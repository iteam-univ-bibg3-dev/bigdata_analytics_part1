{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64b7a2bc-531b-4f70-9cd4-0c931aecea82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
      "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "250b1675-c223-4624-a7a6-090665314399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fdcd780-957c-4ec6-9654-dbd9f3f1d153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting confluent_kafka\n",
      "  Downloading confluent_kafka-2.11.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (23 kB)\n",
      "Downloading confluent_kafka-2.11.1-cp311-cp311-manylinux_2_28_x86_64.whl (3.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: confluent_kafka\n",
      "Successfully installed confluent_kafka-2.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install confluent_kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6982afa-7513-457b-822f-d8ad8bf95a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Downloading kafka_python-2.2.15-py2.py3-none-any.whl.metadata (10.0 kB)\n",
      "Downloading kafka_python-2.2.15-py2.py3-none-any.whl (309 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m309.8/309.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.2.15\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d3dcaea-a75f-487e-b778-08ed3cad8445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic test-topic crÃ©Ã© avec succÃ¨s âœ…\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "\n",
    "admin_client = AdminClient({\n",
    "    \"bootstrap.servers\": \"kafka1:19092,kafka2:19093,kafka3:19094\"\n",
    "})\n",
    "\n",
    "topic_list = [NewTopic(\"test-topic\", num_partitions=3, replication_factor=2)]\n",
    "fs = admin_client.create_topics(topic_list)\n",
    "\n",
    "for topic, f in fs.items():\n",
    "    try:\n",
    "        f.result()\n",
    "        print(f\"Topic {topic} crÃ©Ã© avec succÃ¨s âœ…\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur crÃ©ation topic {topic}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f31c722-69e9-4a59-92ec-fe371852fcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic fly-topic crÃ©Ã© avec succÃ¨s âœ…\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "\n",
    "admin_client = AdminClient({\n",
    "    \"bootstrap.servers\": \"kafka1:19092,kafka2:19093,kafka3:19094\"\n",
    "})\n",
    "\n",
    "topic_list = [NewTopic(\"fly-topic\", num_partitions=3, replication_factor=2)]\n",
    "fs = admin_client.create_topics(topic_list)\n",
    "\n",
    "for topic, f in fs.items():\n",
    "    try:\n",
    "        f.result()\n",
    "        print(f\"Topic {topic} crÃ©Ã© avec succÃ¨s âœ…\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur crÃ©ation topic {topic}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877c3040-e282-4774-ae71-a0f898d827b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producer.py\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "API_URL = \"https://api.aviationstack.com/v1/flights\"\n",
    "API_KEY = \"7f2e58e13fce1821756db3c77f003bca\" # key lina\n",
    "TOPIC = \"fly-topic\"\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[\"kafka1:19092\",\"kafka2:19093\",\"kafka3:19094\"],\n",
    "    value_serializer=lambda v: json.dumps(v).encode(\"utf-8\")\n",
    ")\n",
    "\n",
    "def fetch_flights(limit=5):\n",
    "    params = {\"access_key\": API_KEY, \"limit\": limit}\n",
    "    try:\n",
    "        response = requests.get(API_URL, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return response.json().get(\"data\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ API Error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Boucle producer\n",
    "while True:\n",
    "    flights = fetch_flights(limit=5)\n",
    "    if flights:\n",
    "        for flight in flights:\n",
    "            producer.send(TOPIC, flight)\n",
    "            print(f\"âœ… Sent flight: {flight.get('flight', {}).get('iata', 'N/A')}\")\n",
    "    time.sleep(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bf989c-c2e8-41be-b938-74eabec02b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FlightsStreaming\") \\\n",
    "    .master(\"spark://spark:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# SchÃ©ma JSON du vol\n",
    "flight_schema = StructType([\n",
    "    StructField(\"airline\", StructType([\n",
    "        StructField(\"name\", StringType())\n",
    "    ])),\n",
    "    StructField(\"flight\", StructType([\n",
    "        StructField(\"iata\", StringType())\n",
    "    ])),\n",
    "    StructField(\"departure\", StructType([\n",
    "        StructField(\"airport\", StringType()),\n",
    "        StructField(\"iata\", StringType())\n",
    "    ])),\n",
    "    StructField(\"arrival\", StructType([\n",
    "        StructField(\"airport\", StringType()),\n",
    "        StructField(\"iata\", StringType())\n",
    "    ])),\n",
    "    StructField(\"live\", StructType([\n",
    "        StructField(\"is_ground\", BooleanType()),\n",
    "        StructField(\"altitude\", DoubleType()),\n",
    "        StructField(\"speed_horizontal\", DoubleType())\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Lecture Kafka\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:19092,kafka2:19093,kafka3:19094\") \\\n",
    "    .option(\"subscribe\", \"fly-topic\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Conversion du JSON\n",
    "flights_df = df.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "    .select(from_json(col(\"json\"), flight_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Affichage console pour debug\n",
    "query = flights_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1327a586-92af-4051-88b1-92f6b0fd09d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Downloading plotly-6.3.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting narwhals>=1.15.1 (from plotly)\n",
      "  Downloading narwhals-2.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from plotly) (23.2)\n",
      "Downloading plotly-6.3.0-py3-none-any.whl (9.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading narwhals-2.5.0-py3-none-any.whl (407 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m407.3/407.3 kB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: narwhals, plotly\n",
      "Successfully installed narwhals-2.5.0 plotly-6.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baee92ad-754e-483c-82ce-c4fb46cda386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b9bcc-f5cb-45e4-9476-2a24b89cabd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Exemple de dataframe fictif depuis Mongo\n",
    "flights_data = pd.DataFrame([\n",
    "    {\"flight\": \"ET3857\", \"status\": \"En vol\", \"altitude\": 35000},\n",
    "    {\"flight\": \"VJ3949\", \"status\": \"Au sol\", \"altitude\": 0},\n",
    "])\n",
    "\n",
    "fig = px.bar(flights_data, x=\"flight\", y=\"altitude\", color=\"status\", title=\"Altitude des vols en temps rÃ©el\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3de9ade-c571-4b10-923c-948275c9ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from kafka import KafkaProducer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType, DoubleType\n",
    "\n",
    "# ------------------------\n",
    "# 1ï¸âƒ£ Producer\n",
    "# ------------------------\n",
    "API_URL = \"https://api.aviationstack.com/v1/flights\"\n",
    "API_KEY = \"8fad2c80c2bacd169db6e054e16b19fb\"\n",
    "TOPIC = \"fly-topic\"\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[\"kafka1:19092\",\"kafka2:19093\",\"kafka3:19094\"],\n",
    "    value_serializer=lambda v: json.dumps(v).encode(\"utf-8\")\n",
    ")\n",
    "\n",
    "def fetch_flights(limit=5):\n",
    "    params = {\"access_key\": API_KEY, \"limit\": limit}\n",
    "    try:\n",
    "        response = requests.get(API_URL, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return response.json().get(\"data\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ API Error: {e}\")\n",
    "        return []\n",
    "\n",
    "def producer_loop():\n",
    "    while True:\n",
    "        flights = fetch_flights(limit=5)\n",
    "        for flight in flights:\n",
    "            producer.send(TOPIC, flight)\n",
    "        time.sleep(15)\n",
    "\n",
    "# ------------------------\n",
    "# 2ï¸âƒ£ Spark Streaming\n",
    "# ------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FlightsStreaming\") \\\n",
    "    .master(\"spark://spark:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "flight_schema = StructType([\n",
    "    StructField(\"airline\", StructType([StructField(\"name\", StringType())])),\n",
    "    StructField(\"flight\", StructType([StructField(\"iata\", StringType())])),\n",
    "    StructField(\"departure\", StructType([\n",
    "        StructField(\"airport\", StringType()),\n",
    "        StructField(\"iata\", StringType())\n",
    "    ])),\n",
    "    StructField(\"arrival\", StructType([\n",
    "        StructField(\"airport\", StringType()),\n",
    "        StructField(\"iata\", StringType())\n",
    "    ])),\n",
    "    StructField(\"live\", StructType([\n",
    "        StructField(\"is_ground\", BooleanType()),\n",
    "        StructField(\"altitude\", DoubleType()),\n",
    "        StructField(\"speed_horizontal\", DoubleType())\n",
    "    ]))\n",
    "])\n",
    "\n",
    "def spark_streaming_loop():\n",
    "    df = spark.readStream.format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka1:19092,kafka2:19093,kafka3:19094\") \\\n",
    "        .option(\"subscribe\", TOPIC) \\\n",
    "        .option(\"startingOffsets\", \"latest\").load()\n",
    "    \n",
    "    flights_df = df.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "        .select(from_json(col(\"json\"), flight_schema).alias(\"data\")) \\\n",
    "        .select(\"data.*\")\n",
    "    \n",
    "    query = flights_df.writeStream.outputMode(\"append\") \\\n",
    "        .format(\"console\").start()\n",
    "    \n",
    "    query.awaitTermination()\n",
    "\n",
    "# ------------------------\n",
    "# 3ï¸âƒ£ Dashboard (simple exemple Plotly)\n",
    "# ------------------------\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "def dashboard_loop():\n",
    "    while True:\n",
    "        # Lecture des derniÃ¨res donnÃ©es depuis Mongo ou cache\n",
    "        flights_data = pd.DataFrame([\n",
    "            {\"flight\": \"ET3857\", \"status\": \"En vol\", \"altitude\": 35000},\n",
    "            {\"flight\": \"VJ3949\", \"status\": \"Au sol\", \"altitude\": 0},\n",
    "        ])\n",
    "        fig = px.bar(flights_data, x=\"flight\", y=\"altitude\", color=\"status\")\n",
    "        fig.show()\n",
    "        time.sleep(15)\n",
    "\n",
    "# ------------------------\n",
    "# Lancer tous les threads\n",
    "# ------------------------\n",
    "threads = [\n",
    "    threading.Thread(target=producer_loop),\n",
    "    threading.Thread(target=spark_streaming_loop),\n",
    "    threading.Thread(target=dashboard_loop)\n",
    "]\n",
    "\n",
    "for t in threads:\n",
    "    t.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172be463-08fb-47c2-b269-d6d9ad9ecb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "\n",
    "TOPIC = \"fly-topic\"\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    TOPIC,\n",
    "    bootstrap_servers=[\"kafka1:19092\", \"kafka2:19093\", \"kafka3:19094\"],\n",
    "    auto_offset_reset='latest',\n",
    "    enable_auto_commit=True,\n",
    "    group_id='flight-consumers',\n",
    "    value_deserializer=lambda v: json.loads(v.decode('utf-8'))\n",
    ")\n",
    "\n",
    "# Connexion MongoDB\n",
    "client = MongoClient(\"mongodb://admin:password@mongodb:27017\")\n",
    "db = client.flights\n",
    "collection = db.flights_data\n",
    "\n",
    "for message in consumer:\n",
    "    flight = message.value\n",
    "    collection.insert_one(flight)\n",
    "    print(f\"ğŸ’¾ Saved flight {flight.get('flight', {}).get('iata', 'N/A')} to MongoDB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b306d76a-0f03-4717-a7fa-7336a379d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "\n",
    "TOPIC = \"fly-topic\"\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    TOPIC,\n",
    "    bootstrap_servers=[\"kafka1:19092\", \"kafka2:19093\", \"kafka3:19094\"],\n",
    "    auto_offset_reset='latest',\n",
    "    enable_auto_commit=True,\n",
    "    group_id='flight-consumers',\n",
    "    value_deserializer=lambda v: json.loads(v.decode('utf-8'))\n",
    ")\n",
    "\n",
    "# Connexion MongoDB\n",
    "client = MongoClient(\"mongodb://admin:password@mongodb:27017\")\n",
    "db = client.flights\n",
    "collection = db.flights_data\n",
    "\n",
    "for message in consumer:\n",
    "    flight = message.value\n",
    "    collection.insert_one(flight)\n",
    "    print(f\"ğŸ’¾ Saved flight {flight.get('flight', {}).get('iata', 'N/A')} to MongoDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b3c768-2adf-4da0-ae4d-3bf4274ac309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "\n",
    "TOPIC = \"fly-topic\"\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    TOPIC,\n",
    "    bootstrap_servers=[\"kafka1:19092\", \"kafka2:19093\", \"kafka3:19094\"],\n",
    "    auto_offset_reset='latest',\n",
    "    enable_auto_commit=False,  # dÃ©sactive auto commit\n",
    "    group_id='flight-consumers',\n",
    "    value_deserializer=lambda v: json.loads(v.decode('utf-8')),\n",
    "    consumer_timeout_ms=1000   # permet de timeout pour pouvoir traiter batch\n",
    ")\n",
    "\n",
    "client = MongoClient(\"mongodb://admin:password@mongodb:27017\")\n",
    "db = client.flights\n",
    "collection = db.flights_data\n",
    "\n",
    "batch_size = 10\n",
    "batch = []\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        for message in consumer:\n",
    "            batch.append(message.value)\n",
    "            if len(batch) >= batch_size:\n",
    "                collection.insert_many(batch)\n",
    "                print(f\"ğŸ’¾ Saved batch of {len(batch)} flights to MongoDB\")\n",
    "                consumer.commit()  # commit tous les offsets du batch\n",
    "                batch = []\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79915df5-190c-4d20-a977-bc6e4c1f0b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import time\n",
    "\n",
    "TOPIC = \"fly-topic\"\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    TOPIC,\n",
    "    bootstrap_servers=[\"kafka1:19092\", \"kafka2:19093\", \"kafka3:19094\"],\n",
    "    auto_offset_reset='latest',\n",
    "    enable_auto_commit=True,  # commit automatique\n",
    "    group_id='flight-consumers',\n",
    "    value_deserializer=lambda v: json.loads(v.decode('utf-8')),\n",
    "    consumer_timeout_ms=1000  # timeout pour pouvoir relancer la boucle\n",
    ")\n",
    "\n",
    "# Connexion MongoDB\n",
    "client = MongoClient(\"mongodb://admin:password@mongodb:27017\")\n",
    "db = client.flights\n",
    "collection = db.flights_data\n",
    "\n",
    "print(\"ğŸš€ Consumer started...\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Lire tous les messages disponibles Ã  cet instant\n",
    "        msg_count = 0\n",
    "        for message in consumer:\n",
    "            flight = message.value\n",
    "            collection.insert_one(flight)\n",
    "            print(f\"ğŸ’¾ Saved flight {flight.get('flight', {}).get('iata', 'N/A')} to MongoDB\")\n",
    "            msg_count += 1\n",
    "        \n",
    "        if msg_count == 0:\n",
    "            # Aucun message pour le moment, attendre 1 sec\n",
    "            time.sleep(1)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463c6f82-277f-49d1-8e6f-39f2ec32bffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Imports\n",
    "# ------------------------\n",
    "import threading\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# ------------------------\n",
    "# Configurations\n",
    "# ------------------------\n",
    "API_URL = \"https://api.aviationstack.com/v1/flights\"\n",
    "API_KEY = \"8fad2c80c2bacd169db6e054e16b19fb\"\n",
    "TOPIC = \"fly-topic\"\n",
    "\n",
    "# Kafka brokers (Docker network noms)\n",
    "BROKERS = [\"kafka1:19092\", \"kafka2:19093\", \"kafka3:19094\"]\n",
    "\n",
    "# MongoDB connection\n",
    "client = MongoClient(\"mongodb://admin:password@mongodb:27017\")\n",
    "db = client.flights\n",
    "collection = db.flights_data\n",
    "\n",
    "# ------------------------\n",
    "# Producer\n",
    "# ------------------------\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=BROKERS,\n",
    "    value_serializer=lambda v: json.dumps(v).encode(\"utf-8\")\n",
    ")\n",
    "\n",
    "def fetch_flights(limit=5):\n",
    "    params = {\"access_key\": API_KEY, \"limit\": limit}\n",
    "    try:\n",
    "        response = requests.get(API_URL, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return response.json().get(\"data\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ API Error: {e}\")\n",
    "        return []\n",
    "\n",
    "def producer_loop():\n",
    "    while True:\n",
    "        flights = fetch_flights(limit=5)\n",
    "        for flight in flights:\n",
    "            producer.send(TOPIC, flight)\n",
    "            print(f\"ğŸ“¤ Sent flight {flight.get('flight', {}).get('iata', 'N/A')}\")\n",
    "        producer.flush()  # push tout de suite\n",
    "        time.sleep(15)\n",
    "\n",
    "# ------------------------\n",
    "# Consumer\n",
    "# ------------------------\n",
    "def consumer_loop(batch_size=5):\n",
    "    consumer = KafkaConsumer(\n",
    "        TOPIC,\n",
    "        bootstrap_servers=BROKERS,\n",
    "        group_id=\"flight-consumers\",  # Kafka crÃ©e le groupe automatiquement\n",
    "        auto_offset_reset='latest',\n",
    "        enable_auto_commit=False,\n",
    "        value_deserializer=lambda v: json.loads(v.decode(\"utf-8\")),\n",
    "        consumer_timeout_ms=1000  # pour batch\n",
    "    )\n",
    "\n",
    "    batch = []\n",
    "    while True:\n",
    "        for message in consumer:\n",
    "            batch.append(message.value)\n",
    "            if len(batch) >= batch_size:\n",
    "                collection.insert_many(batch)\n",
    "                consumer.commit()\n",
    "                print(f\"ğŸ’¾ Saved batch of {len(batch)} flights to MongoDB\")\n",
    "                batch = []\n",
    "\n",
    "# ------------------------\n",
    "# Threads pour tourner en parallÃ¨le\n",
    "# ------------------------\n",
    "threads = [\n",
    "    threading.Thread(target=producer_loop, daemon=True),\n",
    "    threading.Thread(target=consumer_loop, daemon=True)\n",
    "]\n",
    "\n",
    "for t in threads:\n",
    "    t.start()\n",
    "\n",
    "# ------------------------\n",
    "# Maintenir le notebook actif\n",
    "# ------------------------\n",
    "while True:\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e9ac9f-2edc-4489-a432-584e441a6c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Imports\n",
    "# ------------------------\n",
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "TOPIC = \"fly-topic\"\n",
    "BROKERS = [\"kafka1:19092\", \"kafka2:19093\", \"kafka3:19094\"]\n",
    "\n",
    "# ------------------------\n",
    "# Kafka Consumer simple\n",
    "# ------------------------\n",
    "consumer = KafkaConsumer(\n",
    "    TOPIC,\n",
    "    bootstrap_servers=BROKERS,\n",
    "    group_id=\"display-consumer\",  # Kafka crÃ©e le groupe automatiquement\n",
    "    auto_offset_reset='latest',\n",
    "    enable_auto_commit=True,       # commit auto pour ne pas relire les mÃªmes messages\n",
    "    value_deserializer=lambda v: json.loads(v.decode(\"utf-8\")),\n",
    ")\n",
    "\n",
    "print(\"â³ Listening for flights...\")\n",
    "\n",
    "for message in consumer:\n",
    "    flight = message.value\n",
    "    airline = flight.get(\"airline\", {}).get(\"name\", \"N/A\")\n",
    "    flight_code = flight.get(\"flight\", {}).get(\"iata\", \"N/A\")\n",
    "    dep_airport = flight.get(\"departure\", {}).get(\"airport\", \"N/A\")\n",
    "    arr_airport = flight.get(\"arrival\", {}).get(\"airport\", \"N/A\")\n",
    "    live = flight.get(\"live\", {})\n",
    "    \n",
    "    status = \"âœˆï¸ En vol\" if live and not live.get(\"is_ground\", True) else \"ğŸ›¬ Au sol\"\n",
    "    altitude = f\"{live.get('altitude', 'N/A')} ft\" if live else \"N/A\"\n",
    "    speed = f\"{live.get('speed_horizontal', 'N/A')} km/h\" if live else \"N/A\"\n",
    "\n",
    "    print(f\"\"\"\n",
    "-------------------------------\n",
    "ğŸ“Œ Compagnie: {airline}\n",
    "ğŸ›« Vol: {flight_code}\n",
    "DÃ©part: {dep_airport}\n",
    "ArrivÃ©e: {arr_airport}\n",
    "Statut: {status}\n",
    "Altitude: {altitude}\n",
    "Vitesse: {speed}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "486e8209-5c7b-4cb6-83bf-fb64e8da09b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.49.1-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /opt/conda/lib/python3.11/site-packages (from streamlit) (5.1.2)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from streamlit) (1.6.3)\n",
      "Collecting cachetools<7,>=4.0 (from streamlit)\n",
      "  Downloading cachetools-6.2.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.11/site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /opt/conda/lib/python3.11/site-packages (from streamlit) (1.24.4)\n",
      "Requirement already satisfied: packaging<26,>=20 in /opt/conda/lib/python3.11/site-packages (from streamlit) (23.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /opt/conda/lib/python3.11/site-packages (from streamlit) (10.1.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in /opt/conda/lib/python3.11/site-packages (from streamlit) (4.24.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /opt/conda/lib/python3.11/site-packages (from streamlit) (13.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /opt/conda/lib/python3.11/site-packages (from streamlit) (2.31.0)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /opt/conda/lib/python3.11/site-packages (from streamlit) (4.8.0)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
      "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/conda/lib/python3.11/site-packages (from streamlit) (3.1.40)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /opt/conda/lib/python3.11/site-packages (from streamlit) (6.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.11/site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.19.1)\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.11/site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.11/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.10)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (2023.7.22)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (3.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.10.6)\n",
      "Downloading streamlit-1.49.1-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-6.2.0-py3-none-any.whl (11 kB)\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: watchdog, toml, tenacity, cachetools, pydeck, streamlit\n",
      "Successfully installed cachetools-6.2.0 pydeck-0.9.1 streamlit-1.49.1 tenacity-9.1.2 toml-0.10.2 watchdog-6.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49cc71b-b092-442c-8e5b-2c6e01a21a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
